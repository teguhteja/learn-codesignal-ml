{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using Language Models in DSPy\n",
                "\n",
                "# Using Language Models in DSPy\n",
                "\n",
                "Language models are the foundational computational engine for all DSPy applications, similar to how a CPU powers a traditional program. They are not just occasional tools but the core upon which modules, signatures, and optimizations are built. DSPy abstracts the complexities of different language model APIs, providing a consistent interface that lets you easily switch between models like GPT-4, Claude, or a local open-source model.\n",
                "\n",
                "-----\n",
                "\n",
                "## Initializing Language Models\n",
                "\n",
                "To start, you must initialize a language model (LM) instance. DSPy uses a unified interface, making it simple to switch providers.\n",
                "\n",
                "You can initialize an LM by creating an instance of `dspy.LM`, specifying the provider and model. Authentication is typically done via an API key.\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "\n",
                "# Initialize an LM instance with an API key\n",
                "lm = dspy.LM('openai/gpt-4o-mini', api_key='YOUR_OPENAI_API_KEY')\n",
                "```\n",
                "\n",
                "For security, it's recommended to use environment variables instead of hardcoding API keys.\n",
                "\n",
                "```python\n",
                "import os\n",
                "import dspy\n",
                "\n",
                "lm = dspy.LM('openai/gpt-4o-mini', api_key=os.environ[\"OPENAI_API_KEY\"])\n",
                "```\n",
                "\n",
                "DSPy supports various other providers with a similar syntax.\n",
                "\n",
                "```python\n",
                "anthropic_lm = dspy.LM('anthropic/claude-3-opus')\n",
                "local_lm = dspy.LM('local/llama-2-7b')\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Making Direct LM Calls\n",
                "\n",
                "After initialization, you can make direct calls to the LM for simple text generation or testing.\n",
                "\n",
                "There are two primary methods for making a call: using a simple string or a structured message format. The response is always returned as a list of strings, as LMs can potentially generate multiple completions.\n",
                "\n",
                "```python\n",
                "# Call with a string\n",
                "response = lm(\"Say this is a test!\")\n",
                "\n",
                "# Call with a structured message\n",
                "response = lm(messages=[{\"role\": \"user\", \"content\": \"Say this is a test!\"}])\n",
                "```\n",
                "\n",
                "You can control the generation process by passing various parameters to the LM constructor.\n",
                "\n",
                "```python\n",
                "# Create an LM instance with custom parameters\n",
                "gpt_4o_mini = dspy.LM('openai/gpt-4o-mini',\n",
                "                      temperature=0.9,      # Higher for more creative outputs\n",
                "                      max_tokens=3000,      # Max length of generated text\n",
                "                      stop=None,            # Stop sequences\n",
                "                      cache=False)          # Disable response caching\n",
                "```\n",
                "\n",
                "**`temperature`**: Controls randomness. Higher values lead to more creative, random output, while lower values result in more deterministic output.\n",
                "**`max_tokens`**: Sets the maximum length of the generated text.\n",
                "**`stop`**: Defines sequences where the model should stop generating.\n",
                "**`cache`**: Determines whether to cache responses to avoid redundant API calls.\n",
                "\n",
                "-----\n",
                "\n",
                "## Global and Local LM Configuration\n",
                "\n",
                "For a consistent LM across your application, you can configure a global default using `dspy.configure()`.\n",
                "\n",
                "```python\n",
                "# Set a global default LM\n",
                "dspy.configure(lm=lm)\n",
                "```\n",
                "\n",
                "Once configured, any DSPy module will automatically use this LM.\n",
                "\n",
                "You can temporarily override the global LM for specific sections of code using a context manager.\n",
                "\n",
                "```python\n",
                "# Temporarily use a different LM\n",
                "with dspy.context(lm=dspy.LM('openai/gpt-3.5-turbo')):\n",
                "    response = qa(question=\"...\")\n",
                "```\n",
                "\n",
                "This is useful for comparing model performance or for using a more powerful model only when needed.\n",
                "\n",
                "-----\n",
                "\n",
                "## Monitoring and Debugging\n",
                "\n",
                "DSPy provides tools to monitor and debug LM interactions. Every LM instance keeps a history of its calls.\n",
                "\n",
                "```python\n",
                "# Check the number of calls\n",
                "print(len(lm.history))\n",
                "\n",
                "# Access the last call with all metadata\n",
                "last_call = lm.history[-1]\n",
                "```\n",
                "\n",
                "The call history provides valuable information such as the prompt sent, the response received, token usage, timestamp, and parameters used. This is essential for debugging and optimizing your application's cost and performance."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Basic LM Operations and History\n",
                "\n",
                "Now that you've learned about the fundamentals of language models in DSPy, let's put that knowledge into practice! In this exercise, you'll create a simple script that demonstrates the core LM operations we just covered.\n",
                "\n",
                "Your task is to build a script that:\n",
                "\n",
                "Initializes a DSPy language model\n",
                "Makes a direct call to the model with a text prompt\n",
                "Retrieves and examines the metadata from the model's history\n",
                "This hands-on experience will help you understand how DSPy interacts with language models behind the scenes and how you can access important information about these interactions. Accessing metadata is particularly useful for debugging and monitoring your LM usage in more complex applications.\n",
                "\n",
                "Complete the TODOs in the starter code to become familiar with these essential DSPy operations. This foundation will be crucial as we build more sophisticated applications in upcoming lessons.\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "\n",
                "# TODO: Initialize a language model\n",
                "# Hint: Use dspy.LM() with an appropriate model name, with api_key=os.environ['OPENAI_API_KEY'] and api_base=os.environ['OPENAI_BASE_URL']\n",
                "\n",
                "# TODO: Make a direct call to the language model\n",
                "# Hint: Pass a simple text prompt to the model and store the response\n",
                "\n",
                "# TODO: Access the history and print metadata keys\n",
                "# Hint: Get the last call from the model's history and print its keys\n",
                "```\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "\n",
                "# Set API keys from environment variables\n",
                "# Note: In a real-world scenario, you would need to have these environment variables set up.\n",
                "# For a hosted environment like CodeSignal, they might be pre-configured.\n",
                "# For local use, you would run:\n",
                "# export OPENAI_API_KEY=\"your_api_key_here\"\n",
                "# export OPENAI_BASE_URL=\"your_api_base_url_here\"\n",
                "\n",
                "# TODO: Initialize a language model\n",
                "lm = dspy.LM('gpt-4o-mini', api_key=os.environ.get('OPENAI_API_KEY'), api_base=os.environ.get('OPENAI_BASE_URL'))\n",
                "\n",
                "# TODO: Make a direct call to the language model\n",
                "prompt_text = \"What is the capital of France?\"\n",
                "response = lm(prompt_text)\n",
                "\n",
                "# TODO: Access the history and print metadata keys\n",
                "if lm.history:\n",
                "    last_call = lm.history[-1]\n",
                "    print(\"Metadata keys from the last LM call:\")\n",
                "    print(list(last_call.keys()))\n",
                "else:\n",
                "    print(\"LM history is empty. No calls were made.\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Switching Models with Context Managers\n",
                "\n",
                "Now that you've mastered basic LM operations, let's explore one of DSPy's most powerful features: the ability to work with multiple language models in the same application.\n",
                "\n",
                "In this exercise, you'll build on your knowledge of LM initialization and direct calls by creating a script that demonstrates how to switch between different models dynamically.\n",
                "\n",
                "Your tasks involve:\n",
                "\n",
                "Initializing two different language models\n",
                "Setting one as the global default with dspy.configure()\n",
                "Making a call with the default model and examining its response\n",
                "Using a context manager to temporarily switch to a second model\n",
                "Making the same call again to compare how different models respond to identical prompts\n",
                "Inspecting the history to verify both interactions were properly tracked\n",
                "This skill is essential when building applications that need to balance cost, performance, and capabilities â€” you might want to use a powerful model for complex reasoning but a faster, cheaper model for simpler tasks.\n",
                "\n",
                "By completing this exercise, you'll gain practical experience with DSPy's flexible model management system, setting you up for building more sophisticated multi-model applications in the future.\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "\n",
                "# TODO: Initialize two different language models\n",
                "# Hint: Create one primary model (e.g., GPT-4o-mini) and one secondary model (e.g., GPT-3.5-turbo). Remember to set api_key to os.environ['OPENAI_API_KEY'] and api_base to os.environ['OPENAI_BASE_URL']\n",
                "\n",
                "# TODO: Set the primary model as the global default\n",
                "# Hint: Use dspy.configure()\n",
                "\n",
                "# Create a prompt that we'll use for both models\n",
                "prompt = \"Explain the concept of recursion in one sentence.\"\n",
                "\n",
                "# TODO: Make a direct call using the global default model\n",
                "print(\"=== Using Primary Model (Global Default) ===\")\n",
                "# Hint: Call the primary model with the prompt and print the response\n",
                "\n",
                "# TODO: Check the history before context switch\n",
                "# Hint: Get the length of the primary model's history and print relevant metadata\n",
                "\n",
                "# TODO: Use a context manager to temporarily switch to the secondary model\n",
                "print(\"\\n=== Using Secondary Model (Context Override) ===\")\n",
                "# Hint: Use dspy.context() to temporarily set the secondary model as the active LM\n",
                "    # TODO: Make the same call with the secondary model\n",
                "    # Hint: Call the secondary model with the same prompt and print the response\n",
                "\n",
                "# TODO: Check the history after context switch\n",
                "# Hint: Compare the history lengths before and after the context switch\n",
                "\n",
                "# TODO: Verify both interactions are in their respective histories\n",
                "print(\"\\n=== History Verification ===\")\n",
                "# Hint: Check if the primary model's history length changed and print a message\n",
                "\n",
                "# TODO: Print metadata from the secondary model's history\n",
                "# Hint: Access the last call in the secondary model's history\n",
                "\n",
                "# TODO: Compare the responses from both models\n",
                "print(\"\\n=== Response Comparison ===\")\n",
                "# Hint: Print both responses side by side to see the differences\n",
                "```\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "\n",
                "# Set API keys from environment variables\n",
                "# Note: In a real-world scenario, you would need to have these environment variables set up.\n",
                "# For a hosted environment like CodeSignal, they might be pre-configured.\n",
                "# For local use, you would run:\n",
                "# export OPENAI_API_KEY=\"your_api_key_here\"\n",
                "# export OPENAI_BASE_URL=\"your_api_base_url_here\"\n",
                "\n",
                "# TODO: Initialize two different language models\n",
                "primary_lm = dspy.LM('gpt-4o-mini', api_key=os.environ.get('OPENAI_API_KEY'), api_base=os.environ.get('OPENAI_BASE_URL'))\n",
                "secondary_lm = dspy.LM('gpt-3.5-turbo', api_key=os.environ.get('OPENAI_API_KEY'), api_base=os.environ.get('OPENAI_BASE_URL'))\n",
                "\n",
                "# TODO: Set the primary model as the global default\n",
                "dspy.configure(lm=primary_lm)\n",
                "\n",
                "# Create a prompt that we'll use for both models\n",
                "prompt = \"Explain the concept of recursion in one sentence.\"\n",
                "\n",
                "# TODO: Make a direct call using the global default model\n",
                "print(\"=== Using Primary Model (Global Default) ===\")\n",
                "primary_response = primary_lm(prompt)\n",
                "print(f\"Primary Model Response: {primary_response[0]}\")\n",
                "\n",
                "# TODO: Check the history before context switch\n",
                "print(\"\\n=== History Before Context Switch ===\")\n",
                "print(f\"Primary LM history length: {len(primary_lm.history)}\")\n",
                "print(f\"Secondary LM history length: {len(secondary_lm.history)}\")\n",
                "\n",
                "# TODO: Use a context manager to temporarily switch to the secondary model\n",
                "print(\"\\n=== Using Secondary Model (Context Override) ===\")\n",
                "with dspy.context(lm=secondary_lm):\n",
                "    # TODO: Make the same call with the secondary model\n",
                "    secondary_response = secondary_lm(prompt)\n",
                "    print(f\"Secondary Model Response: {secondary_response[0]}\")\n",
                "\n",
                "# TODO: Check the history after context switch\n",
                "print(\"\\n=== History After Context Switch ===\")\n",
                "print(f\"Primary LM history length: {len(primary_lm.history)}\")\n",
                "print(f\"Secondary LM history length: {len(secondary_lm.history)}\")\n",
                "\n",
                "# TODO: Verify both interactions are in their respective histories\n",
                "print(\"\\n=== History Verification ===\")\n",
                "if len(primary_lm.history) == 1 and len(secondary_lm.history) == 1:\n",
                "    print(\"Both interactions are correctly recorded in their respective histories.\")\n",
                "else:\n",
                "    print(\"History tracking is not as expected.\")\n",
                "\n",
                "# TODO: Print metadata from the secondary model's history\n",
                "if secondary_lm.history:\n",
                "    last_secondary_call = secondary_lm.history[-1]\n",
                "    print(\"\\nMetadata from Secondary Model's last call:\")\n",
                "    print(f\"Prompt: {last_secondary_call.get('prompt')}\")\n",
                "    print(f\"Response: {last_secondary_call.get('response')}\")\n",
                "    print(f\"Parameters: {last_secondary_call.get('parameters')}\")\n",
                "else:\n",
                "    print(\"\\nSecondary LM history is empty.\")\n",
                "\n",
                "\n",
                "# TODO: Compare the responses from both models\n",
                "print(\"\\n=== Response Comparison ===\")\n",
                "print(f\"Primary Model: {primary_response[0]}\")\n",
                "print(f\"Secondary Model: {secondary_response[0]}\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Structured Messages and History Exploration"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Customizing LM Parameters for Creative Output"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
