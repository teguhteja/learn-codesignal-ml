{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Automatic Instruction Optimization with DSPy\n",
                "\n",
                "## Automatic Instruction Optimization with DSPy\n",
                "\n",
                "Welcome to our lesson on **Automatic Instruction Optimization** with **DSPy**\\! In our previous lesson, we explored how Few-Shot Learning optimizers enhance your DSPy programs by automatically selecting and generating examples to include in your prompts. Today, we'll focus on a different approach: **optimizing the actual instructions** in your prompts.\n",
                "\n",
                "-----\n",
                "\n",
                "While Few-Shot Learning optimizers focus on providing examples to guide the language model, **Instruction Optimization** optimizers focus on improving the natural language instructions themselves. Instead of asking, \"What examples should I show the model?\" these optimizers ask, \"**How should I phrase my request to the model?**\"\n",
                "\n",
                "This distinction is important because the way you phrase your instructions can **significantly impact the model's performance**, even with the same underlying task. A well-crafted instruction can guide the model to produce better outputs without needing additional examples, or it can work alongside examples to further enhance performance.\n",
                "\n",
                "DSPy offers two powerful instruction optimizers:\n",
                "\n",
                "  * **COPRO** (**C**ontrastive **P**rompt **O**ptimization): Generates and refines new instructions for each step in your program, optimizing them through **coordinate ascent**.\n",
                "  * **MIPROv2** (**M**inimum **I**nstruction **P**rompt **O**ptimization v2): Generates instructions that are aware of both your data and any demonstrations, using **Bayesian Optimization** to efficiently search the space of possible instructions.\n",
                "\n",
                "These optimizers are valuable when you want to keep your prompts concise (reducing token usage) or when you're working with models that respond better to clear instructions.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding COPRO (Contrastive Prompt Optimization)\n",
                "\n",
                "**COPRO** is a powerful technique for automatically improving instructions. The core idea is to generate multiple alternative instructions, evaluate them using your metric, and iteratively refine them to find the best-performing set.\n",
                "\n",
                "The \"**contrastive**\" aspect comes from how it learns by comparing instructions that lead to correct outputs with those that fail.\n",
                "\n",
                "COPRO uses **coordinate ascent** (a form of hill-climbing) to optimize instructions:\n",
                "\n",
                "1.  **Generate Alternatives:** For each module, COPRO generates multiple alternative instructions.\n",
                "2.  **Evaluate:** It evaluates each alternative using your metric and training data.\n",
                "3.  **Select & Refine:** It selects the best-performing instruction and repeats the process for multiple iterations, generating new alternatives based on the current best instructions.\n",
                "\n",
                "### Key COPRO Parameters\n",
                "\n",
                "| Parameter | Description | Default |\n",
                "| :--- | :--- | :--- |\n",
                "| `prompt_model` | The language model used to generate new instruction candidates. | N/A |\n",
                "| `metric` | A function that evaluates the performance of your program. | N/A |\n",
                "| `breadth` | The number of new instruction candidates to generate in each iteration. | 16 |\n",
                "| `depth` | The number of iterations to run the optimization process. | 2 |\n",
                "| `init_temperature` | The temperature used when generating new instruction candidates (Higher = more diverse candidates). | 1.0 |\n",
                "| `verbose` | Whether to print detailed information during optimization. | `False` |\n",
                "\n",
                "COPRO is effective when you have a clear metric and want to **optimize instructions without relying on examples**.\n",
                "\n",
                "-----\n",
                "\n",
                "## Implementing COPRO in DSPy\n",
                "\n",
                "Implementing COPRO involves configuring the optimizer with key parameters and using it to compile your program.\n",
                "\n",
                "```python\n",
                "from dspy.teleprompt import COPRO\n",
                "\n",
                "# Define evaluation parameters for the compilation phase\n",
                "eval_kwargs = dict(num_threads=16, display_progress=True, display_table=0)\n",
                "\n",
                "# Create the COPRO optimizer\n",
                "copro_teleprompter = COPRO(\n",
                "    prompt_model=model_to_generate_prompts,  # E.g., dspy.LM('openai/gpt-4')\n",
                "    metric=your_defined_metric,              # Your evaluation metric function\n",
                "    breadth=num_new_prompts_generated,       # E.g., 16\n",
                "    depth=times_to_generate_prompts,         # E.g., 2\n",
                "    init_temperature=prompt_generation_temperature,  # E.g., 1.0\n",
                "    verbose=False\n",
                ")\n",
                "\n",
                "# Compile your program with the optimizer\n",
                "compiled_program_optimized_signature = copro_teleprompter.compile(\n",
                "    your_dspy_program,\n",
                "    trainset=trainset,\n",
                "    eval_kwargs=eval_kwargs\n",
                ")\n",
                "```\n",
                "\n",
                "The `compile()` process will iteratively refine the natural language instructions within your program's modules, selecting the signature that performs best on the provided `trainset` as measured by the `metric`.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding MIPROv2 (Minimum Instruction Prompt Optimization)\n",
                "\n",
                "**MIPROv2** is a more comprehensive optimizer that can optimize both **instructions and few-shot examples**. It generates instructions that are **data-aware** and **demonstration-aware**, meaning they are tailored to work effectively with the specific examples being used.\n",
                "\n",
                "MIPROv2 uses **Bayesian Optimization** to efficiently explore the search space, often finding better instructions with fewer evaluations than COPRO's coordinate ascent.\n",
                "\n",
                "### Key MIPROv2 Parameters\n",
                "\n",
                "| Parameter | Description |\n",
                "| :--- | :--- |\n",
                "| `metric` | A function that evaluates the performance of your program. |\n",
                "| `auto` | Specifies the optimization intensity: `\"light\"`, `\"medium\"`, or `\"heavy\"`. Lighter settings are faster for experimentation, while heavier settings perform more trials for better results. |\n",
                "| `max_bootstrapped_demos` | (Used in `compile`) Maximum number of new examples to self-generate. |\n",
                "| `max_labeled_demos` | (Used in `compile`) Maximum number of examples to use directly from the training set. |\n",
                "\n",
                "MIPROv2 is particularly effective when you have a **reasonable amount of training data** (e.g., 200+ examples) and want to **optimize both instructions and examples in a unified way**.\n",
                "\n",
                "-----\n",
                "\n",
                "## Implementing MIPROv2 in DSPy\n",
                "\n",
                "MIPROv2 supports both few-shot and zero-shot configurations by adjusting the `compile()` parameters.\n",
                "\n",
                "### Few-Shot Configuration (Optimizing Instructions + Examples)\n",
                "\n",
                "```python\n",
                "from dspy.teleprompt import MIPROv2\n",
                "\n",
                "# Create the MIPROv2 optimizer\n",
                "teleprompter = MIPROv2(\n",
                "    metric=gsm8k_metric,\n",
                "    auto=\"light\"  # Start with \"light\" for quick experimentation\n",
                ")\n",
                "\n",
                "# Compile the program with few-shot parameters\n",
                "optimized_program = teleprompter.compile(\n",
                "    program.deepcopy(),\n",
                "    trainset=trainset,\n",
                "    max_bootstrapped_demos=3,  # Generate up to 3 examples\n",
                "    max_labeled_demos=4,       # Use up to 4 existing examples\n",
                "    requires_permission_to_run=False,\n",
                ")\n",
                "```\n",
                "\n",
                "### Zero-Shot Configuration (Optimizing Instructions Only)\n",
                "\n",
                "To run MIPROv2 in a zero-shot mode, simply set both demonstration limits to zero during compilation:\n",
                "\n",
                "```python\n",
                "from dspy.teleprompt import MIPROv2\n",
                "\n",
                "# Create the MIPROv2 optimizer\n",
                "teleprompter = MIPROv2(\n",
                "    metric=gsm8k_metric,\n",
                "    auto=\"light\"\n",
                ")\n",
                "\n",
                "# Compile the program in zero-shot mode\n",
                "optimized_program = teleprompter.compile(\n",
                "    program.deepcopy(),\n",
                "    trainset=trainset,\n",
                "    max_bootstrapped_demos=0,  # No generated examples\n",
                "    max_labeled_demos=0,       # No examples from training set\n",
                "    requires_permission_to_run=False,\n",
                ")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Practice Preview\n",
                "\n",
                "| Optimizer | Primary Focus | Optimization Mechanism | Recommended Use Case |\n",
                "| :--- | :--- | :--- | :--- |\n",
                "| **COPRO** | Instructions Only | Coordinate Ascent (Hill-Climbing) | Focus purely on optimizing instructions, especially for zero-shot prompts. |\n",
                "| **MIPROv2** | Instructions + Examples | Bayesian Optimization | Optimizing instructions and few-shot examples jointly, especially with moderate-to-large training sets (200+). |\n",
                "\n",
                "**Guidelines for Selection:**\n",
                "\n",
                "  * **Instructions Only:** Use **COPRO**.\n",
                "  * **Instructions + Examples (Best Overall):** Use **MIPROv2**.\n",
                "  * **Larger Data / Longer Run:** Use **MIPROv2** with `auto=\"medium\"` or `\"heavy\"` for potentially better results.\n",
                "\n",
                "In the upcoming practice exercises, you'll gain hands-on experience by implementing both **COPRO** and **MIPROv2** to see how they affect your program's behavior.\n",
                "\n",
                "In the next lesson, we'll explore **Automatic Finetuning**, the final optimization category in DSPy, which involves updating the weights of the underlying language model itself."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tuning COPRO Parameters for Better Instructions\n",
                "\n",
                "Now that you understand how COPRO works to optimize instructions, let's experiment with its key parameters! In this exercise, you'll tune the COPRO optimizer to see how different settings affect the instruction generation process.\n",
                "\n",
                "First, you will implement COPRO for a simple math solver. Then, you'll modify two important parameters that control COPRO's behavior:\n",
                "\n",
                "The breadth parameter, which determines how many candidate prompts are generated in each iteration.\n",
                "The init_temperature parameter, which controls how diverse or creative the generated prompts will be.\n",
                "Your task is to implement and run the optimization with different combinations of these parameters and observe how they affect:\n",
                "\n",
                "The variety of instructions generated.\n",
                "The optimization progress shown in the output.\n",
                "The final performance scores.\n",
                "This hands-on experience will help you develop an intuition for configuring instruction optimizers effectively in your own projects. By the end, you'll have a better understanding of the trade-offs between exploration (trying many diverse candidates) and exploitation (focusing on refining promising instructions).\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "from dspy.teleprompt import COPRO\n",
                "from dspy.evaluate import Evaluate\n",
                "from data import get_trainset, get_testset, get_devset, metric\n",
                "\n",
                "# Set up a simple language model\n",
                "lm = dspy.LM('openai/gpt-4o-mini', api_key=os.environ['OPENAI_API_KEY'], api_base=os.environ['OPENAI_BASE_URL'])\n",
                "dspy.configure(lm=lm)\n",
                "\n",
                "# Define a simple math problem solver program\n",
                "class MathSolver(dspy.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.solver = dspy.Predict(\"question -> answer\")\n",
                "    \n",
                "    def forward(self, question):\n",
                "        return self.solver(question=question)\n",
                "\n",
                "# Get data\n",
                "trainset = get_trainset()\n",
                "testset = get_testset()\n",
                "devset = get_devset()\n",
                "\n",
                "# Create the base program\n",
                "base_program = MathSolver()\n",
                "\n",
                "# Define evaluation parameters\n",
                "eval_kwargs = dict(num_threads=4, display_progress=True, display_table=0)\n",
                "\n",
                "\n",
                "# TODO: Create the COPRO optimizer with specified parameters\n",
                "copro_teleprompter = COPRO(\n",
                "    \n",
                ")\n",
                "\n",
                "# TODO: Compile the program with the optimizer\n",
                "optimized_program = None\n",
                "\n",
                "\n",
                "# Set up the evaluator, which can be re-used in your code.\n",
                "evaluator = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
                "\n",
                "# Evaluate the optimized program\n",
                "score = evaluator(optimized_program, metric=metric)\n",
                "\n",
                "# TODO: Show results\n",
                "print(f\"Optimized instruction: {optimized_program.solver.signature}\")\n",
                "\n",
                "\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optimizing QA with COPRO Parameters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Few-Shot and Zero-Shot Optimization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Balancing Optimization Intensity for Better Results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Balancing Optimization Intensity for Better Results"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
