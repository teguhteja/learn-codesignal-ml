{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Automatic Instruction Optimization with DSPy\n",
                "\n",
                "Welcome to our lesson on **Automatic Instruction Optimization** with **DSPy**\\! In our previous lesson, we explored how Few-Shot Learning optimizers enhance your DSPy programs by automatically selecting and generating examples to include in your prompts. Today, we'll focus on a different approach: **optimizing the actual instructions** in your prompts.\n",
                "\n",
                "-----\n",
                "\n",
                "While Few-Shot Learning optimizers focus on providing examples to guide the language model, **Instruction Optimization** optimizers focus on improving the natural language instructions themselves. Instead of asking, \"What examples should I show the model?\" these optimizers ask, \"**How should I phrase my request to the model?**\"\n",
                "\n",
                "This distinction is important because the way you phrase your instructions can **significantly impact the model's performance**, even with the same underlying task. A well-crafted instruction can guide the model to produce better outputs without needing additional examples, or it can work alongside examples to further enhance performance.\n",
                "\n",
                "DSPy offers two powerful instruction optimizers:\n",
                "\n",
                "  * **COPRO** (**C**ontrastive **P**rompt **O**ptimization): Generates and refines new instructions for each step in your program, optimizing them through **coordinate ascent**.\n",
                "  * **MIPROv2** (**M**inimum **I**nstruction **P**rompt **O**ptimization v2): Generates instructions that are aware of both your data and any demonstrations, using **Bayesian Optimization** to efficiently search the space of possible instructions.\n",
                "\n",
                "These optimizers are valuable when you want to keep your prompts concise (reducing token usage) or when you're working with models that respond better to clear instructions.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding COPRO (Contrastive Prompt Optimization)\n",
                "\n",
                "**COPRO** is a powerful technique for automatically improving instructions. The core idea is to generate multiple alternative instructions, evaluate them using your metric, and iteratively refine them to find the best-performing set.\n",
                "\n",
                "The \"**contrastive**\" aspect comes from how it learns by comparing instructions that lead to correct outputs with those that fail.\n",
                "\n",
                "COPRO uses **coordinate ascent** (a form of hill-climbing) to optimize instructions:\n",
                "\n",
                "1.  **Generate Alternatives:** For each module, COPRO generates multiple alternative instructions.\n",
                "2.  **Evaluate:** It evaluates each alternative using your metric and training data.\n",
                "3.  **Select & Refine:** It selects the best-performing instruction and repeats the process for multiple iterations, generating new alternatives based on the current best instructions.\n",
                "\n",
                "### Key COPRO Parameters\n",
                "\n",
                "| Parameter | Description | Default |\n",
                "| :--- | :--- | :--- |\n",
                "| `prompt_model` | The language model used to generate new instruction candidates. | N/A |\n",
                "| `metric` | A function that evaluates the performance of your program. | N/A |\n",
                "| `breadth` | The number of new instruction candidates to generate in each iteration. | 16 |\n",
                "| `depth` | The number of iterations to run the optimization process. | 2 |\n",
                "| `init_temperature` | The temperature used when generating new instruction candidates (Higher = more diverse candidates). | 1.0 |\n",
                "| `verbose` | Whether to print detailed information during optimization. | `False` |\n",
                "\n",
                "COPRO is effective when you have a clear metric and want to **optimize instructions without relying on examples**.\n",
                "\n",
                "-----\n",
                "\n",
                "## Implementing COPRO in DSPy\n",
                "\n",
                "Implementing COPRO involves configuring the optimizer with key parameters and using it to compile your program.\n",
                "\n",
                "```python\n",
                "from dspy.teleprompt import COPRO\n",
                "\n",
                "# Define evaluation parameters for the compilation phase\n",
                "eval_kwargs = dict(num_threads=16, display_progress=True, display_table=0)\n",
                "\n",
                "# Create the COPRO optimizer\n",
                "copro_teleprompter = COPRO(\n",
                "    prompt_model=model_to_generate_prompts,  # E.g., dspy.LM('openai/gpt-4')\n",
                "    metric=your_defined_metric,              # Your evaluation metric function\n",
                "    breadth=num_new_prompts_generated,       # E.g., 16\n",
                "    depth=times_to_generate_prompts,         # E.g., 2\n",
                "    init_temperature=prompt_generation_temperature,  # E.g., 1.0\n",
                "    verbose=False\n",
                ")\n",
                "\n",
                "# Compile your program with the optimizer\n",
                "compiled_program_optimized_signature = copro_teleprompter.compile(\n",
                "    your_dspy_program,\n",
                "    trainset=trainset,\n",
                "    eval_kwargs=eval_kwargs\n",
                ")\n",
                "```\n",
                "\n",
                "The `compile()` process will iteratively refine the natural language instructions within your program's modules, selecting the signature that performs best on the provided `trainset` as measured by the `metric`.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding MIPROv2 (Minimum Instruction Prompt Optimization)\n",
                "\n",
                "**MIPROv2** is a more comprehensive optimizer that can optimize both **instructions and few-shot examples**. It generates instructions that are **data-aware** and **demonstration-aware**, meaning they are tailored to work effectively with the specific examples being used.\n",
                "\n",
                "MIPROv2 uses **Bayesian Optimization** to efficiently explore the search space, often finding better instructions with fewer evaluations than COPRO's coordinate ascent.\n",
                "\n",
                "### Key MIPROv2 Parameters\n",
                "\n",
                "| Parameter | Description |\n",
                "| :--- | :--- |\n",
                "| `metric` | A function that evaluates the performance of your program. |\n",
                "| `auto` | Specifies the optimization intensity: `\"light\"`, `\"medium\"`, or `\"heavy\"`. Lighter settings are faster for experimentation, while heavier settings perform more trials for better results. |\n",
                "| `max_bootstrapped_demos` | (Used in `compile`) Maximum number of new examples to self-generate. |\n",
                "| `max_labeled_demos` | (Used in `compile`) Maximum number of examples to use directly from the training set. |\n",
                "\n",
                "MIPROv2 is particularly effective when you have a **reasonable amount of training data** (e.g., 200+ examples) and want to **optimize both instructions and examples in a unified way**.\n",
                "\n",
                "-----\n",
                "\n",
                "## Implementing MIPROv2 in DSPy\n",
                "\n",
                "MIPROv2 supports both few-shot and zero-shot configurations by adjusting the `compile()` parameters.\n",
                "\n",
                "### Few-Shot Configuration (Optimizing Instructions + Examples)\n",
                "\n",
                "```python\n",
                "from dspy.teleprompt import MIPROv2\n",
                "\n",
                "# Create the MIPROv2 optimizer\n",
                "teleprompter = MIPROv2(\n",
                "    metric=gsm8k_metric,\n",
                "    auto=\"light\"  # Start with \"light\" for quick experimentation\n",
                ")\n",
                "\n",
                "# Compile the program with few-shot parameters\n",
                "optimized_program = teleprompter.compile(\n",
                "    program.deepcopy(),\n",
                "    trainset=trainset,\n",
                "    max_bootstrapped_demos=3,  # Generate up to 3 examples\n",
                "    max_labeled_demos=4,       # Use up to 4 existing examples\n",
                "    requires_permission_to_run=False,\n",
                ")\n",
                "```\n",
                "\n",
                "### Zero-Shot Configuration (Optimizing Instructions Only)\n",
                "\n",
                "To run MIPROv2 in a zero-shot mode, simply set both demonstration limits to zero during compilation:\n",
                "\n",
                "```python\n",
                "from dspy.teleprompt import MIPROv2\n",
                "\n",
                "# Create the MIPROv2 optimizer\n",
                "teleprompter = MIPROv2(\n",
                "    metric=gsm8k_metric,\n",
                "    auto=\"light\"\n",
                ")\n",
                "\n",
                "# Compile the program in zero-shot mode\n",
                "optimized_program = teleprompter.compile(\n",
                "    program.deepcopy(),\n",
                "    trainset=trainset,\n",
                "    max_bootstrapped_demos=0,  # No generated examples\n",
                "    max_labeled_demos=0,       # No examples from training set\n",
                "    requires_permission_to_run=False,\n",
                ")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Practice Preview\n",
                "\n",
                "| Optimizer | Primary Focus | Optimization Mechanism | Recommended Use Case |\n",
                "| :--- | :--- | :--- | :--- |\n",
                "| **COPRO** | Instructions Only | Coordinate Ascent (Hill-Climbing) | Focus purely on optimizing instructions, especially for zero-shot prompts. |\n",
                "| **MIPROv2** | Instructions + Examples | Bayesian Optimization | Optimizing instructions and few-shot examples jointly, especially with moderate-to-large training sets (200+). |\n",
                "\n",
                "**Guidelines for Selection:**\n",
                "\n",
                "  * **Instructions Only:** Use **COPRO**.\n",
                "  * **Instructions + Examples (Best Overall):** Use **MIPROv2**.\n",
                "  * **Larger Data / Longer Run:** Use **MIPROv2** with `auto=\"medium\"` or `\"heavy\"` for potentially better results.\n",
                "\n",
                "In the upcoming practice exercises, you'll gain hands-on experience by implementing both **COPRO** and **MIPROv2** to see how they affect your program's behavior.\n",
                "\n",
                "In the next lesson, we'll explore **Automatic Finetuning**, the final optimization category in DSPy, which involves updating the weights of the underlying language model itself."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tuning COPRO Parameters for Better Instructions\n",
                "\n",
                "Now that you understand how COPRO works to optimize instructions, let's experiment with its key parameters! In this exercise, you'll tune the COPRO optimizer to see how different settings affect the instruction generation process.\n",
                "\n",
                "First, you will implement COPRO for a simple math solver. Then, you'll modify two important parameters that control COPRO's behavior:\n",
                "\n",
                "The breadth parameter, which determines how many candidate prompts are generated in each iteration.\n",
                "The init_temperature parameter, which controls how diverse or creative the generated prompts will be.\n",
                "Your task is to implement and run the optimization with different combinations of these parameters and observe how they affect:\n",
                "\n",
                "The variety of instructions generated.\n",
                "The optimization progress shown in the output.\n",
                "The final performance scores.\n",
                "This hands-on experience will help you develop an intuition for configuring instruction optimizers effectively in your own projects. By the end, you'll have a better understanding of the trade-offs between exploration (trying many diverse candidates) and exploitation (focusing on refining promising instructions).\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "from dspy.teleprompt import COPRO\n",
                "from dspy.evaluate import Evaluate\n",
                "from data import get_trainset, get_testset, get_devset, metric\n",
                "\n",
                "# Set up a simple language model\n",
                "lm = dspy.LM('openai/gpt-4o-mini', api_key=os.environ['OPENAI_API_KEY'], api_base=os.environ['OPENAI_BASE_URL'])\n",
                "dspy.configure(lm=lm)\n",
                "\n",
                "# Define a simple math problem solver program\n",
                "class MathSolver(dspy.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.solver = dspy.Predict(\"question -> answer\")\n",
                "    \n",
                "    def forward(self, question):\n",
                "        return self.solver(question=question)\n",
                "\n",
                "# Get data\n",
                "trainset = get_trainset()\n",
                "testset = get_testset()\n",
                "devset = get_devset()\n",
                "\n",
                "# Create the base program\n",
                "base_program = MathSolver()\n",
                "\n",
                "# Define evaluation parameters\n",
                "eval_kwargs = dict(num_threads=4, display_progress=True, display_table=0)\n",
                "\n",
                "\n",
                "# TODO: Create the COPRO optimizer with specified parameters\n",
                "copro_teleprompter = COPRO(\n",
                "    \n",
                ")\n",
                "\n",
                "# TODO: Compile the program with the optimizer\n",
                "optimized_program = None\n",
                "\n",
                "\n",
                "# Set up the evaluator, which can be re-used in your code.\n",
                "evaluator = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
                "\n",
                "# Evaluate the optimized program\n",
                "score = evaluator(optimized_program, metric=metric)\n",
                "\n",
                "# TODO: Show results\n",
                "print(f\"Optimized instruction: {optimized_program.solver.signature}\")\n",
                "\n",
                "\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optimizing QA with COPRO Parameters\n",
                "\n",
                "Now that you've learned about instruction optimization with DSPy, let's put COPRO into practice! In this exercise, you'll implement COPRO to automatically improve instructions for a question-answering system on the HotPotQA dataset.\n",
                "\n",
                "After learning about the theory behind COPRO and how it uses coordinate ascent to find better instructions, it's time to see it in action. You'll configure the COPRO optimizer with specific parameters and observe how they affect the optimization process.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Load the data from HotPotQA\n",
                "Complete the COPRO optimizer implementation with appropriate parameters.\n",
                "Compile your QA pipeline with the optimizer.\n",
                "Evaluate and compare the performance before and after optimization.\n",
                "Pay special attention to the breadth parameter (which controls how many candidate instructions to generate) and the init_temperature parameter (which controls instruction diversity). These settings determine how thoroughly COPRO explores possible instructions.\n",
                "\n",
                "By completing this exercise, you'll gain practical experience with instruction optimization and see firsthand how better instructions can improve model performance without changing your program's structure or adding examples.\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "from dspy.teleprompt import COPRO\n",
                "from dspy.evaluate import Evaluate\n",
                "from dspy.datasets import HotPotQA\n",
                "\n",
                "# --- Configuration ---\n",
                "# Assuming dspy.OAI fixed the previous error\n",
                "try:\n",
                "    # Use dspy.OAI for OpenAI models (common convention in newer dspy versions)\n",
                "    # Note: Using gpt-4o-mini here, but a better model might be needed for high COPRO performance\n",
                "    lm = dspy.OAI(model='gpt-4o-mini', api_key=os.environ['OPENAI_API_KEY'], api_base=os.environ['OPENAI_BASE_URL'])\n",
                "    dspy.configure(lm=lm)\n",
                "except Exception as e:\n",
                "    print(f\"LM Configuration Error: {e}\")\n",
                "\n",
                "# --- Data Loading ---\n",
                "# FIX 1: Removed 'small=True'\n",
                "# FIX 2: Corrected 'eval_size' to 'dev_size'\n",
                "dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=1, dev_size=50)\n",
                "\n",
                "# Define splits\n",
                "train_set = dataset.train\n",
                "evaluation_set = dataset.dev \n",
                "dev_set_for_copro_search = dataset.dev[:20] \n",
                "\n",
                "# FIX 3: Convert the evaluation sets to dspy.Example objects with inputs defined\n",
                "def prepare_data(data_list):\n",
                "    \"\"\"Converts a list of dicts/examples into dspy.Example objects with specified inputs.\"\"\"\n",
                "    # Ensure all examples have the 'question' field marked as the input\n",
                "    return [dspy.Example(**d).with_inputs(\"question\") for d in data_list]\n",
                "\n",
                "train_set_prepared = prepare_data(train_set)\n",
                "evaluation_set_prepared = prepare_data(evaluation_set)\n",
                "dev_set_for_copro_search_prepared = prepare_data(dev_set_for_copro_search)\n",
                "\n",
                "print(f\"Loaded {len(train_set_prepared)} examples for training (COPRO's search set).\")\n",
                "print(f\"Loaded {len(evaluation_set_prepared)} examples for final evaluation.\")\n",
                "\n",
                "# Define the signature for our QA task\n",
                "class CoTSignature(dspy.Signature):\n",
                "    \"\"\"Answer the question and give the reasoning for the same.\"\"\"\n",
                "\n",
                "    question = dspy.InputField(desc=\"question about something\")\n",
                "    reasoning = dspy.OutputField(desc=\"reasoning for the answer\")\n",
                "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
                "\n",
                "# Create our Chain of Thought pipeline\n",
                "class CoTPipeline(dspy.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.signature = CoTSignature\n",
                "        self.predictor = dspy.ChainOfThought(self.signature)\n",
                "\n",
                "    def forward(self, question):\n",
                "        result = self.predictor(question=question)\n",
                "        return dspy.Prediction(\n",
                "            answer=result.answer,\n",
                "            reasoning=result.reasoning,\n",
                "        )\n",
                "\n",
                "# Define our evaluation metric\n",
                "def validate_context_and_answer(example, pred, trace=None):\n",
                "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
                "    return answer_EM\n",
                "\n",
                "# --- Baseline Evaluation ---\n",
                "cot_baseline = CoTPipeline()\n",
                "\n",
                "# Use the prepared evaluation set\n",
                "evaluate = Evaluate(devset=evaluation_set_prepared, metric=validate_context_and_answer, display_progress=True, display_table=True)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Evaluating baseline model...\")\n",
                "print(\"=\"*50)\n",
                "baseline_score = evaluate(cot_baseline, num_threads=1) \n",
                "print(f\"Baseline Score: {baseline_score:.4f}\")\n",
                "\n",
                "# --- COPRO Optimization ---\n",
                "teleprompter = COPRO(\n",
                "    breadth=10,\n",
                "    init_temperature=1.0,\n",
                "    num_evals=100,\n",
                "    max_bootstrapped_demos=1,\n",
                "    metric=validate_context_and_answer\n",
                ")\n",
                "\n",
                "kwargs = dict(num_threads=1, display_progress=True, display_table=0)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(f\"Starting COPRO Optimization on {len(dev_set_for_copro_search_prepared)} examples...\")\n",
                "print(\"=\"*50)\n",
                "# Use the prepared sets for compilation\n",
                "compiled_prompt_opt = teleprompter.compile(\n",
                "    student=cot_baseline,\n",
                "    trainset=train_set_prepared,\n",
                "    valset=dev_set_for_copro_search_prepared, # COPRO's internal evaluation set\n",
                "    **kwargs\n",
                ")\n",
                "\n",
                "# --- Optimized Evaluation ---\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Evaluating optimized model...\")\n",
                "print(\"=\"*50)\n",
                "optimized_score = evaluate(compiled_prompt_opt, num_threads=1)\n",
                "print(f\"Optimized Score: {optimized_score:.4f}\")\n",
                "\n",
                "# --- Display Results and History ---\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Optimization Results Comparison\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Baseline Score (Answer EM):     {baseline_score:.4f}\")\n",
                "print(f\"Optimized Score (Answer EM):    {optimized_score:.4f}\")\n",
                "improvement = optimized_score - baseline_score\n",
                "print(f\"Improvement: {'+' if improvement >= 0 else ''}{improvement:.4f}\")\n",
                "\n",
                "# Display the final optimized instruction\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Final Optimized Instructions in the Pipeline\")\n",
                "print(\"=\"*50)\n",
                "optimized_description = compiled_prompt_opt.predictor.extended_signature.instructions\n",
                "print(f\"Description: {optimized_description}\")\n",
                "\n",
                "```\n",
                "\n",
                "The `AttributeError: module 'dspy' has no attribute 'OAI'` and `AttributeError: module 'dspy' has no attribute 'DummyLM'` indicate that the code is using **outdated class names** for configuring the Language Model (LM) in the **DSPy** framework.\n",
                "\n",
                "Recent versions of DSPy **unified all language model providers** (like OpenAI, Anthropic, etc.) under the single, generic class: `dspy.LM`.\n",
                "\n",
                "### ✅ The Fix\n",
                "\n",
                "You need to replace the deprecated class names (`dspy.OAI` and `dspy.DummyLM`) with the current, unified class, **`dspy.LM`**.\n",
                "\n",
                "#### **Corrected Code Block**\n",
                "\n",
                "The code should be updated as follows:\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "from dspy.teleprompt import COPRO\n",
                "from dspy.evaluate import Evaluate\n",
                "from dspy.datasets import HotPotQA\n",
                "\n",
                "# --- Configuration ---\n",
                "try:\n",
                "    # FIX: Use dspy.LM for all model providers (OpenAI, Anthropic, etc.)\n",
                "    lm = dspy.LM(\n",
                "        model='openai/gpt-4o-mini', # Use provider/model format\n",
                "        api_key=os.environ['OPENAI_API_KEY'], \n",
                "        api_base=os.environ['OPENAI_BASE_URL']\n",
                "    )\n",
                "    dspy.configure(lm=lm)\n",
                "except Exception as e:\n",
                "    # FIX: The current equivalent for a dummy/no-op LM is also dspy.LM with specific settings\n",
                "    # For a simple fallback, using the DummyLM from dspy.utils is not always available at the top level.\n",
                "    # The safest way is to print the error and continue, but for full functionality, \n",
                "    # the LM must be configured correctly. Since the original intent was a fallback:\n",
                "    print(f\"LM Configuration Error: {e}. Cannot configure an active LM.\")\n",
                "    # For a clean fix that avoids the DummyLM error:\n",
                "    # In a typical setup, you'd ensure environment variables are set or halt.\n",
                "    # If a fallback is necessary without environment variables, the dspy.LM() call will likely fail \n",
                "    # unless you explicitly provide a local/mock model name.\n",
                "    # We will simply leave the dspy.configure(lm=lm) line outside the try/except if using a standard LM name.\n",
                "    \n",
                "# Since the original code had an error handler, we will remove the failing dspy.DummyLM() call \n",
                "# and focus on getting the primary LM configuration correct. \n",
                "# The simplest fix is to ensure the dspy.LM class is used:\n",
                "# Note: For production use, you'd handle the exception more robustly.\n",
                "\n",
                "# ... (rest of the code remains the same)\n",
                "```\n",
                "\n",
                "By changing:\n",
                "\n",
                "1.  `dspy.OAI(...)` to **`dspy.LM('openai/gpt-4o-mini', ...)`**\n",
                "2.  `dspy.DummyLM()` to avoiding the call or using a properly initialized **`dspy.LM`** if required, you adhere to the modern DSPy API.\n",
                "\n",
                "### 🔑 Key Takeaway\n",
                "\n",
                "In modern DSPy:\n",
                "\n",
                "  * Use **`dspy.LM(...)`** for all Language Model instantiation.\n",
                "  * The model name should follow the **`provider/model-name`** format (e.g., `openai/gpt-4o-mini`, `anthropic/claude-3-opus`, etc.) as it integrates with the LiteLLM library."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Few-Shot and Zero-Shot Optimization\n",
                "\n",
                "Now that you've explored COPRO and its parameters, let's dive into MIPROv2 and its unique capabilities! In this exercise, you'll compare few-shot and zero-shot configurations of MIPROv2 on a sentiment analysis task.\n",
                "\n",
                "After learning about how MIPROv2 can work with or without examples, you'll implement both approaches and analyze the differences. You'll configure MIPROv2 twice:\n",
                "\n",
                "Once with positive values for max_bootstrapped_demos and max_labeled_demos (few-shot)\n",
                "Once with both parameters set to 0 (zero-shot)\n",
                "By comparing the performance and examining the optimized instructions from both configurations, you'll gain practical insights into when to use examples versus when to rely solely on instructions. This hands-on experience will help you make informed decisions about optimization strategies in your own projects.\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "from dspy.evaluate import Evaluate\n",
                "from dspy.teleprompt import MIPROv2\n",
                "from data import get_trainset, get_testset, get_devset, sentiment_metric\n",
                "\n",
                "\n",
                "# Set up the language model\n",
                "lm = dspy.LM('openai/gpt-4o-mini', api_key=os.environ['OPENAI_API_KEY'], api_base=os.environ['OPENAI_BASE_URL'])\n",
                "dspy.configure(lm=lm)\n",
                "\n",
                "\n",
                "# Define a simple sentiment analyzer program\n",
                "class SentimentAnalyzer(dspy.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.classifier = dspy.Predict(\"text -> sentiment\")\n",
                "    \n",
                "    def forward(self, text):\n",
                "        return self.classifier(text=text)\n",
                "\n",
                "\n",
                "# Get data\n",
                "trainset = get_trainset()\n",
                "testset = get_testset()\n",
                "devset = get_devset()\n",
                "\n",
                "# Create the base program\n",
                "base_program = SentimentAnalyzer()\n",
                "\n",
                "# Create the MIPROv2 optimizer\n",
                "teleprompter = MIPROv2(\n",
                "    metric=sentiment_metric,\n",
                "    auto=\"light\" # Light optimization run\n",
                ")\n",
                "\n",
                "# TODO: Run few-shot optimization\n",
                "print(\"\\n--- Running MIPROv2 with Few-Shot Configuration ---\\n\")\n",
                "# TODO: Implement few-shot configuration with max_bootstrapped_demos and max_labeled_demos set to positive values\n",
                "\n",
                "# TODO: Run zero-shot optimization\n",
                "print(\"\\n--- Running MIPROv2 with Zero-Shot Configuration ---\\n\")\n",
                "# TODO: Implement zero-shot configuration with max_bootstrapped_demos and max_labeled_demos set to 0\n",
                "\n",
                "# TODO: Evaluate both configurations\n",
                "# TODO: Calculate scores for few-shot, zero-shot, and base programs\n",
                "\n",
                "# TODO: Compare results\n",
                "print(\"\\n--- Comparison of Different MIPROv2 Configurations ---\")\n",
                "# TODO: Print scores for base program, few-shot, and zero-shot configurations\n",
                "\n",
                "# TODO: Print optimized instructions\n",
                "print(\"\\n--- Few-Shot Optimized Instruction ---\")\n",
                "# TODO: Print the few-shot program's optimized instruction\n",
                "\n",
                "print(\"\\n--- Zero-Shot Optimized Instruction ---\")\n",
                "# TODO: Print the zero-shot program's optimized instruction\n",
                "\n",
                "# TODO: Analyze differences\n",
                "print(\"\\n--- Analysis of Differences ---\")\n",
                "# TODO: Compare the performance and instruction differences between the two configurations\n",
                "```\n",
                "\n",
                "This exercise demonstrates the flexibility of **MIPROv2**, showing how it can optimize both instructions and examples (**few-shot**) or instructions alone (**zero-shot**).\n",
                "\n",
                "Here is the completed code that implements both optimization scenarios, evaluates them, and compares the results.\n",
                "\n",
                "### ✅ Completed MIPROv2 Comparison Code\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "from dspy.evaluate import Evaluate\n",
                "from dspy.teleprompt import MIPROv2\n",
                "# NOTE: Assuming the 'data' module provides the necessary functions and metric\n",
                "from data import get_trainset, get_testset, get_devset, sentiment_metric \n",
                "\n",
                "\n",
                "# --- Configuration ---\n",
                "try:\n",
                "    # Set up the language model using dspy.LM (correct class for all providers)\n",
                "    # NOTE: You must have OPENAI_API_KEY and OPENAI_BASE_URL set in your environment\n",
                "    lm = dspy.LM(\n",
                "        model='openai/gpt-4o-mini', \n",
                "        api_key=os.environ['OPENAI_API_KEY'], \n",
                "        api_base=os.environ['OPENAI_BASE_URL']\n",
                "    )\n",
                "    dspy.configure(lm=lm)\n",
                "except Exception as e:\n",
                "    print(f\"LM Configuration Error: {e}. Using DummyLM for simulation/structure review.\")\n",
                "    dspy.configure(lm=dspy.DummyLM())\n",
                "\n",
                "\n",
                "# Define a simple sentiment analyzer program\n",
                "class SentimentAnalyzer(dspy.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        # The Predict module is what MIPROv2 will optimize (its instructions and/or examples)\n",
                "        self.classifier = dspy.Predict(\"text -> sentiment\")\n",
                "    \n",
                "    def forward(self, text):\n",
                "        return self.classifier(text=text)\n",
                "\n",
                "\n",
                "# Get data\n",
                "trainset = get_trainset()\n",
                "testset = get_testset()\n",
                "devset = get_devset()\n",
                "\n",
                "# Create the base program\n",
                "base_program = SentimentAnalyzer()\n",
                "\n",
                "# Create the MIPROv2 optimizer\n",
                "teleprompter = MIPROv2(\n",
                "    metric=sentiment_metric,\n",
                "    auto=\"light\" # Light optimization run for quick testing\n",
                ")\n",
                "\n",
                "# Define evaluation utility\n",
                "evaluate = Evaluate(devset=testset, metric=sentiment_metric, display_progress=True, display_table=True)\n",
                "\n",
                "\n",
                "# --- Baseline Evaluation ---\n",
                "print(\"\\n--- Evaluating Base Program (Zero-Shot, Unoptimized) ---\")\n",
                "base_score = evaluate(base_program, num_threads=1, display_progress=False)\n",
                "print(f\"Base Program Score: {base_score:.4f}\")\n",
                "\n",
                "# --- Few-Shot Optimization ---\n",
                "print(\"\\n--- Running MIPROv2 with Few-Shot Configuration ---\\n\")\n",
                "# Few-shot configuration: Optimize instructions AND generate/use examples.\n",
                "compiled_few_shot = teleprompter.compile(\n",
                "    base_program.deepcopy(),\n",
                "    trainset=trainset,\n",
                "    max_bootstrapped_demos=3,  # Generate up to 3 self-Hashed examples\n",
                "    max_labeled_demos=2,       # Use up to 2 examples directly from the trainset\n",
                "    requires_permission_to_run=False,\n",
                ")\n",
                "\n",
                "# --- Few-Shot Evaluation ---\n",
                "print(\"\\n--- Evaluating Few-Shot Optimized Program ---\")\n",
                "few_shot_score = evaluate(compiled_few_shot, num_threads=1, display_progress=False)\n",
                "print(f\"Few-Shot Optimized Score: {few_shot_score:.4f}\")\n",
                "\n",
                "# --- Zero-Shot Optimization ---\n",
                "print(\"\\n--- Running MIPROv2 with Zero-Shot Configuration ---\\n\")\n",
                "# Zero-shot configuration: Optimize instructions ONLY by setting demo limits to 0.\n",
                "teleprompter_zero = MIPROv2(metric=sentiment_metric, auto=\"light\")\n",
                "compiled_zero_shot = teleprompter_zero.compile(\n",
                "    base_program.deepcopy(),\n",
                "    trainset=trainset,\n",
                "    max_bootstrapped_demos=0,  # No generated examples\n",
                "    max_labeled_demos=0,       # No examples from training set\n",
                "    requires_permission_to_run=False,\n",
                ")\n",
                "\n",
                "# --- Zero-Shot Evaluation ---\n",
                "print(\"\\n--- Evaluating Zero-Shot Optimized Program ---\")\n",
                "zero_shot_score = evaluate(compiled_zero_shot, num_threads=1, display_progress=False)\n",
                "print(f\"Zero-Shot Optimized Score: {zero_shot_score:.4f}\")\n",
                "\n",
                "\n",
                "# --- Comparison of Different MIPROv2 Configurations ---\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Comparison of Different MIPROv2 Configurations\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Baseline Score (Metric):        {base_score:.4f}\")\n",
                "print(f\"Few-Shot Optimized Score:       {few_shot_score:.4f}\")\n",
                "print(f\"Zero-Shot Optimized Score:      {zero_shot_score:.4f}\")\n",
                "\n",
                "\n",
                "# --- Optimized Instructions ---\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Optimized Instructions\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "print(\"\\n--- Few-Shot Optimized Instruction (MIPROv2 with Examples) ---\")\n",
                "# The instructions are stored in the predictor's extended signature\n",
                "few_shot_instructions = compiled_few_shot.classifier.extended_signature.instructions\n",
                "print(f\"Description: {few_shot_instructions}\")\n",
                "\n",
                "print(\"\\n--- Zero-Shot Optimized Instruction (MIPROv2 Instruction-Only) ---\")\n",
                "zero_shot_instructions = compiled_zero_shot.classifier.extended_signature.instructions\n",
                "print(f\"Description: {zero_shot_instructions}\")\n",
                "\n",
                "\n",
                "# --- Analysis of Differences ---\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Analysis of Differences\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Performance Improvement: Few-Shot (+{few_shot_score - base_score:.4f}) vs. Zero-Shot (+{zero_shot_score - base_score:.4f})\")\n",
                "\n",
                "if few_shot_score > zero_shot_score:\n",
                "    print(\"\\n**Few-Shot Analysis:** The Few-Shot configuration typically performs **better** (or equal) because it leverages both an **optimized instruction** AND **contextual examples** (demonstrations). The MIPROv2 optimizer selects a set of examples and then finds an instruction tailored to maximize performance *with those specific examples*.\")\n",
                "else:\n",
                "    print(\"\\n**Zero-Shot Analysis:** In some cases, the Zero-Shot configuration may perform better. This happens if the task is highly sensitive to the *quality* of the instructions, and the added examples (demos) selected by MIPROv2 were noisy or misleading. The Zero-Shot mode forces the optimizer to find the **most robust, self-contained instruction** possible.\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Balancing Optimization Intensity for Better Results\n",
                "\n",
                "Now that you've explored both COPRO and MIPROv2, let's dive deeper into MIPROv2's optimization intensity settings! In this exercise, you'll compare how different optimization intensities affect performance and runtime.\n",
                "\n",
                "You'll work with a sentiment analysis program that's already set up with a \"light\" optimization. Your tasks are to:\n",
                "\n",
                "Add code to run MIPROv2 with the \"heavy\" setting.\n",
                "Track and compare the runtime of both optimization processes.\n",
                "Analyze the differences in performance and generated instructions.\n",
                "Determine if the extra computation time is worth the potential gains.\n",
                "This exercise will help you make informed decisions about when to use more intensive optimization in your own projects. By comparing the number of trials, runtime, and performance improvements, you'll develop a practical understanding of the trade-offs involved in instruction optimization.\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "import time\n",
                "from dspy.teleprompt import MIPROv2\n",
                "from dspy.evaluate import Evaluate\n",
                "from data import get_trainset, get_testset, get_devset, sentiment_metric\n",
                "\n",
                "\n",
                "# Set up the language model\n",
                "lm = dspy.LM('openai/gpt-4o-mini', api_key=os.environ['OPENAI_API_KEY'], api_base=os.environ['OPENAI_BASE_URL'])\n",
                "dspy.configure(lm=lm)\n",
                "\n",
                "\n",
                "# Define a simple sentiment analyzer program\n",
                "class SentimentAnalyzer(dspy.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.classifier = dspy.Predict(\"text -> sentiment\")\n",
                "    \n",
                "    def forward(self, text):\n",
                "        return self.classifier(text=text)\n",
                "\n",
                "# Get data\n",
                "trainset = get_trainset()\n",
                "testset = get_testset()\n",
                "devset = get_devset()\n",
                "\n",
                "# Create the base program\n",
                "base_program = SentimentAnalyzer()\n",
                "\n",
                "# Set up the evaluator, which can be re-used in your code.\n",
                "evaluator = Evaluate(devset=devset, display_progress=True, display_table=5)\n",
                "\n",
                "# Evaluate the base program\n",
                "base_score = evaluator(base_program, metric=sentiment_metric)\n",
                "print(f\"Base program score: {base_score:.4f}\")\n",
                "\n",
                "# Run light optimization\n",
                "print(\"\\n--- Running MIPROv2 with 'light' optimization ---\\n\")\n",
                "light_teleprompter = MIPROv2(\n",
                "    metric=sentiment_metric,\n",
                "    auto=\"light\",  # Light optimization run (fewer trials)\n",
                ")\n",
                "\n",
                "# Time the light optimization\n",
                "light_start_time = time.time()\n",
                "light_program = light_teleprompter.compile(\n",
                "    base_program.deepcopy(),\n",
                "    trainset=trainset,\n",
                "    max_bootstrapped_demos=2,\n",
                "    max_labeled_demos=3,\n",
                "    requires_permission_to_run=False\n",
                ")\n",
                "light_end_time = time.time()\n",
                "light_duration = light_end_time - light_start_time\n",
                "\n",
                "# TODO: Create a new MIPROv2 optimizer with \"heavy\" optimization\n",
                "# Add your code here to create a heavy teleprompter\n",
                "\n",
                "# TODO: Time the heavy optimization\n",
                "# Add your code here to measure the runtime of heavy optimization\n",
                "\n",
                "# TODO: Evaluate both optimized programs\n",
                "light_score = evaluator(light_program, metric=sentiment_metric)\n",
                "# Add your code here to evaluate the heavy program\n",
                "\n",
                "# Compare results\n",
                "print(\"\\n--- Comparison of Different MIPROv2 Intensities ---\")\n",
                "print(f\"Base program score: {base_score:.4f}\")\n",
                "print(f\"Light optimization score: {light_score:.4f}\")\n",
                "# TODO: Add code to print the heavy optimization score\n",
                "\n",
                "# TODO: Compare runtime\n",
                "print(\"\\n--- Runtime Comparison ---\")\n",
                "print(f\"Light optimization runtime: {light_duration:.2f} seconds\")\n",
                "# TODO: Add code to print the heavy optimization runtime and comparison\n",
                "\n",
                "# Print optimized instructions\n",
                "print(\"\\n--- Light Optimization Instruction ---\")\n",
                "print(light_program.classifier.signature)\n",
                "\n",
                "# TODO: Print the heavy optimization instruction\n",
                "# Add your code here to print the heavy program's instruction\n",
                "\n",
                "# TODO: Analyze differences\n",
                "print(\"\\n--- Analysis of Differences ---\")\n",
                "# TODO: Add code to compare performance, runtime, and instructions\n",
                "# Add your observations about the differences between light and heavy optimization\n",
                "\n",
                "```\n",
                "\n",
                "This exercise requires implementing the **\"heavy\"** MIPROv2 optimization to compare its trade-offs against the **\"light\"** setting. The core difference will be observed in runtime, cost (due to more trials), and potentially the final performance score.\n",
                "\n",
                "Here is the completed code with the required additions.\n",
                "\n",
                "### ✅ Completed Optimization Intensity Comparison\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "import os\n",
                "import time\n",
                "from dspy.teleprompt import MIPROv2\n",
                "from dspy.evaluate import Evaluate\n",
                "# NOTE: Assuming the 'data' module provides the necessary functions and metric\n",
                "from data import get_trainset, get_testset, get_devset, sentiment_metric \n",
                "\n",
                "# --- Configuration ---\n",
                "try:\n",
                "    # Set up the language model using dspy.LM (correct class for all providers)\n",
                "    # NOTE: You must have OPENAI_API_KEY and OPENAI_BASE_URL set in your environment\n",
                "    lm = dspy.LM(\n",
                "        model='openai/gpt-4o-mini', \n",
                "        api_key=os.environ['OPENAI_API_KEY'], \n",
                "        api_base=os.environ['OPENAI_BASE_URL']\n",
                "    )\n",
                "    dspy.configure(lm=lm)\n",
                "except Exception as e:\n",
                "    print(f\"LM Configuration Error: {e}. Using DummyLM for simulation/structure review.\")\n",
                "    dspy.configure(lm=dspy.DummyLM())\n",
                "\n",
                "\n",
                "# Define a simple sentiment analyzer program\n",
                "class SentimentAnalyzer(dspy.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.classifier = dspy.Predict(\"text -> sentiment\")\n",
                "    \n",
                "    def forward(self, text):\n",
                "        result = self.classifier(text=text)\n",
                "        return dspy.Prediction(sentiment=result.sentiment) # Ensure output matches the signature if needed\n",
                "\n",
                "# Get data\n",
                "trainset = get_trainset()\n",
                "testset = get_testset()\n",
                "devset = get_devset()\n",
                "\n",
                "# Create the base program\n",
                "base_program = SentimentAnalyzer()\n",
                "\n",
                "# Set up the evaluator, which can be re-used in your code.\n",
                "evaluator = Evaluate(devset=devset, metric=sentiment_metric, display_progress=False, display_table=False)\n",
                "\n",
                "# Evaluate the base program\n",
                "base_score = evaluator(base_program, metric=sentiment_metric)\n",
                "print(f\"Base program score: {base_score:.4f}\")\n",
                "\n",
                "# --- Light Optimization ---\n",
                "print(\"\\n--- Running MIPROv2 with 'light' optimization ---\\n\")\n",
                "light_teleprompter = MIPROv2(\n",
                "    metric=sentiment_metric,\n",
                "    auto=\"light\",  # Light optimization run (fewer trials)\n",
                ")\n",
                "\n",
                "# Time the light optimization\n",
                "light_start_time = time.time()\n",
                "light_program = light_teleprompter.compile(\n",
                "    base_program.deepcopy(),\n",
                "    trainset=trainset,\n",
                "    max_bootstrapped_demos=2,\n",
                "    max_labeled_demos=3,\n",
                "    requires_permission_to_run=False\n",
                ")\n",
                "light_end_time = time.time()\n",
                "light_duration = light_end_time - light_start_time\n",
                "\n",
                "# --- Heavy Optimization ---\n",
                "# TODO: Create a new MIPROv2 optimizer with \"heavy\" optimization\n",
                "print(\"\\n--- Running MIPROv2 with 'heavy' optimization ---\\n\")\n",
                "heavy_teleprompter = MIPROv2(\n",
                "    metric=sentiment_metric,\n",
                "    auto=\"heavy\",  # Heavy optimization run (more trials/candidates)\n",
                ")\n",
                "\n",
                "# TODO: Time the heavy optimization\n",
                "heavy_start_time = time.time()\n",
                "heavy_program = heavy_teleprompter.compile(\n",
                "    base_program.deepcopy(),\n",
                "    trainset=trainset,\n",
                "    max_bootstrapped_demos=2,\n",
                "    max_labeled_demos=3,\n",
                "    requires_permission_to_run=False\n",
                ")\n",
                "heavy_end_time = time.time()\n",
                "heavy_duration = heavy_end_time - heavy_start_time\n",
                "\n",
                "# --- Final Evaluation ---\n",
                "light_score = evaluator(light_program, metric=sentiment_metric)\n",
                "# TODO: Add your code here to evaluate the heavy program\n",
                "heavy_score = evaluator(heavy_program, metric=sentiment_metric)\n",
                "\n",
                "# --- Comparison of Results ---\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Comparison of Different MIPROv2 Intensities\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "# Compare scores\n",
                "print(f\"Base program score:       {base_score:.4f}\")\n",
                "print(f\"Light optimization score: {light_score:.4f}\")\n",
                "print(f\"Heavy optimization score: {heavy_score:.4f}\") # Print the heavy optimization score\n",
                "\n",
                "# Compare runtime\n",
                "print(\"\\n--- Runtime Comparison ---\")\n",
                "print(f\"Light optimization runtime: {light_duration:.2f} seconds\")\n",
                "print(f\"Heavy optimization runtime: {heavy_duration:.2f} seconds\") # Print the heavy optimization runtime\n",
                "time_ratio = heavy_duration / light_duration if light_duration > 0 else float('inf')\n",
                "print(f\"Heavy optimization ran approximately {time_ratio:.1f}x longer than Light.\")\n",
                "\n",
                "# Print optimized instructions\n",
                "print(\"\\n--- Optimized Instructions ---\")\n",
                "print(\"\\n--- Light Optimization Instruction ---\")\n",
                "print(f\"Instruction: {light_program.classifier.extended_signature.instructions}\")\n",
                "\n",
                "# TODO: Print the heavy optimization instruction\n",
                "print(\"\\n--- Heavy Optimization Instruction ---\")\n",
                "print(f\"Instruction: {heavy_program.classifier.extended_signature.instructions}\")\n",
                "\n",
                "# --- Analysis of Differences ---\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Analysis of Differences\")\n",
                "print(\"=\"*50)\n",
                "score_diff = heavy_score - light_score\n",
                "print(f\"Performance Difference (Heavy - Light): {score_diff:+.4f}\")\n",
                "\n",
                "# TODO: Add code to compare performance, runtime, and instructions\n",
                "print(\"\\n**Performance:**\")\n",
                "if score_diff > 0.0:\n",
                "    print(f\"The Heavy optimization was better, improving the score by {score_diff:.4f} over the Light setting.\")\n",
                "elif score_diff < 0.0:\n",
                "    print(f\"The Light optimization unexpectedly performed better, with the Heavy setting resulting in a {-score_diff:.4f} lower score. This suggests the limited data led to overfitting during the longer Heavy run.\")\n",
                "else:\n",
                "    print(\"Both Light and Heavy optimization achieved similar final scores.\")\n",
                "\n",
                "print(\"\\n**Runtime & Cost:**\")\n",
                "print(f\"The Heavy setting took {time_ratio:.1f}x longer than the Light setting. Since MIPROv2 is inference-heavy (especially the Instruction Proposal and Bayesian Optimization stages), the Heavy setting involves significantly more API calls and computational time.\")\n",
                "\n",
                "print(\"\\n**Instruction/Demo Differences:**\")\n",
                "print(\"The primary difference between the resulting programs is the **instruction phrasing** and the **set of few-shot examples** (demos). The Heavy setting explores a much larger combination space of these elements (more trials in the Bayesian Optimization) to find a marginal or substantial performance gain. The two optimized instructions are likely phrased differently, reflecting the optimizer's best effort to guide the model for its specific, selected few-shot set.\")\n",
                "\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
