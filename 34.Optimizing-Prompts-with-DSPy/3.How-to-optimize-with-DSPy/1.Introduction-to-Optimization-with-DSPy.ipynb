{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction to Optimization with DSPy\n",
                "\n",
                "# Welcome to the first lesson in our course on optimizing with DSPy\\!\n",
                "\n",
                "In this course, you'll learn how to leverage **DSPy's powerful optimization capabilities** to improve the performance of your language model applications.\n",
                "\n",
                "So far, we've been focusing on building DSPy programs that can solve specific tasks using large language models (LLMs). While these programs can be effective with carefully crafted prompts, there's often room for improvement. This is where optimization comes in.\n",
                "\n",
                "In DSPy, **optimization** refers to the process of **automatically tuning the parameters of your program** — primarily the prompts and potentially the language model weights — to improve performance on your specific task. Rather than manually tweaking prompts through trial and error, DSPy provides optimizers that can systematically explore the space of possible prompts and find ones that work better for your application.\n",
                "\n",
                "-----\n",
                "\n",
                "## Why is optimization so important?\n",
                "\n",
                "Even expert prompt engineers can't always predict what prompt formulations will work best for a given task and model. The space of possible prompts is vast, and small changes in wording can sometimes lead to significant differences in performance. **DSPy optimizers automate this exploration process**, saving you time and often finding better solutions than manual tuning.\n",
                "\n",
                "The key advantage of DSPy's approach is that it transforms prompt engineering from an art into a more systematic process. Instead of relying solely on intuition and manual iteration, you can leverage data-driven optimization techniques to improve your program's performance.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding the Optimization Process\n",
                "\n",
                "The basic workflow for optimization in DSPy follows a straightforward pattern:\n",
                "\n",
                "1.  You start with a DSPy program that defines your task.\n",
                "2.  You apply a DSPy optimizer to improve the program.\n",
                "3.  You get back an optimized version of your program with better performance.\n",
                "\n",
                "Let's break down the key components involved in this process:\n",
                "\n",
                "  * **Programs**: These are the DSPy modules or pipelines you've created to solve your task. They can be simple single-step programs or complex multi-module pipelines.\n",
                "  * **Optimizers**: These are algorithms that tune your program's parameters. DSPy provides various optimizers for different scenarios, which we'll explore shortly.\n",
                "  * **Metrics**: These are functions that evaluate how well your program is performing. They provide the feedback signal that guides the optimization process.\n",
                "  * **Datasets**: These are collections of examples that the optimizer uses to learn from and evaluate performance.\n",
                "\n",
                "-----\n",
                "\n",
                "## Data used in the Optimization Process\n",
                "\n",
                "When preparing data for optimization, it's useful to split your dataset into different subsets:\n",
                "\n",
                "  * **Training set**: Used by the optimizer to learn and improve your program.\n",
                "  * **Validation set**: Used to evaluate and select the best program during optimization.\n",
                "  * **Test set**: Used for final evaluation after optimization is complete.\n",
                "\n",
                "For effective optimization, you'll need to collect enough data. Here are some guidelines:\n",
                "\n",
                "  * For the training set (and its subset, validation set), aim for at least **300 examples**, though you can often get substantial value out of as few as 30 examples.\n",
                "  * Some optimizers accept only a training set, while others require both training and validation sets.\n",
                "  * For prompt optimizers, it's recommended to start with a **20% split for training and 80% for validation**, which is the opposite of what's typically done for deep neural networks.\n",
                "\n",
                "This data-splitting approach helps ensure that your optimized program generalizes well to new examples rather than just memorizing the training data.\n",
                "\n",
                "-----\n",
                "\n",
                "## Types of Optimizers in DSPy\n",
                "\n",
                "In the next lessons, we will be learning what optimizers DSPy provides. There are three main categories of optimizers, each with different approaches to improving your program:\n",
                "\n",
                "1.  **Automatic Few-Shot Learning Optimizers**: These optimizers focus on finding and incorporating effective examples (demonstrations) into your prompts. They implement few-shot learning, where the model is shown examples of the task before being asked to solve a new instance.\n",
                "2.  **Automatic Instruction Optimization Optimizers**: These optimizers focus on improving the natural language instructions in your prompts. They can generate and refine instructions to better guide the language model.\n",
                "3.  **Automatic Finetuning Optimizers**: These optimizers go beyond prompt engineering to actually modify the weights of the language model itself. They can distill knowledge from a prompt-based program into model weights. As this is a more advanced setup, we will not be covering them in this course.\n",
                "\n",
                "-----\n",
                "\n",
                "## Setting Up for Optimization\n",
                "\n",
                "Before you can optimize your DSPy program, you need to prepare a few things:\n",
                "\n",
                "### 1\\. Prepare Your DSPy Program\n",
                "\n",
                "Your program should be properly defined using DSPy modules and signatures. Make sure it works correctly before optimization, as optimizers can improve performance but won't fix fundamental flaws in your program's design.\n",
                "\n",
                "### 2\\. Create Evaluation Metrics\n",
                "\n",
                "As we saw in last course, metrics tell the optimizer what to optimize for. A simple metric might check if the program's output matches a reference answer:\n",
                "\n",
                "```python\n",
                "def accuracy_metric(example, prediction):\n",
                "    # Check if the predicted answer matches the reference answer\n",
                "    return prediction.answer.lower() == example.answer.lower()\n",
                "```\n",
                "\n",
                "You can create more sophisticated metrics for specific tasks. For example, a QA metric might check if the predicted answer contains the correct information, even if the wording is different.\n",
                "\n",
                "### 3\\. Structure Your Datasets\n",
                "\n",
                "Your datasets should be collections of examples that your program can process. Each example should contain the inputs your program expects and, for training data, the expected outputs.\n",
                "\n",
                "Here's an example of how you might prepare a dataset for a question-answering task:\n",
                "\n",
                "```python\n",
                "# Create a list of examples\n",
                "trainset = []\n",
                "for question, answer in zip(questions, answers):\n",
                "    # Create an example with the input fields your program expects\n",
                "    example = dspy.Example(\n",
                "        question=question,\n",
                "        answer=answer\n",
                "    )\n",
                "    trainset.append(example)\n",
                "\n",
                "# Split into training and validation sets\n",
                "train_size = int(len(trainset) * 0.2)  # 20% for training, 80% for validation\n",
                "trainset, valset = trainset[:train_size], trainset[train_size:]\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Next Steps\n",
                "\n",
                "In this lesson, we've introduced the concept of optimization in DSPy and explored how it can help improve the performance of your language model applications. Let's recap the key points:\n",
                "\n",
                "  * Optimization in DSPy is the process of **automatically tuning prompts** and potentially model weights to improve performance.\n",
                "  * The optimization process involves **programs, optimizers, metrics, and datasets** working together.\n",
                "  * DSPy provides three main types of optimizers: **Few-Shot Learning, Instruction Optimization, and Finetuning**.\n",
                "  * To set up for optimization, you need to prepare your program, create evaluation metrics, and structure your datasets.\n",
                "\n",
                "In the practice exercises that follow, you'll get hands-on experience with setting up basic optimization workflows and understanding how different components work together. You'll prepare datasets, define metrics, and run simple optimizations to see the improvement in your program's performance.\n",
                "\n",
                "In the next lesson, we'll dive deeper into **Automatic Few-Shot Learning optimizers**, exploring how they can automatically find and incorporate effective examples into your prompts. You'll learn how to use optimizers like `LabeledFewShot` and `BootstrapFewShot` to improve your program's performance with minimal data.\n",
                "\n",
                "Remember, optimization is an iterative process. After optimizing your program, you might want to revisit your program design, collect more data, or try different optimization strategies to further improve performance. The tools and techniques you'll learn in this course will help you systematically improve your DSPy applications."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Splitting Data for Prompt Optimization\n",
                "\n",
                "Now that you understand the importance of data preparation for DSPy optimization, let's put that knowledge into practice! In this exercise, you'll create a small dataset of question-answer pairs and split it using the recommended 20%/80% ratio for prompt optimization that we just learned about.\n",
                "\n",
                "Remember, this split ratio (a smaller training set and a larger validation set) is the opposite of what's typically used in traditional machine learning, but it's particularly effective for prompt optimization.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Calculate the correct split index based on the dataset size\n",
                "Split the dataset into training and validation sets\n",
                "Print the sizes of both sets to verify your split is correct\n",
                "This hands-on experience with dataset preparation will give you the foundation needed before we dive into specific optimizers in the upcoming lessons. Properly structured data is the first step toward successful optimization!\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "\n",
                "# Create a small dataset of question-answer pairs\n",
                "qa_pairs = [\n",
                "    (\"What is the capital of France?\", \"Paris\"),\n",
                "    (\"Who wrote 'Romeo and Juliet'?\", \"William Shakespeare\"),\n",
                "    (\"What is the largest planet in our solar system?\", \"Jupiter\"),\n",
                "    (\"What is the chemical symbol for gold?\", \"Au\"),\n",
                "    (\"Who painted the Mona Lisa?\", \"Leonardo da Vinci\"),\n",
                "    (\"What is the tallest mountain in the world?\", \"Mount Everest\"),\n",
                "    (\"What year did World War II end?\", \"1945\"),\n",
                "    (\"What is the smallest prime number?\", \"2\"),\n",
                "    (\"Who was the first person to step on the moon?\", \"Neil Armstrong\"),\n",
                "    (\"What is the main ingredient in guacamole?\", \"Avocado\")\n",
                "]\n",
                "\n",
                "# Convert the pairs into DSPy examples\n",
                "dataset = []\n",
                "for question, answer in qa_pairs:\n",
                "    example = dspy.Example(\n",
                "        question=question,\n",
                "        answer=answer\n",
                "    )\n",
                "    dataset.append(example)\n",
                "\n",
                "# TODO: Calculate the split index for 20% training, 80% validation\n",
                "# For prompt optimization, we use a smaller training set and larger validation set\n",
                "\n",
                "# TODO: Split the dataset into trainset and valset\n",
                "\n",
                "# TODO: Print the sizes to verify the split\n",
                "\n",
                "# Print a sample from each set\n",
                "print(\"\\nSample from training set:\")\n",
                "for i, example in enumerate(trainset):\n",
                "    print(f\"  Example {i+1}: {example.question} → {example.answer}\")\n",
                "\n",
                "print(\"\\nSample from validation set (first 3):\")\n",
                "for i, example in enumerate(valset[:3]):\n",
                "    print(f\"  Example {i+1}: {example.question} → {example.answer}\")\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "\n",
                "# Create a small dataset of question-answer pairs\n",
                "qa_pairs = [\n",
                "    (\"What is the capital of France?\", \"Paris\"),\n",
                "    (\"Who wrote 'Romeo and Juliet'?\", \"William Shakespeare\"),\n",
                "    (\"What is the largest planet in our solar system?\", \"Jupiter\"),\n",
                "    (\"What is the chemical symbol for gold?\", \"Au\"),\n",
                "    (\"Who painted the Mona Lisa?\", \"Leonardo da Vinci\"),\n",
                "    (\"What is the tallest mountain in the world?\", \"Mount Everest\"),\n",
                "    (\"What year did World War II end?\", \"1945\"),\n",
                "    (\"What is the smallest prime number?\", \"2\"),\n",
                "    (\"Who was the first person to step on the moon?\", \"Neil Armstrong\"),\n",
                "    (\"What is the main ingredient in guacamole?\", \"Avocado\")\n",
                "]\n",
                "\n",
                "# Convert the pairs into DSPy examples\n",
                "dataset = []\n",
                "for question, answer in qa_pairs:\n",
                "    example = dspy.Example(\n",
                "        question=question,\n",
                "        answer=answer\n",
                "    )\n",
                "    dataset.append(example)\n",
                "\n",
                "# TODO: Calculate the split index for 20% training, 80% validation\n",
                "# For prompt optimization, we use a smaller training set and larger validation set\n",
                "split_index = int(len(dataset) * 0.2)\n",
                "\n",
                "# TODO: Split the dataset into trainset and valset\n",
                "trainset = dataset[:split_index]\n",
                "valset = dataset[split_index:]\n",
                "\n",
                "# TODO: Print the sizes to verify the split\n",
                "print(f\"Training set size: {len(trainset)}\")\n",
                "print(f\"Validation set size: {len(valset)}\")\n",
                "\n",
                "# Print a sample from each set\n",
                "print(\"\\nSample from training set:\")\n",
                "for i, example in enumerate(trainset):\n",
                "    print(f\"  Example {i+1}: {example.question} → {example.answer}\")\n",
                "\n",
                "print(\"\\nSample from validation set (first 3):\")\n",
                "for i, example in enumerate(valset[:3]):\n",
                "    print(f\"  Example {i+1}: {example.question} → {example.answer}\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Explanation of the Code\n",
                "\n",
                "In this exercise, we prepared a dataset for **prompt optimization** within a DSPy workflow. The key step was splitting the data using a **20%/80% ratio** for the training and validation sets, respectively.\n",
                "\n",
                "  * `split_index = int(len(dataset) * 0.2)`: This line calculates the index where the dataset should be split. By multiplying the total number of examples (10) by `0.2`, we get an index of `2`, meaning the first two examples will be used for training.\n",
                "\n",
                "  * `trainset = dataset[:split_index]` and `valset = dataset[split_index:]`: These lines use Python's list slicing to create the two subsets. The `trainset` contains all examples from the beginning up to (but not including) the `split_index`. The `valset` contains all examples from the `split_index` to the end of the list.\n",
                "\n",
                "The resulting output confirms that the training set has a size of 2, and the validation set has a size of 8, which aligns with the recommended ratio for this specific task. This smaller training set is used by the optimizer to learn effective prompts, while the larger validation set provides a more robust and reliable evaluation of the program's performance."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading External Data for Optimization\n",
                "\n",
                "Now that you've learned how to split data for prompt optimization, let's take the next step by working with external data files! In this exercise, you'll create a data loader function that reads question-answer pairs from a JSON file and prepares them for DSPy optimization.\n",
                "\n",
                "Working with external data files is a common practice in real-world applications, as it allows you to separate your code from your data and makes it easier to update your dataset without changing your code.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Complete the load_qa_data() function that reads and processes a JSON file.\n",
                "Use this function in the solution.py script to load your dataset.\n",
                "Print statistics and some examples to verify your data is loaded correctly.\n",
                "This skill of loading and preparing external data will be essential as you work with larger datasets for optimization in the upcoming lessons. Properly structured data is the foundation of effective DSPy optimization!\n",
                "\n",
                "```python\n",
                "# data_loader.py\n",
                "import json\n",
                "import dspy\n",
                "\n",
                "def load_qa_data(file_path):\n",
                "    \"\"\"\n",
                "    Load question-answer pairs from a JSON file and convert them to DSPy Examples.\n",
                "    \n",
                "    Args:\n",
                "        file_path (str): Path to the JSON file containing question-answer pairs\n",
                "        \n",
                "    Returns:\n",
                "        list: A list of dspy.Example objects with question and answer fields\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # TODO: Open and read the JSON file\n",
                "        \n",
                "        # TODO: Convert the pairs into DSPy examples\n",
                "        dataset = []\n",
                "        \n",
                "        # TODO: Return the dataset\n",
                "        \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File '{file_path}' not found.\")\n",
                "        return []\n",
                "    except json.JSONDecodeError:\n",
                "        print(f\"Error: File '{file_path}' contains invalid JSON.\")\n",
                "        return []\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading data: {str(e)}\")\n",
                "        return []\n",
                "\n",
                "# solution.py\n",
                "import dspy\n",
                "# TODO: Import the load_qa_data function from data_loader\n",
                "\n",
                "# TODO: Load the dataset from the JSON file\n",
                "\n",
                "# TODO: Print the size to verify the load\n",
                "\n",
                "# TODO: Print some examples from the dataset\n",
                "\n",
                "```\n",
                "\n",
                "To complete the task, you need to first fill in the `load_qa_data` function in the `data_loader.py` file to read the JSON data and convert it into a list of DSPy `Example` objects. Then, in `solution.py`, you'll import and use this function to load the data, printing its size and a few examples to confirm it works correctly.\n",
                "\n",
                "### **`data_loader.py`**\n",
                "\n",
                "Here is the completed `data_loader.py` file with the `load_qa_data` function fully implemented. The code first opens and loads the JSON file, then iterates through each question-answer pair to create a new `dspy.Example` for each pair.\n",
                "\n",
                "```python\n",
                "import json\n",
                "import dspy\n",
                "\n",
                "def load_qa_data(file_path):\n",
                "    \"\"\"\n",
                "    Load question-answer pairs from a JSON file and convert them to DSPy Examples.\n",
                "    \n",
                "    Args:\n",
                "        file_path (str): Path to the JSON file containing question-answer pairs\n",
                "        \n",
                "    Returns:\n",
                "        list: A list of dspy.Example objects with question and answer fields\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Open and read the JSON file\n",
                "        with open(file_path, 'r') as f:\n",
                "            data = json.load(f)\n",
                "        \n",
                "        # Convert the pairs into DSPy examples\n",
                "        dataset = []\n",
                "        for item in data:\n",
                "            question = item.get(\"question\")\n",
                "            answer = item.get(\"answer\")\n",
                "            if question and answer:\n",
                "                dataset.append(dspy.Example(question=question, answer=answer))\n",
                "        \n",
                "        # Return the dataset\n",
                "        return dataset\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File '{file_path}' not found.\")\n",
                "        return []\n",
                "    except json.JSONDecodeError:\n",
                "        print(f\"Error: File '{file_path}' contains invalid JSON.\")\n",
                "        return []\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading data: {str(e)}\")\n",
                "        return []\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### **`solution.py`**\n",
                "\n",
                "The `solution.py` script below demonstrates how to use the completed `load_qa_data` function. It imports the function, specifies a file path (assuming a `qa_data.json` file exists), loads the dataset, and then prints the total number of examples and the content of the first three examples to verify the process was successful.\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "from data_loader import load_qa_data\n",
                "\n",
                "# Load the dataset from the JSON file\n",
                "data_file = 'qa_data.json' # Assuming your JSON file is named 'qa_data.json'\n",
                "dataset = load_qa_data(data_file)\n",
                "\n",
                "# Print the size to verify the load\n",
                "print(f\"Loaded {len(dataset)} examples from {data_file}\")\n",
                "\n",
                "# Print some examples from the dataset\n",
                "if dataset:\n",
                "    print(\"\\nFirst 3 examples:\")\n",
                "    for i in range(min(3, len(dataset))):\n",
                "        example = dataset[i]\n",
                "        print(f\"Example {i+1}:\")\n",
                "        print(f\"  Question: {example.question}\")\n",
                "        print(f\"  Answer: {example.answer}\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementing the DSPy Data Split Ratio\n",
                "\n",
                "Now that we've learned about the importance of data preparation for DSPy optimization, let's practice implementing a dynamic train/validation split! In this exercise, you'll work with a sample dataset of question-answer pairs and apply a customizable split ratio, which is particularly effective for prompt optimization.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Implement a function that calculates the split index based on the dataset size and a given percentage\n",
                "Use this function to split the dataset into training and validation sets\n",
                "Print statistics and some samples about your split to verify it's correct\n",
                "This practical skill will serve as the foundation for all the optimization work we'll do in upcoming lessons. Getting your data split right is the first crucial step toward building more effective language model applications!\n",
                "\n",
                "```python\n",
                "# solution.py\n",
                "import dspy\n",
                "from data_loader import load_qa_data\n",
                "\n",
                "def split_dataset(dataset, train_percentage):\n",
                "    \"\"\"\n",
                "    Split the dataset into training and validation sets based on the given percentage.\n",
                "    \n",
                "    Args:\n",
                "        dataset (list): The dataset to be split\n",
                "        train_percentage (float): The percentage of data to be used for training (e.g., 0.2 for 20%)\n",
                "        \n",
                "    Returns:\n",
                "        tuple: A tuple containing the training set and validation set\n",
                "    \"\"\"\n",
                "    # TODO: Calculate the split index for the given training percentage\n",
                "    # TODO: Split the dataset into trainset and valset\n",
                "    # TODO: Return the trainset and valset\n",
                "\n",
                "# Load the dataset from the JSON file\n",
                "dataset = load_qa_data('qa_data.json')\n",
                "\n",
                "# TODO: Use the function to split the dataset with a desired training/validation ratio\n",
                "\n",
                "# TODO: Print statistics about the split (total size, training size, validation size)\n",
                "\n",
                "# TODO: Print a sample from each set\n",
                "\n",
                "#data_loader.py\n",
                "import json\n",
                "import dspy\n",
                "\n",
                "def load_qa_data(file_path):\n",
                "    \"\"\"\n",
                "    Load question-answer pairs from a JSON file and convert them to DSPy Examples.\n",
                "    \n",
                "    Args:\n",
                "        file_path (str): Path to the JSON file containing question-answer pairs\n",
                "        \n",
                "    Returns:\n",
                "        list: A list of dspy.Example objects with question and answer fields\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Open and read the JSON file\n",
                "        with open(file_path, 'r') as file:\n",
                "            qa_pairs = json.load(file)\n",
                "        \n",
                "        # Convert the pairs into DSPy examples\n",
                "        dataset = []\n",
                "        for item in qa_pairs:\n",
                "            example = dspy.Example(\n",
                "                question=item[\"question\"],\n",
                "                answer=item[\"answer\"]\n",
                "            )\n",
                "            dataset.append(example)\n",
                "        \n",
                "        return dataset\n",
                "    \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File '{file_path}' not found.\")\n",
                "        return []\n",
                "    except json.JSONDecodeError:\n",
                "        print(f\"Error: File '{file_path}' contains invalid JSON.\")\n",
                "        return []\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading data: {str(e)}\")\n",
                "        return []\n",
                "\n",
                "```\n",
                "\n",
                "To split the dataset, you first need to complete the `split_dataset` function in `solution.py` by calculating the split index and using it to divide the dataset. Then, you'll call this function with your desired ratio and print the resulting statistics.\n",
                "\n",
                "### **`solution.py`**\n",
                "\n",
                "Here is the completed `solution.py` code. The `split_dataset` function uses the `train_percentage` to calculate the **split index**, which determines the point at which the dataset is divided into training and validation sets. The `math.ceil` function is used to ensure a whole number for the split index. After the split, the script prints the size of each set and a sample from both to confirm the split was successful.\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "from data_loader import load_qa_data\n",
                "import math\n",
                "\n",
                "def split_dataset(dataset, train_percentage):\n",
                "    \"\"\"\n",
                "    Split the dataset into training and validation sets based on the given percentage.\n",
                "    \n",
                "    Args:\n",
                "        dataset (list): The dataset to be split\n",
                "        train_percentage (float): The percentage of data to be used for training (e.g., 0.2 for 20%)\n",
                "        \n",
                "    Returns:\n",
                "        tuple: A tuple containing the training set and validation set\n",
                "    \"\"\"\n",
                "    # Calculate the split index for the given training percentage\n",
                "    split_index = math.ceil(len(dataset) * train_percentage)\n",
                "    \n",
                "    # Split the dataset into trainset and valset\n",
                "    trainset = dataset[:split_index]\n",
                "    valset = dataset[split_index:]\n",
                "    \n",
                "    # Return the trainset and valset\n",
                "    return trainset, valset\n",
                "\n",
                "# Load the dataset from the JSON file\n",
                "dataset = load_qa_data('qa_data.json')\n",
                "\n",
                "# Use the function to split the dataset with a desired training/validation ratio\n",
                "# Example: 80% for training and 20% for validation\n",
                "train_percentage = 0.8\n",
                "trainset, valset = split_dataset(dataset, train_percentage)\n",
                "\n",
                "# Print statistics about the split\n",
                "print(f\"Total dataset size: {len(dataset)}\")\n",
                "print(f\"Training set size ({train_percentage*100}%): {len(trainset)}\")\n",
                "print(f\"Validation set size ({(1-train_percentage)*100}%): {len(valset)}\")\n",
                "\n",
                "# Print a sample from each set\n",
                "if trainset:\n",
                "    print(\"\\nSample from Training Set:\")\n",
                "    print(trainset[0])\n",
                "\n",
                "if valset:\n",
                "    print(\"\\nSample from Validation Set:\")\n",
                "    print(valset[0])\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### **`data_loader.py`**\n",
                "\n",
                "The provided `data_loader.py` is already functional and does not require any changes. It correctly handles file loading and data conversion to DSPy `Example` objects, which is a necessary prerequisite for the splitting task."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Working with Built-in DSPy Datasets\n",
                "\n",
                "After learning about data preparation for DSPy optimization, let's take it to the next level by working with built-in datasets! In this exercise, you'll use DSPy's built-in GSM8K dataset, which contains math word problems — perfect for practicing our optimization techniques.\n",
                "\n",
                "You've already mastered creating and splitting custom datasets. Now you'll learn how to work with larger, pre-built datasets that come with DSPy, which is a common workflow in real applications.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Load the GSM8K dataset using DSPy's dataset functionality\n",
                "Create helper functions to access different parts of the dataset (train, test, dev)\n",
                "Explore the dataset to better understand what you're working with\n",
                "This exercise builds directly on your previous work with data splitting but now applies those skills to a real-world dataset that's commonly used in LLM research. Understanding how to work with these larger datasets is essential before we dive into specific optimizers in the upcoming lessons!\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "from dspy.datasets import gsm8k\n",
                "\n",
                "# Load the GSM8K dataset\n",
                "dataset = gsm8k.GSM8K()\n",
                "\n",
                "def get_trainset():\n",
                "    \"\"\"\n",
                "    Returns the training set with only 'question' as input.\n",
                "    \"\"\"\n",
                "    # TODO: Return the training set with only 'question' as input using with_inputs()\n",
                "    pass\n",
                "\n",
                "def get_testset():\n",
                "    \"\"\"\n",
                "    Returns the test set with only 'question' as input.\n",
                "    \"\"\"\n",
                "    # TODO: Return the test set with only 'question' as input using with_inputs()\n",
                "    pass\n",
                "\n",
                "def get_devset():\n",
                "    \"\"\"\n",
                "    Returns the development set with only 'question' as input.\n",
                "    \"\"\"\n",
                "    # TODO: Return the development set with only 'question' as input using with_inputs()\n",
                "    pass\n",
                "\n",
                "\n",
                "# Get the training set\n",
                "# TODO: Call get_trainset() and store the result in a variable called trainset\n",
                "\n",
                "# Print dataset statistics\n",
                "# TODO: Print the sizes of the full training set, test set, and dev set\n",
                "\n",
                "# TODO: Display a sample example from the dataset\n",
                "\n",
                "```\n",
                "\n",
                "Your `TypeError: 'GSM8K' object is not subscriptable` error occurs because you're trying to use bracket notation, like `dataset['train']`, on the `GSM8K` object. The `gsm8k.GSM8K()` function does not return a dictionary; it returns a `GSM8K` object that contains the data as attributes.\n",
                "\n",
                "To fix this, you must access the train, test, and dev sets using **dot notation** (`.train`, `.test`, `.dev`) instead of bracket notation (`['train']`). The correct code is to use `dataset.train`, `dataset.test`, and `dataset.dev`.\n",
                "\n",
                "### Corrected Solution\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "from dspy.datasets import gsm8k\n",
                "\n",
                "# Load the GSM8K dataset, which returns a GSM8K object with attributes\n",
                "dataset = gsm8k.GSM8K()\n",
                "\n",
                "def get_trainset():\n",
                "    \"\"\"\n",
                "    Returns the training set with only 'question' as input.\n",
                "    \"\"\"\n",
                "    # Use dot notation to access the .train attribute\n",
                "    return [x.with_inputs('question') for x in dataset.train]\n",
                "\n",
                "def get_testset():\n",
                "    \"\"\"\n",
                "    Returns the test set with only 'question' as input.\n",
                "    \"\"\"\n",
                "    # Use dot notation to access the .test attribute\n",
                "    return [x.with_inputs('question') for x in dataset.test]\n",
                "\n",
                "def get_devset():\n",
                "    \"\"\"\n",
                "    Returns the development set with only 'question' as input.\n",
                "    \"\"\"\n",
                "    # Use dot notation to access the .dev attribute\n",
                "    return [x.with_inputs('question') for x in dataset.dev]\n",
                "\n",
                "\n",
                "# Get the training set\n",
                "trainset = get_trainset()\n",
                "testset = get_testset()\n",
                "devset = get_devset()\n",
                "\n",
                "# Print dataset statistics\n",
                "print(f\"Full training set size: {len(trainset)}\")\n",
                "print(f\"Test set size: {len(testset)}\")\n",
                "print(f\"Development set size: {len(devset)}\")\n",
                "\n",
                "# Display a sample example from the dataset\n",
                "if trainset:\n",
                "    print(\"\\nSample from the training set:\")\n",
                "    print(trainset[0])\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## DSPy Optimization Basics Quiz\n",
                "\n",
                "Here are the answers to your questions about optimization in **DSPy** (Declarative Self-improving Programs):\n",
                "\n",
                "***\n",
                "\n",
                "## 1. What is the primary goal of optimization in DSPy?\n",
                "\n",
                "The primary goal of optimization in DSPy is:\n",
                "* **To automatically tune prompts and model weights for improved performance**\n",
                "\n",
                "> **Explanation:** DSPy's core idea is to *declare* the program logic and let the optimizers automatically learn the best textual prompts (and sometimes model weights) required to maximize the program's defined performance metric.\n",
                "\n",
                "***\n",
                "\n",
                "## 2. Which components are essential in the DSPy optimization process?\n",
                "\n",
                "The essential components in the DSPy optimization process are:\n",
                "* **Programs** (The declarative logic you define)\n",
                "* **Optimizers** (The algorithms that tune the program)\n",
                "* **Metrics** (The functions used to evaluate performance and guide tuning)\n",
                "* **Datasets** (The examples used to train the optimizer and evaluate the program)\n",
                "\n",
                "> **Note:** **Manual prompt adjustments** is what DSPy aims to *eliminate*, not an essential component of its *automatic* optimization process.\n",
                "\n",
                "***\n",
                "\n",
                "## 3. What is the recommended data split for prompt optimizers in DSPy?\n",
                "\n",
                "The recommended data split for prompt optimizers in DSPy is:\n",
                "* **50% training, 50% validation**\n",
                "\n",
                "> **Explanation:** DSPy optimizers like `BootstrapFewShot` and `BayesianSignatureOptimizer` typically use a validation set to evaluate the quality of the prompts/signatures they generate. A 50/50 split is often recommended to give a sufficiently large set of examples for the optimizer to *train* on (i.e., generate few-shot examples or optimize instructions) and a sufficiently large set to *validate* the resulting program's performance.\n",
                "\n",
                "***\n",
                "\n",
                "## 4. Which type of optimizer in DSPy focuses on modifying the weights of the language model?\n",
                "\n",
                "The type of optimizer in DSPy that focuses on modifying the weights of the language model is:\n",
                "* **Automatic Finetuning Optimizers**\n",
                "\n",
                "> **Explanation:** While most DSPy optimizers focus on finding the best *prompts* or *few-shot examples* for a frozen (pre-trained) LLM, the finetuning optimizers (like **`FineTune`**) are designed to actually update the weights of a smaller model to better execute the defined DSPy program.\n",
                "\n",
                "***\n",
                "\n",
                "## 5. What is the role of evaluation metrics in the DSPy optimization process?\n",
                "\n",
                "The role of evaluation metrics in the DSPy optimization process is:\n",
                "* **To provide feedback signals that guide the optimization process**\n",
                "\n",
                "> **Explanation:** Metrics (like accuracy, F1-score, or custom functions) are what the optimizer attempts to **maximize**. The optimizer iteratively tests different prompts/weights and uses the metric's output to determine if the changes were an improvement, thus guiding the search for the optimal program configuration.\n",
                ">\n",
                "> That is an excellent detail to double-check, as DSPy's recommendation for prompt optimizers is a bit unusual compared to standard deep learning practices.\n",
                "\n",
                "The initial answer was based on a common ML split, but the **DSPy documentation** for most prompt optimizers explicitly recommends a **reversed split** to ensure better generalization.\n",
                "\n",
                "The correct answer for Question 3 is:\n",
                "\n",
                "## 3. What is the recommended data split for prompt optimizers in DSPy?\n",
                "\n",
                "* **20% training, 80% validation**\n",
                "\n",
                "### **Why the reverse split?**\n",
                "\n",
                "DSPy's prompt-based optimizers (like `BootstrapFewShot` and `MIPRO`) often use a small training set to *generate* the initial optimized prompt/few-shot examples. They then use a larger **validation** set to rigorously *evaluate and select* the best-performing generated prompt.\n",
                "\n",
                "This reverse allocation (**small training, large validation**) is recommended because prompt-based optimizers can easily **overfit** to a small number of training examples, and a large validation set is crucial for ensuring the resulting optimized program generalizes well."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
