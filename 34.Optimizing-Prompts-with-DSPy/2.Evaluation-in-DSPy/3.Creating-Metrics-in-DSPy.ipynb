{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluation in DSPy: Creating Metrics\n",
                "\n",
                "Welcome to the third lesson of the \"Evaluation in DSPy\" course. This lesson focuses on creating metrics, which are crucial for quantifying the performance of system outputs in DSPy. Metrics provide a basis for improving and optimizing a system's performance.\n",
                "\n",
                "-----\n",
                "\n",
                "## Basic Metric Functions\n",
                "\n",
                "Basic metric functions are fundamental for evaluating system responses.\n",
                "\n",
                "### `validate_answer`\n",
                "\n",
                "The `validate_answer` function checks if a predicted answer is an exact match to the expected answer, ignoring case differences. It returns `True` for a match and `False` otherwise, making it useful for tasks requiring exact answers.\n",
                "\n",
                "```python\n",
                "def validate_answer(example, pred, trace=None):\n",
                "    return example.answer.lower() == pred.answer.lower()\n",
                "```\n",
                "\n",
                "### `answer_exact_match` and `answer_passage_match`\n",
                "\n",
                "DSPy also provides built-in functions like `answer_exact_match` and `answer_passage_match` that offer more flexibility.\n",
                "\n",
                "  * `answer_exact_match`: This function uses a helper function `_answer_match` to check if a prediction matches any of the given answers. It allows for partial matches based on a `frac` parameter.\n",
                "  * `answer_passage_match`: This function evaluates whether a predicted answer is present within a provided passage or context. It uses a helper function `_passage_match` to check if any of the expected answers are found in the predicted response's context.\n",
                "\n",
                "-----\n",
                "\n",
                "## Completeness and Groundedness Evaluation\n",
                "\n",
                "Evaluating a system's response for **completeness** and **groundedness** is vital for understanding its quality. The `CompleteAndGrounded` built-in class provides a structured way to perform this evaluation.\n",
                "\n",
                "The `CompleteAndGrounded` class uses an F1 score to combine two key components: `AnswerCompleteness` and `AnswerGroundedness`.\n",
                "\n",
                "  * **`AnswerCompleteness`**: This component estimates how well the system's response covers the \"ground truth\" or correct answer. It involves enumerating key ideas from both the ground truth and the system's response to discuss their overlap and report a completeness score.\n",
                "  * **`AnswerGroundedness`**: This component assesses the degree to which a system's response is supported by \"retrieved documents\" and common sense reasoning. It involves enumerating claims made in the system's response and discussing how they are supported by the provided context.\n",
                "\n",
                "-----\n",
                "\n",
                "## Semantic Evaluation with F1 Score\n",
                "\n",
                "Semantic evaluation assesses the quality of a response based on its semantic content. The built-in `SemanticF1` class performs this evaluation using recall, precision, and an F1 score.\n",
                "\n",
                "  * **Recall**: Measures the fraction of the ground truth covered by the system's response.\n",
                "  * **Precision**: Measures the fraction of the system's response that is covered by the ground truth.\n",
                "  * **F1 Score**: Combines precision and recall to provide a balanced evaluation metric.\n",
                "\n",
                "The `SemanticF1` class can perform both standard and \"decompositional\" semantic evaluations, offering flexibility in assessing response quality.\n",
                "\n",
                "-----\n",
                "\n",
                "## Practical Example: Evaluating a Tweet\n",
                "\n",
                "You can create **custom metrics** to evaluate specific criteria. As a practical example, a custom metric can be used to evaluate a tweet's quality. The goal is to check if a generated tweet correctly answers a question, is engaging, and adheres to a character limit.\n",
                "\n",
                "This custom metric uses an `Assess` class with a `dspy.Signature` to define an automatic assessment. The `metric` function then evaluates the tweet's correctness, engagement, and length, returning a score that reflects its overall quality.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, you learned how to create and use metrics in DSPy to evaluate system outputs. This includes using basic functions, evaluating for completeness and groundedness, and performing semantic evaluation with F1 scores. These skills are fundamental for assessing the performance of DSPy systems and provide a foundation for more advanced topics."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementing a Case Insensitive Metric\n",
                "\n",
                "Now that you've learned about basic metric functions in DSPy, let's put that knowledge into practice! In this exercise, you'll implement a fundamental metric: a case-insensitive exact match validator.\n",
                "\n",
                "The validate_answer function we discussed earlier is essential for many question-answering tasks. Your job is to complete this function in the provided file. The function should compare the predicted answer with the expected answer while ignoring differences in letter case.\n",
                "\n",
                "To complete this exercise:\n",
                "\n",
                "Implement the comparison logic in the validate_answer function.\n",
                "Ensure it returns True when answers match (ignoring case) and False otherwise.\n",
                "Run the provided test cases to verify your implementation works correctly.\n",
                "This simple metric will serve as a building block for more complex evaluation methods you'll explore later in the course. Mastering these basic metrics is the first step toward creating sophisticated evaluation systems for your DSPy applications.\n",
                "\n",
                "```python\n",
                "from dspy import Example, Prediction\n",
                "\n",
                "\n",
                "def validate_answer(example, pred, trace=None):\n",
                "    \"\"\"\n",
                "    Validates if the predicted answer matches the expected answer, ignoring case.\n",
                "    \n",
                "    Args:\n",
                "        example: The example containing the expected answer\n",
                "        pred: The prediction containing the predicted answer\n",
                "        trace: Optional trace information (not used in this function)\n",
                "        \n",
                "    Returns:\n",
                "        True if the answers match (ignoring case), False otherwise\n",
                "    \"\"\"\n",
                "    # TODO: Implement case-insensitive comparison between example.answer and pred.answer\n",
                "    pass\n",
                "\n",
                "\n",
                "# Test cases with matching answers (ignoring case)\n",
                "matching_cases = [\n",
                "    (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "     Prediction(answer=\"paris\")),\n",
                "    \n",
                "    (Example(question=\"Who wrote Romeo and Juliet?\", answer=\"william shakespeare\"), \n",
                "     Prediction(answer=\"William Shakespeare\")),\n",
                "    \n",
                "    (Example(question=\"What is the chemical symbol for gold?\", answer=\"Au\"), \n",
                "     Prediction(answer=\"au\")),\n",
                "]\n",
                "\n",
                "# Test cases with non-matching answers\n",
                "non_matching_cases = [\n",
                "    (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "     Prediction(answer=\"London\")),\n",
                "    \n",
                "    (Example(question=\"Who wrote Romeo and Juliet?\", answer=\"William Shakespeare\"), \n",
                "     Prediction(answer=\"Charles Dickens\")),\n",
                "    \n",
                "    (Example(question=\"What is the chemical symbol for gold?\", answer=\"Au\"), \n",
                "     Prediction(answer=\"Ag\")),\n",
                "]\n",
                "\n",
                "\n",
                "def run_tests():\n",
                "    print(\"Testing matching cases (should all be True):\")\n",
                "    matching_results = []\n",
                "    for i, (example, pred) in enumerate(matching_cases):\n",
                "        result = validate_answer(example, pred)\n",
                "        matching_results.append(result)\n",
                "        print(f\"  Test {i+1}: Expected '{example.answer}', Got '{pred.answer}', Match: {result}\")\n",
                "    \n",
                "    print(\"\\nTesting non-matching cases (should all be False):\")\n",
                "    non_matching_results = []\n",
                "    for i, (example, pred) in enumerate(non_matching_cases):\n",
                "        result = validate_answer(example, pred)\n",
                "        non_matching_results.append(result)\n",
                "        print(f\"  Test {i+1}: Expected '{example.answer}', Got '{pred.answer}', Match: {result}\")\n",
                "    \n",
                "    # Summary\n",
                "    matching_success = all(matching_results)\n",
                "    non_matching_success = not any(non_matching_results)\n",
                "    overall_success = matching_success and non_matching_success\n",
                "    \n",
                "    print(\"\\nSummary:\")\n",
                "    print(f\"  Matching cases: {'All passed' if matching_success else 'Some failed'}\")\n",
                "    print(f\"  Non-matching cases: {'All passed' if non_matching_success else 'Some failed'}\")\n",
                "    print(f\"  Overall: {'All tests passed!' if overall_success else 'Some tests failed.'}\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    run_tests()\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from dspy import Example, Prediction\n",
                "\n",
                "\n",
                "def validate_answer(example, pred, trace=None):\n",
                "    \"\"\"\n",
                "    Validates if the predicted answer matches the expected answer, ignoring case.\n",
                "    \n",
                "    Args:\n",
                "        example: The example containing the expected answer\n",
                "        pred: The prediction containing the predicted answer\n",
                "        trace: Optional trace information (not used in this function)\n",
                "        \n",
                "    Returns:\n",
                "        True if the answers match (ignoring case), False otherwise\n",
                "    \"\"\"\n",
                "    # Implement case-insensitive comparison between example.answer and pred.answer\n",
                "    return example.answer.lower() == pred.answer.lower()\n",
                "\n",
                "\n",
                "# Test cases with matching answers (ignoring case)\n",
                "matching_cases = [\n",
                "    (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "     Prediction(answer=\"paris\")),\n",
                "    \n",
                "    (Example(question=\"Who wrote Romeo and Juliet?\", answer=\"william shakespeare\"), \n",
                "     Prediction(answer=\"William Shakespeare\")),\n",
                "    \n",
                "    (Example(question=\"What is the chemical symbol for gold?\", answer=\"Au\"), \n",
                "     Prediction(answer=\"au\")),\n",
                "]\n",
                "\n",
                "# Test cases with non-matching answers\n",
                "non_matching_cases = [\n",
                "    (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "     Prediction(answer=\"London\")),\n",
                "    \n",
                "    (Example(question=\"Who wrote Romeo and Juliet?\", answer=\"William Shakespeare\"), \n",
                "     Prediction(answer=\"Charles Dickens\")),\n",
                "    \n",
                "    (Example(question=\"What is the chemical symbol for gold?\", answer=\"Au\"), \n",
                "     Prediction(answer=\"Ag\")),\n",
                "]\n",
                "\n",
                "\n",
                "def run_tests():\n",
                "    print(\"Testing matching cases (should all be True):\")\n",
                "    matching_results = []\n",
                "    for i, (example, pred) in enumerate(matching_cases):\n",
                "        result = validate_answer(example, pred)\n",
                "        matching_results.append(result)\n",
                "        print(f\"  Test {i+1}: Expected '{example.answer}', Got '{pred.answer}', Match: {result}\")\n",
                "    \n",
                "    print(\"\\nTesting non-matching cases (should all be False):\")\n",
                "    non_matching_results = []\n",
                "    for i, (example, pred) in enumerate(non_matching_cases):\n",
                "        result = validate_answer(example, pred)\n",
                "        non_matching_results.append(result)\n",
                "        print(f\"  Test {i+1}: Expected '{example.answer}', Got '{pred.answer}', Match: {result}\")\n",
                "    \n",
                "    # Summary\n",
                "    matching_success = all(matching_results)\n",
                "    non_matching_success = not any(non_matching_results)\n",
                "    overall_success = matching_success and non_matching_success\n",
                "    \n",
                "    print(\"\\nSummary:\")\n",
                "    print(f\"  Matching cases: {'All passed' if matching_success else 'Some failed'}\")\n",
                "    print(f\"  Non-matching cases: {'All passed' if non_matching_success else 'Some failed'}\")\n",
                "    print(f\"  Overall: {'All tests passed!' if overall_success else 'Some tests failed.'}\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    run_tests()\n",
                "\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Flexible Answer Matching for Multiple Formats\n",
                "\n",
                "Now that you've learned about basic metric functions and seen how validate_answer works, let's dive deeper into DSPy's evaluation capabilities! In this exercise, you'll enhance the answer_exact_match function to handle different types of answers.\n",
                "\n",
                "The function needs to properly process both single string answers and lists of acceptable answers. Your task is to complete the implementation that correctly delegates to the _answer_match helper function in both cases.\n",
                "\n",
                "To complete this exercise:\n",
                "\n",
                "Implement the logic to handle when example.answer is a string.\n",
                "Implement the logic to handle when example.answer is a list.\n",
                "Ensure the frac parameter is passed correctly to allow for partial matching.\n",
                "This enhanced metric will be valuable for real-world applications where multiple correct answers might exist. Building flexible evaluation metrics is a key skill for developing robust DSPy applications that can handle diverse response formats.\n",
                "\n",
                "```python\n",
                "from dspy import Example, Prediction\n",
                "from utils import EM, F1\n",
                "\n",
                "\n",
                "def _answer_match(prediction, answers, frac=1.0):\n",
                "    \"\"\"Returns True if the prediction matches any of the answers.\"\"\"\n",
                "    if frac >= 1.0:\n",
                "        return EM(prediction, answers)\n",
                "\n",
                "    return F1(prediction, answers) >= frac\n",
                "\n",
                "\n",
                "def answer_exact_match(example, pred, trace=None, frac=1.0):\n",
                "    \"\"\"\n",
                "    Checks if the predicted answer matches any of the expected answers.\n",
                "    \n",
                "    Args:\n",
                "        example: The example containing the expected answer(s)\n",
                "        pred: The prediction containing the predicted answer\n",
                "        trace: Optional trace information (not used in this function)\n",
                "        frac: The fraction threshold for partial matching (default: 1.0 for exact match)\n",
                "        \n",
                "    Returns:\n",
                "        True if the prediction matches any of the expected answers, False otherwise\n",
                "        \n",
                "    Raises:\n",
                "        ValueError: If the answer type is neither a string nor a list\n",
                "    \"\"\"\n",
                "    # TODO: Handle the case when example.answer is a string\n",
                "    \n",
                "    # TODO: Handle the case when example.answer is a list\n",
                "    \n",
                "    raise ValueError(f\"Invalid answer type: {type(example.answer)}\")\n",
                "\n",
                "\n",
                "def run_tests():\n",
                "    print(\"Testing answer_exact_match function...\")\n",
                "    \n",
                "    # Test case 1: String answer, exact match\n",
                "    example = Example(answer=\"Paris\")\n",
                "    pred = Prediction(answer=\"Paris\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 1 (String, Exact Match): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 2: String answer, case-insensitive match\n",
                "    example = Example(answer=\"Paris\")\n",
                "    pred = Prediction(answer=\"paris\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 2 (String, Case-Insensitive): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 3: String answer, no match\n",
                "    example = Example(answer=\"Paris\")\n",
                "    pred = Prediction(answer=\"London\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 3 (String, No Match): {'✓ Passed' if not result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 4: List answer, exact match with first item\n",
                "    example = Example(answer=[\"Paris\", \"City of Light\"])\n",
                "    pred = Prediction(answer=\"Paris\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 4 (List, Match First): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 5: List answer, exact match with second item\n",
                "    example = Example(answer=[\"City of Light\", \"Paris\"])\n",
                "    pred = Prediction(answer=\"Paris\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 5 (List, Match Second): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 6: List answer, no match\n",
                "    example = Example(answer=[\"Paris\", \"City of Light\"])\n",
                "    pred = Prediction(answer=\"London\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 6 (List, No Match): {'✓ Passed' if not result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 7: String answer, partial match with frac=0.5\n",
                "    example = Example(answer=\"The capital of France is Paris\")\n",
                "    pred = Prediction(answer=\"Paris is the capital\")\n",
                "    result = answer_exact_match(example, pred, frac=0.5)\n",
                "    print(f\"Test 7 (String, Partial Match): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 8: List answer, partial match with frac=0.5\n",
                "    example = Example(answer=[\"The capital of France is Paris\", \"Paris is in France\"])\n",
                "    pred = Prediction(answer=\"Paris is the capital\")\n",
                "    result = answer_exact_match(example, pred, frac=0.5)\n",
                "    print(f\"Test 8 (List, Partial Match): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 9: Invalid answer type\n",
                "    try:\n",
                "        example = Example(answer=123)  # Integer is not a valid type\n",
                "        pred = Prediction(answer=\"Paris\")\n",
                "        answer_exact_match(example, pred)\n",
                "        print(\"Test 9 (Invalid Type): ✗ Failed - No exception raised\")\n",
                "    except ValueError:\n",
                "        print(\"Test 9 (Invalid Type): ✓ Passed - ValueError raised\")\n",
                "    except Exception as e:\n",
                "        print(f\"Test 9 (Invalid Type): ✗ Failed - Wrong exception: {type(e).__name__}\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    run_tests()\n",
                "```\n",
                "\n",
                "```python\n",
                "from dspy import Example, Prediction\n",
                "from utils import EM, F1\n",
                "\n",
                "\n",
                "def _answer_match(prediction, answers, frac=1.0):\n",
                "    \"\"\"Returns True if the prediction matches any of the answers.\"\"\"\n",
                "    if frac >= 1.0:\n",
                "        return EM(prediction, answers)\n",
                "\n",
                "    return F1(prediction, answers) >= frac\n",
                "\n",
                "\n",
                "def answer_exact_match(example, pred, trace=None, frac=1.0):\n",
                "    \"\"\"\n",
                "    Checks if the predicted answer matches any of the expected answers.\n",
                "    \n",
                "    Args:\n",
                "        example: The example containing the expected answer(s)\n",
                "        pred: The prediction containing the predicted answer\n",
                "        trace: Optional trace information (not used in this function)\n",
                "        frac: The fraction threshold for partial matching (default: 1.0 for exact match)\n",
                "        \n",
                "    Returns:\n",
                "        True if the prediction matches any of the expected answers, False otherwise\n",
                "        \n",
                "    Raises:\n",
                "        ValueError: If the answer type is neither a string nor a list\n",
                "    \"\"\"\n",
                "    if isinstance(example.answer, str):\n",
                "        answers = [example.answer]\n",
                "        return _answer_match(pred.answer, answers, frac)\n",
                "    \n",
                "    if isinstance(example.answer, list):\n",
                "        return _answer_match(pred.answer, example.answer, frac)\n",
                "    \n",
                "    raise ValueError(f\"Invalid answer type: {type(example.answer)}\")\n",
                "\n",
                "\n",
                "def run_tests():\n",
                "    print(\"Testing answer_exact_match function...\")\n",
                "    \n",
                "    # Test case 1: String answer, exact match\n",
                "    example = Example(answer=\"Paris\")\n",
                "    pred = Prediction(answer=\"Paris\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 1 (String, Exact Match): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 2: String answer, case-insensitive match\n",
                "    example = Example(answer=\"Paris\")\n",
                "    pred = Prediction(answer=\"paris\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 2 (String, Case-Insensitive): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 3: String answer, no match\n",
                "    example = Example(answer=\"Paris\")\n",
                "    pred = Prediction(answer=\"London\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 3 (String, No Match): {'✓ Passed' if not result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 4: List answer, exact match with first item\n",
                "    example = Example(answer=[\"Paris\", \"City of Light\"])\n",
                "    pred = Prediction(answer=\"Paris\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 4 (List, Match First): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 5: List answer, exact match with second item\n",
                "    example = Example(answer=[\"City of Light\", \"Paris\"])\n",
                "    pred = Prediction(answer=\"Paris\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 5 (List, Match Second): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 6: List answer, no match\n",
                "    example = Example(answer=[\"Paris\", \"City of Light\"])\n",
                "    pred = Prediction(answer=\"London\")\n",
                "    result = answer_exact_match(example, pred)\n",
                "    print(f\"Test 6 (List, No Match): {'✓ Passed' if not result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 7: String answer, partial match with frac=0.5\n",
                "    example = Example(answer=\"The capital of France is Paris\")\n",
                "    pred = Prediction(answer=\"Paris is the capital\")\n",
                "    result = answer_exact_match(example, pred, frac=0.5)\n",
                "    print(f\"Test 7 (String, Partial Match): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 8: List answer, partial match with frac=0.5\n",
                "    example = Example(answer=[\"The capital of France is Paris\", \"Paris is in France\"])\n",
                "    pred = Prediction(answer=\"Paris is the capital\")\n",
                "    result = answer_exact_match(example, pred, frac=0.5)\n",
                "    print(f\"Test 8 (List, Partial Match): {'✓ Passed' if result else '✗ Failed'}\")\n",
                "    \n",
                "    # Test case 9: Invalid answer type\n",
                "    try:\n",
                "        example = Example(answer=123)  # Integer is not a valid type\n",
                "        pred = Prediction(answer=\"Paris\")\n",
                "        answer_exact_match(example, pred)\n",
                "        print(\"Test 9 (Invalid Type): ✗ Failed - No exception raised\")\n",
                "    except ValueError:\n",
                "        print(\"Test 9 (Invalid Type): ✓ Passed - ValueError raised\")\n",
                "    except Exception as e:\n",
                "        print(f\"Test 9 (Invalid Type): ✗ Failed - Wrong exception: {type(e).__name__}\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    run_tests()\n",
                "```\n",
                "\n",
                "### Explanation of the Solution\n",
                "\n",
                "The solution correctly handles both string and list inputs for the `example.answer` by using `isinstance` to check the data type.\n",
                "\n",
                "  * When `example.answer` is a **string**, the code wraps it in a single-element list (`[example.answer]`). This is crucial because the `_answer_match` helper function is designed to work with a list of acceptable answers. This approach makes the code cleaner and avoids duplicating logic.\n",
                "  * When `example.answer` is a **list**, the code directly passes this list to the `_answer_match` helper function.\n",
                "  * In both cases, the `pred.answer` and the `frac` parameter are passed correctly to the `_answer_match` function, ensuring that both exact (`frac=1.0`) and partial (`frac < 1.0`) matching are handled correctly.\n",
                "\n",
                "Finally, if the `example.answer` is neither a string nor a list, the function raises a `ValueError`, as specified in the instructions. This robust type-checking ensures the function behaves predictably and handles unexpected inputs gracefully. The provided test cases validate that the completed function works as expected for all scenarios, including exact matches, partial matches, and incorrect input types."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Passage Matching for Retrieval Systems\n",
                "\n",
                "Excellent work on implementing the flexible answer-matching function! Now let's build on your understanding of DSPy metrics by implementing the answer_passage_match function, which is crucial for retrieval-based question-answering systems.\n",
                "\n",
                "This function checks whether any passage in the predicted context contains the expected answer — a key metric for evaluating if retrieved information is actually useful for answering questions.\n",
                "\n",
                "Your task is to complete the answer_passage_match function that handles both single-string answers and lists of acceptable answers. The helper function _passage_match is already implemented for you, so you'll need to:\n",
                "\n",
                "Add code to handle when example.answer is a string\n",
                "Add code to handle when example.answer is a list of strings\n",
                "Properly call the _passage_match helper with the right parameters\n",
                "This metric is particularly valuable when working with retrieval-augmented generation systems, as it helps verify that your system is finding relevant information before generating answers. Mastering this function will give you a powerful tool for evaluating and improving information retrieval components in your DSPy applications.\n",
                "\n",
                "```python\n",
                "from dspy import Example, Prediction\n",
                "\n",
                "\n",
                "def _passage_match(passages: list[str], answers: list[str]) -> bool:\n",
                "    \"\"\"Returns True if any of the passages contains the answer.\"\"\"\n",
                "    from utils import DPR_normalize, has_answer, normalize_text\n",
                "\n",
                "    def passage_has_answers(passage: str, answers: list[str]) -> bool:\n",
                "        \"\"\"Returns True if the passage contains the answer.\"\"\"\n",
                "        return has_answer(\n",
                "            tokenized_answers=[DPR_normalize(normalize_text(ans)) for ans in answers],\n",
                "            text=normalize_text(passage),\n",
                "        )\n",
                "\n",
                "    return any(passage_has_answers(psg, answers) for psg in passages)\n",
                "\n",
                "\n",
                "def answer_passage_match(example, pred, trace=None):\n",
                "    \"\"\"\n",
                "    Checks if any passage in the predicted context contains the expected answer.\n",
                "    \n",
                "    Args:\n",
                "        example: The example containing the expected answer(s)\n",
                "        pred: The prediction containing the context passages\n",
                "        trace: Optional trace information (not used in this function)\n",
                "        \n",
                "    Returns:\n",
                "        True if any passage contains any acceptable answer, False otherwise\n",
                "    \"\"\"\n",
                "    # TODO: Implement handling for when example.answer is a string\n",
                "    # Hint: Call _passage_match with pred.context and a list containing the single answer\n",
                "    \n",
                "    # TODO: Implement handling for when example.answer is a list\n",
                "    # Hint: Call _passage_match with pred.context and the list of answers\n",
                "    \n",
                "    raise ValueError(f\"Invalid answer type: {type(example.answer)}\")\n",
                "\n",
                "\n",
                "def run_tests():\n",
                "    # Test cases for string answers\n",
                "    string_cases = [\n",
                "        # Answer present in first passage\n",
                "        (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "         Prediction(context=[\"Paris is the capital of France.\", \"France is in Europe.\"]), \n",
                "         True),\n",
                "        \n",
                "        # Answer present in second passage\n",
                "        (Example(question=\"Who wrote Romeo and Juliet?\", answer=\"Shakespeare\"), \n",
                "         Prediction(context=[\"Romeo and Juliet is a famous play.\", \"It was written by Shakespeare in the 16th century.\"]), \n",
                "         True),\n",
                "        \n",
                "        # Answer not present in any passage\n",
                "        (Example(question=\"What is the capital of Japan?\", answer=\"Tokyo\"), \n",
                "         Prediction(context=[\"Japan is an island nation.\", \"It has a rich cultural history.\"]), \n",
                "         False),\n",
                "        \n",
                "        # Case insensitive matching\n",
                "        (Example(question=\"What element has symbol Au?\", answer=\"Gold\"), \n",
                "         Prediction(context=[\"The symbol Au represents gold on the periodic table.\"]), \n",
                "         True),\n",
                "    ]\n",
                "    \n",
                "    # Test cases for list answers\n",
                "    list_cases = [\n",
                "        # One of the answers present\n",
                "        (Example(question=\"Name a primary color\", answer=[\"Red\", \"Blue\", \"Yellow\"]), \n",
                "         Prediction(context=[\"The rainbow contains many colors.\", \"Red is considered a primary color.\"]), \n",
                "         True),\n",
                "        \n",
                "        # Multiple answers present\n",
                "        (Example(question=\"Name a fruit\", answer=[\"Apple\", \"Banana\", \"Orange\"]), \n",
                "         Prediction(context=[\"Apples and oranges are popular fruits.\", \"Bananas are rich in potassium.\"]), \n",
                "         True),\n",
                "        \n",
                "        # No answers present\n",
                "        (Example(question=\"Name a planet\", answer=[\"Mercury\", \"Venus\", \"Mars\"]), \n",
                "         Prediction(context=[\"The Earth is the third planet from the Sun.\", \"It is the only known planet with life.\"]), \n",
                "         False),\n",
                "    ]\n",
                "    \n",
                "    # Edge cases\n",
                "    edge_cases = [\n",
                "        # Empty answer\n",
                "        (Example(question=\"Empty answer\", answer=\"\"), \n",
                "         Prediction(context=[\"This is a test passage.\"]), \n",
                "         True),  # Empty string is considered to be in any text\n",
                "        \n",
                "        # Empty passage\n",
                "        (Example(question=\"Empty passage\", answer=\"Something\"), \n",
                "         Prediction(context=[\"\"]), \n",
                "         False),\n",
                "        \n",
                "        # Empty list of answers\n",
                "        (Example(question=\"Empty list\", answer=[]), \n",
                "         Prediction(context=[\"This is a test passage.\"]), \n",
                "         False),\n",
                "        \n",
                "        # Empty list of passages\n",
                "        (Example(question=\"Empty passages\", answer=\"Something\"), \n",
                "         Prediction(context=[]), \n",
                "         False),\n",
                "    ]\n",
                "    \n",
                "    # Run all test cases\n",
                "    all_test_cases = {\n",
                "        \"String Answer Cases\": string_cases,\n",
                "        \"List Answer Cases\": list_cases,\n",
                "        \"Edge Cases\": edge_cases,\n",
                "    }\n",
                "    \n",
                "    all_passed = True\n",
                "    \n",
                "    for category, test_cases in all_test_cases.items():\n",
                "        print(f\"\\nTesting {category}:\")\n",
                "        category_passed = True\n",
                "        \n",
                "        for i, (example, pred, expected) in enumerate(test_cases):\n",
                "            try:\n",
                "                result = answer_passage_match(example, pred)\n",
                "                passed = result == expected\n",
                "                category_passed = category_passed and passed\n",
                "                \n",
                "                status = \"✓\" if passed else \"✗\"\n",
                "                print(f\"  Test {i+1}: {status} Expected: {expected}, Got: {result}\")\n",
                "                if not passed:\n",
                "                    if isinstance(example.answer, list):\n",
                "                        answer_display = f\"[{', '.join(example.answer)}]\"\n",
                "                    else:\n",
                "                        answer_display = example.answer\n",
                "                    print(f\"    Example answer: {answer_display}\")\n",
                "                    print(f\"    Prediction context: {pred.context}\")\n",
                "            except Exception as e:\n",
                "                category_passed = False\n",
                "                print(f\"  Test {i+1}: ✗ Error: {str(e)}\")\n",
                "        \n",
                "        category_status = \"PASSED\" if category_passed else \"FAILED\"\n",
                "        print(f\"{category}: {category_status}\")\n",
                "        all_passed = all_passed and category_passed\n",
                "    \n",
                "    print(\"\\nOverall Result:\", \"ALL TESTS PASSED!\" if all_passed else \"SOME TESTS FAILED\")\n",
                "    return all_passed\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    run_tests()\n",
                "```\n",
                "\n",
                "```python\n",
                "from dspy import Example, Prediction\n",
                "\n",
                "\n",
                "def _passage_match(passages: list[str], answers: list[str]) -> bool:\n",
                "    \"\"\"Returns True if any of the passages contains the answer.\"\"\"\n",
                "    from utils import DPR_normalize, has_answer, normalize_text\n",
                "\n",
                "    def passage_has_answers(passage: str, answers: list[str]) -> bool:\n",
                "        \"\"\"Returns True if the passage contains the answer.\"\"\"\n",
                "        return has_answer(\n",
                "            tokenized_answers=[DPR_normalize(normalize_text(ans)) for ans in answers],\n",
                "            text=normalize_text(passage),\n",
                "        )\n",
                "\n",
                "    return any(passage_has_answers(psg, answers) for psg in passages)\n",
                "\n",
                "\n",
                "def answer_passage_match(example, pred, trace=None):\n",
                "    \"\"\"\n",
                "    Checks if any passage in the predicted context contains the expected answer.\n",
                "    \n",
                "    Args:\n",
                "        example: The example containing the expected answer(s)\n",
                "        pred: The prediction containing the context passages\n",
                "        trace: Optional trace information (not used in this function)\n",
                "        \n",
                "    Returns:\n",
                "        True if any passage contains any acceptable answer, False otherwise\n",
                "    \"\"\"\n",
                "    if isinstance(example.answer, str):\n",
                "        answers = [example.answer]\n",
                "        return _passage_match(pred.context, answers)\n",
                "    \n",
                "    if isinstance(example.answer, list):\n",
                "        return _passage_match(pred.context, example.answer)\n",
                "    \n",
                "    raise ValueError(f\"Invalid answer type: {type(example.answer)}\")\n",
                "\n",
                "\n",
                "def run_tests():\n",
                "    # Test cases for string answers\n",
                "    string_cases = [\n",
                "        # Answer present in first passage\n",
                "        (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "         Prediction(context=[\"Paris is the capital of France.\", \"France is in Europe.\"]), \n",
                "         True),\n",
                "        \n",
                "        # Answer present in second passage\n",
                "        (Example(question=\"Who wrote Romeo and Juliet?\", answer=\"Shakespeare\"), \n",
                "         Prediction(context=[\"Romeo and Juliet is a famous play.\", \"It was written by Shakespeare in the 16th century.\"]), \n",
                "         True),\n",
                "        \n",
                "        # Answer not present in any passage\n",
                "        (Example(question=\"What is the capital of Japan?\", answer=\"Tokyo\"), \n",
                "         Prediction(context=[\"Japan is an island nation.\", \"It has a rich cultural history.\"]), \n",
                "         False),\n",
                "        \n",
                "        # Case insensitive matching\n",
                "        (Example(question=\"What element has symbol Au?\", answer=\"Gold\"), \n",
                "         Prediction(context=[\"The symbol Au represents gold on the periodic table.\"]), \n",
                "         True),\n",
                "    ]\n",
                "    \n",
                "    # Test cases for list answers\n",
                "    list_cases = [\n",
                "        # One of the answers present\n",
                "        (Example(question=\"Name a primary color\", answer=[\"Red\", \"Blue\", \"Yellow\"]), \n",
                "         Prediction(context=[\"The rainbow contains many colors.\", \"Red is considered a primary color.\"]), \n",
                "         True),\n",
                "        \n",
                "        # Multiple answers present\n",
                "        (Example(question=\"Name a fruit\", answer=[\"Apple\", \"Banana\", \"Orange\"]), \n",
                "         Prediction(context=[\"Apples and oranges are popular fruits.\", \"Bananas are rich in potassium.\"]), \n",
                "         True),\n",
                "        \n",
                "        # No answers present\n",
                "        (Example(question=\"Name a planet\", answer=[\"Mercury\", \"Venus\", \"Mars\"]), \n",
                "         Prediction(context=[\"The Earth is the third planet from the Sun.\", \"It is the only known planet with life.\"]), \n",
                "         False),\n",
                "    ]\n",
                "    \n",
                "    # Edge cases\n",
                "    edge_cases = [\n",
                "        # Empty answer\n",
                "        (Example(question=\"Empty answer\", answer=\"\"), \n",
                "         Prediction(context=[\"This is a test passage.\"]), \n",
                "         True),  # Empty string is considered to be in any text\n",
                "        \n",
                "        # Empty passage\n",
                "        (Example(question=\"Empty passage\", answer=\"Something\"), \n",
                "         Prediction(context=[\"\"]), \n",
                "         False),\n",
                "        \n",
                "        # Empty list of answers\n",
                "        (Example(question=\"Empty list\", answer=[]), \n",
                "         Prediction(context=[\"This is a test passage.\"]), \n",
                "         False),\n",
                "        \n",
                "        # Empty list of passages\n",
                "        (Example(question=\"Empty passages\", answer=\"Something\"), \n",
                "         Prediction(context=[]), \n",
                "         False),\n",
                "    ]\n",
                "    \n",
                "    # Run all test cases\n",
                "    all_test_cases = {\n",
                "        \"String Answer Cases\": string_cases,\n",
                "        \"List Answer Cases\": list_cases,\n",
                "        \"Edge Cases\": edge_cases,\n",
                "    }\n",
                "    \n",
                "    all_passed = True\n",
                "    \n",
                "    for category, test_cases in all_test_cases.items():\n",
                "        print(f\"\\nTesting {category}:\")\n",
                "        category_passed = True\n",
                "        \n",
                "        for i, (example, pred, expected) in enumerate(test_cases):\n",
                "            try:\n",
                "                result = answer_passage_match(example, pred)\n",
                "                passed = result == expected\n",
                "                category_passed = category_passed and passed\n",
                "                \n",
                "                status = \"✓\" if passed else \"✗\"\n",
                "                print(f\"  Test {i+1}: {status} Expected: {expected}, Got: {result}\")\n",
                "                if not passed:\n",
                "                    if isinstance(example.answer, list):\n",
                "                        answer_display = f\"[{', '.join(example.answer)}]\"\n",
                "                    else:\n",
                "                        answer_display = example.answer\n",
                "                    print(f\"    Example answer: {answer_display}\")\n",
                "                    print(f\"    Prediction context: {pred.context}\")\n",
                "            except Exception as e:\n",
                "                category_passed = False\n",
                "                print(f\"  Test {i+1}: ✗ Error: {str(e)}\")\n",
                "        \n",
                "        category_status = \"PASSED\" if category_passed else \"FAILED\"\n",
                "        print(f\"{category}: {category_status}\")\n",
                "        all_passed = all_passed and category_passed\n",
                "    \n",
                "    print(\"\\nOverall Result:\", \"ALL TESTS PASSED!\" if all_passed else \"SOME TESTS FAILED\")\n",
                "    return all_passed\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    run_tests()\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building a Holistic QA Evaluation Metric\n",
                "\n",
                "You've done a fantastic job implementing both the basic validate_answer function and the more advanced answer_exact_match and answer_passage_match functions! Now, let's take your skills to the next level by combining these metrics into a more powerful evaluation tool.\n",
                "\n",
                "In real-world question-answering systems, we often care about two things: whether the system produced the correct answer and whether that answer is supported by the retrieved context. This is where a composite metric becomes valuable.\n",
                "\n",
                "Your task is to implement the composite_metric function that:\n",
                "\n",
                "Checks if the predicted answer matches the expected answer using answer_exact_match\n",
                "Verifies if the expected answer appears in the context using answer_passage_match\n",
                "Combines these results into a single score between 0 and 1\n",
                "The function should handle both string and list answers and properly weigh the importance of exact matches versus passage matches.\n",
                "\n",
                "This composite metric will help you evaluate question-answering systems more holistically, ensuring they not only give correct answers but also find relevant supporting information. This is a key skill for building trustworthy AI systems that can explain their reasoning.\n",
                "\n",
                "```python\n",
                "from dspy import Example, Prediction\n",
                "from dspy.evaluate import answer_exact_match, answer_passage_match\n",
                "from utils import f1_score\n",
                "\n",
                "\n",
                "def composite_metric(example, pred, trace=None, exact_weight=0.6, passage_weight=0.4):\n",
                "    \"\"\"\n",
                "    A composite metric that combines answer_exact_match and answer_passage_match.\n",
                "    \n",
                "    Args:\n",
                "        example: The example containing the expected answer(s)\n",
                "        pred: The prediction containing the predicted answer and context\n",
                "        trace: Optional trace information (not used in this function)\n",
                "        exact_weight: Weight for the exact match component (default: 0.6)\n",
                "        passage_weight: Weight for the passage match component (default: 0.4)\n",
                "        \n",
                "    Returns:\n",
                "        A score between 0 and 1, with higher values indicating better performance\n",
                "        \n",
                "    Raises:\n",
                "        ValueError: If the answer type is neither a string nor a list\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    \n",
                "\n",
                "\n",
                "def run_tests():\n",
                "    print(\"Testing composite_metric function...\")\n",
                "    \n",
                "    # Test cases where both conditions are met\n",
                "    both_true_cases = [\n",
                "        (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "         Prediction(answer=\"Paris\", context=[\"Paris is the capital of France.\"]), \n",
                "         1.0),\n",
                "        \n",
                "        (Example(question=\"Who wrote Romeo and Juliet?\", answer=[\"William Shakespeare\", \"Shakespeare\"]), \n",
                "         Prediction(answer=\"Shakespeare\", context=[\"Romeo and Juliet was written by Shakespeare.\"]), \n",
                "         1.0),\n",
                "    ]\n",
                "    \n",
                "    # Test cases where only exact match is true\n",
                "    only_exact_cases = [\n",
                "        (Example(question=\"What is the capital of Japan?\", answer=\"Tokyo\"), \n",
                "         Prediction(answer=\"Tokyo\", context=[\"Japan is an island nation in East Asia.\"]), \n",
                "         0.6),  # exact_weight\n",
                "        \n",
                "        (Example(question=\"What is the chemical symbol for gold?\", answer=\"Au\"), \n",
                "         Prediction(answer=\"Au\", context=[\"Silver's chemical symbol is Ag.\"]), \n",
                "         0.6),  # exact_weight\n",
                "    ]\n",
                "    \n",
                "    # Test cases where only passage match is true\n",
                "    only_passage_cases = [\n",
                "        (Example(question=\"What is the capital of Italy?\", answer=\"Rome\"), \n",
                "         Prediction(answer=\"Milan\", context=[\"Rome is the capital of Italy.\"]), \n",
                "         0.4),  # passage_weight\n",
                "        \n",
                "        (Example(question=\"Who discovered penicillin?\", answer=\"Alexander Fleming\"), \n",
                "         Prediction(answer=\"Louis Pasteur\", context=[\"Alexander Fleming discovered penicillin in 1928.\"]), \n",
                "         0.4),  # passage_weight\n",
                "    ]\n",
                "    \n",
                "    # Test cases where neither condition is met\n",
                "    neither_true_cases = [\n",
                "        (Example(question=\"What is the capital of Germany?\", answer=\"Berlin\"), \n",
                "         Prediction(answer=\"Munich\", context=[\"Germany is a country in Europe.\"]), \n",
                "         0.0),\n",
                "        \n",
                "        (Example(question=\"Who painted the Mona Lisa?\", answer=\"Leonardo da Vinci\"), \n",
                "         Prediction(answer=\"Michelangelo\", context=[\"The Sistine Chapel was painted by Michelangelo.\"]), \n",
                "         0.0),\n",
                "    ]\n",
                "    \n",
                "    # Edge cases\n",
                "    edge_cases = [\n",
                "        (Example(question=\"Empty answer\", answer=\"\"), \n",
                "         Prediction(answer=\"\", context=[\"This is a test passage.\"]), \n",
                "         1.0),  # Both match\n",
                "        \n",
                "        (Example(question=\"Empty passage\", answer=\"Something\"), \n",
                "         Prediction(answer=\"Something\", context=[\"\"]), \n",
                "         0.6),  # Only exact match\n",
                "        \n",
                "        (Example(question=\"Empty list\", answer=\"Empty list\"), \n",
                "         Prediction(answer=\"\", context=[\"This is a test passage.\"]), \n",
                "         0.0),  # Neither match\n",
                "    ]\n",
                "    \n",
                "    # Run all test cases\n",
                "    all_test_cases = {\n",
                "        \"Both Conditions Met\": both_true_cases,\n",
                "        \"Only Exact Match\": only_exact_cases,\n",
                "        \"Only Passage Match\": only_passage_cases,\n",
                "        \"Neither Condition Met\": neither_true_cases,\n",
                "        \"Edge Cases\": edge_cases,\n",
                "    }\n",
                "    \n",
                "    all_passed = True\n",
                "    \n",
                "    for category, test_cases in all_test_cases.items():\n",
                "        print(f\"\\nTesting {category}:\")\n",
                "        category_passed = True\n",
                "        \n",
                "        for i, (example, pred, expected) in enumerate(test_cases):\n",
                "            try:\n",
                "                result = composite_metric(example, pred)\n",
                "                # Allow for small floating-point differences\n",
                "                passed = abs(result - expected) < 0.01\n",
                "                category_passed = category_passed and passed\n",
                "                \n",
                "                status = \"✓\" if passed else \"✗\"\n",
                "                print(f\"  Test {i+1}: {status} Expected: {expected:.2f}, Got: {result:.2f}\")\n",
                "                if not passed:\n",
                "                    if isinstance(example.answer, list):\n",
                "                        answer_display = f\"[{', '.join(example.answer)}]\"\n",
                "                    else:\n",
                "                        answer_display = example.answer\n",
                "                    print(f\"    Example answer: {answer_display}\")\n",
                "                    print(f\"    Prediction answer: {pred.answer}\")\n",
                "                    print(f\"    Prediction context: {pred.context}\")\n",
                "            except Exception as e:\n",
                "                category_passed = False\n",
                "                print(f\"  Test {i+1}: ✗ Error: {str(e)}\")\n",
                "        \n",
                "        category_status = \"PASSED\" if category_passed else \"FAILED\"\n",
                "        print(f\"{category}: {category_status}\")\n",
                "        all_passed = all_passed and category_passed\n",
                "    \n",
                "    print(\"\\nOverall Result:\", \"ALL TESTS PASSED!\" if all_passed else \"SOME TESTS FAILED\")\n",
                "    return all_passed\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    run_tests()\n",
                "```\n",
                "\n",
                "```python\n",
                "from dspy import Example, Prediction\n",
                "from dspy.evaluate import answer_exact_match, answer_passage_match\n",
                "from utils import f1_score\n",
                "\n",
                "\n",
                "def composite_metric(example, pred, trace=None, exact_weight=0.6, passage_weight=0.4):\n",
                "    \"\"\"\n",
                "    A composite metric that combines answer_exact_match and answer_passage_match.\n",
                "    \n",
                "    Args:\n",
                "        example: The example containing the expected answer(s)\n",
                "        pred: The prediction containing the predicted answer and context\n",
                "        trace: Optional trace information (not used in this function)\n",
                "        exact_weight: Weight for the exact match component (default: 0.6)\n",
                "        passage_weight: Weight for the passage match component (default: 0.4)\n",
                "        \n",
                "    Returns:\n",
                "        A score between 0 and 1, with higher values indicating better performance\n",
                "        \n",
                "    Raises:\n",
                "        ValueError: If the answer type is neither a string nor a list\n",
                "    \"\"\"\n",
                "    # Check if the predicted answer matches the expected answer\n",
                "    exact_match_score = int(answer_exact_match(example, pred))\n",
                "    \n",
                "    # Check if the expected answer appears in the context\n",
                "    passage_match_score = int(answer_passage_match(example, pred))\n",
                "    \n",
                "    # Combine the scores using the given weights\n",
                "    score = (exact_match_score * exact_weight) + (passage_match_score * passage_weight)\n",
                "    \n",
                "    return score\n",
                "\n",
                "\n",
                "def run_tests():\n",
                "    print(\"Testing composite_metric function...\")\n",
                "    \n",
                "    # Test cases where both conditions are met\n",
                "    both_true_cases = [\n",
                "        (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "         Prediction(answer=\"Paris\", context=[\"Paris is the capital of France.\"]), \n",
                "         1.0),\n",
                "        \n",
                "        (Example(question=\"Who wrote Romeo and Juliet?\", answer=[\"William Shakespeare\", \"Shakespeare\"]), \n",
                "         Prediction(answer=\"Shakespeare\", context=[\"Romeo and Juliet was written by Shakespeare.\"]), \n",
                "         1.0),\n",
                "    ]\n",
                "    \n",
                "    # Test cases where only exact match is true\n",
                "    only_exact_cases = [\n",
                "        (Example(question=\"What is the capital of Japan?\", answer=\"Tokyo\"), \n",
                "         Prediction(answer=\"Tokyo\", context=[\"Japan is an island nation in East Asia.\"]), \n",
                "         0.6),  # exact_weight\n",
                "        \n",
                "        (Example(question=\"What is the chemical symbol for gold?\", answer=\"Au\"), \n",
                "         Prediction(answer=\"Au\", context=[\"Silver's chemical symbol is Ag.\"]), \n",
                "         0.6),  # exact_weight\n",
                "    ]\n",
                "    \n",
                "    # Test cases where only passage match is true\n",
                "    only_passage_cases = [\n",
                "        (Example(question=\"What is the capital of Italy?\", answer=\"Rome\"), \n",
                "         Prediction(answer=\"Milan\", context=[\"Rome is the capital of Italy.\"]), \n",
                "         0.4),  # passage_weight\n",
                "        \n",
                "        (Example(question=\"Who discovered penicillin?\", answer=\"Alexander Fleming\"), \n",
                "         Prediction(answer=\"Louis Pasteur\", context=[\"Alexander Fleming discovered penicillin in 1928.\"]), \n",
                "         0.4),  # passage_weight\n",
                "    ]\n",
                "    \n",
                "    # Test cases where neither condition is met\n",
                "    neither_true_cases = [\n",
                "        (Example(question=\"What is the capital of Germany?\", answer=\"Berlin\"), \n",
                "         Prediction(answer=\"Munich\", context=[\"Germany is a country in Europe.\"]), \n",
                "         0.0),\n",
                "        \n",
                "        (Example(question=\"Who painted the Mona Lisa?\", answer=\"Leonardo da Vinci\"), \n",
                "         Prediction(answer=\"Michelangelo\", context=[\"The Sistine Chapel was painted by Michelangelo.\"]), \n",
                "         0.0),\n",
                "    ]\n",
                "    \n",
                "    # Edge cases\n",
                "    edge_cases = [\n",
                "        (Example(question=\"Empty answer\", answer=\"\"), \n",
                "         Prediction(answer=\"\", context=[\"This is a test passage.\"]), \n",
                "         1.0),  # Both match\n",
                "        \n",
                "        (Example(question=\"Empty passage\", answer=\"Something\"), \n",
                "         Prediction(answer=\"Something\", context=[\"\"]), \n",
                "         0.6),  # Only exact match\n",
                "        \n",
                "        (Example(question=\"Empty list\", answer=\"Empty list\"), \n",
                "         Prediction(answer=\"\", context=[\"This is a test passage.\"]), \n",
                "         0.0),  # Neither match\n",
                "    ]\n",
                "    \n",
                "    # Run all test cases\n",
                "    all_test_cases = {\n",
                "        \"Both Conditions Met\": both_true_cases,\n",
                "        \"Only Exact Match\": only_exact_cases,\n",
                "        \"Only Passage Match\": only_passage_cases,\n",
                "        \"Neither Condition Met\": neither_true_cases,\n",
                "        \"Edge Cases\": edge_cases,\n",
                "    }\n",
                "    \n",
                "    all_passed = True\n",
                "    \n",
                "    for category, test_cases in all_test_cases.items():\n",
                "        print(f\"\\nTesting {category}:\")\n",
                "        category_passed = True\n",
                "        \n",
                "        for i, (example, pred, expected) in enumerate(test_cases):\n",
                "            try:\n",
                "                result = composite_metric(example, pred)\n",
                "                # Allow for small floating-point differences\n",
                "                passed = abs(result - expected) < 0.01\n",
                "                category_passed = category_passed and passed\n",
                "                \n",
                "                status = \"✓\" if passed else \"✗\"\n",
                "                print(f\"  Test {i+1}: {status} Expected: {expected:.2f}, Got: {result:.2f}\")\n",
                "                if not passed:\n",
                "                    if isinstance(example.answer, list):\n",
                "                        answer_display = f\"[{', '.join(example.answer)}]\"\n",
                "                    else:\n",
                "                        answer_display = example.answer\n",
                "                    print(f\"    Example answer: {answer_display}\")\n",
                "                    print(f\"    Prediction answer: {pred.answer}\")\n",
                "                    print(f\"    Prediction context: {pred.context}\")\n",
                "            except Exception as e:\n",
                "                category_passed = False\n",
                "                print(f\"  Test {i+1}: ✗ Error: {str(e)}\")\n",
                "        \n",
                "        category_status = \"PASSED\" if category_passed else \"FAILED\"\n",
                "        print(f\"{category}: {category_status}\")\n",
                "        all_passed = all_passed and category_passed\n",
                "    \n",
                "    print(\"\\nOverall Result:\", \"ALL TESTS PASSED!\" if all_passed else \"SOME TESTS FAILED\")\n",
                "    return all_passed\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    run_tests()\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### Solution Explanation\n",
                "\n",
                "The `composite_metric` function combines two evaluation criteria to provide a more holistic score for a QA system's performance: the **correctness of the answer** and the **relevance of the retrieved context**.\n",
                "\n",
                "The solution correctly implements this by:\n",
                "\n",
                "1.  **Evaluating Answer Correctness:** It calls `answer_exact_match(example, pred)` to check if the predicted answer matches the expected answer. The boolean result is converted to an integer (`True` becomes `1`, `False` becomes `0`).\n",
                "2.  **Evaluating Context Relevance:** It calls `answer_passage_match(example, pred)` to check if the expected answer is present in the provided context. This result is also converted to an integer.\n",
                "3.  **Combining the Scores:** The function then uses the provided weights (`exact_weight` and `passage_weight`) to create a final score. The formula used is a weighted sum: `(exact_match_score * exact_weight) + (passage_match_score * passage_weight)`.\n",
                "\n",
                "This approach provides a flexible and customizable metric. For example, if you consider a correct answer more important than the presence of the answer in the context, you can assign `exact_weight` a higher value, like the default `0.6`. Conversely, if the focus is more on the retrieval component, the weights can be adjusted accordingly.  This kind of composite metric is invaluable for a comprehensive evaluation of a Retrieval-Augmented Generation (RAG) system, as it tests both the retrieval and generation stages."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Semantic Evaluation with F1 Score\n",
                "\n",
                "Now that you've worked with exact matching and passage matching, let's explore semantic evaluation — a more nuanced way to assess language model outputs. In this exercise, you'll implement a SemanticEvaluator class that measures how well a system's response captures the meaning of ground truth answers.\n",
                "\n",
                "Unlike exact matching, semantic evaluation focuses on the overlap of key ideas between responses, making it ideal for evaluating longer, more complex outputs. Your evaluator will calculate precision (how much of the system's response is supported by ground truth) and recall (how much of the ground truth is covered by the system's response), then combine them into an F1 score.\n",
                "\n",
                "To complete this exercise:\n",
                "\n",
                "Implement the constructor to support both standard and decompositional evaluation modes.\n",
                "Complete the forward method to calculate semantic similarity scores.\n",
                "Handle the threshold parameter correctly for binary evaluation.\n",
                "This metric will be particularly valuable for evaluating summarization, question answering, and other tasks where the exact wording matters less than capturing the right concepts. By implementing this evaluator, you'll add a powerful tool to your DSPy evaluation toolkit.\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "from dspy import Example, Prediction\n",
                "import os\n",
                "\n",
                "\n",
                "def f1_score(precision, recall):\n",
                "    \"\"\"Calculate F1 score from precision and recall values.\"\"\"\n",
                "    precision, recall = max(0.0, min(1.0, precision)), max(0.0, min(1.0, recall))\n",
                "    return 0.0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
                "\n",
                "\n",
                "class SemanticEvaluator(dspy.Module):\n",
                "    \"\"\"\n",
                "    A module that evaluates the semantic similarity between ground truth and system responses.\n",
                "    \n",
                "    This evaluator calculates precision (how much of the system response is supported by ground truth),\n",
                "    recall (how much of the ground truth is covered by the system response), and F1 score.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, threshold=0.66, decompositional=False):\n",
                "        \"\"\"\n",
                "        Initialize the semantic evaluator.\n",
                "        \n",
                "        Args:\n",
                "            threshold: The threshold for considering a response good enough (default: 0.66)\n",
                "            decompositional: Whether to use decompositional evaluation (default: False)\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.threshold = threshold\n",
                "        \n",
                "        # TODO: Set self.module based on the decompositional parameter\n",
                "        # If decompositional is True, use DecompositionalSemanticRecallPrecision\n",
                "        # Otherwise, use SemanticRecallPrecision\n",
                "        # Wrap the chosen signature with dspy.ChainOfThought\n",
                "    \n",
                "    def forward(self, example, pred, trace=None):\n",
                "        \"\"\"\n",
                "        Evaluate the semantic similarity between the example's ground truth and the prediction.\n",
                "        \n",
                "        Args:\n",
                "            example: The example containing the question and ground truth response\n",
                "            pred: The prediction containing the system response\n",
                "            trace: Optional trace information\n",
                "            \n",
                "        Returns:\n",
                "            If trace is None: A score between 0 and 1 representing semantic similarity\n",
                "            If trace is not None: True if the score exceeds the threshold, False otherwise\n",
                "        \"\"\"\n",
                "        # TODO: Call the appropriate module to get precision and recall\n",
                "        # The module should receive question, ground_truth, and system_response\n",
                "        \n",
                "        # TODO: Calculate F1 score from precision and recall\n",
                "        \n",
                "        # TODO: Return appropriate value based on trace parameter\n",
                "        # If trace is not None, return whether score exceeds threshold\n",
                "        # Otherwise, return the raw score\n",
                "        pass\n",
                "\n",
                "\n",
                "class SemanticRecallPrecision(dspy.Signature):\n",
                "    \"\"\"\n",
                "    Compare a system's response to the ground truth to compute its recall and precision.\n",
                "    If asked to reason, enumerate key ideas in each response, and whether they are present in the other response.\n",
                "    \"\"\"\n",
                "\n",
                "    question: str = dspy.InputField()\n",
                "    ground_truth: str = dspy.InputField()\n",
                "    system_response: str = dspy.InputField()\n",
                "    recall: float = dspy.OutputField(desc=\"fraction (out of 1.0) of ground truth covered by the system response\")\n",
                "    precision: float = dspy.OutputField(desc=\"fraction (out of 1.0) of system response covered by the ground truth\")\n",
                "\n",
                "\n",
                "class DecompositionalSemanticRecallPrecision(dspy.Signature):\n",
                "    \"\"\"\n",
                "    Compare a system's response to the ground truth to compute recall and precision of key ideas.\n",
                "    You will first enumerate key ideas in each response, discuss their overlap, and then report recall and precision.\n",
                "    \"\"\"\n",
                "\n",
                "    question: str = dspy.InputField()\n",
                "    ground_truth: str = dspy.InputField()\n",
                "    system_response: str = dspy.InputField()\n",
                "    ground_truth_key_ideas: str = dspy.OutputField(desc=\"enumeration of key ideas in the ground truth\")\n",
                "    system_response_key_ideas: str = dspy.OutputField(desc=\"enumeration of key ideas in the system response\")\n",
                "    discussion: str = dspy.OutputField(desc=\"discussion of the overlap between ground truth and system response\")\n",
                "    recall: float = dspy.OutputField(desc=\"fraction (out of 1.0) of ground truth covered by the system response\")\n",
                "    precision: float = dspy.OutputField(desc=\"fraction (out of 1.0) of system response covered by the ground truth\")\n",
                "    \n",
                "    \n",
                "def run_tests():\n",
                "    # Create test cases\n",
                "    perfect_match = (\n",
                "        Example(\n",
                "            question=\"What are the benefits of exercise?\",\n",
                "            ground_truth=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        Prediction(\n",
                "            response=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        1.0  # Expected score\n",
                "    )\n",
                "    \n",
                "    high_overlap = (\n",
                "        Example(\n",
                "            question=\"What are the benefits of exercise?\",\n",
                "            ground_truth=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        Prediction(\n",
                "            response=\"Exercise has many benefits including better heart health, stronger muscles, and improved mood.\"\n",
                "        ),\n",
                "        0.8  # Expected score (approximate)\n",
                "    )\n",
                "    \n",
                "    partial_overlap = (\n",
                "        Example(\n",
                "            question=\"What are the benefits of exercise?\",\n",
                "            ground_truth=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        Prediction(\n",
                "            response=\"Exercise is good for your heart and can help you build stronger muscles.\"\n",
                "        ),\n",
                "        0.5  # Expected score (approximate)\n",
                "    )\n",
                "    \n",
                "    low_overlap = (\n",
                "        Example(\n",
                "            question=\"What are the benefits of exercise?\",\n",
                "            ground_truth=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        Prediction(\n",
                "            response=\"Physical activity is an important part of a healthy lifestyle.\"\n",
                "        ),\n",
                "        0.2  # Expected score (approximate)\n",
                "    )\n",
                "    \n",
                "    no_overlap = (\n",
                "        Example(\n",
                "            question=\"What are the benefits of exercise?\",\n",
                "            ground_truth=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        Prediction(\n",
                "            response=\"The weather forecast predicts rain tomorrow.\"\n",
                "        ),\n",
                "        0.0  # Expected score\n",
                "    )\n",
                "    \n",
                "    # Test both standard and decompositional modes\n",
                "    test_cases = [\n",
                "        (\"Standard Mode\", False, [perfect_match, high_overlap, partial_overlap, low_overlap, no_overlap]),\n",
                "        (\"Decompositional Mode\", True, [perfect_match, high_overlap, partial_overlap, low_overlap, no_overlap])\n",
                "    ]\n",
                "    \n",
                "    # Run tests\n",
                "    for mode_name, decompositional, cases in test_cases:\n",
                "        print(f\"\\nTesting {mode_name}:\")\n",
                "        evaluator = SemanticEvaluator(threshold=0.6, decompositional=decompositional)\n",
                "        \n",
                "        for i, (example, pred, expected) in enumerate(cases):\n",
                "            score = evaluator(example, pred)\n",
                "            threshold_result = evaluator(example, pred, trace={})\n",
                "            \n",
                "            # For approximate scores, check if within reasonable range\n",
                "            if expected in (0.0, 1.0):\n",
                "                score_ok = abs(score - expected) < 0.1\n",
                "            else:\n",
                "                # For approximate scores, we're more lenient\n",
                "                score_ok = abs(score - expected) < 0.3\n",
                "                \n",
                "            expected_threshold = expected >= 0.6\n",
                "            \n",
                "            print(f\"  Test {i+1}: Score: {score:.2f} (Expected ~{expected:.1f}) - {'✓' if score_ok else '✗'}\")\n",
                "            print(f\"    Threshold check: {threshold_result} (Expected {expected_threshold}) - {'✓' if threshold_result == expected_threshold else '✗'}\")\n",
                "            \n",
                "            # Print details for failed tests\n",
                "            if not score_ok:\n",
                "                print(f\"    Question: {example.question}\")\n",
                "                print(f\"    Ground truth: {example.ground_truth}\")\n",
                "                print(f\"    Response: {pred.response}\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    # Set up a language model\n",
                "    lm = dspy.LM('openai/gpt-4o-mini', api_key=os.environ['OPENAI_API_KEY'], api_base=os.environ['OPENAI_BASE_URL'])\n",
                "    dspy.configure(lm=lm)\n",
                "    run_tests()\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "import dspy\n",
                "from dspy import Example, Prediction\n",
                "import os\n",
                "\n",
                "\n",
                "def f1_score(precision, recall):\n",
                "    \"\"\"Calculate F1 score from precision and recall values.\"\"\"\n",
                "    precision, recall = max(0.0, min(1.0, precision)), max(0.0, min(1.0, recall))\n",
                "    return 0.0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
                "\n",
                "\n",
                "class SemanticEvaluator(dspy.Module):\n",
                "    \"\"\"\n",
                "    A module that evaluates the semantic similarity between ground truth and system responses.\n",
                "    \n",
                "    This evaluator calculates precision (how much of the system response is supported by ground truth),\n",
                "    recall (how much of the ground truth is covered by the system response), and F1 score.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, threshold=0.66, decompositional=False):\n",
                "        \"\"\"\n",
                "        Initialize the semantic evaluator.\n",
                "        \n",
                "        Args:\n",
                "            threshold: The threshold for considering a response good enough (default: 0.66)\n",
                "            decompositional: Whether to use decompositional evaluation (default: False)\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.threshold = threshold\n",
                "        \n",
                "        if decompositional:\n",
                "            self.module = dspy.ChainOfThought(DecompositionalSemanticRecallPrecision)\n",
                "        else:\n",
                "            self.module = dspy.ChainOfThought(SemanticRecallPrecision)\n",
                "    \n",
                "    def forward(self, example, pred, trace=None):\n",
                "        \"\"\"\n",
                "        Evaluate the semantic similarity between the example's ground truth and the prediction.\n",
                "        \n",
                "        Args:\n",
                "            example: The example containing the question and ground truth response\n",
                "            pred: The prediction containing the system response\n",
                "            trace: Optional trace information\n",
                "            \n",
                "        Returns:\n",
                "            If trace is None: A score between 0 and 1 representing semantic similarity\n",
                "            If trace is not None: True if the score exceeds the threshold, False otherwise\n",
                "        \"\"\"\n",
                "        result = self.module(\n",
                "            question=example.question,\n",
                "            ground_truth=example.ground_truth,\n",
                "            system_response=pred.response\n",
                "        )\n",
                "        \n",
                "        precision = result.precision\n",
                "        recall = result.recall\n",
                "        \n",
                "        score = f1_score(precision, recall)\n",
                "        \n",
                "        if trace is not None:\n",
                "            return score >= self.threshold\n",
                "        else:\n",
                "            return score\n",
                "\n",
                "\n",
                "class SemanticRecallPrecision(dspy.Signature):\n",
                "    \"\"\"\n",
                "    Compare a system's response to the ground truth to compute its recall and precision.\n",
                "    If asked to reason, enumerate key ideas in each response, and whether they are present in the other response.\n",
                "    \"\"\"\n",
                "\n",
                "    question: str = dspy.InputField()\n",
                "    ground_truth: str = dspy.InputField()\n",
                "    system_response: str = dspy.InputField()\n",
                "    recall: float = dspy.OutputField(desc=\"fraction (out of 1.0) of ground truth covered by the system response\")\n",
                "    precision: float = dspy.OutputField(desc=\"fraction (out of 1.0) of system response covered by the ground truth\")\n",
                "\n",
                "\n",
                "class DecompositionalSemanticRecallPrecision(dspy.Signature):\n",
                "    \"\"\"\n",
                "    Compare a system's response to the ground truth to compute recall and precision of key ideas.\n",
                "    You will first enumerate key ideas in each response, discuss their overlap, and then report recall and precision.\n",
                "    \"\"\"\n",
                "\n",
                "    question: str = dspy.InputField()\n",
                "    ground_truth: str = dspy.InputField()\n",
                "    system_response: str = dspy.InputField()\n",
                "    ground_truth_key_ideas: str = dspy.OutputField(desc=\"enumeration of key ideas in the ground truth\")\n",
                "    system_response_key_ideas: str = dspy.OutputField(desc=\"enumeration of key ideas in the system response\")\n",
                "    discussion: str = dspy.OutputField(desc=\"discussion of the overlap between ground truth and system response\")\n",
                "    recall: float = dspy.OutputField(desc=\"fraction (out of 1.0) of ground truth covered by the system response\")\n",
                "    precision: float = dspy.OutputField(desc=\"fraction (out of 1.0) of system response covered by the ground truth\")\n",
                "    \n",
                "    \n",
                "def run_tests():\n",
                "    # Create test cases\n",
                "    perfect_match = (\n",
                "        Example(\n",
                "            question=\"What are the benefits of exercise?\",\n",
                "            ground_truth=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        Prediction(\n",
                "            response=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        1.0  # Expected score\n",
                "    )\n",
                "    \n",
                "    high_overlap = (\n",
                "        Example(\n",
                "            question=\"What are the benefits of exercise?\",\n",
                "            ground_truth=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        Prediction(\n",
                "            response=\"Exercise has many benefits including better heart health, stronger muscles, and improved mood.\"\n",
                "        ),\n",
                "        0.8  # Expected score (approximate)\n",
                "    )\n",
                "    \n",
                "    partial_overlap = (\n",
                "        Example(\n",
                "            question=\"What are the benefits of exercise?\",\n",
                "            ground_truth=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        Prediction(\n",
                "            response=\"Exercise is good for your heart and can help you build stronger muscles.\"\n",
                "        ),\n",
                "        0.5  # Expected score (approximate)\n",
                "    )\n",
                "    \n",
                "    low_overlap = (\n",
                "        Example(\n",
                "            question=\"What are the benefits of exercise?\",\n",
                "            ground_truth=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        Prediction(\n",
                "            response=\"Physical activity is an important part of a healthy lifestyle.\"\n",
                "        ),\n",
                "        0.2  # Expected score (approximate)\n",
                "    )\n",
                "    \n",
                "    no_overlap = (\n",
                "        Example(\n",
                "            question=\"What are the benefits of exercise?\",\n",
                "            ground_truth=\"Regular exercise improves cardiovascular health, builds muscle strength, and enhances mental wellbeing.\"\n",
                "        ),\n",
                "        Prediction(\n",
                "            response=\"The weather forecast predicts rain tomorrow.\"\n",
                "        ),\n",
                "        0.0  # Expected score\n",
                "    )\n",
                "    \n",
                "    # Test both standard and decompositional modes\n",
                "    test_cases = [\n",
                "        (\"Standard Mode\", False, [perfect_match, high_overlap, partial_overlap, low_overlap, no_overlap]),\n",
                "        (\"Decompositional Mode\", True, [perfect_match, high_overlap, partial_overlap, low_overlap, no_overlap])\n",
                "    ]\n",
                "    \n",
                "    # Run tests\n",
                "    for mode_name, decompositional, cases in test_cases:\n",
                "        print(f\"\\nTesting {mode_name}:\")\n",
                "        evaluator = SemanticEvaluator(threshold=0.6, decompositional=decompositional)\n",
                "        \n",
                "        for i, (example, pred, expected) in enumerate(cases):\n",
                "            score = evaluator(example, pred)\n",
                "            threshold_result = evaluator(example, pred, trace={})\n",
                "            \n",
                "            # For approximate scores, check if within reasonable range\n",
                "            if expected in (0.0, 1.0):\n",
                "                score_ok = abs(score - expected) < 0.1\n",
                "            else:\n",
                "                # For approximate scores, we're more lenient\n",
                "                score_ok = abs(score - expected) < 0.3\n",
                "                \n",
                "            expected_threshold = expected >= 0.6\n",
                "            \n",
                "            print(f\"  Test {i+1}: Score: {score:.2f} (Expected ~{expected:.1f}) - {'✓' if score_ok else '✗'}\")\n",
                "            print(f\"    Threshold check: {threshold_result} (Expected {expected_threshold}) - {'✓' if threshold_result == expected_threshold else '✗'}\")\n",
                "            \n",
                "            # Print details for failed tests\n",
                "            if not score_ok:\n",
                "                print(f\"    Question: {example.question}\")\n",
                "                print(f\"    Ground truth: {example.ground_truth}\")\n",
                "                print(f\"    Response: {pred.response}\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    # Set up a language model\n",
                "    lm = dspy.LM('openai/gpt-4o-mini', api_key=os.environ['OPENAI_API_KEY'], api_base=os.environ['OPENAI_BASE_URL'])\n",
                "    dspy.configure(lm=lm)\n",
                "    run_tests()\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### Solution Explanation\n",
                "\n",
                "The `SemanticEvaluator` class is a powerful example of how to build a custom evaluation metric in DSPy that goes beyond simple keyword matching. Here's a breakdown of the completed implementation:\n",
                "\n",
                "### 1\\. The `__init__` Method\n",
                "\n",
                "The constructor is responsible for setting up the core logic based on the `decompositional` parameter.\n",
                "\n",
                "  * It checks the value of `decompositional`.\n",
                "  * If `True`, it initializes `self.module` with a **`DecompositionalSemanticRecallPrecision`** signature. This signature is designed to first break down the key ideas in each text, enabling a more granular, human-like comparison.\n",
                "  * If `False`, it uses the simpler **`SemanticRecallPrecision`** signature.\n",
                "  * In both cases, the signature is wrapped in a `dspy.ChainOfThought` module. This is a crucial step that instructs the Language Model (LLM) to \"think step by step\" and provides the reasoning for its final recall and precision scores, leading to more robust and accurate evaluations.\n",
                "\n",
                "### 2\\. The `forward` Method\n",
                "\n",
                "This method orchestrates the evaluation process.\n",
                "\n",
                "  * It calls `self.module`, passing the **`question`**, **`ground_truth`**, and **`system_response`** fields from the `example` and `pred` objects.\n",
                "  * It extracts the `precision` and `recall` values from the LLM's output. The LLM's `ChainOfThought` process computes these scores as part of its output.\n",
                "  * It then uses the provided `f1_score` helper function to combine the precision and recall into a single F1 score, which is a harmonic mean that balances both metrics.\n",
                "  * Finally, it checks the `trace` parameter. If `trace` is provided, it returns a binary result (`True` or `False`) based on whether the calculated F1 score meets or exceeds the `self.threshold`. This is useful for pass/fail evaluations. If `trace` is not provided, it returns the raw F1 score, which is valuable for quantitative analysis and ranking.\n",
                "\n",
                "This composite approach—using an LLM to perform semantic reasoning and then calculating a standard F1 score—makes the `SemanticEvaluator` a versatile tool for assessing the quality of generative AI outputs."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
