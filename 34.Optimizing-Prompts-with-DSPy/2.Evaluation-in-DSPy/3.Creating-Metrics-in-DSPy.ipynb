{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Metrics in DSPy\n",
                "\n",
                "# Evaluation in DSPy: Creating Metrics\n",
                "\n",
                "Welcome to the third lesson of the \"Evaluation in DSPy\" course. This lesson focuses on creating metrics, which are crucial for quantifying the performance of system outputs in DSPy. Metrics provide a basis for improving and optimizing a system's performance.\n",
                "\n",
                "-----\n",
                "\n",
                "## Basic Metric Functions\n",
                "\n",
                "Basic metric functions are fundamental for evaluating system responses.\n",
                "\n",
                "### `validate_answer`\n",
                "\n",
                "The `validate_answer` function checks if a predicted answer is an exact match to the expected answer, ignoring case differences. It returns `True` for a match and `False` otherwise, making it useful for tasks requiring exact answers.\n",
                "\n",
                "```python\n",
                "def validate_answer(example, pred, trace=None):\n",
                "    return example.answer.lower() == pred.answer.lower()\n",
                "```\n",
                "\n",
                "### `answer_exact_match` and `answer_passage_match`\n",
                "\n",
                "DSPy also provides built-in functions like `answer_exact_match` and `answer_passage_match` that offer more flexibility.\n",
                "\n",
                "  * `answer_exact_match`: This function uses a helper function `_answer_match` to check if a prediction matches any of the given answers. It allows for partial matches based on a `frac` parameter.\n",
                "  * `answer_passage_match`: This function evaluates whether a predicted answer is present within a provided passage or context. It uses a helper function `_passage_match` to check if any of the expected answers are found in the predicted response's context.\n",
                "\n",
                "-----\n",
                "\n",
                "## Completeness and Groundedness Evaluation\n",
                "\n",
                "Evaluating a system's response for **completeness** and **groundedness** is vital for understanding its quality. The `CompleteAndGrounded` built-in class provides a structured way to perform this evaluation.\n",
                "\n",
                "The `CompleteAndGrounded` class uses an F1 score to combine two key components: `AnswerCompleteness` and `AnswerGroundedness`.\n",
                "\n",
                "  * **`AnswerCompleteness`**: This component estimates how well the system's response covers the \"ground truth\" or correct answer. It involves enumerating key ideas from both the ground truth and the system's response to discuss their overlap and report a completeness score.\n",
                "  * **`AnswerGroundedness`**: This component assesses the degree to which a system's response is supported by \"retrieved documents\" and common sense reasoning. It involves enumerating claims made in the system's response and discussing how they are supported by the provided context.\n",
                "\n",
                "-----\n",
                "\n",
                "## Semantic Evaluation with F1 Score\n",
                "\n",
                "Semantic evaluation assesses the quality of a response based on its semantic content. The built-in `SemanticF1` class performs this evaluation using recall, precision, and an F1 score.\n",
                "\n",
                "  * **Recall**: Measures the fraction of the ground truth covered by the system's response.\n",
                "  * **Precision**: Measures the fraction of the system's response that is covered by the ground truth.\n",
                "  * **F1 Score**: Combines precision and recall to provide a balanced evaluation metric.\n",
                "\n",
                "The `SemanticF1` class can perform both standard and \"decompositional\" semantic evaluations, offering flexibility in assessing response quality.\n",
                "\n",
                "-----\n",
                "\n",
                "## Practical Example: Evaluating a Tweet\n",
                "\n",
                "You can create **custom metrics** to evaluate specific criteria. As a practical example, a custom metric can be used to evaluate a tweet's quality. The goal is to check if a generated tweet correctly answers a question, is engaging, and adheres to a character limit.\n",
                "\n",
                "This custom metric uses an `Assess` class with a `dspy.Signature` to define an automatic assessment. The `metric` function then evaluates the tweet's correctness, engagement, and length, returning a score that reflects its overall quality.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, you learned how to create and use metrics in DSPy to evaluate system outputs. This includes using basic functions, evaluating for completeness and groundedness, and performing semantic evaluation with F1 scores. These skills are fundamental for assessing the performance of DSPy systems and provide a foundation for more advanced topics."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementing a Case Insensitive Metric\n",
                "\n",
                "Now that you've learned about basic metric functions in DSPy, let's put that knowledge into practice! In this exercise, you'll implement a fundamental metric: a case-insensitive exact match validator.\n",
                "\n",
                "The validate_answer function we discussed earlier is essential for many question-answering tasks. Your job is to complete this function in the provided file. The function should compare the predicted answer with the expected answer while ignoring differences in letter case.\n",
                "\n",
                "To complete this exercise:\n",
                "\n",
                "Implement the comparison logic in the validate_answer function.\n",
                "Ensure it returns True when answers match (ignoring case) and False otherwise.\n",
                "Run the provided test cases to verify your implementation works correctly.\n",
                "This simple metric will serve as a building block for more complex evaluation methods you'll explore later in the course. Mastering these basic metrics is the first step toward creating sophisticated evaluation systems for your DSPy applications.\n",
                "\n",
                "```python\n",
                "from dspy import Example, Prediction\n",
                "\n",
                "\n",
                "def validate_answer(example, pred, trace=None):\n",
                "    \"\"\"\n",
                "    Validates if the predicted answer matches the expected answer, ignoring case.\n",
                "    \n",
                "    Args:\n",
                "        example: The example containing the expected answer\n",
                "        pred: The prediction containing the predicted answer\n",
                "        trace: Optional trace information (not used in this function)\n",
                "        \n",
                "    Returns:\n",
                "        True if the answers match (ignoring case), False otherwise\n",
                "    \"\"\"\n",
                "    # TODO: Implement case-insensitive comparison between example.answer and pred.answer\n",
                "    pass\n",
                "\n",
                "\n",
                "# Test cases with matching answers (ignoring case)\n",
                "matching_cases = [\n",
                "    (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "     Prediction(answer=\"paris\")),\n",
                "    \n",
                "    (Example(question=\"Who wrote Romeo and Juliet?\", answer=\"william shakespeare\"), \n",
                "     Prediction(answer=\"William Shakespeare\")),\n",
                "    \n",
                "    (Example(question=\"What is the chemical symbol for gold?\", answer=\"Au\"), \n",
                "     Prediction(answer=\"au\")),\n",
                "]\n",
                "\n",
                "# Test cases with non-matching answers\n",
                "non_matching_cases = [\n",
                "    (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "     Prediction(answer=\"London\")),\n",
                "    \n",
                "    (Example(question=\"Who wrote Romeo and Juliet?\", answer=\"William Shakespeare\"), \n",
                "     Prediction(answer=\"Charles Dickens\")),\n",
                "    \n",
                "    (Example(question=\"What is the chemical symbol for gold?\", answer=\"Au\"), \n",
                "     Prediction(answer=\"Ag\")),\n",
                "]\n",
                "\n",
                "\n",
                "def run_tests():\n",
                "    print(\"Testing matching cases (should all be True):\")\n",
                "    matching_results = []\n",
                "    for i, (example, pred) in enumerate(matching_cases):\n",
                "        result = validate_answer(example, pred)\n",
                "        matching_results.append(result)\n",
                "        print(f\"  Test {i+1}: Expected '{example.answer}', Got '{pred.answer}', Match: {result}\")\n",
                "    \n",
                "    print(\"\\nTesting non-matching cases (should all be False):\")\n",
                "    non_matching_results = []\n",
                "    for i, (example, pred) in enumerate(non_matching_cases):\n",
                "        result = validate_answer(example, pred)\n",
                "        non_matching_results.append(result)\n",
                "        print(f\"  Test {i+1}: Expected '{example.answer}', Got '{pred.answer}', Match: {result}\")\n",
                "    \n",
                "    # Summary\n",
                "    matching_success = all(matching_results)\n",
                "    non_matching_success = not any(non_matching_results)\n",
                "    overall_success = matching_success and non_matching_success\n",
                "    \n",
                "    print(\"\\nSummary:\")\n",
                "    print(f\"  Matching cases: {'All passed' if matching_success else 'Some failed'}\")\n",
                "    print(f\"  Non-matching cases: {'All passed' if non_matching_success else 'Some failed'}\")\n",
                "    print(f\"  Overall: {'All tests passed!' if overall_success else 'Some tests failed.'}\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    run_tests()\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from dspy import Example, Prediction\n",
                "\n",
                "\n",
                "def validate_answer(example, pred, trace=None):\n",
                "    \"\"\"\n",
                "    Validates if the predicted answer matches the expected answer, ignoring case.\n",
                "    \n",
                "    Args:\n",
                "        example: The example containing the expected answer\n",
                "        pred: The prediction containing the predicted answer\n",
                "        trace: Optional trace information (not used in this function)\n",
                "        \n",
                "    Returns:\n",
                "        True if the answers match (ignoring case), False otherwise\n",
                "    \"\"\"\n",
                "    # Implement case-insensitive comparison between example.answer and pred.answer\n",
                "    return example.answer.lower() == pred.answer.lower()\n",
                "\n",
                "\n",
                "# Test cases with matching answers (ignoring case)\n",
                "matching_cases = [\n",
                "    (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "     Prediction(answer=\"paris\")),\n",
                "    \n",
                "    (Example(question=\"Who wrote Romeo and Juliet?\", answer=\"william shakespeare\"), \n",
                "     Prediction(answer=\"William Shakespeare\")),\n",
                "    \n",
                "    (Example(question=\"What is the chemical symbol for gold?\", answer=\"Au\"), \n",
                "     Prediction(answer=\"au\")),\n",
                "]\n",
                "\n",
                "# Test cases with non-matching answers\n",
                "non_matching_cases = [\n",
                "    (Example(question=\"What is the capital of France?\", answer=\"Paris\"), \n",
                "     Prediction(answer=\"London\")),\n",
                "    \n",
                "    (Example(question=\"Who wrote Romeo and Juliet?\", answer=\"William Shakespeare\"), \n",
                "     Prediction(answer=\"Charles Dickens\")),\n",
                "    \n",
                "    (Example(question=\"What is the chemical symbol for gold?\", answer=\"Au\"), \n",
                "     Prediction(answer=\"Ag\")),\n",
                "]\n",
                "\n",
                "\n",
                "def run_tests():\n",
                "    print(\"Testing matching cases (should all be True):\")\n",
                "    matching_results = []\n",
                "    for i, (example, pred) in enumerate(matching_cases):\n",
                "        result = validate_answer(example, pred)\n",
                "        matching_results.append(result)\n",
                "        print(f\"  Test {i+1}: Expected '{example.answer}', Got '{pred.answer}', Match: {result}\")\n",
                "    \n",
                "    print(\"\\nTesting non-matching cases (should all be False):\")\n",
                "    non_matching_results = []\n",
                "    for i, (example, pred) in enumerate(non_matching_cases):\n",
                "        result = validate_answer(example, pred)\n",
                "        non_matching_results.append(result)\n",
                "        print(f\"  Test {i+1}: Expected '{example.answer}', Got '{pred.answer}', Match: {result}\")\n",
                "    \n",
                "    # Summary\n",
                "    matching_success = all(matching_results)\n",
                "    non_matching_success = not any(non_matching_results)\n",
                "    overall_success = matching_success and non_matching_success\n",
                "    \n",
                "    print(\"\\nSummary:\")\n",
                "    print(f\"  Matching cases: {'All passed' if matching_success else 'Some failed'}\")\n",
                "    print(f\"  Non-matching cases: {'All passed' if non_matching_success else 'Some failed'}\")\n",
                "    print(f\"  Overall: {'All tests passed!' if overall_success else 'Some tests failed.'}\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    run_tests()\n",
                "\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Flexible Answer Matching for Multiple Formats"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Passage Matching for Retrieval Systems"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building a Holistic QA Evaluation Metric"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Semantic Evaluation with F1 Score"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
