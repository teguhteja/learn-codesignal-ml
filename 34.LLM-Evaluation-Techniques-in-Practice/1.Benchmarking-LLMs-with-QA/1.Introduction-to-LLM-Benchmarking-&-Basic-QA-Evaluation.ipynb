{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction to LLM Benchmarking & Basic QA Evaluation\n",
                "\n",
                "# Introduction to LLM Benchmarking\n",
                "\n",
                "-----\n",
                "\n",
                "Welcome to the first lesson of our course, \"Benchmarking LLMs with QA.\" In this lesson, we will explore the fundamentals of benchmarking large language models (LLMs). **Benchmarking** is the process of evaluating the performance of a system or component by comparing it against a set of predefined standards or datasets. It is crucial in understanding how well a model performs in various tasks, identifying its strengths and weaknesses, and guiding improvements.\n",
                "\n",
                "## Why Benchmarking?\n",
                "\n",
                "Benchmarking is essential for the development and refinement of LLMs, as it provides a systematic way to measure their capabilities and progress over time. By using standardized datasets and evaluation metrics, we can objectively assess the performance of different models and make informed decisions about their deployment and further development.\n",
                "\n",
                "Some common types of LLM benchmarks include:\n",
                "\n",
                "  * **Factual QA** (like TriviaQA, SQuAD)\n",
                "  * **Multiple-choice reasoning** (like MMLU, ARC)\n",
                "  * **Truthfulness & bias detection** (like TruthfulQA)\n",
                "  * **Perplexity-based evaluation** (language fluency prediction)\n",
                "  * **Semantic similarity** (embedding-based matching)\n",
                "  * **Domain-specific tests** (custom internal benchmarks)\n",
                "\n",
                "In this course, we’ll begin with factual QA benchmarks before expanding to other types in later lessons.\n",
                "\n",
                "## Working with the TriviaQA Dataset\n",
                "\n",
                "We will use the **TriviaQA** dataset, which contains a large collection of real-world question-answer pairs gathered from trivia websites. While TriviaQA is not a multiple-choice dataset, it is well-suited for evaluating factual question-answering capabilities.\n",
                "\n",
                "For simplicity and performance, we’ve pre-selected and stored a 100-example subset for you, available at:\n",
                "\n",
                "`triviaqa.csv`\n",
                "\n",
                "This subset contains pairs of factual questions and short answers. Here are a few sample entries from the dataset:\n",
                "\n",
                "```text\n",
                "Question: What is the capital of France?\n",
                "Answer: Paris\n",
                "\n",
                "Question: In which year did the Titanic sink?\n",
                "Answer: 1912\n",
                "```\n",
                "\n",
                "## Setting Up the Environment\n",
                "\n",
                "Before we dive into the code, let's ensure that your environment is ready. For this lesson, you will need the `openai` and `csv` libraries. If you are working on your local machine, you can install the `openai` library using pip:\n",
                "\n",
                "```bash\n",
                "pip install openai\n",
                "```\n",
                "\n",
                "The `csv` module is part of Python's standard library, so no additional installation is needed. However, if you are using the CodeSignal environment, these libraries are already pre-installed, so you can focus on the lesson without worrying about setup.\n",
                "\n",
                "## Loading and Understanding the TriviaQA Dataset\n",
                "\n",
                "To load the dataset, we’ll use Python’s built-in `csv` module. Here is how you can read it:\n",
                "\n",
                "```python\n",
                "import csv\n",
                "\n",
                "with open(\"triviaqa.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "```\n",
                "\n",
                "This code opens the `triviaqa.csv` file and reads its contents into a list of dictionaries, where each dictionary represents a question-answer pair. Understanding the structure of this dataset is crucial, as it will be the basis for our evaluation.\n",
                "\n",
                "## Implementing Normalized Match Evaluation\n",
                "\n",
                "To evaluate the performance of an LLM, we will use a technique called **normalized match**. This involves comparing the model's response to the correct answer by normalizing both texts. Normalization helps in removing any discrepancies due to case sensitivity or punctuation.\n",
                "\n",
                "Let's look at the code that implements this evaluation:\n",
                "\n",
                "```python\n",
                "import re\n",
                "from openai import OpenAI\n",
                "\n",
                "# Initialize the OpenAI client\n",
                "client = OpenAI()\n",
                "\n",
                "def normalize(text):\n",
                "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
                "\n",
                "correct = 0\n",
                "for q in qa_pairs:\n",
                "    prompt = f\"Answer the following question with a short and direct fact:\\n\\n{q['question']}\"\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    ).choices[0].message.content.strip()\n",
                "    if normalize(q['answer']) == normalize(response):\n",
                "        correct += 1\n",
                "```\n",
                "\n",
                "In this code, the `normalize` function removes all non-alphanumeric characters and converts the text to lowercase. This ensures that the comparison between the model's response and the correct answer is fair and consistent. We then iterate over each question-answer pair, generate a response using the `openai` library, and compare the normalized texts.\n",
                "\n",
                "## Example: Calculating Normalized Accuracy\n",
                "\n",
                "In this lesson, we introduced the concept of LLM benchmarking and demonstrated how to evaluate a language model using the TriviaQA dataset. We covered the setup of the environment, loading the dataset, and implementing a normalized match evaluation. However, we will not calculate the normalized accuracy just yet. The results with the current setup may not be optimal, but in the next unit, we will explore techniques like one-shot or few-shot learning to improve the model's performance. By doing so, you will gain a deeper understanding of how to enhance LLM capabilities through advanced evaluation techniques.\n",
                "\n",
                "## Summary and Next Steps\n",
                "\n",
                "In this lesson, we introduced the concept of LLM benchmarking and demonstrated how to evaluate a language model using the TriviaQA dataset. We covered the setup of the environment, loading the dataset, and implementing a normalized match evaluation. Although we have not calculated the normalized accuracy yet, we will explore more advanced techniques in the next unit to improve the model's performance. As you move forward, practice these concepts with the exercises provided. These will reinforce your understanding and prepare you for more advanced evaluation techniques in future lessons. Remember, benchmarking is a powerful tool in improving language models, and mastering it will enhance your skills in working with LLMs.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading and Exploring TriviaQA Dataset\n",
                "\n",
                "Now that you've learned about the importance of benchmarking LLMs, let's get hands-on with the TriviaQA dataset! In this exercise, you'll take your first step toward implementing a benchmark by loading and exploring the dataset.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Open the TriviaQA dataset file at \"triviaqa.csv\".\n",
                "Load the data using Python's csv module as a list of dictionaries.\n",
                "Print the first three question-answer pairs in a readable format.\n",
                "This practical experience with dataset handling will build the foundation for the evaluation techniques we'll explore next. Understanding your benchmark data is the first crucial step in any effective LLM evaluation process.\n",
                "\n",
                "```python\n",
                "# Loading and exploring the TriviaQA dataset\n",
                "\n",
                "import csv\n",
                "\n",
                "# TODO: Open the TriviaQA dataset file and read it using csv.DictReader\n",
                "# The file is located at \"triviaqa.csv\"\n",
                "\n",
                "\n",
                "# TODO: Print the first 3 question-answer pairs from the dataset\n",
                "# Format each pair as \"Question: [question]\" and \"Answer: [answer]\"\n",
                "# Add an empty line between pairs for better readability\n",
                "\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "# Loading and exploring the TriviaQA dataset\n",
                "\n",
                "import csv\n",
                "\n",
                "# Open the TriviaQA dataset file and read it using csv.DictReader\n",
                "with open(\"triviaqa.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "\n",
                "# Print the first 3 question-answer pairs from the dataset\n",
                "# Format each pair as \"Question: [question]\" and \"Answer: [answer]\"\n",
                "# Add an empty line between pairs for better readability\n",
                "for i in range(3):\n",
                "    print(f\"Question: {qa_pairs[i]['question']}\")\n",
                "    print(f\"Answer: {qa_pairs[i]['answer']}\\n\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Text Normalization for Fair Comparisons"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Answers Beyond Surface Formatting"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating a Single LLM Response"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating a Single LLM Response"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
