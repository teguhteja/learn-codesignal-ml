{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 1 Handling Pagination in Web Scraping with Python and Beautiful Soup\n",
                                    "\n",
                                    "# Introduction to Pagination with BeautifulSoup\n",
                                    "\n",
                                    "Hello and welcome\\! In this lesson, we will focus on handling **pagination** in web scraping using Python and Beautiful Soup. Pagination is essential when scraping large datasets from websites that display their content over multiple pages. By the end of this lesson, you will be equipped with the skills to navigate multiple web pages, extract necessary data, and handle pagination effectively.\n",
                                    "\n",
                                    "### What is Pagination?\n",
                                    "\n",
                                    "Pagination is a web design technique used to divide extensive content into multiple pages, commonly seen in search results, blogs, and forums. Each page shows a subset of the total data, and navigation links (typically labeled \"Next\", \"Previous\", or page numbers) let users move through the data.\n",
                                    "\n",
                                    "**Challenges:**\n",
                                    "\n",
                                    "  * Identifying and following \"Next\" buttons programmatically.\n",
                                    "  * Constructing URLs dynamically to request subsequent pages.\n",
                                    "  * Ensuring consistent data extraction amidst varying page layouts.\n",
                                    "\n",
                                    "Understanding pagination is essential for effective web scraping since it allows you to gather comprehensive datasets.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "### Implementing Pagination in Web Scraping\n",
                                    "\n",
                                    "Let's consider a scenario where we scrape quotes from a website that paginates its content. The website displays quotes on multiple pages, with a \"Next\" button to navigate to the next page. The following code demonstrates how to scrape quotes from multiple pages:\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "next_url = '/page/1/'\n",
                                    "\n",
                                    "while next_url:\n",
                                    "    response = requests.get(f\"{base_url}{next_url}\")\n",
                                    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "    quotes = soup.find_all(\"div\", class_=\"quote\")\n",
                                    "\n",
                                    "    for quote in quotes:\n",
                                    "        print(quote.find(\"span\", class_=\"text\").text)\n",
                                    "\n",
                                    "    next_button = soup.find(\"li\", class_=\"next\")\n",
                                    "    next_url = next_button.find(\"a\")[\"href\"] if next_button else None\n",
                                    "```\n",
                                    "\n",
                                    "The code iterates through multiple pages of the website and extracts quotes using Beautiful Soup.\n",
                                    "The `while` loop continues as long as the `next_url` is available, extracting the next URL dynamically from the \"Next\" button link. This code elegantly handles pagination by recursively following \"Next\" links until no more pages are available.\n",
                                    "\n",
                                    "We use `soup.find_all` to locate all `div` tags with class `quote`. Within each `quote` `div`, we find the `span` with the class `text` to extract the quote text.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "### Lesson Summary\n",
                                    "\n",
                                    "In this lesson, we explored how to handle pagination while scraping web data using Python and Beautiful Soup. We started with the concept of pagination, broke down the example code, and implemented a full pagination logic to scrape multiple pages.\n",
                                    "\n",
                                    "Let's practice and reinforce the concepts we learned. Happy scraping\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Scraping Quotes with Pagination\n",
                                    "\n",
                                    "Great job on understanding the lesson! Let's now run the example code to see pagination in action.\n",
                                    "\n",
                                    "Run the code to observe how it scrapes quotes from multiple pages.\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "next_url = '/page/1/'\n",
                                    "while next_url:\n",
                                    "    response = requests.get(f\"{base_url}{next_url}\")\n",
                                    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "\n",
                                    "    quotes = soup.find_all(\"div\", class_=\"quote\")\n",
                                    "    for quote in quotes:\n",
                                    "        print(quote.find(\"span\", class_=\"text\").text)\n",
                                    "\n",
                                    "    next_button = soup.find(\"li\", class_=\"next\")\n",
                                    "    next_url = next_button.find(\"a\")[\"href\"] if next_button else None\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Enhance Quote Scraping Script\n",
                                    "\n",
                                    "Great job so far! Now, let's take one step further and modify our existing code.\n",
                                    "\n",
                                    "Currently, the code prints only the text of the quotes. Change the code to also print the author of each quote.\n",
                                    "\n",
                                    "Remember, analyzing the structure of the page is crucial in web scraping. So, if you are not sure where to find the author's name, you can open the URL in another tab and inspect the page.\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "next_url = '/page/1/'\n",
                                    "while next_url:\n",
                                    "    response = requests.get(f\"{base_url}{next_url}\")\n",
                                    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "\n",
                                    "    quotes = soup.find_all(\"div\", class_=\"quote\")\n",
                                    "\n",
                                    "    # TODO: Modify the code to print the author of each quote along with the text\n",
                                    "    for quote in quotes:\n",
                                    "        print(quote.find(\"span\", class_=\"text\").text)\n",
                                    "\n",
                                    "    next_button = soup.find(\"li\", class_=\"next\")\n",
                                    "    next_url = next_button.find(\"a\")[\"href\"] if next_button else None\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "next_url = '/page/1/'\n",
                                    "while next_url:\n",
                                    "    response = requests.get(f\"{base_url}{next_url}\")\n",
                                    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "\n",
                                    "    quotes = soup.find_all(\"div\", class_=\"quote\")\n",
                                    "    \n",
                                    "    # TODO: Modify the code to print the author of each quote along with the text\n",
                                    "    for quote in quotes:\n",
                                    "        text = quote.find(\"span\", class_=\"text\").text\n",
                                    "        author = quote.find(\"small\", class_=\"author\").text\n",
                                    "        print(f'\"{text}\" - {author}')\n",
                                    "\n",
                                    "    next_button = soup.find(\"li\", class_=\"next\")\n",
                                    "    next_url = next_button.find(\"a\")[\"href\"] if next_button else None\n",
                                    "```\n",
                                    "\n",
                                    "**Penjelasan Modifikasi:**\n",
                                    "\n",
                                    "1.  **Mencari Elemen Penulis**: Saya menganalisis struktur HTML dari halaman `quotes.toscrape.com` dan menemukan bahwa nama penulis berada di dalam tag `<small>` dengan `class=\"author\"`.\n",
                                    "2.  **Menyimpan Teks dan Penulis**: Di dalam loop `for`, saya sekarang mengambil teks kutipan dan nama penulis secara terpisah dan menyimpannya ke dalam variabel `text` dan `author`.\n",
                                    "3.  **Mencetak Format Baru**: Saya menggunakan f-string (`f'\"{text}\" - {author}'`) untuk menggabungkan teks kutipan dan nama penulis ke dalam satu baris output yang mudah dibaca."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Enhance Pagination Handling Script"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Handle Next Button"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Handle Pagination in Web Scraping"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Creating a Pagination Scraper"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
