{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 4 Scraping Best Practices\n",
                                    "\n",
                                    "Welcome\\! In this lesson, we will focus on vital aspects of web scraping that ensure your scrapers are efficient, scalable, and respectful to the websites you are scraping from. We'll cover a variety of techniques and best practices to improve your scraping scripts using Python and BeautifulSoup.\n",
                                    "\n",
                                    "## Importance and Ethics of Web Scraping\n",
                                    "\n",
                                    "When you scrape data from a website, it's important to do it ethically. This means understanding and respecting the website's terms of service and its `robots.txt` file, which outlines what parts of the site can be crawled by web scrapers and bots. Ignoring this can lead to your IP being blocked and potentially legal consequences.\n",
                                    "\n",
                                    "Ethical scraping involves:\n",
                                    "\n",
                                    "  * **Honoring the `robots.txt` file:** Always check if the data you wish to scrape is allowed. This file usually can be found at the root of the website (e.g., `https://example.com/robots.txt`).\n",
                                    "  * **Avoiding overloading the server:** Make your scraper polite by controlling the rate of requests to avoid putting unnecessary load on the server.\n",
                                    "  * **Understanding data ownership:** Some data might be protected by copyright or require permission to be scraped.\n",
                                    "\n",
                                    "Aggressive scraping behaviors can degrade the performance of target websites, making them slow and potentially unresponsive for users. This is why polite crawling, rate limiting, and adhering to best practices is crucial.\n",
                                    "\n",
                                    "## Rate Limiting\n",
                                    "\n",
                                    "Rate limiting involves adding delays between requests to avoid overwhelming the server. You can use the `time.sleep()` function to achieve this.\n",
                                    "\n",
                                    "### Python\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "import time\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "\n",
                                    "url = \"https://quotes.toscrape.com\"\n",
                                    "page = \"/page/1/\"\n",
                                    "\n",
                                    "while page:\n",
                                    "    response = requests.get(url + page)\n",
                                    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "    print(f'Parsed page {url + page}')\n",
                                    "    next_page = soup.select_one('.next a')\n",
                                    "    page = next_page['href'] if next_page else None\n",
                                    "    time.sleep(1)  # Add a delay of 1 second between requests to avoid overloading the server\n",
                                    "```\n",
                                    "\n",
                                    "The above code snippet demonstrates how to add a delay of 1 second between requests. This helps in controlling the rate of requests and ensures that the server is not overwhelmed.\n",
                                    "\n",
                                    "## Handling Timeouts\n",
                                    "\n",
                                    "When making requests to a server, it's important to handle timeouts gracefully. You can set a timeout value for your requests to avoid waiting indefinitely for a response.\n",
                                    "\n",
                                    "### Python\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "\n",
                                    "url = \"https://httpbin.org/delay/20\"  # This URL introduces a delay of 20 seconds\n",
                                    "try:\n",
                                    "    response = requests.get(url, timeout=2)  # Set a timeout of 2 seconds\n",
                                    "    print(response.text)\n",
                                    "except requests.Timeout:\n",
                                    "    print(\"The request timed out\")\n",
                                    "```\n",
                                    "\n",
                                    "If we don't set a timeout, the request will wait indefinitely for a response, which can lead to performance issues.\n",
                                    "\n",
                                    "## Blending in the Regular Traffic\n",
                                    "\n",
                                    "Setting the `User-Agent` header can help your scraper blend in with regular browser traffic. This header provides information about the client's software environment.\n",
                                    "\n",
                                    "### Python\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "\n",
                                    "url = \"https://quotes.toscrape.com\"\n",
                                    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36' }\n",
                                    "\n",
                                    "response = requests.get(url, headers=headers)\n",
                                    "```\n",
                                    "\n",
                                    "By setting the `User-Agent` header, you can make your scraper appear more like a regular browser, reducing the chances of being blocked. This information varies based on the browser and operating system you use. You can find a list of common user agents online for different browsers and operating systems.\n",
                                    "\n",
                                    "## Lesson Summary\n",
                                    "\n",
                                    "We've covered essential best practices, ensuring that your web scraper is efficient, respectful, and robust. By following these guidelines, you can build reliable scrapers that extract data effectively without causing disruptions to the websites you scrape from. Remember, ethical scraping is the key to successful and sustainable web scraping practices. Happy scraping\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Best Practices of Web Scraper in Action\n",
                                    "\n",
                                    "Great progress so far! Now, let's run a Python script that demonstrates best practices for web scraping. The script scrapes quotes from a website, collecting content from multiple pages while respecting the server.\n",
                                    "\n",
                                    "This script uses:\n",
                                    "\n",
                                    "Requests and BeautifulSoup: For fetching and parsing web page content.\n",
                                    "\n",
                                    "Timeouts: To ensure the scraper doesn't hang indefinitely.\n",
                                    "\n",
                                    "Error Handling: To manage request errors gracefully.\n",
                                    "\n",
                                    "Polite Crawling: Adding delays between requests using time.sleep().\n",
                                    "\n",
                                    "Navigation: Handling multiple pages by detecting \"Next\" page links.\n",
                                    "\n",
                                    "When you run this code, it will print each quote's text and author name. Pay attention to how errors are handled, how pages are navigated, and how the server is respected by limiting requests.\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import time\n",
                                    "\n",
                                    "def scrape(base_url, start_page):\n",
                                    "    current_page = start_page\n",
                                    "    while current_page:\n",
                                    "        try:\n",
                                    "            response = requests.get(base_url + current_page, timeout=10)\n",
                                    "            response.raise_for_status()\n",
                                    "        except requests.exceptions.RequestException as e:\n",
                                    "            print(e)\n",
                                    "            break\n",
                                    "\n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "        for quote in soup.select(\".quote\"):\n",
                                    "            text = quote.select_one(\".text\").get_text(strip=True)\n",
                                    "            author = quote.select_one(\".author\").get_text(strip=True)\n",
                                    "            print(f\"'{text}' by {author}\")\n",
                                    "\n",
                                    "        next_page = soup.select_one(\"li.next > a\")\n",
                                    "        current_page = next_page[\"href\"] if next_page else None\n",
                                    "        time.sleep(1)  # Respectful crawling by sleeping for 1 second\n",
                                    "        print(\"\\n\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "start_page = '/page/1/'\n",
                                    "scrape(base_url, start_page)\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Fix the Web Scraper Bugs\n",
                                    "\n",
                                    "You've done a great job so far! Let's now identify and fix an issue in the code to improve its performance.\n",
                                    "\n",
                                    "Currently, the code is making a request to a URL and waiting for a response. The response takes too long, slowing down the code execution. We need to fix this issue by setting a timeout for the request for 2 seconds.\n",
                                    "\n",
                                    "Can you identify the issue and fix it to improve the code performance?\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "\n",
                                    "url = \"https://postman-echo.com/delay/10\"\n",
                                    "\n",
                                    "try:\n",
                                    "    response = requests.get(url)\n",
                                    "    print(response.text)\n",
                                    "except requests.Timeout:\n",
                                    "    print(\"The request timed out\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To fix the issue of the code taking too long to execute, you need to add a `timeout` parameter to the `requests.get()` function. The problem is that the request is set to an endpoint with a 10-second delay, but the code doesn't have a timeout, so it will wait the full 10 seconds before continuing.\n",
                                    "\n",
                                    "Here is the corrected code with a 2-second timeout:\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "\n",
                                    "url = \"https://postman-echo.com/delay/10\"\n",
                                    "\n",
                                    "try:\n",
                                    "    response = requests.get(url, timeout=2)\n",
                                    "    print(response.text)\n",
                                    "except requests.Timeout:\n",
                                    "    print(\"The request timed out\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Complete the Web Scraper\n",
                                    "\n",
                                    "Excellent progress so far! The scraper should use best practices, such as adding a delay between requests.\n",
                                    "\n",
                                    "Your task is to fill in the missing part of the code to have a delay to not overwhelm the server.\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import time\n",
                                    "\n",
                                    "def scrape_heroes(base_url, first_page):\n",
                                    "    next_page = first_page\n",
                                    "    while next_page:\n",
                                    "        try:\n",
                                    "            response = requests.get(base_url + next_page, timeout=10)\n",
                                    "            response.raise_for_status()\n",
                                    "        except requests.exceptions.RequestException as e:\n",
                                    "            print(e)\n",
                                    "            break\n",
                                    "        \n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "        for hero in soup.select(\".quote\"):\n",
                                    "            name = hero.select_one(\".text\").get_text(strip=True)\n",
                                    "            power = hero.select_one(\".author\").get_text(strip=True)\n",
                                    "            print(f\"'{name}' has power: {power}\")\n",
                                    "\n",
                                    "        next_page_link = soup.select_one(\"li.next > a\")\n",
                                    "        next_page = next_page_link[\"href\"] if next_page_link else None\n",
                                    "        \n",
                                    "        # TODO: Add wait time of 1 second before continuing the scraping to not overwhelm the server\n",
                                    "        \n",
                                    "        print(\"\\n\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "first_page = '/page/1/'\n",
                                    "scrape_heroes(base_url, first_page)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To add a delay to the web scraper, you should use the `time.sleep()` function. As per the instructions, a 1-second delay is needed to avoid overwhelming the server.\n",
                                    "\n",
                                    "Here is the completed code with the missing part filled in:\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import time\n",
                                    "\n",
                                    "def scrape_heroes(base_url, first_page):\n",
                                    "    next_page = first_page\n",
                                    "    while next_page:\n",
                                    "        try:\n",
                                    "            response = requests.get(base_url + next_page, timeout=10)\n",
                                    "            response.raise_for_status()\n",
                                    "        except requests.exceptions.RequestException as e:\n",
                                    "            print(e)\n",
                                    "            break\n",
                                    "        \n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "        for hero in soup.select(\".quote\"):\n",
                                    "            name = hero.select_one(\".text\").get_text(strip=True)\n",
                                    "            power = hero.select_one(\".author\").get_text(strip=True)\n",
                                    "            print(f\"'{name}' has power: {power}\")\n",
                                    "\n",
                                    "        next_page_link = soup.select_one(\"li.next > a\")\n",
                                    "        next_page = next_page_link[\"href\"] if next_page_link else None\n",
                                    "        \n",
                                    "        # TODO: Add wait time of 1 second before continuing the scraping to not overwhelm the server\n",
                                    "        time.sleep(1)\n",
                                    "        \n",
                                    "        print(\"\\n\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "first_page = '/page/1/'\n",
                                    "scrape_heroes(base_url, first_page)\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
