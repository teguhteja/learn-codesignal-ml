{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 3 Structured Data Extraction and Storage with Python\n",
                                    "\n",
                                    "## Topic Overview\n",
                                    "\n",
                                    "Hello and welcome\\! In this lesson, we'll be focusing on **Structured Data Extraction and Storage**. Specifically, we'll use **Python**, along with **BeautifulSoup** and **Pandas**, to scrape data from web pages and store it in a CSV file. This process involves retrieving HTML content, parsing it to extract data, handling pagination, and finally saving the structured data.\n",
                                    "\n",
                                    "### Introduction to CSV Files\n",
                                    "\n",
                                    "When scraping data from web pages, it's essential to store the extracted data in a structured format for further analysis. One common way to store structured data is by using a CSV (Comma-Separated Values) file. CSV files are easy to create, read, and share, making them a popular choice for storing tabular data. Here is an example of a CSV file:\n",
                                    "\n",
                                    "```csv\n",
                                    "actor,character,movie\n",
                                    "Tom Hanks,Forrest Gump,Forrest Gump\n",
                                    "Leonardo DiCaprio,Dominick Cobb,Inception\n",
                                    "```\n",
                                    "\n",
                                    "### Pandas Library\n",
                                    "\n",
                                    "**Pandas** is a powerful library in Python for data manipulation and analysis. It provides data structures like `DataFrame` and tools for reading and writing data in various formats, including CSV files. By using **Pandas**, we can easily store structured data in a CSV file.\n",
                                    "\n",
                                    "Here is an example of how to create a `DataFrame` and save it to a CSV file using **Pandas**:\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "data = {\n",
                                    "    'actor': ['Tom Hanks', 'Leonardo DiCaprio'],\n",
                                    "    'character': ['Forrest Gump', 'Dominick Cobb'],\n",
                                    "    'movie': ['Forrest Gump', 'Inception']\n",
                                    "}\n",
                                    "\n",
                                    "df = pd.DataFrame(data)\n",
                                    "df.to_csv('actors.csv', index=False)\n",
                                    "```\n",
                                    "\n",
                                    "After running this code, a CSV file named `actors.csv` will be created with the following content:\n",
                                    "\n",
                                    "```csv\n",
                                    "actor,character,movie\n",
                                    "Tom Hanks,Forrest Gump,Forrest Gump\n",
                                    "Leonardo DiCaprio,Dominick Cobb,Inception\n",
                                    "```\n",
                                    "\n",
                                    "Now that we have an understanding of CSV files and the **Pandas** library, let's move on to web scraping and data extraction.\n",
                                    "\n",
                                    "### Storing Scraped Data in a CSV File\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "\n",
                                    "def extract_to_csv(base_url, start_page, filename):\n",
                                    "    all_quotes = []\n",
                                    "\n",
                                    "    current_page = start_page\n",
                                    "    while current_page:\n",
                                    "        response = requests.get(f\"{base_url}{current_page}\")\n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "        quotes = soup.find_all('div', class_='quote')\n",
                                    "        for quote in quotes:\n",
                                    "            text = quote.find('span', class_='text').text\n",
                                    "            author = quote.find('small', class_='author').text\n",
                                    "            tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
                                    "            all_quotes.append({\"text\": text, \"author\": author, \"tags\": tags})\n",
                                    "\n",
                                    "        next_link = soup.find('li', class_='next')\n",
                                    "        current_page = next_link.find('a')['href'] if next_link else None\n",
                                    "\n",
                                    "    df = pd.DataFrame(all_quotes)\n",
                                    "    df.to_csv(filename, index=False)\n",
                                    "    print(f\"Data saved to {filename}\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "start_page = '/page/1/'\n",
                                    "filename = 'quotes.csv'\n",
                                    "extract_to_csv(base_url, start_page, filename)\n",
                                    "```\n",
                                    "\n",
                                    "In this code:\n",
                                    "\n",
                                    "  * We define the `extract_to_csv` function to handle the entire process.\n",
                                    "  * `all_quotes` collects all the quotes from all the pages.\n",
                                    "  * We loop through each page, extract quotes in the format: `{\"text\": text, \"author\": author, \"tags\": tags}`, and append them to `all_quotes`.\n",
                                    "  * The loop is controlled by the `current_page` variable, which is updated to the next page URL until there are no more pages.\n",
                                    "  * The next page URL is extracted from the `li` element with the class `next`.\n",
                                    "  * `pd.DataFrame(all_quotes)` creates a DataFrame.\n",
                                    "  * `df.to_csv(filename, index=False)` saves the DataFrame to a CSV file.\n",
                                    "\n",
                                    "The output of the above code will be the `quotes.csv` file containing the extracted data in a structured format.\n",
                                    "\n",
                                    "### Lesson Summary\n",
                                    "\n",
                                    "In this lesson, we covered the process of extracting structured data from web pages and storing it in a CSV file. We used **Python**, **BeautifulSoup**, and **Pandas** to scrape quotes from a website and save them in a CSV file.\n",
                                    "\n",
                                    "Make sure to practice this on your own and explore other web scraping projects to enhance your skills. Happy coding\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Store Scraped Data in CSV\n",
                                    "\n",
                                    "Great job on learning the basics of structured data extraction and storage! Now, let's run the code from the lesson to see it in action and understand it better.\n",
                                    "\n",
                                    "This task will scrape quotes from a website and store them in a CSV file using Python, BeautifulSoup, and Pandas.\n",
                                    "\n",
                                    "Here's a brief overview of the code:\n",
                                    "\n",
                                    "Import Libraries: Use requests to handle HTTP requests, BeautifulSoup to parse HTML, and pandas to manage and save data.\n",
                                    "\n",
                                    "Function extract_to_csv:\n",
                                    "\n",
                                    "Fetch HTML content.\n",
                                    "Parse HTML to find quotes, authors, and tags.\n",
                                    "Handle multiple pages.\n",
                                    "Store data in a CSV file.\n",
                                    "Make sure to review the code and its structure. Running it will save the scraped data into a CSV file, making it ready for further analysis.\n",
                                    "\n",
                                    "Remember to check out the quotes.csv file to observe the saved data.\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "def extract_to_csv(base_url, start_page, filename):\n",
                                    "    all_quotes = []\n",
                                    "    current_page = start_page\n",
                                    "\n",
                                    "    while current_page:\n",
                                    "        response = requests.get(f\"{base_url}{current_page}\")\n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "\n",
                                    "        quotes = soup.find_all('div', class_='quote')\n",
                                    "        for quote in quotes:\n",
                                    "            text = quote.find('span', class_='text').text\n",
                                    "            author = quote.find('small', class_='author').text\n",
                                    "            tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
                                    "            all_quotes.append({\"text\": text, \"author\": author, \"tags\": tags})\n",
                                    "\n",
                                    "        next_link = soup.find('li', class_='next')\n",
                                    "        current_page = next_link.find('a')['href'] if next_link else None\n",
                                    "\n",
                                    "    df = pd.DataFrame(all_quotes)\n",
                                    "    df.to_csv(filename, index=False)\n",
                                    "    print(f\"Data saved to {filename}\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "start_page = '/page/1/'\n",
                                    "filename = 'quotes.csv'\n",
                                    "extract_to_csv(base_url, start_page, filename)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "You've provided a complete and correct Python script for scraping quotes and saving them to a CSV file. The code is well-structured and follows the best practices discussed in the lesson.\n",
                                    "\n",
                                    "When you run this script, it will perform the following actions:\n",
                                    "\n",
                                    "1.  **Initialize**: It starts with the `base_url` and `start_page` for the website and the desired `filename` for the output.\n",
                                    "2.  **Loop Through Pages**: The `while current_page:` loop will continue as long as it finds a \"next\" button, automatically handling pagination.\n",
                                    "3.  **Scrape Data**: On each page, it finds all the quote `div` elements, then extracts the quote text, author, and tags.\n",
                                    "4.  **Append to List**: The extracted data for each quote is stored as a dictionary and added to the `all_quotes` list.\n",
                                    "5.  **Create DataFrame**: Once all pages have been scraped, the `all_quotes` list is converted into a pandas `DataFrame`.\n",
                                    "6.  **Save to CSV**: The `df.to_csv()` method saves the DataFrame to a file named `quotes.csv`. The `index=False` argument prevents pandas from writing the DataFrame's index as a column in the CSV.\n",
                                    "7.  **Confirmation**: Finally, a confirmation message is printed, letting you know that the data has been successfully saved.\n",
                                    "\n",
                                    "By running this code, you'll have a `quotes.csv` file in the same directory, containing all the scraped quotes, authors, and tags from the website, ready for you to explore and analyze!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adding Author's Description\n",
                                    "\n",
                                    "You have done well with the basics of extracting and storing structured data.\n",
                                    "\n",
                                    "In this task, we will update the existing logic to save a description of each author instead of their birth date. To complete the task, follow the instructions in the starter code.\n",
                                    "\n",
                                    "Remember to check out the quotes.csv file to observe the saved data after running the code.\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "def extract_to_csv(base_url, start_page, filename):\n",
                                    "    all_quotes = []\n",
                                    "    current_page = start_page\n",
                                    "\n",
                                    "    while current_page:\n",
                                    "        response = requests.get(f\"{base_url}{current_page}\")\n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "        quotes = soup.find_all('div', class_='quote')[:2]\n",
                                    "\n",
                                    "        for quote in quotes:\n",
                                    "            text = quote.find('span', class_='text').text\n",
                                    "            author = quote.find('small', class_='author').text\n",
                                    "            tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
                                    "\n",
                                    "            # Get author's detail page URL and fetch the birthdate\n",
                                    "            author_url_tag = quote.select_one('span a')\n",
                                    "            author_url = author_url_tag['href']\n",
                                    "\n",
                                    "            author_response = requests.get(f\"{base_url}{author_url}\")\n",
                                    "            author_soup = BeautifulSoup(author_response.text, 'html.parser')\n",
                                    "\n",
                                    "            # TODO: Update the lines below to fetch the author description instead of the birth date\n",
                                    "            # Hint: It can be found in the element with class 'author-description'\n",
                                    "            birthdate_tag = author_soup.find('span', class_='author-born-date')\n",
                                    "            birthdate = birthdate_tag.text\n",
                                    "\n",
                                    "            # TODO: Update the append statement to include the description\n",
                                    "            all_quotes.append({\"text\": text, \"author\": author, \"tags\": tags, \"birthdate\": birthdate})\n",
                                    "\n",
                                    "        next_link = soup.find('li', class_='next')\n",
                                    "        current_page = next_link.find('a')['href'] if next_link else None\n",
                                    "\n",
                                    "    df = pd.DataFrame(all_quotes)\n",
                                    "    df.to_csv(filename, index=False)\n",
                                    "    print(f\"Data saved to {filename}\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "start_page = '/page/1/'\n",
                                    "filename = 'quotes.csv'\n",
                                    "extract_to_csv(base_url, start_page, filename)\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Complete the Scraping Code\n",
                                    "\n",
                                    "Great work so far!\n",
                                    "\n",
                                    "In this task, you'll fill in the missing parts of the code needed to extract quotes from a website and save them in a CSV file.\n",
                                    "\n",
                                    "The function extract_to_csv retrieves HTML content, parses it to find quotes, authors, and tags, and stores the data in a structured format.\n",
                                    "\n",
                                    "Your job is to complete the TODO sections of the given code.\n",
                                    "\n",
                                    "Remember to check out the quotes.csv file to observe the saved data after running the code.\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "def extract_to_csv(base_url, start_page, filename):\n",
                                    "    all_quotes = []\n",
                                    "    current_page = start_page\n",
                                    "\n",
                                    "    while current_page:\n",
                                    "        response = requests.get(f\"{base_url}{current_page}\")  # Get the page content\n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')   # Parse the HTML content\n",
                                    "\n",
                                    "        quotes = soup.find_all('div', class_='quote')  # Find all quote blocks\n",
                                    "\n",
                                    "        for quote in quotes:\n",
                                    "            # TODO: Extract the quote text - span elements with class 'text'\n",
                                    "\n",
                                    "            # TODO: Extract the author name - span elements with class 'author'\n",
                                    "\n",
                                    "            tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
                                    "            all_quotes.append({\"text\": text, \"author\": author, \"tags\": tags})  # Append the quote data\n",
                                    "\n",
                                    "        next_link = soup.find('li', class_='next')  # Find the next page link\n",
                                    "        current_page = next_link.find('a')['href'] if next_link else None  # Update the current page\n",
                                    "\n",
                                    "    # TODO: Create Pandas DataFrame called 'df' with all_quotes that we constucted in the loop\n",
                                    "\n",
                                    "    df.to_csv(filename, index=False)\n",
                                    "    print(f\"Data saved to {filename}\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "start_page = '/page/1/'\n",
                                    "filename = 'quotes.csv'\n",
                                    "extract_to_csv(base_url, start_page, filename)\n",
                                    "\n",
                                    "```\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "def extract_to_csv(base_url, start_page, filename):\n",
                                    "    all_quotes = []\n",
                                    "    current_page = start_page\n",
                                    "\n",
                                    "    while current_page:\n",
                                    "        response = requests.get(f\"{base_url}{current_page}\")  # Get the page content\n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')   # Parse the HTML content\n",
                                    "\n",
                                    "        quotes = soup.find_all('div', class_='quote')  # Find all quote blocks\n",
                                    "\n",
                                    "        for quote in quotes:\n",
                                    "            # TODO: Extract the quote text - span elements with class 'text'\n",
                                    "            text = quote.find('span', class_='text').text.strip()\n",
                                    "\n",
                                    "            # TODO: Extract the author name - span elements with class 'author'\n",
                                    "            author = quote.find('small', class_='author').text.strip()\n",
                                    "\n",
                                    "            tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
                                    "            all_quotes.append({\"text\": text, \"author\": author, \"tags\": tags})  # Append the quote data\n",
                                    "\n",
                                    "        next_link = soup.find('li', class_='next')  # Find the next page link\n",
                                    "        current_page = next_link.find('a')['href'] if next_link else None  # Update the current page\n",
                                    "\n",
                                    "    # TODO: Create Pandas DataFrame called 'df' with all_quotes that we constucted in the loop\n",
                                    "    df = pd.DataFrame(all_quotes)\n",
                                    "\n",
                                    "    df.to_csv(filename, index=False)\n",
                                    "    print(f\"Data saved to {filename}\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "start_page = '/page/1/'\n",
                                    "filename = 'quotes.csv'\n",
                                    "extract_to_csv(base_url, start_page, filename)\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Add Extra Data in Scraping\n",
                                    "\n",
                                    "Great job on learning the basics of structured data extraction and storage.\n",
                                    "\n",
                                    "In this task, we will enhance our scraping function to not only extract quotes but also save the author's birthdate. Modify the existing code by adding the necessary parts to fetch the birthdate from the author's detail page. Ensure that the final output includes the quote text, author, tags, and birthdate in the CSV file.\n",
                                    "\n",
                                    "Remember to check out the quotes.csv file to observe the saved data after running the code.\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "def extract_to_csv(base_url, start_page, filename):\n",
                                    "    all_quotes = []\n",
                                    "    current_page = start_page\n",
                                    "\n",
                                    "    while current_page:\n",
                                    "        response = requests.get(f\"{base_url}{current_page}\")\n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "        quotes = soup.find_all('div', class_='quote')\n",
                                    "\n",
                                    "        for quote in quotes:\n",
                                    "            text = quote.find('span', class_='text').text\n",
                                    "            author = quote.find('small', class_='author').text\n",
                                    "            tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
                                    "\n",
                                    "            # Get author's detail page URL and fetch the birthdate\n",
                                    "            author_url_tag = quote.select_one('span a')\n",
                                    "            author_url = author_url_tag['href']\n",
                                    "\n",
                                    "            author_response = requests.get(f\"{base_url}{author_url}\")\n",
                                    "            author_soup = BeautifulSoup(author_response.text, 'html.parser')\n",
                                    "\n",
                                    "            # TODO: Find the span containing the birthdate with class 'author-born-date'\n",
                                    "            \n",
                                    "            # TODO: Extract the birthdate text\n",
                                    "\n",
                                    "            # TODO: Extend the object below to include birthdate field with the extracted birthdate\n",
                                    "            all_quotes.append({\"text\": text, \"author\": author, \"tags\": tags})\n",
                                    "\n",
                                    "        next_link = soup.find('li', class_='next')\n",
                                    "        current_page = next_link.find('a')['href'] if next_link else None\n",
                                    "\n",
                                    "    # TODO: Construct Pandas DataFrame with all_quotes\n",
                                    "    \n",
                                    "    # TODO: Write the data to a CSV file\n",
                                    "    print(f\"Data saved to {filename}\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "start_page = '/page/1/'\n",
                                    "filename = 'quotes.csv'\n",
                                    "extract_to_csv(base_url, start_page, filename)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "def extract_to_csv(base_url, start_page, filename):\n",
                                    "    all_quotes = []\n",
                                    "    current_page = start_page\n",
                                    "\n",
                                    "    while current_page:\n",
                                    "        response = requests.get(f\"{base_url}{current_page}\")\n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "        quotes = soup.find_all('div', class_='quote')\n",
                                    "\n",
                                    "        for quote in quotes:\n",
                                    "            text = quote.find('span', class_='text').text\n",
                                    "            author = quote.find('small', class_='author').text\n",
                                    "            tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
                                    "\n",
                                    "            # Get author's detail page URL and fetch the birthdate\n",
                                    "            author_url_tag = quote.select_one('span a')\n",
                                    "            author_url = author_url_tag['href']\n",
                                    "\n",
                                    "            author_response = requests.get(f\"{base_url}{author_url}\")\n",
                                    "            author_soup = BeautifulSoup(author_response.text, 'html.parser')\n",
                                    "\n",
                                    "            # TODO: Find the span containing the birthdate with class 'author-born-date'\n",
                                    "            born_date_tag = author_soup.find('span', class_='author-born-date')\n",
                                    "\n",
                                    "            # TODO: Extract the birthdate text\n",
                                    "            birthdate = born_date_tag.text if born_date_tag else None\n",
                                    "\n",
                                    "            # TODO: Extend the object below to include birthdate field with the extracted birthdate\n",
                                    "            all_quotes.append({\"text\": text, \"author\": author, \"tags\": tags, \"birthdate\": birthdate})\n",
                                    "\n",
                                    "        next_link = soup.find('li', class_='next')\n",
                                    "        current_page = next_link.find('a')['href'] if next_link else None\n",
                                    "\n",
                                    "    # TODO: Construct Pandas DataFrame with all_quotes\n",
                                    "    df = pd.DataFrame(all_quotes)\n",
                                    "\n",
                                    "    # TODO: Write the data to a CSV file\n",
                                    "    df.to_csv(filename, index=False)\n",
                                    "    print(f\"Data saved to {filename}\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "start_page = '/page/1/'\n",
                                    "filename = 'quotes.csv'\n",
                                    "extract_to_csv(base_url, start_page, filename)\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Scrape and Save Quotes Data\n",
                                    "\n",
                                    "Awesome progress so far! This final practice will let you put everything you've learned into action.\n",
                                    "\n",
                                    "Your task is to write a Python function to scrape quotes from a website, handle pagination, and store the data in a CSV file using requests, BeautifulSoup, and pandas.\n",
                                    "\n",
                                    "Follow the TODO comments to complete the task.\n",
                                    "\n",
                                    "Remember to check out the quotes.csv file to observe the saved data after running the code.\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "def extract_to_csv(base_url, start_page, filename):\n",
                                    "    # TODO: Initialize an empty list to collect all quotes\n",
                                    "\n",
                                    "    # TODO: Set the current page to the start page\n",
                                    "\n",
                                    "    # TODO: Start a while loop that continues as long as there is a current page\n",
                                    "        # TODO: Retrieve the page content using requests.get\n",
                                    "\n",
                                    "        # TODO: Parse the page content using BeautifulSoup\n",
                                    "\n",
                                    "        # TODO: Find all the quote elements on the page\n",
                                    "\n",
                                    "        # TODO: Loop through each quote element and extract the text, author, and tags\n",
                                    "\n",
                                    "        # TODO: Append the extracted data to the list\n",
                                    "\n",
                                    "        # TODO: Find the link to the next page and update the current page\n",
                                    "\n",
                                    "    # TODO: Create a DataFrame from the collected data\n",
                                    "\n",
                                    "    # TODO: Save the DataFrame to a CSV file\n",
                                    "\n",
                                    "    # TODO: Print a confirmation message with the filename\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "start_page = '/page/1/'\n",
                                    "filename = 'quotes.csv'\n",
                                    "extract_to_csv(base_url, start_page, filename)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import requests\n",
                                    "from bs4 import BeautifulSoup\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "def extract_to_csv(base_url, start_page, filename):\n",
                                    "    # Initialize an empty list to collect all quotes\n",
                                    "    all_quotes = []\n",
                                    "\n",
                                    "    # Set the current page to the start page\n",
                                    "    current_page = start_page\n",
                                    "\n",
                                    "    # Start a while loop that continues as long as there is a current page\n",
                                    "    while current_page:\n",
                                    "        # Retrieve the page content using requests.get\n",
                                    "        response = requests.get(f\"{base_url}{current_page}\")\n",
                                    "        \n",
                                    "        # Parse the page content using BeautifulSoup\n",
                                    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                                    "\n",
                                    "        # Find all the quote elements on the page\n",
                                    "        quotes = soup.find_all('div', class_='quote')\n",
                                    "\n",
                                    "        # Loop through each quote element and extract the text, author, and tags\n",
                                    "        for quote in quotes:\n",
                                    "            text = quote.find('span', class_='text').text.strip()\n",
                                    "            author = quote.find('small', class_='author').text.strip()\n",
                                    "            tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
                                    "\n",
                                    "            # Append the extracted data to the list\n",
                                    "            all_quotes.append({\"text\": text, \"author\": author, \"tags\": tags})\n",
                                    "\n",
                                    "        # Find the link to the next page and update the current page\n",
                                    "        next_link = soup.find('li', class_='next')\n",
                                    "        current_page = next_link.find('a')['href'] if next_link else None\n",
                                    "\n",
                                    "    # Create a DataFrame from the collected data\n",
                                    "    df = pd.DataFrame(all_quotes)\n",
                                    "\n",
                                    "    # Save the DataFrame to a CSV file\n",
                                    "    df.to_csv(filename, index=False)\n",
                                    "\n",
                                    "    # Print a confirmation message with the filename\n",
                                    "    print(f\"Data saved to {filename}\")\n",
                                    "\n",
                                    "base_url = 'http://quotes.toscrape.com'\n",
                                    "start_page = '/page/1/'\n",
                                    "filename = 'quotes.csv'\n",
                                    "extract_to_csv(base_url, start_page, filename)\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
