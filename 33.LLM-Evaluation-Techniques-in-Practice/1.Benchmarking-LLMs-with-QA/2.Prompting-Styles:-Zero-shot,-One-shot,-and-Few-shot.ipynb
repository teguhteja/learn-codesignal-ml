{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prompting Styles: Zero-shot, One-shot, and Few-shot\n",
                "\n",
                "### Introduction to Prompting Styles\n",
                "\n",
                "Welcome back\\! In the previous lesson, we explored the fundamentals of benchmarking large language models (LLMs) using the TriviaQA dataset. We discussed the importance of benchmarking and how it helps in evaluating the performance of LLMs. As a reminder, benchmarking allows us to measure a model's capabilities and guide improvements by comparing it against standardized datasets and evaluation metrics.\n",
                "\n",
                "In this lesson, we will delve into the concept of **prompting styles**, which play a crucial role in enhancing LLM performance in question-answering (QA) tasks. Specifically, we will explore **zero-shot**, **one-shot**, and **few-shot** prompting styles. These strategies involve providing different amounts of context or examples to the model, which can significantly impact its ability to generate accurate responses.\n",
                "\n",
                "### Implementing Zero-shot Prompting\n",
                "\n",
                "Zero-shot prompting is a technique where the model is given a question without any prior examples or context. This approach tests the model's ability to generate an answer based solely on its pre-trained knowledge. Let's look at how to implement zero-shot prompting using the provided code snippet.\n",
                "\n",
                "In the code, we define a function `get_prompt` that generates a prompt for the model based on the specified mode. For zero-shot prompting, the function returns a simple question with the instruction to answer with a short fact. We then iterate over the question-answer pairs from the TriviaQA dataset, generate a response using the OpenAI API, and compare the normalized response with the correct answer. The accuracy is calculated by counting the number of correct responses.\n",
                "\n",
                "```python\n",
                "import csv\n",
                "import re\n",
                "from openai import OpenAI\n",
                "client = OpenAI()  # Initialize the OpenAI client\n",
                "\n",
                "def normalize(text):\n",
                "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
                "\n",
                "with open(\"triviaqa.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "\n",
                "def get_zero_shot_prompt(question):\n",
                "    return f\"Question: {question}\\n Provide a short, direct answer.\"\n",
                "\n",
                "correct = 0\n",
                "for q in qa_pairs:\n",
                "    prompt = get_zero_shot_prompt(q['question'])\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    ).choices[0].message.content.strip()\n",
                "    if normalize(response) == normalize(q['answer']):\n",
                "        correct += 1\n",
                "\n",
                "print(f\"Zero-shot Accuracy: {correct}/{len(qa_pairs)}\")\n",
                "```\n",
                "\n",
                "The output will display the accuracy of the model in zero-shot mode, indicating how well it performs without any additional context. Zero-shot prompting is advantageous for its simplicity and speed, but it may not always yield the most accurate results due to the lack of context.\n",
                "\n",
                "### Implementing One-shot Prompting\n",
                "\n",
                "One-shot prompting involves providing the model with a single example before asking it to answer a new question. This approach helps the model understand the format and context of the task. Let's implement one-shot prompting using the code snippet.\n",
                "\n",
                "In the `get_prompt` function, we modify the prompt to include a single example question and answer before the actual question. This example helps the model infer the expected response format. We then follow the same process as before to generate responses and calculate accuracy.\n",
                "\n",
                "```python\n",
                "def get_one_shot_prompt(question):\n",
                "    return (\n",
                "        \"Answer each question with a short, direct factual answer.\\n\\n\"\n",
                "        \"Q: Who directed the movie La Dolce Vita?\\n\"\n",
                "        \"A: Federico Fellini\\n\\n\"\n",
                "        f\"Q: {question}\\nA:\"\n",
                "    )\n",
                "\n",
                "correct = 0\n",
                "for q in qa_pairs:\n",
                "    prompt = get_one_shot_prompt(q['question'])\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    ).choices[0].message.content.strip()\n",
                "    if normalize(response) == normalize(q['answer']):\n",
                "        correct += 1\n",
                "\n",
                "print(f\"One-shot Accuracy: {correct}/{len(qa_pairs)}\")\n",
                "```\n",
                "\n",
                "The output will show the accuracy of the model in one-shot mode. One-shot prompting can improve performance by providing a clear example, but it may still be limited by the single context provided.\n",
                "\n",
                "### Implementing Few-shot Prompting\n",
                "\n",
                "Few-shot prompting extends the concept of one-shot by providing multiple examples before the question. This approach gives the model more context and can significantly enhance its performance. Let's see how to implement few-shot prompting.\n",
                "\n",
                "In the `get_prompt` function, we include multiple example question-answer pairs before the actual question. This additional context helps the model better understand the task and generate more accurate responses. We then calculate the accuracy as before.\n",
                "\n",
                "```python\n",
                "def get_few_shot_prompt(question):\n",
                "    return (\n",
                "        \"Answer each question with a short, direct factual answer.\\n\\n\"\n",
                "        \"Q: Who directed the movie La Dolce Vita?\\n\"\n",
                "        \"A: Federico Fellini\\n\"\n",
                "        \"Q: What is Bruce Willis' real first name?\\n\"\n",
                "        \"A: Walter\\n\"\n",
                "        \"Q: Which country does the airline Air Pacific come from?\\n\"\n",
                "        \"A: Fiji\\n\\n\"\n",
                "        f\"Q: {question}\\nA:\"\n",
                "    )\n",
                "\n",
                "correct = 0\n",
                "for q in qa_pairs:\n",
                "    prompt = get_few_shot_prompt(q['question'])\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    ).choices[0].message.content.strip()\n",
                "    if normalize(response) == normalize(q['answer']):\n",
                "        correct += 1\n",
                "\n",
                "print(f\"Few-shot Accuracy: {correct}/{len(qa_pairs)}\")\n",
                "```\n",
                "\n",
                "The output will display the accuracy of the model in few-shot mode. Few-shot prompting is powerful because it provides rich context, but it requires more computational resources due to the additional examples.\n",
                "\n",
                "### Comparing Prompting Strategies\n",
                "\n",
                "Now that we have implemented zero-shot, one-shot, and few-shot prompting, let's compare their performance. By analyzing the accuracy results from each mode, we can see how the amount of context provided affects the model's ability to generate correct answers.\n",
                "\n",
                "  * Zero-shot prompting is quick and simple but may lack accuracy due to the absence of context.\n",
                "  * One-shot prompting provides a single example, which can improve performance but may still be limited.\n",
                "  * Few-shot prompting offers multiple examples, leading to better accuracy but at the cost of increased computational resources.\n",
                "\n",
                "In real-world applications, the choice of prompting style depends on the specific task and available resources. For tasks requiring high accuracy, few-shot prompting may be preferred, while zero-shot or one-shot prompting may be suitable for tasks with limited resources or time constraints.\n",
                "\n",
                "### Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, we explored different prompting styles and their impact on LLM performance in QA tasks. We implemented zero-shot, one-shot, and few-shot prompting using the TriviaQA dataset and compared their effectiveness. By understanding these strategies, you can optimize LLM performance for various applications.\n",
                "\n",
                "As you move forward, practice these concepts with the exercises provided. Experiment with different prompting styles and observe how they affect model performance. This hands-on experience will reinforce your understanding and prepare you for more advanced evaluation techniques in future lessons. Remember, mastering prompting styles is key to enhancing LLM capabilities and achieving better results in QA tasks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Your First Zero Shot Prompt\n",
                "\n",
                "Now that we've learned about different prompting styles, let's start by implementing the simplest one: zero-shot prompting. In the lesson, we saw how zero-shot prompting tests a model's ability to answer questions without any examples.\n",
                "\n",
                "Your task is to complete the get_zero_shot_prompt() function that formats questions for zero-shot prompting. The function should take a question as input and return a properly formatted string that includes the question and an instruction to \"Provide a short, direct answer.\"\n",
                "\n",
                "Make sure your function:\n",
                "\n",
                "Correctly inserts the question into the prompt\n",
                "Includes the proper line break between the question and instruction\n",
                "Returns the exact format shown in the example\n",
                "This exercise will help you understand how to structure basic prompts for language models, which is the foundation for all the prompting styles we'll explore in this unit.\n",
                "\n",
                "```python\n",
                "def get_zero_shot_prompt(question):\n",
                "    # TODO: Return a string that includes the question and the instruction\n",
                "    # Format: \"Question: {question}\\n Provide a short, direct answer.\"\n",
                "    return \"\"\n",
                "\n",
                "# Test the function with a sample question\n",
                "sample_question = \"What is the capital of Japan?\"\n",
                "prompt = get_zero_shot_prompt(sample_question)\n",
                "print(\"Generated prompt:\")\n",
                "print(prompt)\n",
                "\n",
                "# Expected output:\n",
                "# Question: What is the capital of Japan?\n",
                "# Provide a short, direct answer.\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "def get_zero_shot_prompt(question):\n",
                "    # TODO: Return a string that includes the question and the instruction\n",
                "    # Format: \"Question: {question}\\n Provide a short, direct answer.\"\n",
                "    return f\"Question: {question}\\n Provide a short, direct answer.\"\n",
                "\n",
                "# Test the function with a sample question\n",
                "sample_question = \"What is the capital of Japan?\"\n",
                "prompt = get_zero_shot_prompt(sample_question)\n",
                "print(\"Generated prompt:\")\n",
                "print(prompt)\n",
                "\n",
                "# Expected output:\n",
                "# Question: What is the capital of Japan?\n",
                "# Provide a short, direct answer.\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Your First One Shot Prompt\n",
                "\n",
                "Excellent work on implementing your first zero-shot prompt! Now let's move on to the next level: one-shot prompting. As we learned in the lesson, one-shot prompting provides the model with a single example to learn from before asking it to answer a new question.\n",
                "\n",
                "Your task is to complete the get_one_shot_prompt() function that takes a question as input and returns a properly formatted prompt string. The prompt should include one example Q&A pair about the capital of France, followed by the actual question.\n",
                "\n",
                "When implementing your function, make sure to:\n",
                "\n",
                "Include the example \"Q: What is the capital of France?\" and \"A: Paris\"\n",
                "Add proper spacing between the example and your actual question\n",
                "Format the question with \"Q:\" and \"A:\" prefixes\n",
                "Follow the exact format shown in the lesson\n",
                "This exercise will help you understand how providing even a single example can significantly improve a model's performance by giving it context about what you're asking.\n",
                "\n",
                "```python\n",
                "def get_one_shot_prompt(question):\n",
                "    return(  \n",
                "    \"Answer each question with a short, direct factual answer.\\n\\n\"\n",
                "    # TODO: Return a string that includes one example Q&A pair followed by the actual question\n",
                "    # The example should be about the capital of France\n",
                "    # Format: \"Q: What is the capital of France?\\nA: Paris\\n\\nQ: {question}\\nA:\"\n",
                "    # Make sure to include proper spacing between the example and the question\n",
                "    )\n",
                "\n",
                "# Test the function with a sample question\n",
                "sample_question = \"What is the capital of Japan?\"\n",
                "prompt = get_one_shot_prompt(sample_question)\n",
                "print(\"Generated prompt:\")\n",
                "print(prompt)\n",
                "\n",
                "# Expected output:\n",
                "# Answer each question with a short, direct factual answer.\n",
                "# Q: What is the capital of France?\n",
                "# A: Paris\n",
                "#\n",
                "# Q: What is the capital of Japan?\n",
                "# A:\n",
                "```\n",
                "\n",
                "```python\n",
                "def get_one_shot_prompt(question):\n",
                "    return(\n",
                "    \"Answer each question with a short, direct factual answer.\\n\\n\"\n",
                "    # TODO: Return a string that includes one example Q&A pair followed by the actual question\n",
                "    # The example should be about the capital of France\n",
                "    # Format: \"Q: What is the capital of France?\\nA: Paris\\n\\nQ: {question}\\nA:\"\n",
                "    # Make sure to include proper spacing between the example and the question\n",
                "    \"Q: What is the capital of France?\\n\"\n",
                "    \"A: Paris\\n\\n\"\n",
                "    f\"Q: {question}\\n\"\n",
                "    \"A:\"\n",
                "    )\n",
                "\n",
                "# Test the function with a sample question\n",
                "sample_question = \"What is the capital of Japan?\"\n",
                "prompt = get_one_shot_prompt(sample_question)\n",
                "print(\"Generated prompt:\")\n",
                "print(prompt)\n",
                "\n",
                "# Expected output:\n",
                "# Answer each question with a short, direct factual answer.\n",
                "#\n",
                "# Q: What is the capital of France?\n",
                "# A: Paris\n",
                "#\n",
                "# Q: What is the capital of Japan?\n",
                "# A:\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Making Accuracy Functions More Flexible"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementing Few Shot Prompting"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementing Few Shot Prompting"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
