{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction to LLM Benchmarking & Basic QA Evaluation\n",
                "\n",
                "# Introduction to LLM Benchmarking\n",
                "\n",
                "-----\n",
                "\n",
                "Welcome to the first lesson of our course, \"Benchmarking LLMs with QA.\" In this lesson, we will explore the fundamentals of benchmarking large language models (LLMs). **Benchmarking** is the process of evaluating the performance of a system or component by comparing it against a set of predefined standards or datasets. It is crucial in understanding how well a model performs in various tasks, identifying its strengths and weaknesses, and guiding improvements.\n",
                "\n",
                "## Why Benchmarking?\n",
                "\n",
                "Benchmarking is essential for the development and refinement of LLMs, as it provides a systematic way to measure their capabilities and progress over time. By using standardized datasets and evaluation metrics, we can objectively assess the performance of different models and make informed decisions about their deployment and further development.\n",
                "\n",
                "Some common types of LLM benchmarks include:\n",
                "\n",
                "  * **Factual QA** (like TriviaQA, SQuAD)\n",
                "  * **Multiple-choice reasoning** (like MMLU, ARC)\n",
                "  * **Truthfulness & bias detection** (like TruthfulQA)\n",
                "  * **Perplexity-based evaluation** (language fluency prediction)\n",
                "  * **Semantic similarity** (embedding-based matching)\n",
                "  * **Domain-specific tests** (custom internal benchmarks)\n",
                "\n",
                "In this course, we’ll begin with factual QA benchmarks before expanding to other types in later lessons.\n",
                "\n",
                "## Working with the TriviaQA Dataset\n",
                "\n",
                "We will use the **TriviaQA** dataset, which contains a large collection of real-world question-answer pairs gathered from trivia websites. While TriviaQA is not a multiple-choice dataset, it is well-suited for evaluating factual question-answering capabilities.\n",
                "\n",
                "For simplicity and performance, we’ve pre-selected and stored a 100-example subset for you, available at:\n",
                "\n",
                "`triviaqa.csv`\n",
                "\n",
                "This subset contains pairs of factual questions and short answers. Here are a few sample entries from the dataset:\n",
                "\n",
                "```text\n",
                "Question: What is the capital of France?\n",
                "Answer: Paris\n",
                "\n",
                "Question: In which year did the Titanic sink?\n",
                "Answer: 1912\n",
                "```\n",
                "\n",
                "## Setting Up the Environment\n",
                "\n",
                "Before we dive into the code, let's ensure that your environment is ready. For this lesson, you will need the `openai` and `csv` libraries. If you are working on your local machine, you can install the `openai` library using pip:\n",
                "\n",
                "```bash\n",
                "pip install openai\n",
                "```\n",
                "\n",
                "The `csv` module is part of Python's standard library, so no additional installation is needed. However, if you are using the CodeSignal environment, these libraries are already pre-installed, so you can focus on the lesson without worrying about setup.\n",
                "\n",
                "## Loading and Understanding the TriviaQA Dataset\n",
                "\n",
                "To load the dataset, we’ll use Python’s built-in `csv` module. Here is how you can read it:\n",
                "\n",
                "```python\n",
                "import csv\n",
                "\n",
                "with open(\"triviaqa.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "```\n",
                "\n",
                "This code opens the `triviaqa.csv` file and reads its contents into a list of dictionaries, where each dictionary represents a question-answer pair. Understanding the structure of this dataset is crucial, as it will be the basis for our evaluation.\n",
                "\n",
                "## Implementing Normalized Match Evaluation\n",
                "\n",
                "To evaluate the performance of an LLM, we will use a technique called **normalized match**. This involves comparing the model's response to the correct answer by normalizing both texts. Normalization helps in removing any discrepancies due to case sensitivity or punctuation.\n",
                "\n",
                "Let's look at the code that implements this evaluation:\n",
                "\n",
                "```python\n",
                "import re\n",
                "from openai import OpenAI\n",
                "\n",
                "# Initialize the OpenAI client\n",
                "client = OpenAI()\n",
                "\n",
                "def normalize(text):\n",
                "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
                "\n",
                "correct = 0\n",
                "for q in qa_pairs:\n",
                "    prompt = f\"Answer the following question with a short and direct fact:\\n\\n{q['question']}\"\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    ).choices[0].message.content.strip()\n",
                "    if normalize(q['answer']) == normalize(response):\n",
                "        correct += 1\n",
                "```\n",
                "\n",
                "In this code, the `normalize` function removes all non-alphanumeric characters and converts the text to lowercase. This ensures that the comparison between the model's response and the correct answer is fair and consistent. We then iterate over each question-answer pair, generate a response using the `openai` library, and compare the normalized texts.\n",
                "\n",
                "## Example: Calculating Normalized Accuracy\n",
                "\n",
                "In this lesson, we introduced the concept of LLM benchmarking and demonstrated how to evaluate a language model using the TriviaQA dataset. We covered the setup of the environment, loading the dataset, and implementing a normalized match evaluation. However, we will not calculate the normalized accuracy just yet. The results with the current setup may not be optimal, but in the next unit, we will explore techniques like one-shot or few-shot learning to improve the model's performance. By doing so, you will gain a deeper understanding of how to enhance LLM capabilities through advanced evaluation techniques.\n",
                "\n",
                "## Summary and Next Steps\n",
                "\n",
                "In this lesson, we introduced the concept of LLM benchmarking and demonstrated how to evaluate a language model using the TriviaQA dataset. We covered the setup of the environment, loading the dataset, and implementing a normalized match evaluation. Although we have not calculated the normalized accuracy yet, we will explore more advanced techniques in the next unit to improve the model's performance. As you move forward, practice these concepts with the exercises provided. These will reinforce your understanding and prepare you for more advanced evaluation techniques in future lessons. Remember, benchmarking is a powerful tool in improving language models, and mastering it will enhance your skills in working with LLMs.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading and Exploring TriviaQA Dataset\n",
                "\n",
                "Now that you've learned about the importance of benchmarking LLMs, let's get hands-on with the TriviaQA dataset! In this exercise, you'll take your first step toward implementing a benchmark by loading and exploring the dataset.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Open the TriviaQA dataset file at \"triviaqa.csv\".\n",
                "Load the data using Python's csv module as a list of dictionaries.\n",
                "Print the first three question-answer pairs in a readable format.\n",
                "This practical experience with dataset handling will build the foundation for the evaluation techniques we'll explore next. Understanding your benchmark data is the first crucial step in any effective LLM evaluation process.\n",
                "\n",
                "```python\n",
                "# Loading and exploring the TriviaQA dataset\n",
                "\n",
                "import csv\n",
                "\n",
                "# TODO: Open the TriviaQA dataset file and read it using csv.DictReader\n",
                "# The file is located at \"triviaqa.csv\"\n",
                "\n",
                "\n",
                "# TODO: Print the first 3 question-answer pairs from the dataset\n",
                "# Format each pair as \"Question: [question]\" and \"Answer: [answer]\"\n",
                "# Add an empty line between pairs for better readability\n",
                "\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "# Loading and exploring the TriviaQA dataset\n",
                "\n",
                "import csv\n",
                "\n",
                "# Open the TriviaQA dataset file and read it using csv.DictReader\n",
                "with open(\"triviaqa.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "\n",
                "# Print the first 3 question-answer pairs from the dataset\n",
                "# Format each pair as \"Question: [question]\" and \"Answer: [answer]\"\n",
                "# Add an empty line between pairs for better readability\n",
                "for i in range(3):\n",
                "    print(f\"Question: {qa_pairs[i]['question']}\")\n",
                "    print(f\"Answer: {qa_pairs[i]['answer']}\\n\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Text Normalization for Fair Comparisons\n",
                "\n",
                "Now that you've successfully loaded the TriviaQA dataset, let's dive into a key component of LLM evaluation: text normalization! In benchmarking, we need to compare model outputs with reference answers fairly, regardless of capitalization or punctuation differences.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Review the provided normalize function, which converts text to lowercase and removes non-alphanumeric characters.\n",
                "Test this function with the provided example strings that contain various formatting styles.\n",
                "Print both the original and normalized versions of each test string.\n",
                "Observe how different text formats are standardized to a common form.\n",
                "Understanding text normalization will help you build more robust evaluation systems that focus on content rather than formatting when comparing LLM outputs to reference answers.\n",
                "\n",
                "```python\n",
                "# Implementing and testing the normalize function for LLM evaluation\n",
                "\n",
                "import re\n",
                "\n",
                "# TODO: Review this normalize function and make sure it correctly converts text \n",
                "# to lowercase and removes all non-alphanumeric characters\n",
                "def normalize(text):\n",
                "    \"\"\"\n",
                "    Normalizes text by converting to lowercase and removing all non-alphanumeric characters.\n",
                "    This helps in fair comparison of LLM outputs with reference answers.\n",
                "    \"\"\"\n",
                "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
                "\n",
                "# Test cases with varied punctuation, spaces, and capitalization\n",
                "test_strings = [\n",
                "    \"Paris\",\n",
                "    \"Paris, France\",\n",
                "    \"PARIS\",\n",
                "    \"paris\",\n",
                "    \"1912\",\n",
                "    \"Year 1912!\",\n",
                "    \"Albert Einstein\",\n",
                "    \"Albert   Einstein\",\n",
                "    \"albert-einstein\",\n",
                "    \"The Pacific Ocean\",\n",
                "    \"42.195 kilometers\",\n",
                "    \"H2O (water)\"\n",
                "]\n",
                "\n",
                "# TODO: Implement code to test the normalize function with each string\n",
                "# For each test string, print both the original and normalized versions\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "# Implementing and testing the normalize function for LLM evaluation\n",
                "\n",
                "import re\n",
                "\n",
                "# TODO: Review this normalize function and make sure it correctly converts text\n",
                "# to lowercase and removes all non-alphanumeric characters\n",
                "def normalize(text):\n",
                "    \"\"\"\n",
                "    Normalizes text by converting to lowercase and removing all non-alphanumeric characters.\n",
                "    This helps in fair comparison of LLM outputs with reference answers.\n",
                "    \"\"\"\n",
                "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
                "\n",
                "# Test cases with varied punctuation, spaces, and capitalization\n",
                "test_strings = [\n",
                "    \"Paris\",\n",
                "    \"Paris, France\",\n",
                "    \"PARIS\",\n",
                "    \"paris\",\n",
                "    \"1912\",\n",
                "    \"Year 1912!\",\n",
                "    \"Albert Einstein\",\n",
                "    \"Albert   Einstein\",\n",
                "    \"albert-einstein\",\n",
                "    \"The Pacific Ocean\",\n",
                "    \"42.195 kilometers\",\n",
                "    \"H2O (water)\"\n",
                "]\n",
                "\n",
                "# TODO: Implement code to test the normalize function with each string\n",
                "# For each test string, print both the original and normalized versions\n",
                "\n",
                "print(\"--- Text Normalization Examples ---\")\n",
                "print(\"Original Text                      | Normalized Text\")\n",
                "print(\"-----------------------------------|------------------\")\n",
                "\n",
                "for s in test_strings:\n",
                "    normalized_s = normalize(s)\n",
                "    # Using ljust for alignment\n",
                "    print(f\"{s.ljust(35)}| {normalized_s}\")\n",
                "\n",
                "print(\"-----------------------------------|------------------\")\n",
                "print(\"\\nObservations:\")\n",
                "print(\"1. All characters are converted to lowercase.\")\n",
                "print(\"2. Punctuation (commas, exclamation marks, parentheses, hyphens) and spaces are removed.\")\n",
                "print(\"3. Only alphanumeric characters (a-z, 0-9) remain.\")\n",
                "print(\"4. This process standardizes different variations of the same answer, allowing for fair comparison.\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Answers Beyond Surface Formatting\n",
                "\n",
                "Now that you've explored text normalization with test strings, let's put this knowledge into practice! In this exercise, you'll see how normalization helps us accurately compare answers that look different but contain the same information.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Observe two strings that represent the same answer but with different formatting (one as an expected answer and one as a model's response).\n",
                "Apply the normalize function to both strings.\n",
                "Compare the normalized versions to check if they match.\n",
                "Print the original strings, their normalized forms, and the comparison result.\n",
                "This hands-on practice with string comparison demonstrates why normalization is essential for fair LLM evaluation — it helps us focus on the content rather than superficial differences in formatting when determining if an answer is correct.\n",
                "\n",
                "```python\n",
                "# Comparing answers using text normalization\n",
                "\n",
                "import re\n",
                "\n",
                "def normalize(text):\n",
                "    \"\"\"\n",
                "    Normalizes text by converting to lowercase and removing all non-alphanumeric characters.\n",
                "    This helps in fair comparison of LLM outputs with reference answers.\n",
                "    \"\"\"\n",
                "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
                "\n",
                "# Define the expected answer and model response\n",
                "expected_answer = \"Albert Einstein born in 1879, Germany\"\n",
                "model_response = \"ALBERT EINSTEIN (Born in 1879, Germany)\"\n",
                "\n",
                "# TODO: Normalize both strings using the normalize function\n",
                "\n",
                "\n",
                "# TODO: Compare the normalized strings and store the result in a boolean variable\n",
                "\n",
                "\n",
                "# TODO: Print the original strings, their normalized versions, and whether they match\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "# Comparing answers using text normalization\n",
                "\n",
                "import re\n",
                "\n",
                "def normalize(text):\n",
                "    \"\"\"\n",
                "    Normalizes text by converting to lowercase and removing all non-alphanumeric characters.\n",
                "    This helps in fair comparison of LLM outputs with reference answers.\n",
                "    \"\"\"\n",
                "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
                "\n",
                "# Define the expected answer and model response\n",
                "expected_answer = \"Albert Einstein born in 1879, Germany\"\n",
                "model_response = \"ALBERT EINSTEIN (Born in 1879, Germany)\"\n",
                "\n",
                "# TODO: Normalize both strings using the normalize function\n",
                "normalized_expected = normalize(expected_answer)\n",
                "normalized_response = normalize(model_response)\n",
                "\n",
                "# TODO: Compare the normalized strings and store the result in a boolean variable\n",
                "answers_match = normalized_expected == normalized_response\n",
                "\n",
                "# TODO: Print the original strings, their normalized versions, and whether they match\n",
                "print(\"--- Answer Comparison with Normalization ---\")\n",
                "print(f\"Original Expected Answer: '{expected_answer}'\")\n",
                "print(f\"Original Model Response:  '{model_response}'\")\n",
                "print(\"-\" * 40)\n",
                "print(f\"Normalized Expected: '{normalized_expected}'\")\n",
                "print(f\"Normalized Response: '{normalized_response}'\")\n",
                "print(\"-\" * 40)\n",
                "print(f\"Do the normalized answers match? {answers_match}\")\n",
                "print(\"\\nConclusion:\")\n",
                "print(\"Although the original strings look different due to capitalization, punctuation, and wording,\")\n",
                "print(\"their normalized forms are identical. This demonstrates that text normalization is crucial\")\n",
                "print(\"for evaluating LLM outputs fairly, as it allows us to correctly identify answers that are\")\n",
                "print(\"semantically the same, regardless of minor formatting variations.\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating a Single LLM Response\n",
                "\n",
                "Excellent work with text normalization! Now let's take the next step and apply what you've learned to evaluate an actual LLM response. In this exercise, you'll work with a real model to see how your normalization techniques help in fair answer comparison.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Load the first three questions from the TriviaQA dataset.\n",
                "Send each question to an LLM using the OpenAI API.\n",
                "Compare the model's response with the expected answer using normalization.\n",
                "Print the results to see how well the model performed.\n",
                "This hands-on experience with a real LLM will show you how the evaluation techniques we've been discussing work in practice, bringing you one step closer to building complete benchmarking systems.\n",
                "\n",
                "```python\n",
                "# Evaluating a single LLM response against a reference answer\n",
                "\n",
                "import csv\n",
                "import re\n",
                "from openai import OpenAI\n",
                "\n",
                "def normalize(text):\n",
                "    \"\"\"\n",
                "    Normalizes text by converting to lowercase and removing all non-alphanumeric characters.\n",
                "    This helps in fair comparison of LLM outputs with reference answers.\n",
                "    \"\"\"\n",
                "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
                "\n",
                "# Initialize the OpenAI client\n",
                "client = OpenAI()\n",
                "\n",
                "# Load the first three questions from the TriviaQA dataset\n",
                "with open(\"triviaqa.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "    questions_data = qa_pairs[:3]  # Take the first three questions\n",
                "\n",
                "for question_data in questions_data:\n",
                "    question = question_data['question']\n",
                "    expected_answer = question_data['answer']\n",
                "\n",
                "    # TODO: Create a prompt for the LLM that includes the question\n",
                "    prompt = \"\"\n",
                "\n",
                "    # TODO: Send the prompt to the OpenAI API and get the response\n",
                "    response = \"\"\n",
                "\n",
                "    # TODO: Normalize both the expected answer and the model's response\n",
                "\n",
                "    # TODO: Check if the normalized texts match\n",
                "\n",
                "    # TODO: Print the results including the question, expected answer, model response,\n",
                "    # normalized versions of both, and whether they match\n",
                "```\n",
                "\n",
                "Unfortunately, I am unable to directly use the OpenAI API to complete this task. The provided code requires an API key, which is a private credential, and I do not have the ability to access or use external APIs with personal authentication.\n",
                "\n",
                "However, I can still provide you with the completed code structure that you can run yourself once you have your API key configured. The code below shows how to integrate the normalization function with an API call to a hypothetical LLM and then compare the results.\n",
                "\n",
                "You would need to install the OpenAI Python library (`pip install openai`) and then set up your API key, for example, by setting an environment variable named `OPENAI_API_KEY`.\n",
                "\n",
                "```python\n",
                "# Evaluating a single LLM response against a reference answer\n",
                "\n",
                "import csv\n",
                "import re\n",
                "from openai import OpenAI\n",
                "\n",
                "def normalize(text):\n",
                "    \"\"\"\n",
                "    Normalizes text by converting to lowercase and removing all non-alphanumeric characters.\n",
                "    This helps in fair comparison of LLM outputs with reference answers.\n",
                "    \"\"\"\n",
                "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
                "\n",
                "# Initialize the OpenAI client\n",
                "# Ensure your OPENAI_API_KEY environment variable is set\n",
                "client = OpenAI()\n",
                "\n",
                "# Load the first three questions from the TriviaQA dataset\n",
                "with open(\"triviaqa.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "    questions_data = qa_pairs[:3]  # Take the first three questions\n",
                "\n",
                "print(\"--- LLM Evaluation Results ---\")\n",
                "for question_data in questions_data:\n",
                "    question = question_data['question']\n",
                "    expected_answer = question_data['answer']\n",
                "\n",
                "    # Create a prompt for the LLM that includes the question\n",
                "    # The model parameter might need to be adjusted based on the latest available models\n",
                "    prompt = f\"Answer the following question concisely: {question}\"\n",
                "\n",
                "    # Send the prompt to the OpenAI API and get the response\n",
                "    # We use a try-except block to handle potential API errors\n",
                "    try:\n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",  # Or another suitable model\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "                {\"role\": \"user\", \"content\": prompt}\n",
                "            ]\n",
                "        )\n",
                "        model_response = completion.choices[0].message.content\n",
                "    except Exception as e:\n",
                "        model_response = f\"API Error: {e}\"\n",
                "\n",
                "    # Normalize both the expected answer and the model's response\n",
                "    normalized_expected = normalize(expected_answer)\n",
                "    normalized_response = normalize(model_response)\n",
                "\n",
                "    # Check if the normalized texts match\n",
                "    answers_match = normalized_expected == normalized_response\n",
                "\n",
                "    # Print the results\n",
                "    print(\"-\" * 40)\n",
                "    print(f\"Question: {question}\")\n",
                "    print(f\"Expected Answer: {expected_answer}\")\n",
                "    print(f\"Model Response: {model_response}\")\n",
                "    print(f\"Normalized Expected: {normalized_expected}\")\n",
                "    print(f\"Normalized Response: {normalized_response}\")\n",
                "    print(f\"Match: {'Yes' if answers_match else 'No'}\")\n",
                "\n",
                "```\n",
                "\n",
                "This video explains how to get your own OpenAI API key, which is necessary to run the provided code."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
