{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Improving Evaluation: Fuzzy Answer Matching\n",
                "\n",
                "# Introduction to Evaluation Challenges\n",
                "\n",
                "Welcome back\\! In the previous lesson, we explored different prompting styles and their impact on the performance of large language models (LLMs) in question-answering (QA) tasks. We learned how zero-shot, one-shot, and few-shot prompting can influence the accuracy of model responses. As a reminder, these prompting styles help provide context to the model, which can significantly affect its ability to generate accurate answers.\n",
                "\n",
                "In this lesson, we will address a common challenge in evaluating QA systems: the limitations of exact match evaluation. Often, correct answers are not counted due to minor variations in wording or phrasing. For example, if the expected answer is \"New York City\" and the model responds with \"NYC,\" an exact match evaluation would mark this as incorrect, even though the response is valid. To overcome this, we will introduce the concept of **fuzzy matching**, which allows for more flexible and reliable evaluation by considering the similarity between responses.\n",
                "\n",
                "For this unit, you will not need to call the model, as we will prepare the results for you. Your task will be to evaluate these results using the techniques discussed.\n",
                "\n",
                "## Understanding Similarity Scoring\n",
                "\n",
                "**Similarity scoring** is a technique used to measure how closely two pieces of text resemble each other. This approach is particularly useful in QA evaluations, where minor variations in wording can lead to incorrect assessments. By using similarity scoring, we can increase the reliability of our evaluations and ensure that valid responses are recognized.\n",
                "\n",
                "In Python, one of the tools we can use for similarity scoring is the `SequenceMatcher` from the `difflib` library. This tool compares two strings and returns a ratio indicating their similarity. A ratio closer to 1 means the strings are very similar, while a ratio closer to 0 indicates they are quite different. By setting a threshold, we can determine what level of similarity is acceptable for considering two responses as equivalent.\n",
                "\n",
                "## Implementing Fuzzy Matching in Python\n",
                "\n",
                "Let's dive into the implementation of fuzzy matching using Python. We will use the `SequenceMatcher` from the `difflib` library to compare the model's response with the expected answer. Additionally, we'll include a simple visualization to represent the fuzzy accuracy as a percentage bar. Here's the code snippet from the `solution.py` file:\n",
                "\n",
                "```python\n",
                "from difflib import SequenceMatcher\n",
                "import csv\n",
                "\n",
                "def is_similar(a, b, threshold=0.75):\n",
                "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > threshold\n",
                "\n",
                "def create_visual_bar(percentage, max_length=20):\n",
                "    \"\"\"Create a simple visual bar to represent a percentage.\"\"\"\n",
                "    filled_length = int(percentage / 100 * max_length)\n",
                "    bar = 'â–ˆ' * filled_length + 'â–‘' * (max_length - filled_length)\n",
                "    return bar\n",
                "\n",
                "with open(\"triviaqa.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "\n",
                "correct = 0\n",
                "for q in qa_pairs:\n",
                "    response = q['model_response']  # Pre-prepared model response\n",
                "    if is_similar(response, q['answer']):\n",
                "        correct += 1\n",
                "\n",
                "fuzzy_accuracy = (correct / len(qa_pairs)) * 100\n",
                "print(f\"Fuzzy Accuracy: {correct}/{len(qa_pairs)}\")\n",
                "print(f\"Visual Representation: {create_visual_bar(fuzzy_accuracy)}\")\n",
                "```\n",
                "\n",
                "In this code, we define a function `is_similar` that takes two strings, `a` and `b`, and a similarity `threshold`. The function uses `SequenceMatcher` to calculate the similarity ratio between the two strings, ignoring case differences. If the ratio exceeds the threshold, the function returns `True`, indicating that the strings are similar enough to be considered equivalent. We then iterate over the question-answer pairs from the `TriviaQA` dataset, using the pre-prepared model responses, and use the `is_similar` function to evaluate the response. The fuzzy accuracy is calculated by counting the number of similar responses.\n",
                "\n",
                "To visually represent the fuzzy accuracy, we use the `create_visual_bar` function, which generates a simple bar chart to illustrate the percentage of correct responses. This visual aid helps in quickly assessing the model's performance.\n",
                "\n",
                "## Example: Evaluating Trivia QA with Fuzzy Matching\n",
                "\n",
                "Let's walk through an example of evaluating the `TriviaQA` dataset using fuzzy matching. Suppose we have a question-answer pair where the question is \"What is the capital of France?\" and the expected answer is \"Paris.\" If the model responds with \"The capital of France is Paris,\" an exact match evaluation would mark this as incorrect. However, using fuzzy matching, the `is_similar` function would likely return `True`, as the response is sufficiently similar to the expected answer.\n",
                "\n",
                "By running the provided code, you will see an output indicating the fuzzy accuracy of the model. This metric reflects the number of responses that were considered similar to the expected answers, providing a more reliable assessment of the model's performance.\n",
                "\n",
                "## Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, we addressed the limitations of exact match evaluation in QA systems and introduced the concept of fuzzy matching. We explored how similarity scoring can improve evaluation reliability by considering variations in phrasing. By implementing fuzzy matching in Python, we demonstrated how to evaluate model responses more accurately.\n",
                "\n",
                "As you move forward, practice these concepts with the exercises provided. Experiment with different similarity thresholds and observe how they affect the evaluation accuracy. This hands-on experience will reinforce your understanding and prepare you for more advanced evaluation techniques in future lessons. Remember, mastering fuzzy matching is key to enhancing the reliability of QA evaluations and achieving better results in real-world applications."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building Your First Fuzzy Matcher\n",
                "\n",
                "Now that we've learned about the limitations of exact match evaluation, let's build our first fuzzy matching function! In this exercise, you'll implement the core component that makes flexible answer evaluation possible.\n",
                "\n",
                "Your task is to complete the is_similar function using Python's SequenceMatcher. This function needs to:\n",
                "\n",
                "Convert both input strings to lowercase.\n",
                "Calculate the similarity ratio between them.\n",
                "Return True if the ratio exceeds the threshold; False otherwise.\n",
                "The test cases included will show you how this function behaves with different types of string pairs â€” from abbreviations to reordered sentences. Pay attention to the similarity ratios that get printed, as they'll help you understand the nuances of fuzzy matching and why it's so valuable for QA evaluation.\n",
                "\n",
                "Mastering this function is your first step toward building more reliable evaluation systems that can recognize correct answers even when they're phrased differently!\n",
                "\n",
                "```python\n",
                "from difflib import SequenceMatcher\n",
                "\n",
                "def is_similar(a, b, threshold=0.75):\n",
                "    # TODO: Use SequenceMatcher to calculate the similarity ratio between \n",
                "    # the lowercase versions of strings a and b, then return True if the \n",
                "    # ratio exceeds the threshold, False otherwise\n",
                "\n",
                "# Test cases\n",
                "test_cases = [\n",
                "    (\"New York City\", \"NYC\", \"City abbreviation\"),\n",
                "    (\"apple\", \"apples\", \"Singular vs plural\"),\n",
                "    (\"The president lives in the White House\", \"The White House is where the president lives\", \"Reordered sentence\"),\n",
                "    (\"cat\", \"dog\", \"Different words\"),\n",
                "    (\"Python programming\", \"Python coding\", \"Similar concept, different wording\")\n",
                "]\n",
                "\n",
                "print(\"Testing fuzzy matching with different string pairs:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for str1, str2, description in test_cases:\n",
                "    similarity = SequenceMatcher(None, str1.lower(), str2.lower()).ratio()\n",
                "    result = is_similar(str1, str2)\n",
                "    \n",
                "    print(f\"Test: {description}\")\n",
                "    print(f\"String 1: '{str1}'\")\n",
                "    print(f\"String 2: '{str2}'\")\n",
                "    print(f\"Similarity ratio: {similarity:.4f}\")\n",
                "    print(f\"Similar enough (threshold={0.75})? {result}\")\n",
                "    print(\"-\" * 60)\n",
                "```\n",
                "\n",
                "```python\n",
                "from difflib import SequenceMatcher\n",
                "\n",
                "def is_similar(a, b, threshold=0.75):\n",
                "    # TODO: Use SequenceMatcher to calculate the similarity ratio between \n",
                "    # the lowercase versions of strings a and b, then return True if the \n",
                "    # ratio exceeds the threshold, False otherwise\n",
                "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > threshold\n",
                "\n",
                "# Test cases\n",
                "test_cases = [\n",
                "    (\"New York City\", \"NYC\", \"City abbreviation\"),\n",
                "    (\"apple\", \"apples\", \"Singular vs plural\"),\n",
                "    (\"The president lives in the White House\", \"The White House is where the president lives\", \"Reordered sentence\"),\n",
                "    (\"cat\", \"dog\", \"Different words\"),\n",
                "    (\"Python programming\", \"Python coding\", \"Similar concept, different wording\")\n",
                "]\n",
                "\n",
                "print(\"Testing fuzzy matching with different string pairs:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for str1, str2, description in test_cases:\n",
                "    similarity = SequenceMatcher(None, str1.lower(), str2.lower()).ratio()\n",
                "    result = is_similar(str1, str2)\n",
                "    \n",
                "    print(f\"Test: {description}\")\n",
                "    print(f\"String 1: '{str1}'\")\n",
                "    print(f\"String 2: '{str2}'\")\n",
                "    print(f\"Similarity ratio: {similarity:.4f}\")\n",
                "    print(f\"Similar enough (threshold={0.75})? {result}\")\n",
                "    print(\"-\" * 60)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating Model Responses with Fuzzy Matching\n",
                "\n",
                "Excellent work on building your fuzzy matching function! Now let's put it to practical use by evaluating actual model responses against expected answers.\n",
                "\n",
                "In this exercise, you'll use your is_similar() function to analyze results from a zero-shot prompting experiment. Your tasks are to:\n",
                "\n",
                "Read the question-answer pairs and model responses from the provided CSV file.\n",
                "Compare each model response to its expected answer using fuzzy matching.\n",
                "Count how many responses are similar enough to be considered correct.\n",
                "Calculate and display the final accuracy score.\n",
                "This hands-on application will show you the real value of fuzzy matching in evaluation scenarios. By the end, you'll see firsthand how this approach provides a fairer assessment of model performance than strict exact matching would!\n",
                "\n",
                "```python\n",
                "from difflib import SequenceMatcher\n",
                "import csv\n",
                "\n",
                "def is_similar(a, b, threshold=0.75):\n",
                "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > threshold\n",
                "\n",
                "# TODO: Open and read the CSV file\n",
                "\n",
                "# TODO: Initialize a counter for correct answers\n",
                "\n",
                "# TODO: Loop through each question-answer pair in the CSV\n",
                "    # TODO: Extract the model response and expected answer\n",
                "    \n",
                "    # TODO: Use the is_similar function to check if the response matches the expected answer\n",
                "    \n",
                "    # TODO: Increment the counter if the answer is correct\n",
                "\n",
                "# TODO: Print the final accuracy score (format: \"Fuzzy Accuracy: X/Y\")\n",
                "```\n",
                "\n",
                "Thank you for providing the specific file name and the contents of the CSV. This is extremely helpful\\!\n",
                "\n",
                "The correct file to open is `results_zero_shot.csv`, not `triviaqa.csv`, and the relevant column names are `model_answer` and `expected_answer`.\n",
                "\n",
                "Here is the corrected code that should pass the evaluation:\n",
                "\n",
                "```python\n",
                "from difflib import SequenceMatcher\n",
                "import csv\n",
                "\n",
                "def is_similar(a, b, threshold=0.75):\n",
                "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > threshold\n",
                "\n",
                "# TODO: Open and read the CSV file\n",
                "with open(\"results_zero_shot.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "\n",
                "# TODO: Initialize a counter for correct answers\n",
                "correct_answers_count = 0\n",
                "\n",
                "# TODO: Loop through each question-answer pair in the CSV\n",
                "for q in qa_pairs:\n",
                "    # TODO: Extract the model response and expected answer\n",
                "    response = q['model_answer']\n",
                "    expected_answer = q['expected_answer']\n",
                "    \n",
                "    # TODO: Use the is_similar function to check if the response matches the expected answer\n",
                "    if is_similar(response, expected_answer):\n",
                "    \n",
                "    # TODO: Increment the counter if the answer is correct\n",
                "        correct_answers_count += 1\n",
                "\n",
                "# TODO: Print the final accuracy score (format: \"Fuzzy Accuracy: X/Y\")\n",
                "total_questions = len(qa_pairs)\n",
                "print(f\"Fuzzy Accuracy: {correct_answers_count}/{total_questions}\")\n",
                "```\n",
                "\n",
                "This corrected code now properly handles the file name and the specific column names present in your provided data, which will resolve the `KeyError`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Finding the Perfect Similarity Threshold\n",
                "\n",
                "Now that you've successfully built and applied your fuzzy matching function, let's take it to the next level! In real-world evaluation scenarios, finding the right similarity threshold is crucial â€” too low, and you'll accept incorrect answers; too high, and you'll reject valid ones.\n",
                "\n",
                "In this exercise, you'll enhance your evaluation system by experimenting with different threshold values. Your tasks are to:\n",
                "\n",
                "Create a function called calculate_accuracy that calculates accuracy using a given threshold. Important: This function must use results loaded from the results_one_shot.csv file.\n",
                "Test a range of thresholds (0.5 to 0.9) on our one-shot prompting results.\n",
                "Visualize how accuracy changes with different thresholds.\n",
                "Observe the trade-off between strictness and flexibility.\n",
                "This exploration will give you practical insight into tuning evaluation systems for different use cases. By the end, you'll understand how to balance precision and recall in your QA evaluations â€” a key skill for anyone working with language models!\n",
                "\n",
                "\n",
                "\n",
                "```python\n",
                "from difflib import SequenceMatcher\n",
                "import csv\n",
                "\n",
                "def is_similar(a, b, threshold=0.75):\n",
                "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > threshold\n",
                "\n",
                "# TODO: Create a function called calculate_accuracy that takes qa_pairs and a threshold\n",
                "# parameter, then returns the number of correct answers and total questions\n",
                "\n",
                "# Open and read the CSV file\n",
                "with open(\"results_one_shot.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "\n",
                "# TODO: Create a loop to test different threshold values (0.5, 0.6, 0.7, 0.8, 0.9)\n",
                "\n",
                "# TODO: For each threshold, calculate and print the accuracy\n",
                "# Format should show threshold value, correct/total count, and percentage\n",
                "\n",
                "# TODO: Add a visual element (like a simple text bar) to help visualize the differences\n",
                "```\n",
                "\n",
                "After analyzing the error you encountered, I have fixed the code and run it for you. The original error, `KeyError: 'Generated Answer'`, occurred because the column names used in the code did not match the actual column headers in the CSV file you provided.\n",
                "\n",
                "The correct column names from the CSV are `model_answer` and `expected_answer`, which I have updated in the script. I also created the `results_one_shot.csv` file with the data you shared so the script could run successfully.\n",
                "\n",
                "Below is the fixed code, along with the output from its execution.\n",
                "\n",
                "### Fixed Code\n",
                "\n",
                "```python\n",
                "from difflib import SequenceMatcher\n",
                "import csv\n",
                "\n",
                "def is_similar(a, b, threshold=0.75):\n",
                "    \"\"\"\n",
                "    Compares two strings for similarity using SequenceMatcher.\n",
                "    \n",
                "    Args:\n",
                "        a (str): The first string.\n",
                "        b (str): The second string.\n",
                "        threshold (float): The minimum similarity ratio to consider them similar.\n",
                "        \n",
                "    Returns:\n",
                "        bool: True if the similarity ratio is greater than the threshold, False otherwise.\n",
                "    \"\"\"\n",
                "    if not a or not b:\n",
                "        return False\n",
                "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > threshold\n",
                "\n",
                "def calculate_accuracy(qa_pairs, threshold):\n",
                "    \"\"\"\n",
                "    Calculates the accuracy of a set of QA pairs using a given similarity threshold.\n",
                "\n",
                "    Args:\n",
                "        qa_pairs (list): A list of dictionaries, where each dictionary represents a QA pair.\n",
                "        threshold (float): The similarity threshold to use for evaluation.\n",
                "\n",
                "    Returns:\n",
                "        tuple: A tuple containing the number of correct answers and the total number of questions.\n",
                "    \"\"\"\n",
                "    correct_answers = 0\n",
                "    total_questions = len(qa_pairs)\n",
                "    \n",
                "    for pair in qa_pairs:\n",
                "        # Corrected: Use the actual column names from the CSV file\n",
                "        if is_similar(pair['model_answer'], pair['expected_answer'], threshold=threshold):\n",
                "            correct_answers += 1\n",
                "            \n",
                "    return correct_answers, total_questions\n",
                "\n",
                "# Open and read the CSV file\n",
                "with open(\"results_one_shot.csv\") as f:\n",
                "    qa_pairs = list(csv.DictReader(f))\n",
                "\n",
                "# Define the range of thresholds to test\n",
                "thresholds_to_test = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
                "\n",
                "print(\"--- Evaluating Accuracy at Different Similarity Thresholds ---\")\n",
                "for threshold in thresholds_to_test:\n",
                "    correct, total = calculate_accuracy(qa_pairs, threshold)\n",
                "    accuracy_percentage = (correct / total) * 100\n",
                "\n",
                "    # Create a simple text bar for visualization\n",
                "    bar = \"â–ˆ\" * int(accuracy_percentage / 5)  # Each block represents 5%\n",
                "\n",
                "    print(f\"\\nThreshold: {threshold:.1f}\")\n",
                "    print(f\"Correct: {correct}/{total}\")\n",
                "    print(f\"Accuracy: {accuracy_percentage:.2f}%\")\n",
                "    print(f\"Visualization: [{bar:<20}]\") # Pad the bar to a length of 20\n",
                "print(\"\\n--- End of Evaluation ---\")\n",
                "```\n",
                "\n",
                "### Output\n",
                "\n",
                "The execution of the corrected code produced the following results, showing that the accuracy remains consistent across the tested thresholds:\n",
                "\n",
                "```\n",
                "--- Evaluating Accuracy at Different Similarity Thresholds ---\n",
                "\n",
                "Threshold: 0.5\n",
                "Correct: 1/4\n",
                "Accuracy: 25.00%\n",
                "Visualization: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               ]\n",
                "\n",
                "Threshold: 0.6\n",
                "Correct: 1/4\n",
                "Accuracy: 25.00%\n",
                "Visualization: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               ]\n",
                "\n",
                "Threshold: 0.7\n",
                "Correct: 1/4\n",
                "Accuracy: 25.00%\n",
                "Visualization: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               ]\n",
                "\n",
                "Threshold: 0.8\n",
                "Correct: 1/4\n",
                "Accuracy: 25.00%\n",
                "Visualization: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               ]\n",
                "\n",
                "Threshold: 0.9\n",
                "Correct: 1/4\n",
                "Accuracy: 25.00%\n",
                "Visualization: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               ]\n",
                "\n",
                "--- End of Evaluation ---\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prompting Strategies Showdown with Fuzzy Matching\n",
                "\n",
                "\n",
                "You've made impressive progress with your fuzzy matching skills! Now, let's take a big step forward by comparing different prompting strategies head-to-head using your evaluation techniques.\n",
                "\n",
                "In this exercise, you'll build a comprehensive evaluation system that processes results from two different prompting approaches: one-shot and few-shot. Your system will:\n",
                "\n",
                "Process CSV files containing QA results from each prompting strategy\n",
                "Calculate and compare accuracy scores using fuzzy matching\n",
                "Generate a clear leaderboard showing which strategy performs best\n",
                "Visualize the differences with simple text-based graphics\n",
                "This comparison will provide you with concrete evidence about which prompting strategy works best for your specific QA task. The insights you gain will help you make informed decisions about which approach to use in your own applications!\n",
                "\n",
                "```python\n",
                "from difflib import SequenceMatcher\n",
                "import csv\n",
                "\n",
                "def is_similar(a, b, threshold=0.75):\n",
                "    \"\"\"Compare two strings and return True if they're similar enough.\"\"\"\n",
                "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > threshold\n",
                "\n",
                "# TODO: Create a function called evaluate_file that takes a filename and threshold\n",
                "# parameter, then returns the number of correct answers, total questions, and accuracy percentage\n",
                "\n",
                "def create_visual_bar(percentage, max_length=20):\n",
                "    \"\"\"Create a simple visual bar to represent a percentage.\"\"\"\n",
                "    # TODO: Calculate the filled length based on the percentage (hint: use percentage/100 * max_length)\n",
                "    # TODO: Create a bar using filled and empty characters (â–ˆ for filled, â–‘ for empty)\n",
                "    return bar\n",
                "\n",
                "def compare_prompting_strategies():\n",
                "    # List of files to evaluate\n",
                "    files = [\n",
                "        (\"results_one_shot.csv\", \"One-shot\"),\n",
                "        (\"results_few_shot.csv\", \"Few-shot\")\n",
                "    ]\n",
                "    \n",
                "    # Set the similarity threshold\n",
                "    threshold = 0.75\n",
                "    \n",
                "    # TODO: Create an empty list to store results for each file\n",
                "    \n",
                "    # TODO: Loop through each file, evaluate it, and store the results\n",
                "    # Format: (prompt_type, correct, total, accuracy)\n",
                "    \n",
                "    # TODO: Sort results by accuracy (highest first)\n",
                "    \n",
                "    # Print the leaderboard header\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(f\"PROMPTING STRATEGY LEADERBOARD (threshold={threshold})\")\n",
                "    print(\"=\"*60)\n",
                "    print(f\"{'Rank':<6}{'Strategy':<12}{'Accuracy':<12}{'Score':<15}{'Performance'}\")\n",
                "    print(\"-\"*60)\n",
                "    \n",
                "    # TODO: Print each result with rank, including the visual bar\n",
                "    \n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # TODO: Print the winner and calculate improvement over second place\n",
                "    \n",
                "compare_prompting_strategies()\n",
                "```\n",
                "\n",
                "```python\n",
                "from difflib import SequenceMatcher\n",
                "import csv\n",
                "\n",
                "def is_similar(a, b, threshold=0.75):\n",
                "    \"\"\"Compare two strings and return True if they're similar enough.\"\"\"\n",
                "    # Ensure strings are not empty before comparison\n",
                "    if not a or not b:\n",
                "        return False\n",
                "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > threshold\n",
                "\n",
                "def evaluate_file(filename, threshold):\n",
                "    \"\"\"\n",
                "    Evaluates the accuracy of QA pairs in a CSV file using a given threshold.\n",
                "\n",
                "    Args:\n",
                "        filename (str): The name of the CSV file.\n",
                "        threshold (float): The similarity threshold for fuzzy matching.\n",
                "    \n",
                "    Returns:\n",
                "        tuple: A tuple containing the number of correct answers, total questions, \n",
                "               and the accuracy percentage. Returns a default tuple on error.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        with open(filename, 'r', encoding='utf-8') as f:\n",
                "            reader = csv.DictReader(f)\n",
                "            qa_pairs = list(reader)\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: The file '{filename}' was not found.\")\n",
                "        return 0, 0, 0.0\n",
                "    except KeyError as e:\n",
                "        print(f\"Error: Missing key in file '{filename}'. Check column headers. ({e})\")\n",
                "        return 0, 0, 0.0\n",
                "    \n",
                "    correct_answers = 0\n",
                "    total_questions = len(qa_pairs)\n",
                "    \n",
                "    if total_questions == 0:\n",
                "        return 0, 0, 0.0\n",
                "\n",
                "    for pair in qa_pairs:\n",
                "        # Use the correct column names from the data structure\n",
                "        if is_similar(pair['model_answer'], pair['expected_answer'], threshold):\n",
                "            correct_answers += 1\n",
                "            \n",
                "    accuracy = (correct_answers / total_questions) * 100\n",
                "    \n",
                "    return correct_answers, total_questions, accuracy\n",
                "\n",
                "def create_visual_bar(percentage, max_length=20):\n",
                "    \"\"\"Create a simple visual bar to represent a percentage.\"\"\"\n",
                "    if percentage < 0: percentage = 0\n",
                "    if percentage > 100: percentage = 100\n",
                "    \n",
                "    filled_length = int(percentage / 100 * max_length)\n",
                "    empty_length = max_length - filled_length\n",
                "    \n",
                "    bar = \"â–ˆ\" * filled_length + \"â–‘\" * empty_length\n",
                "    return bar\n",
                "\n",
                "def compare_prompting_strategies():\n",
                "    # List of files to evaluate\n",
                "    files = [\n",
                "        (\"results_one_shot.csv\", \"One-shot\"),\n",
                "        (\"results_few_shot.csv\", \"Few-shot\")\n",
                "    ]\n",
                "    \n",
                "    # Set the similarity threshold\n",
                "    threshold = 0.75\n",
                "    \n",
                "    # Create an empty list to store results for each file\n",
                "    results = []\n",
                "    \n",
                "    # Loop through each file, evaluate it, and store the results\n",
                "    for filename, prompt_type in files:\n",
                "        correct, total, accuracy = evaluate_file(filename, threshold)\n",
                "        if total > 0: # Only add if file was processed successfully\n",
                "            results.append((prompt_type, correct, total, accuracy))\n",
                "    \n",
                "    # Sort results by accuracy (highest first)\n",
                "    results.sort(key=lambda x: x[3], reverse=True)\n",
                "    \n",
                "    # Print the leaderboard header\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(f\"PROMPTING STRATEGY LEADERBOARD (threshold={threshold})\")\n",
                "    print(\"=\"*60)\n",
                "    print(f\"{'Rank':<6}{'Strategy':<12}{'Accuracy':<12}{'Score':<15}{'Performance'}\")\n",
                "    print(\"-\"*60)\n",
                "    \n",
                "    # Print each result with rank, including the visual bar\n",
                "    for i, (prompt_type, correct, total, accuracy) in enumerate(results):\n",
                "        rank = i + 1\n",
                "        score = f\"{correct}/{total}\"\n",
                "        bar = create_visual_bar(accuracy)\n",
                "        print(f\"{rank:<6}{prompt_type:<12}{accuracy:<12.2f}{score:<15}{bar}\")\n",
                "        \n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # Print the winner and calculate improvement over second place\n",
                "    if len(results) > 1:\n",
                "        winner = results[0]\n",
                "        runner_up = results[1]\n",
                "        \n",
                "        winner_strategy, _, _, winner_accuracy = winner\n",
                "        runner_up_strategy, _, _, runner_up_accuracy = runner_up\n",
                "\n",
                "        if runner_up_accuracy > 0:\n",
                "            improvement = (winner_accuracy - runner_up_accuracy) / runner_up_accuracy * 100\n",
                "            print(f\"ðŸ¥‡ The winner is the {winner_strategy} strategy, which showed a {improvement:.2f}% improvement over {runner_up_strategy}.\")\n",
                "        else:\n",
                "            print(f\"ðŸ¥‡ The winner is the {winner_strategy} strategy. The runner-up had 0% accuracy.\")\n",
                "    elif len(results) == 1:\n",
                "        winner_strategy, _, _, _ = results[0]\n",
                "        print(f\"ðŸ¥‡ The only strategy evaluated was {winner_strategy}.\")\n",
                "    else:\n",
                "        print(\"No results to compare.\")\n",
                "\n",
                "compare_prompting_strategies()\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
