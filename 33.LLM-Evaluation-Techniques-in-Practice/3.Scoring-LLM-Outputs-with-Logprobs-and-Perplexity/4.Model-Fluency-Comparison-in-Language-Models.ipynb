{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 4"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Fluency Comparison in Language Models\n",
                "\n",
                "### Introduction to Model Fluency\n",
                "\n",
                "Welcome to the final lesson of the \"Scoring LLM Outputs with Logprobs and Perplexity\" course. In previous lessons, you explored how to extract log probabilities and calculate perplexity to evaluate language models. Building on that foundation, this lesson will focus on comparing the **fluency** of different language models. Model fluency is a crucial aspect of evaluating how well a model can generate coherent and natural-sounding text. By the end of this lesson, you will be able to assess model fluency using log probabilities and perplexity, providing you with a deeper understanding of model performance.\n",
                "\n",
                "### Setting Up the Environment\n",
                "\n",
                "Before we dive into the code, let's ensure your environment is ready. If you're working on your local machine, you'll need to install the `openai` library. You can do this using `pip`:\n",
                "\n",
                "```bash\n",
                "pip install openai\n",
                "```\n",
                "\n",
                "The `math` library is part of Python's standard library, so no installation is needed for it. However, if you're using the CodeSignal environment, these libraries are already pre-installed, allowing you to focus on the code without worrying about setup.\n",
                "\n",
                "### Understanding the Code Structure\n",
                "\n",
                "Let's break down the code snippet you'll be working with. This code is designed to evaluate the fluency of a sentence across different language models using log probabilities obtained from OpenAI's API. We start by importing the necessary libraries: `math` for mathematical operations and `OpenAI` for interacting with the language model. Next, we initialize the `OpenAI` client, which allows us to send requests to the model. We define a list of models and a sentence that we want to evaluate. The code processes the sentence for each model individually, creating a chat completion request for each one. This request specifies the model to use, the message content, and parameters such as `max_tokens`, `logprobs`, and `top_logprobs`. These parameters control the number of tokens generated, whether to return log probabilities, and how many top token probabilities to retrieve, respectively.\n",
                "\n",
                "### Example: Evaluating Sentence Fluency Across Models\n",
                "\n",
                "Now, let's see the code in action with a practical example. We have a sentence: \"The president addressed the nation on live television.\" By running the code, we can evaluate the fluency of this sentence across different models based on the log probabilities of the first token generated by each model.\n",
                "\n",
                "```python\n",
                "import math\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "models = [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
                "sentence = \"The president addressed the nation on live television.\"\n",
                "\n",
                "print(f\"Evaluating sentence fluency: \\\"{sentence}\\\"\\n\")\n",
                "\n",
                "for model in models:\n",
                "    response = client.chat.completions.create(\n",
                "        model=model,\n",
                "        messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "        max_tokens=1,  # Required, can't be 0\n",
                "        logprobs=True,\n",
                "        top_logprobs=5\n",
                "    )\n",
                "    \n",
                "    # Use logprob of first generated token as approximation\n",
                "    logprob = response.choices[0].logprobs.content[0].logprob\n",
                "    perplexity = math.exp(-logprob)\n",
                "    \n",
                "    print(f\"{model}:\")\n",
                "    print(f\"  Log Probability (1st token): {logprob:.4f}\")\n",
                "    print(f\"  Approx. Perplexity: {perplexity:.2f}\\n\")\n",
                "```\n",
                "\n",
                "When you run this code, you might see an output similar to:\n",
                "\n",
                "```\n",
                "Evaluating sentence fluency: \"The president addressed the nation on live television.\"\n",
                "gpt-3.5-turbo:\n",
                "  Log Probability (1st token): -0.1500\n",
                "  Approx. Perplexity: 1.16\n",
                "\n",
                "gpt-4:\n",
                "  Log Probability (1st token): -0.1000\n",
                "  Approx. Perplexity: 1.11\n",
                "```\n",
                "\n",
                "In this example, the `gpt-4` model has a lower perplexity, indicating that it finds the sentence more fluent and predictable compared to `gpt-3.5-turbo`. This demonstrates how you can use log probabilities and perplexity to compare the fluency of different models."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Token Text from API Responses\n",
                "\n",
                "Now that you've seen how to extract log probabilities from model responses, let's dig deeper into the API response structure. In this exercise, you'll enhance our fluency comparison by extracting not just the log probability but also the actual text of the first token generated by each model.\n",
                "\n",
                "Look for the TODO comment in the code. Your task is to add a line that extracts the token's text from the response object, similar to how we're already extracting the log probability. The token text is stored in the same nested object where we find the log probability.\n",
                "\n",
                "By displaying both the token text and its probability, you'll gain more insight into how different models interpret and continue the same input text. This skill of navigating complex API responses will be valuable as you work with more advanced language model applications.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "models = [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
                "sentence = \"The president addressed the nation on live television.\"\n",
                "\n",
                "print(f\"Evaluating sentence fluency: \\\"{sentence}\\\"\\n\")\n",
                "\n",
                "for model in models:\n",
                "    response = client.chat.completions.create(\n",
                "        model=model,\n",
                "        messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "        max_tokens=1,  # Required, can't be 0\n",
                "        logprobs=True,\n",
                "        top_logprobs=5\n",
                "    )\n",
                "\n",
                "    # TODO: Extract the token text from the response, similar to how we extract the log probability below\n",
                "    \n",
                "    # Use logprob of first generated token as approximation\n",
                "    logprob = response.choices[0].logprobs.content[0].logprob\n",
                "\n",
                "    print(f\"{model}:\")\n",
                "    print(f\"  First token: \\\"[Your code should extract this]\\\"\")\n",
                "    print(f\"  Log Probability (1st token): {logprob:.4f}\\n\")\n",
                "\n",
                "```\n",
                "\n",
                "The text of the token is located in the same nested object as the log probability. You can access it using the `.token` attribute.\n",
                "\n",
                "Here is the updated code:\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "models = [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
                "sentence = \"The president addressed the nation on live television.\"\n",
                "\n",
                "print(f\"Evaluating sentence fluency: \\\"{sentence}\\\"\\n\")\n",
                "\n",
                "for model in models:\n",
                "    response = client.chat.completions.create(\n",
                "        model=model,\n",
                "        messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "        max_tokens=1,\n",
                "        logprobs=True,\n",
                "        top_logprobs=5\n",
                "    )\n",
                "\n",
                "    # Extract the token text and its log probability\n",
                "    first_token_info = response.choices[0].logprobs.content[0]\n",
                "    token_text = first_token_info.token\n",
                "    logprob = first_token_info.logprob\n",
                "\n",
                "    print(f\"{model}:\")\n",
                "    print(f\"  First token: \\\"{token_text}\\\"\")\n",
                "    print(f\"  Log Probability (1st token): {logprob:.4f}\\n\")\n",
                "```\n",
                "\n",
                "### Explanation\n",
                "\n",
                "The `response` object from the OpenAI API has a structured hierarchy. When you request `logprobs=True`, the `logprobs` object is added to the response. The token data is then found within this object at `response.choices[0].logprobs.content[0]`.\n",
                "\n",
                "This object contains two key pieces of information:\n",
                "\n",
                "  - `.token`: The actual text of the token (e.g., \"The\").\n",
                "  - `.logprob`: The log probability of that token.\n",
                "\n",
                "By extracting both pieces of information, you can now see exactly what the model predicts as the next token and how confident it is in that prediction."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Calculating Perplexity for Model Comparison\n",
                "\n",
                "Perfect! You've learned how to work with API response structures and extract log probabilities from model outputs. Now it's time to complete the fluency comparison by adding the missing piece — perplexity calculation.\n",
                "\n",
                "Your objective is to calculate the approximate perplexity for each model's first token using the mathematical formula math.exp(-logprob). Look for the TODO comment in the code and add the perplexity calculation right after you extract the log probability.\n",
                "\n",
                "Remember that lower perplexity values indicate better fluency — they show that a model finds the text more predictable and natural. This calculation will give you the complete picture for comparing how different models perceive the same input sentence.\n",
                "\n",
                "```python\n",
                "import math\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "models = [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
                "sentence = \"The president addressed the nation on live television.\"\n",
                "\n",
                "print(f\"Evaluating sentence fluency: \\\"{sentence}\\\"\\n\")\n",
                "\n",
                "for model in models:\n",
                "    response = client.chat.completions.create(\n",
                "        model=model,\n",
                "        messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "        max_tokens=1,  # Required, can't be 0\n",
                "        logprobs=True,\n",
                "        top_logprobs=5\n",
                "    )\n",
                "\n",
                "    # Use logprob of first generated token as approximation\n",
                "    logprob = response.choices[0].logprobs.content[0].logprob\n",
                "    # TODO: Calculate the approximate perplexity using math.exp\n",
                "\n",
                "    print(f\"{model}:\")\n",
                "    print(f\"  Log Probability (1st token): {logprob:.4f}\")\n",
                "    print(f\"  Approx. Perplexity: {perplexity:.2f}\\n\")\n",
                "```\n",
                "\n",
                "You're right, the final step is to calculate the perplexity. The formula is `math.exp(-logprob)`.\n",
                "\n",
                "Here is the completed code:\n",
                "\n",
                "```python\n",
                "import math\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "models = [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
                "sentence = \"The president addressed the nation on live television.\"\n",
                "\n",
                "print(f\"Evaluating sentence fluency: \\\"{sentence}\\\"\\n\")\n",
                "\n",
                "for model in models:\n",
                "    response = client.chat.completions.create(\n",
                "        model=model,\n",
                "        messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "        max_tokens=1,  # Required, can't be 0\n",
                "        logprobs=True,\n",
                "        top_logprobs=5\n",
                "    )\n",
                "\n",
                "    # Use logprob of first generated token as approximation\n",
                "    logprob = response.choices[0].logprobs.content[0].logprob\n",
                "    # Calculate the approximate perplexity\n",
                "    perplexity = math.exp(-logprob)\n",
                "\n",
                "    print(f\"{model}:\")\n",
                "    print(f\"  Log Probability (1st token): {logprob:.4f}\")\n",
                "    print(f\"  Approx. Perplexity: {perplexity:.2f}\\n\")\n",
                "```\n",
                "\n",
                "When you run this code, you'll see the perplexity values for each model, which will give you a direct comparison of their fluency for the given sentence. As you noted, the model with the lower perplexity value is considered more \"fluent\" or less \"surprised\" by the text."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating Multiple Sentences for Fluency\n",
                "\n",
                "Cosmo\n",
                "Just now\n",
                "Read message aloud\n",
                "Excellent work on mastering single-sentence fluency evaluation! Now it's time to scale up your analysis and see how model fluency varies across different types of text.\n",
                "\n",
                "Your objective is to modify the code to evaluate multiple sentences at once, giving you a broader view of model performance patterns. This will help you understand how the same model might handle different sentence structures or topics with varying levels of fluency.\n",
                "\n",
                "Here's what you need to do:\n",
                "\n",
                "Replace the single-sentence variable with a list of sentences\n",
                "Add a loop to iterate through each sentence\n",
                "Update the print statements to show which sentence is being evaluated\n",
                "**Restrict your evaluation to only the \"gpt-4\" model.\n",
                "Add error handling for math.exp(-logprob) using a try/except block to catch OverflowError and set perplexity to float('inf') if it occurs\n",
                "By testing multiple sentences with the \"gpt-4\" model, you'll develop a more complete understanding of how a state-of-the-art model handles various text types and gain valuable experience working with nested data structures and robust error handling.\n",
                "\n",
                "```python\n",
                "import math\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "models = [\"gpt-4\"]\n",
                "# TODO: Replace this single sentence with a list of sentences to evaluate\n",
                "sentence = \"The president addressed the nation on live television.\"\n",
                "\n",
                "# TODO: Add a loop here to iterate through each sentence in your list\n",
                "\n",
                "print(f\"Evaluating sentence fluency: \\\"{sentence}\\\"\\n\")\n",
                "\n",
                "for model in models:\n",
                "    response = client.chat.completions.create(\n",
                "        model=model,\n",
                "        messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "        max_tokens=1,  # Required, can't be 0\n",
                "        logprobs=True,\n",
                "        top_logprobs=5\n",
                "    )\n",
                "\n",
                "    # Use logprob of first generated token as approximation\n",
                "    logprob = response.choices[0].logprobs.content[0].logprob\n",
                "    # TODO: Add error handling for math.exp(-logprob) to catch OverflowError\n",
                "    perplexity = math.exp(-logprob)\n",
                "\n",
                "    print(f\"{model}:\")\n",
                "    print(f\"  Log Probability (1st token): {logprob:.4f}\")\n",
                "    print(f\"  Approx. Perplexity: {perplexity:.2f}\\n\")\n",
                "```\n",
                "\n",
                "Here is the modified code that evaluates multiple sentences using a list, loops through them, and includes error handling for the `math.exp()` function.\n",
                "\n",
                "```python\n",
                "import math\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "models = [\"gpt-4\"]\n",
                "# Replace the single sentence with a list of sentences to evaluate\n",
                "sentences = [\n",
                "    \"The president addressed the nation on live television.\",\n",
                "    \"The sun rises in the east and sets in the west.\",\n",
                "    \"A computer is a machine that can be programmed.\",\n",
                "    \"Unusual phrases often lead to high perplexity.\"\n",
                "]\n",
                "\n",
                "# Process each sentence for the specified model\n",
                "for model in models:\n",
                "    for sentence in sentences:\n",
                "        print(f\"Evaluating sentence fluency for '{model}': \\\"{sentence}\\\"\")\n",
                "        try:\n",
                "            response = client.chat.completions.create(\n",
                "                model=model,\n",
                "                messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "                max_tokens=1,\n",
                "                logprobs=True,\n",
                "                top_logprobs=5\n",
                "            )\n",
                "\n",
                "            # Use logprob of first generated token as approximation\n",
                "            logprob = response.choices[0].logprobs.content[0].logprob\n",
                "\n",
                "            # Add error handling for math.exp(-logprob) to catch OverflowError\n",
                "            try:\n",
                "                perplexity = math.exp(-logprob)\n",
                "            except OverflowError:\n",
                "                # Set perplexity to infinity if the logprob is too negative\n",
                "                perplexity = float('inf')\n",
                "\n",
                "            print(f\"  Log Probability (1st token): {logprob:.4f}\")\n",
                "            print(f\"  Approx. Perplexity: {perplexity:.2f}\\n\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"An error occurred while processing the sentence: {e}\\n\")\n",
                "```\n",
                "\n",
                "### Key Changes\n",
                "\n",
                "  * **List of Sentences**: The `sentence` variable has been replaced with a list called `sentences`, containing several different examples.\n",
                "  * **Nested Loop**: A new `for sentence in sentences:` loop has been added. This loop is nested inside the `for model in models:` loop, ensuring that each sentence is processed by each model in the list.\n",
                "  * **Updated Print Statements**: The print statements have been modified to clearly indicate which model and sentence are currently being evaluated.\n",
                "  * **OverflowError Handling**: A `try-except OverflowError` block is now wrapped around the `math.exp(-logprob)` calculation. This prevents the program from crashing if `logprob` is an extremely large negative number, which can happen with highly predictable tokens. If an `OverflowError` occurs, the perplexity is set to `float('inf')` to represent an extremely high value. This helps in understanding that the model found the sentence highly predictable, but the value exceeded the float limit.\n",
                "  * **General Exception Handling**: A broader `try-except` block has been added to catch any other potential API or data parsing errors, ensuring the script continues to run even if a request fails."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
