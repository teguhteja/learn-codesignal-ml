{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Sentence Likelihoods Using Log Probabilities\n",
                "\n",
                "### Introduction to Sentence Likelihoods\n",
                "\n",
                "Welcome back to the course \"Scoring LLM Outputs with Logprobs and Perplexity.\" In the previous lesson, we explored how log probabilities provide insights into a language model’s confidence when generating tokens. Now, we’ll build on that foundation by comparing sentence likelihoods using log probabilities.\n",
                "\n",
                "Evaluating sentence likelihoods helps us understand how models judge different formulations of language. In this lesson, you’ll learn how to use the OpenAI API to score sentences based on the log probability of the model’s next token prediction.\n",
                "\n",
                "### The Importance and Applications of Likelihoods\n",
                "\n",
                "Likelihoods are a fundamental concept in language modeling and natural language processing. They measure how probable a sequence of words is according to a model, allowing us to quantify how “natural” or “expected” a sentence is. This is crucial for a variety of tasks:\n",
                "\n",
                "  * **Model Evaluation:** Likelihoods are widely used to compare different language models and assess their performance on tasks like text generation, translation, and summarization.\n",
                "  * **Error Detection:** By identifying sentences or tokens with unusually low likelihoods, we can spot errors, anomalies, or unnatural phrasing in generated text.\n",
                "  * **Data Filtering:** Likelihood scores help filter out low-quality or irrelevant data when building datasets for training or evaluation.\n",
                "  * **Downstream Applications:** Many applications—such as speech recognition, machine translation, and autocomplete—rely on likelihoods to rank candidate outputs and select the most plausible one.\n",
                "\n",
                "Because of their versatility and interpretability, likelihoods (and their log-transformed versions, log probabilities) are a standard tool for both researchers and practitioners working with language models.\n",
                "\n",
                "### Understanding the Code Structure\n",
                "\n",
                "Let’s break down the code you’ll use in this unit. We begin by initializing the OpenAI client and defining a list of candidate sentences. For each sentence, we’ll pass it to the model and extract the log probability of the first predicted token, which gives us a proxy for how likely or “natural” the sentence feels to the model.\n",
                "\n",
                "We use:\n",
                "\n",
                "  * `logprobs=True` to return log probability data.\n",
                "  * `top_logprobs=5` to retrieve scores for the top 5 candidate tokens.\n",
                "  * `max_tokens=1` to generate exactly one token prediction.\n",
                "\n",
                "### Extracting and Interpreting Log Probabilities\n",
                "\n",
                "When you request `logprobs=True` from the OpenAI API, the response includes a `logprobs` object for each generated token. This object contains the log probability assigned to each token, as well as the top alternative tokens and their logprobs. The structure looks like this:\n",
                "\n",
                "```json\n",
                "response.choices[0].logprobs.content[0] = {\n",
                "    \"token\": \"<generated_token>\",\n",
                "    \"logprob\": <log_probability>,\n",
                "    \"top_logprobs\": {\n",
                "        \"<token_1>\": <logprob_1>,\n",
                "        \"<token_2>\": <logprob_2>,\n",
                "        ...\n",
                "    }\n",
                "}\n",
                "```\n",
                "\n",
                "A log probability closer to 0 means the model is more confident in that token. The plot below illustrates how log probability values relate to model confidence:\n",
                "\n",
                "```\n",
                "Confidence (probability)   Log Probability\n",
                "-----------------------    ---------------\n",
                "      1.0                        0\n",
                "      0.5                   -0.693\n",
                "      0.1                   -2.303\n",
                "      0.01                  -4.605\n",
                "```\n",
                "\n",
                "By comparing logprob values for different sentences, you can infer which one the model finds more plausible.\n",
                "\n",
                "### Example: Comparing Sentence Fluency\n",
                "\n",
                "Let’s look at two example sentences:\n",
                "\n",
                "  * \"The sun is a star.\"\n",
                "  * \"The sun is a sandwich.\"\n",
                "\n",
                "While both are syntactically valid, one is clearly more semantically coherent. We’ll use logprobs to see how the model scores them.\n",
                "\n",
                "```python\n",
                "import openai\n",
                "client = openai.OpenAI()\n",
                "\n",
                "sentences = [\n",
                "    \"The sun is a star.\",\n",
                "    \"The sun is a sandwich.\"\n",
                "]\n",
                "\n",
                "for sentence in sentences:\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "        max_tokens=1,\n",
                "        logprobs=True,\n",
                "        top_logprobs=5\n",
                "    )\n",
                "    # Extract logprob for the first generated token\n",
                "    logprob = response.choices[0].logprobs.content[0].logprob\n",
                "    print(f\"Sentence: {sentence}\")\n",
                "    print(f\"Log probability of next token: {logprob:.3f}\\n\")\n",
                "```\n",
                "\n",
                "You might get an output like:\n",
                "\n",
                "```\n",
                "Sentence: The sun is a star.\n",
                "Log probability of next token: -0.276\n",
                "\n",
                "Sentence: The sun is a sandwich.\n",
                "Log probability of next token: -1.841\n",
                "```\n",
                "\n",
                "The model assigns a much higher log probability to the first sentence, indicating it considers it more likely.\n",
                "\n",
                "### Summary and Next Steps\n",
                "\n",
                "In this lesson, you learned how to compare sentence plausibility by examining token-level log probabilities. This method allows you to go beyond just generating responses—now you can measure how confident the model is in its next move.\n",
                "\n",
                "In the next unit, you’ll use this idea to calculate perplexity, a popular metric that quantifies overall sentence fluency using log probability averages. You're getting closer to evaluating language models like a pro—let’s keep going\\!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Log Probabilities from Responses\n",
                "\n",
                "Now that you've learned about log probabilities and how they help measure a model's confidence, let's put this knowledge into practice! In this exercise, you'll work with a single sentence and extract its log probability score from the OpenAI API response.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Extract the log probability from the nested API response structure\n",
                "Format and print this value to three decimal places\n",
                "Display both the sentence and its corresponding log probability\n",
                "This hands-on experience will help you understand how to access the specific fields in the API response that contain log probability data. By completing this exercise, you'll take your first step toward comparing different sentences based on their likelihood scores.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "sentence = \"The sun is a star.\"\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "    max_tokens=1,  \n",
                "    logprobs=True,\n",
                "    top_logprobs=5\n",
                ")\n",
                "\n",
                "# TODO: Extract the logprob of the first generated token from the response\n",
                "logprob = None\n",
                "\n",
                "# TODO: Print the sentence and its log probability formatted to 3 decimal places\n",
                "```\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "sentence = \"The sun is a star.\"\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "    max_tokens=1,\n",
                "    logprobs=True,\n",
                "    top_logprobs=5\n",
                ")\n",
                "\n",
                "# TODO: Extract the logprob of the first generated token from the response\n",
                "logprob = response.choices[0].logprobs.content[0].logprob\n",
                "\n",
                "# TODO: Print the sentence and its log probability formatted to 3 decimal places\n",
                "print(f\"Sentence: {sentence}\")\n",
                "print(f\"Log probability of the next token: {logprob:.3f}\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Sentences with Log Probabilities\n",
                "\n",
                "Excellent work extracting log probabilities from a single sentence! Now let's take it a step further by comparing multiple sentences. In this exercise, you'll implement the example we discussed in the lesson to see how log probabilities reveal a model's preference for plausible statements.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Create a list with two sentences — one factual and one nonsensical.\n",
                "Write a loop to process each sentence with the API.\n",
                "Extract and display the log probability for each sentence.\n",
                "When you complete this exercise, you'll see firsthand how the model assigns higher log probabilities (values closer to 0) to sentences that make more sense. This practical comparison will deepen your understanding of how language models evaluate different statements based on their likelihood.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# TODO: Create a list containing two sentences:\n",
                "# 1. \"The sun is a star.\" (semantically coherent)\n",
                "# 2. \"The sun is a sandwich.\" (nonsensical)\n",
                "sentences = []\n",
                "\n",
                "# TODO: Write a loop to process each sentence in your list\n",
                "# The code below needs to be inside your loop\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"The sun is a star.\"}],\n",
                "    max_tokens=1,  # Must be >= 1\n",
                "    logprobs=True,\n",
                "    top_logprobs=5\n",
                ")\n",
                "\n",
                "# TODO: Extract the logprob of the first generated token\n",
                "logprob = None\n",
                "\n",
                "# TODO: Print the sentence and its log probability formatted to 3 decimal places\n",
                "# Make sure to add a blank line between different sentence results\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# TODO: Create a list containing two sentences:\n",
                "# 1. \"The sun is a star.\" (semantically coherent)\n",
                "# 2. \"The sun is a sandwich.\" (nonsensical)\n",
                "sentences = [\n",
                "    \"The sun is a star.\",\n",
                "    \"The sun is a sandwich.\"\n",
                "]\n",
                "\n",
                "# TODO: Write a loop to process each sentence in your list\n",
                "# The code below needs to be inside your loop\n",
                "for sentence in sentences:\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "        max_tokens=1,  # Must be >= 1\n",
                "        logprobs=True,\n",
                "        top_logprobs=5\n",
                "    )\n",
                "\n",
                "    # TODO: Extract the logprob of the first generated token\n",
                "    logprob = response.choices[0].logprobs.content[0].logprob\n",
                "\n",
                "    # TODO: Print the sentence and its log probability formatted to 3 decimal places\n",
                "    # Make sure to add a blank line between different sentence results\n",
                "    print(f\"Sentence: {sentence}\")\n",
                "    print(f\"Log probability of the next token: {logprob:.3f}\\n\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Finding the Most Plausible Sentence\n",
                "\n",
                "antastic job comparing sentences with log probabilities! Now let's put your skills to practical use by creating a function that automatically finds the most likely sentence from a group of options. In this exercise, you'll build a reusable tool that can identify which sentence a language model considers most natural.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Complete the find_most_likely_sentence function that processes multiple sentences.\n",
                "Make API calls to get log probabilities for each sentence.\n",
                "Track and update which sentence has the highest log probability.\n",
                "Return both the winning sentence and its log probability score.\n",
                "This function represents a real-world application of what you've learned — it could be used in systems that need to choose the most natural-sounding option from several alternatives. By completing this exercise, you'll have a practical tool that demonstrates how log probabilities can guide decision-making in language processing applications.\n",
                "\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def find_most_likely_sentence(sentences):\n",
                "    \"\"\"\n",
                "    Find the sentence with the highest log probability of the first token.\n",
                "    \n",
                "    Args:\n",
                "        sentences: A list of sentences to compare\n",
                "        \n",
                "    Returns:\n",
                "        A tuple containing (most_likely_sentence, highest_logprob)\n",
                "    \"\"\"\n",
                "    # TODO: Initialize variables to track the highest logprob and most likely sentence\n",
                "    \n",
                "    for sentence in sentences:\n",
                "        # TODO: Make an API call to get the log probability for this sentence\n",
                "        \n",
                "        # TODO: Extract the logprob of the first generated token\n",
                "        \n",
                "        print(f\"Sentence: {sentence}\")\n",
                "        print(f\"Log probability: {logprob:.3f}\\n\")\n",
                "        \n",
                "        # TODO: Update tracking variables if this sentence has a higher logprob\n",
                "    \n",
                "    # TODO: Return the most likely sentence and its log probability\n",
                "\n",
                "# Test sentences with varying degrees of plausibility\n",
                "test_sentences = [\n",
                "    \"The sun is a star in our solar system.\",\n",
                "    \"The sun is a planet in our solar system.\",\n",
                "    \"The sun is a sandwich in our solar system.\",\n",
                "    \"The sun provides light and heat to Earth.\"\n",
                "]\n",
                "\n",
                "# TODO: Call the function and store the result\n",
                "\n",
                "# TODO: Print the most likely sentence and its log probability\n",
                "```\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def find_most_likely_sentence(sentences):\n",
                "    \"\"\"\n",
                "    Find the sentence with the highest log probability of the first token.\n",
                "    \n",
                "    Args:\n",
                "        sentences: A list of sentences to compare\n",
                "        \n",
                "    Returns:\n",
                "        A tuple containing (most_likely_sentence, highest_logprob)\n",
                "    \"\"\"\n",
                "    # TODO: Initialize variables to track the highest logprob and most likely sentence\n",
                "    highest_logprob = float('-inf')\n",
                "    most_likely_sentence = \"\"\n",
                "\n",
                "    for sentence in sentences:\n",
                "        # TODO: Make an API call to get the log probability for this sentence\n",
                "        response = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=[{\"role\": \"user\", \"content\": sentence}],\n",
                "            max_tokens=1,\n",
                "            logprobs=True,\n",
                "            top_logprobs=5\n",
                "        )\n",
                "        \n",
                "        # TODO: Extract the logprob of the first generated token\n",
                "        logprob = response.choices[0].logprobs.content[0].logprob\n",
                "        \n",
                "        print(f\"Sentence: {sentence}\")\n",
                "        print(f\"Log probability: {logprob:.3f}\\n\")\n",
                "        \n",
                "        # TODO: Update tracking variables if this sentence has a higher logprob\n",
                "        if logprob > highest_logprob:\n",
                "            highest_logprob = logprob\n",
                "            most_likely_sentence = sentence\n",
                "    \n",
                "    # TODO: Return the most likely sentence and its log probability\n",
                "    return (most_likely_sentence, highest_logprob)\n",
                "\n",
                "# Test sentences with varying degrees of plausibility\n",
                "test_sentences = [\n",
                "    \"The sun is a star in our solar system.\",\n",
                "    \"The sun is a planet in our solar system.\",\n",
                "    \"The sun is a sandwich in our solar system.\",\n",
                "    \"The sun provides light and heat to Earth.\"\n",
                "]\n",
                "\n",
                "# TODO: Call the function and store the result\n",
                "most_likely, likelihood_score = find_most_likely_sentence(test_sentences)\n",
                "\n",
                "# TODO: Print the most likely sentence and its log probability\n",
                "print(\"---\")\n",
                "print(f\"The most plausible sentence is:\\n'{most_likely}'\")\n",
                "print(f\"With a log probability of: {likelihood_score:.3f}\")\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
