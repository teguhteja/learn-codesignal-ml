{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Log Probabilities for Tokens\n",
                "\n",
                "# Introduction to Log Probabilities in LLMs\n",
                "\n",
                "Welcome to the first lesson of the course **\"Scoring LLM Outputs with Logprobs and Perplexity.\"** In this lesson, we will explore the concept of **log probabilities** in language models. Log probabilities help measure how likely a model considers each possible next word or token given a specific prompt. This internal signal is essential for understanding how models like GPT-3.5 make predictions and how confident they are about each token.\n",
                "\n",
                "By the end of this lesson, you will be able to extract and interpret log probabilities from a model’s response and get a deeper look into how it evaluates different word choices.\n",
                "\n",
                "-----\n",
                "\n",
                "## What Are Log Probabilities?\n",
                "\n",
                "A **log probability** is the natural logarithm of a probability value. In the context of language models, the model assigns a probability ($$p$$) to each possible next token given the preceding context. The log probability is then calculated as:\n",
                "\n",
                "$$\\text{logprob} = \\log(p)$$\n",
                "\n",
                "where $$\\log$$ denotes the natural logarithm (base $$e$$). Since probabilities ($$p$$) are always between 0 and 1, their log probabilities are always negative or zero.\n",
                "\n",
                "### Why use log probabilities?\n",
                "\n",
                "  * Log probabilities are numerically more stable, especially when dealing with very small probabilities (as is common in language modeling).\n",
                "  * They make it easier to sum probabilities across sequences, since multiplying probabilities corresponds to adding their log probabilities:\n",
                "\n",
                "$$\\log(p_1 \\times p_2 \\times \\dots \\times p_n) = \\log(p_1) + \\log(p_2) + \\dots + \\log(p_n)$$\n",
                "\n",
                "Log probabilities are widely used in evaluating and comparing model outputs, such as in perplexity calculations and sequence scoring.\n",
                "\n",
                "In summary, log probabilities provide a convenient and robust way to represent and manipulate the likelihoods assigned by language models to tokens and sequences.\n",
                "\n",
                "-----\n",
                "\n",
                "## Setting Up the Environment\n",
                "\n",
                "To follow along, ensure you have the **OpenAI** library installed. If you're using your local environment, run:\n",
                "\n",
                "```bash\n",
                "pip install openai\n",
                "```\n",
                "\n",
                "If you're working inside the CodeSignal environment, the necessary libraries are already installed for you.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding the Code Structure\n",
                "\n",
                "Let’s take a look at a code snippet that extracts log probabilities using OpenAI’s API. We’ll use a more open-ended prompt that can yield multiple possible completions. This gives us a better opportunity to inspect how confident the model is in its different predictions.\n",
                "\n",
                "Notice that we set `max_tokens=1` in the API call. This is essential because it tells the model to generate only a single token as output. By limiting the output to one token, we can clearly examine the log probabilities for just the immediate next token, making it much easier to interpret the model’s confidence and the alternatives at that specific position. If we allowed more tokens, the response would include log probabilities for each generated token in the sequence, which could make the analysis more complex and less focused for this introductory example.\n",
                "\n",
                "Before iterating through the log probability data, it’s important to understand the structure of `response.choices[0].logprobs.content`. This object is a list, where each element corresponds to a generated token. Each element contains:\n",
                "\n",
                "  * **token:** the generated token (e.g., \"apple\")\n",
                "  * **top\\_logprobs:** a list of the top alternative tokens and their log probabilities for that position. Each entry in `top_logprobs` has:\n",
                "      * **token:** the alternative token\n",
                "      * **logprob:** the log probability assigned to that token\n",
                "\n",
                "This structure allows you to see not only the token the model generated, but also the model’s confidence in the top alternative tokens at that position.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "client = OpenAI()\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"A common fruit people eat is\"}],\n",
                "    max_tokens=1,\n",
                "    temperature=1.0,\n",
                "    logprobs=True,      # Request log probabilities for generated tokens\n",
                "    top_logprobs=5      # Return the top 5 most likely tokens at each position\n",
                ")\n",
                "print(\"Top token probabilities:\")\n",
                "try:\n",
                "    # response.choices[0].logprobs.content is a list of token info objects.\n",
                "    # Each token info object contains:\n",
                "    #   - token: the generated token\n",
                "    #   - top_logprobs: a list of alternative tokens and their logprobs\n",
                "    if response.choices[0].logprobs and response.choices[0].logprobs.content:\n",
                "        for token_info in response.choices[0].logprobs.content:\n",
                "            print(f\"\\nGenerated token: {token_info.token}\")\n",
                "            if token_info.top_logprobs:\n",
                "                for alt in token_info.top_logprobs:\n",
                "                    print(f\"  {alt.token} → {alt.logprob:.3f}\")\n",
                "    else:\n",
                "        print(\"No token probabilities returned.\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Extracting and Interpreting Log Probabilities\n",
                "\n",
                "Once the model responds, we inspect the returned token and its associated alternatives. Each alternative token is associated with a **log probability**, which represents the model’s confidence. Log probabilities are typically negative, with values closer to zero indicating higher confidence. The more negative the log probability, the less likely the model considers that token as the next word.\n",
                "\n",
                "For example, you may see:\n",
                "\n",
                "```text\n",
                "Generated token: apple\n",
                "  apple → -0.018\n",
                "  banana → -0.576\n",
                "  orange → -1.043\n",
                "  grapes → -1.832\n",
                "  fruit → -2.067\n",
                "```\n",
                "\n",
                "In this example, \"apple\" has the highest confidence with a log probability of -0.018, making it the most likely token. \"Banana\" follows with a log probability of -0.576, indicating it is less likely than \"apple\" but more likely than \"orange\", \"grapes\", and \"fruit\". Understanding these values helps in assessing the model's prediction confidence and the relative likelihood of different tokens.\n",
                "\n",
                "-----\n",
                "\n",
                "## Visualizing Log Probabilities and Probabilities\n",
                "\n",
                "To better understand the relationship between log probabilities and probabilities, you can visualize both for the top tokens using a simple plot. This helps you see how a small difference in log probability can correspond to a much larger difference in actual probability.\n",
                "\n",
                "```python\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Example log probabilities returned by the model\n",
                "token_probs = {\n",
                "    \"apple\": -0.018,\n",
                "    \"banana\": -0.576,\n",
                "    \"orange\": -1.043,\n",
                "    \"grapes\": -1.832,\n",
                "    \"fruit\": -2.067\n",
                "}\n",
                "tokens = list(token_probs.keys())\n",
                "log_probs = np.array(list(token_probs.values()))\n",
                "probs = np.exp(log_probs)  # Convert logprobs to probabilities\n",
                "\n",
                "x = np.arange(len(tokens))\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "\n",
                "# Plot probabilities\n",
                "plt.plot(x, probs, marker='o', linestyle='-', color='navy', label='Probability (exp(log(p)))')\n",
                "\n",
                "# Plot log probabilities\n",
                "plt.plot(x, log_probs, marker='s', linestyle='--', color='darkgreen', label='Log Probability (log(p))')\n",
                "\n",
                "# Annotate each point with its value\n",
                "for i in x:\n",
                "    plt.text(i, probs[i] + 0.01, f\"p={probs[i]:.3f}\", ha='center', va='bottom', fontsize=9, color='blue')\n",
                "    plt.text(i, log_probs[i] - 0.1, f\"log(p)={log_probs[i]:.3f}\", ha='center', va='top', fontsize=9, color='green')\n",
                "\n",
                "plt.xticks(x, tokens)\n",
                "plt.title(\"Log Probabilities vs Probabilities of Tokens\", fontsize=14)\n",
                "plt.xlabel(\"Tokens\")\n",
                "plt.ylabel(\"Value\")\n",
                "plt.grid(True, linestyle='--', alpha=0.5)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "```\n",
                "\n",
                "The output plot looks like this:\n",
                "\n",
                "This plot shows:\n",
                "\n",
                "  * The log probability for each token (dashed green line)\n",
                "  * The corresponding probability (solid blue line)\n",
                "  * Value annotations for both log probability and probability\n",
                "\n",
                "Notice how the token with the log probability closest to zero (\"apple\") also has the highest probability, and how the differences in log probability translate to much larger differences in actual probability. This visualization can help you build intuition for interpreting log probabilities in practice.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Next Steps\n",
                "\n",
                "In this lesson, you learned what log probabilities are, how to retrieve them using the OpenAI API, and how to interpret them in practice. This technique is useful for gaining deeper insight into model behavior and understanding how confident it is in its generated outputs.\n",
                "\n",
                "In the next lesson, we’ll build on this by using log probabilities to compare sentence likelihoods—giving us a tool for scoring how \"natural\" different sentences sound to a language model.\n",
                "\n",
                "Stay curious and experiment with different prompts to see how token predictions vary\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fixing Token Probability Display Code\n",
                "\n",
                "Now that you understand what log probabilities are and how they help us peek into a language model's \"thinking,\" let's put that knowledge into practice! In this exercise, you'll fix a piece of code that extracts log probabilities from an OpenAI model but has some issues with processing and displaying the results.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Fix the loop that iterates through tokens in the response.\n",
                "Correct how the code accesses log probability data.\n",
                "Format the output to show token → logprob with 3 decimal places.\n",
                "Add proper error handling for when no tokens are returned.\n",
                "By completing this exercise, you'll gain hands-on experience with extracting and interpreting log probabilities — a fundamental skill for analyzing model confidence that we'll build upon in future lessons.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Name a color that people often wear:\"}],\n",
                "    max_tokens=1,\n",
                "    temperature=1.0,\n",
                "    logprobs=True,\n",
                "    top_logprobs=5\n",
                ")\n",
                "\n",
                "print(\"Top token probabilities:\")\n",
                "try:\n",
                "    if response.choices[0].logprobs:\n",
                "        # TODO: Fix this loop to correctly iterate through the tokens in the response\n",
                "        for token in response.choices[0].logprobs:\n",
                "            print(f\"\\nGenerated token: {token}\")\n",
                "            # TODO: Fix this loop to correctly access the top log probabilities\n",
                "            for alt_token, prob in token.items():\n",
                "                # TODO: Update this print statement to format as \"token → logprob\" with 3 decimal places\n",
                "                print(f\"  {alt_token}: {prob}\")\n",
                "    # TODO: Add an else clause to handle cases when no token probabilities are returned\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "```\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Name a color that people often wear:\"}],\n",
                "    max_tokens=1,\n",
                "    temperature=1.0,\n",
                "    logprobs=True,\n",
                "    top_logprobs=5\n",
                ")\n",
                "\n",
                "print(\"Top token probabilities:\")\n",
                "try:\n",
                "    if response.choices[0].logprobs and response.choices[0].logprobs.content:\n",
                "        # Fixed: Correctly iterate through the list of token information objects\n",
                "        for token_info in response.choices[0].logprobs.content:\n",
                "            print(f\"\\nGenerated token: {token_info.token}\")\n",
                "            \n",
                "            # Fixed: Correctly access the top log probabilities, which is a list of objects\n",
                "            if token_info.top_logprobs:\n",
                "                for alt in token_info.top_logprobs:\n",
                "                    # Fixed: Correctly format the output using the object's attributes\n",
                "                    print(f\"  {alt.token} → {alt.logprob:.3f}\")\n",
                "    else:\n",
                "        # Added: Handle the case where no token probabilities are returned\n",
                "        print(\"No token probabilities returned.\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Making Token Probabilities Dynamic\n",
                "\n",
                "Excellent work on fixing the token probability display code! Now, let's make our code more flexible by allowing it to show different numbers of alternative tokens. Currently, we're always requesting the top 5 token probabilities, but what if we want to see more or fewer options?\n",
                "\n",
                "In this exercise, you'll modify the code to use a variable for the number of top log probabilities instead of a hardcoded value. This will allow you to easily experiment with different settings to see how the model ranks its token choices.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Create a variable to control how many top token probabilities to return.\n",
                "Update the API call to use this variable instead of the fixed value.\n",
                "Run your code with different values (3, 5, and 7) to observe the differences.\n",
                "Make sure the output message reflects the number you've chosen.\n",
                "By making this improvement, you'll gain a more flexible tool for exploring model predictions at different levels of detail — an important skill for analyzing model behavior in various contexts.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# TODO: Create a variable to control how many top token probabilities to return\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Name a color that people often wear:\"}],\n",
                "    max_tokens=1,\n",
                "    temperature=1.0,\n",
                "    logprobs=True,\n",
                "    # TODO: Replace the hardcoded value with your variable\n",
                "    top_logprobs=5\n",
                ")\n",
                "\n",
                "print(\"Top token probabilities:\")\n",
                "try:\n",
                "    if response.choices[0].logprobs and response.choices[0].logprobs.content:\n",
                "        for token_info in response.choices[0].logprobs.content:\n",
                "            print(f\"\\nGenerated token: {token_info.token}\")\n",
                "            if token_info.top_logprobs:\n",
                "                for alt in token_info.top_logprobs:\n",
                "                    print(f\"  {alt.token} → {alt.logprob:.3f}\")\n",
                "    else:\n",
                "        print(\"No token probabilities returned.\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "\n",
                "# TODO: Run your code with different values (3, 5, and 7) for the number of top log probabilities\n",
                "# TODO: Compare the results and notice how the number of alternative tokens changes\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# Task 1: Create a variable to control how many top token probabilities to return\n",
                "num_top_logprobs = 5  # You can change this to 3, 5, or 7 to test\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Name a color that people often wear:\"}],\n",
                "    max_tokens=1,\n",
                "    temperature=1.0,\n",
                "    logprobs=True,\n",
                "    # Task 2: Replace the hardcoded value with your variable\n",
                "    top_logprobs=num_top_logprobs\n",
                ")\n",
                "\n",
                "# Task 4: Make the output message reflect the number chosen\n",
                "print(f\"Top {num_top_logprobs} token probabilities:\")\n",
                "try:\n",
                "    if response.choices[0].logprobs and response.choices[0].logprobs.content:\n",
                "        for token_info in response.choices[0].logprobs.content:\n",
                "            print(f\"\\nGenerated token: {token_info.token}\")\n",
                "            if token_info.top_logprobs:\n",
                "                for alt in token_info.top_logprobs:\n",
                "                    print(f\"  {alt.token} → {alt.logprob:.3f}\")\n",
                "    else:\n",
                "        print(\"No token probabilities returned.\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "\n",
                "# Task 3: Run your code with different values (3, 5, and 7) for the number of top log probabilities\n",
                "# When you set num_top_logprobs = 3, you will see the top 3 alternatives.\n",
                "# When you set num_top_logprobs = 5, you will see the top 5 alternatives.\n",
                "# When you set num_top_logprobs = 7, you will see the top 7 alternatives.\n",
                "# By changing this single variable and rerunning the script, you can easily control the number of returned probabilities.\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Filtering Tokens by Probability Threshold\n",
                "\n",
                "\n",
                "Now that you've made your token probability code more flexible, let's take it a step further by focusing on the most relevant information. In real-world applications, we often want to filter out low-probability tokens and focus only on the most likely options.\n",
                "\n",
                "In this exercise, you'll add a filtering mechanism to your code that shows only tokens with log probabilities above a certain threshold. This helps clean up your output and allows you to focus on what matters most.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Add a threshold variable (try starting with -1.0)\n",
                "Modify the display loop to show only tokens above this threshold\n",
                "Track how many tokens pass your filter\n",
                "Add a summary showing how many tokens were filtered out\n",
                "Try experimenting with different threshold values (-0.5, -1.0, -2.0) to see how they affect your results. This filtering technique will help you develop a more practical understanding of how to interpret and use log probabilities in your analytical work.\n",
                "\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# TODO: Add a threshold variable for filtering log probabilities\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Name a color that people often wear:\"}],\n",
                "    max_tokens=1,\n",
                "    temperature=1.0,\n",
                "    logprobs=True,\n",
                "    top_logprobs=5\n",
                ")\n",
                "\n",
                "print(\"Top token probabilities:\")\n",
                "try:\n",
                "    if response.choices[0].logprobs and response.choices[0].logprobs.content:\n",
                "        for token_info in response.choices[0].logprobs.content:\n",
                "            print(f\"\\nGenerated token: {token_info.token}\")\n",
                "            if token_info.top_logprobs:\n",
                "                # TODO: Add variables to track total and filtered token counts\n",
                "                \n",
                "                for alt in token_info.top_logprobs:\n",
                "                    # TODO: Add a conditional check to only print tokens above the threshold\n",
                "                    print(f\"  {alt.token} → {alt.logprob:.3f}\")\n",
                "                    # TODO: Update the counter for filtered tokens if needed\n",
                "                \n",
                "                # TODO: Add a summary showing how many tokens were filtered out\n",
                "    else:\n",
                "        print(\"No token probabilities returned.\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "\n",
                "# Try different threshold values (-0.5, -1.0, -2.0) to see how they affect the results\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# Task 1: Add a threshold variable for filtering log probabilities\n",
                "logprob_threshold = -1.0  # You can change this to -0.5, -1.0, or -2.0\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Name a color that people often wear:\"}],\n",
                "    max_tokens=1,\n",
                "    temperature=1.0,\n",
                "    logprobs=True,\n",
                "    top_logprobs=5\n",
                ")\n",
                "\n",
                "print(\"Top token probabilities (filtered with a threshold of \" + str(logprob_threshold) + \"):\")\n",
                "try:\n",
                "    if response.choices[0].logprobs and response.choices[0].logprobs.content:\n",
                "        for token_info in response.choices[0].logprobs.content:\n",
                "            print(f\"\\nGenerated token: {token_info.token}\")\n",
                "            if token_info.top_logprobs:\n",
                "                # Task 2: Add variables to track total and filtered token counts\n",
                "                total_tokens = len(token_info.top_logprobs)\n",
                "                filtered_tokens_count = 0\n",
                "                \n",
                "                print(\"Tokens above threshold:\")\n",
                "                \n",
                "                for alt in token_info.top_logprobs:\n",
                "                    # Task 3: Add a conditional check to only print tokens above the threshold\n",
                "                    if alt.logprob >= logprob_threshold:\n",
                "                        print(f\"  {alt.token} → {alt.logprob:.3f}\")\n",
                "                    else:\n",
                "                        # Task 4: Update the counter for filtered tokens\n",
                "                        filtered_tokens_count += 1\n",
                "                \n",
                "                # Task 5: Add a summary showing how many tokens were filtered out\n",
                "                print(f\"\\nSummary: {total_tokens - filtered_tokens_count} of {total_tokens} tokens shown.\")\n",
                "    else:\n",
                "        print(\"No token probabilities returned.\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "\n",
                "# Try different threshold values (-0.5, -1.0, -2.0) to see how they affect the results\n",
                "# For a threshold of -0.5, you will likely see only a few of the top tokens.\n",
                "# For a threshold of -1.0, you will see a broader range of high-confidence tokens.\n",
                "# For a threshold of -2.0, most of the top 5 tokens will likely be displayed.\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
