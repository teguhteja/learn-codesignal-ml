{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exploring Temperature Sensitivity in LLM Outputs\n",
                "\n",
                "# Introduction: What Is Temperature in LLMs?\n",
                "\n",
                "Welcome back\\! In the previous lesson, you learned how to measure and interpret token usage in large language models (LLMs). That knowledge is important for understanding how efficiently a model processes information. In this lesson, we will focus on a different aspect of LLM behavior: the **temperature** parameter.\n",
                "\n",
                "The **temperature** setting is a key way to control how creative or predictable a model’s responses are. When you send a prompt to an LLM, the model can generate many possible outputs. The **temperature** parameter lets you adjust how much randomness the model uses when picking its next word or phrase. A low **temperature** makes the model more focused and deterministic, while a higher **temperature** encourages more variety and creativity in the responses.\n",
                "\n",
                "Understanding **temperature** is important for benchmarking because it helps you see how the same model can behave differently depending on this setting. By the end of this lesson, you will know how to run simple experiments to observe these differences and interpret what they mean for your use case.\n",
                "\n",
                "-----\n",
                "\n",
                "## How Temperature Changes Model Responses\n",
                "\n",
                "The **temperature** parameter is a number, usually between 0 and 2, that you can set when generating text with an LLM. When the **temperature** is set to **0**, the model will always pick the most likely next word, making its responses very predictable and consistent. As you increase the **temperature**, the model becomes more willing to take risks and choose less likely words, which can make its responses more creative or surprising.\n",
                "\n",
                "If you set the **temperature** above **1**, the model becomes even more random and unpredictable. While the model is still valid and will generate output, the responses may start to lose coherence or relevance, as the model is more likely to select unusual or unexpected words. This can be useful for brainstorming or creative writing, but may not be suitable if you need reliable or factual answers.\n",
                "\n",
                "On the other hand, setting the **temperature** to a negative value is not valid. Most LLM APIs will return an error or ignore the setting if you try to use a negative **temperature**. Always use a value of **0** or higher to ensure the model behaves as expected.\n",
                "\n",
                "-----\n",
                "\n",
                "## Example: Comparing Responses at Different Temperatures\n",
                "\n",
                "Let’s look at a practical example to see how **temperature** affects model outputs. In this example, you will use the OpenAI Python client to send the same prompt to the model three times, each with a different **temperature** setting. The prompt asks the model to describe a completely fictional animal found in a magical forest.\n",
                "\n",
                "Here is the code you will use:\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "client = OpenAI()\n",
                "prompt = \"Describe a completely fictional animal found in a magical forest.\"\n",
                "temperatures = [0.0, 0.7, 1.2]\n",
                "for temp in temperatures:\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "        temperature=temp,\n",
                "        max_tokens=150\n",
                "    )\n",
                "    reply = response.choices[0].message.content.strip()\n",
                "    print(f\"\\nTemperature: {temp}\")\n",
                "    print(f\"Response: {reply}\\n\")\n",
                "```\n",
                "\n",
                "In this code, you first define your **prompt** and a list of three **temperature** values: **0.0**, **0.7**, and **1.2**. For each **temperature**, you send the **prompt** to the model and print out the response. The only thing that changes between each run is the **temperature** value.\n",
                "\n",
                "When you run this code, you might see output like the following (your results may vary):\n",
                "\n",
                "```\n",
                "Temperature: 0.0\n",
                "Response: The Glimmerfox is a small, agile creature with shimmering silver fur and bright blue eyes. It is known for its ability to blend into the moonlit mist of the magical forest, making it nearly invisible to predators and travelers alike.\n",
                "\n",
                "Temperature: 0.7\n",
                "Response: The Glimmerfox is a rare animal with iridescent fur that changes color depending on its mood. It has long, feathery ears and a tail that glows softly in the dark. The Glimmerfox is said to bring good luck to anyone who spots it in the magical forest.\n",
                "\n",
                "Temperature: 1.2\n",
                "Response: Deep in the magical forest lives the Whimsyhorn, a floating, jelly-like creature with rainbow stripes and a single spiraled antler. It sings lullabies to the trees at night and leaves trails of sparkling dust wherever it drifts.\n",
                "```\n",
                "\n",
                "Notice how the response at **temperature 0.0** is very straightforward and safe, while the response at **0.7** adds more creative details. At **1.2**, the model invents a completely new animal with imaginative features. This shows how increasing the **temperature** leads to more diverse and creative outputs.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary And What’s Next\n",
                "\n",
                "In this lesson, you learned what the **temperature** parameter is and how it affects the behavior of large language models. You saw that low **temperature** values make the model’s responses more predictable, while higher **temperature** values encourage creativity and variety. You also worked through a code example that compared model outputs at different **temperature** settings, helping you see these effects in action.\n",
                "\n",
                "Next, you will get a chance to practice running your own **temperature** experiments. Try different prompts and **temperature** values to see how the model’s responses change. This hands-on practice will help you build intuition for when to use different **temperature** settings in your own projects."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Low and High Temperature Outputs\n",
                "\n",
                "Now that you understand what the temperature parameter is, let's see it in action! In this exercise, you'll experiment with how different temperature values affect the creativity of LLM outputs.\n",
                "\n",
                "You'll start with a script that uses a low temperature (0.0) to generate a description of a fictional animal. After observing the initial output, you'll modify the code to use a higher temperature (1.2) instead.\n",
                "\n",
                "Your tasks are:\n",
                "\n",
                "Run the code with the initial low temperature setting\n",
                "Change the temperatures list to use the higher value (1.2)\n",
                "Run the code again and observe the differences\n",
                "Add a print statement explaining what you notice about the outputs\n",
                "By comparing these outputs directly, you'll develop a better intuition for how temperature controls the balance between predictability and creativity in LLM responses.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "prompt = \"Describe a completely fictional animal found in a magical forest.\"\n",
                "\n",
                "# TODO: After running the code with this temperature value,\n",
                "# replace it with a higher value (1.2) to see how the output changes\n",
                "temperatures = [0.0]\n",
                "\n",
                "for temp in temperatures:\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "        temperature=temp,\n",
                "        max_tokens=150\n",
                "    )\n",
                "    reply = response.choices[0].message.content.strip()\n",
                "    print(f\"\\nTemperature: {temp}\")\n",
                "    print(f\"Response: {reply}\\n\")\n",
                "```\n",
                "\n",
                "### How Temperature Affects LLM Outputs: A Practical Example\n",
                "\n",
                "In this exercise, you will run a Python script to see the effect of the `temperature` parameter on the creativity and predictability of a large language model's output. You will start with a low temperature and then increase it to see the difference.\n",
                "\n",
                "**Step 1: Run the code with a low temperature (0.0)**\n",
                "When you run the provided script with `temperatures = [0.0]`, the model is highly deterministic. It will likely produce a very similar, if not identical, response each time it is run.\n",
                "\n",
                "**Example Output with temperature 0.0:**\n",
                "\n",
                "```\n",
                "Temperature: 0.0\n",
                "Response: The Luminafox is a small, ethereal creature with shimmering silver fur and large, luminous blue eyes. It is known for its ability to absorb and emit moonlight, allowing it to navigate the darkest parts of the magical forest with ease.\n",
                "```\n",
                "\n",
                "The output is factual and straightforward, almost like an entry in a bestiary. It's a \"safe\" and predictable response.\n",
                "\n",
                "**Step 2: Change the temperature to a higher value (1.2)**\n",
                "Now, you will edit the script to change the `temperatures` list to `[1.2]`. A higher temperature value encourages the model to be more random and creative.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "prompt = \"Describe a completely fictional animal found in a magical forest.\"\n",
                "\n",
                "# TODO: After running the code with this temperature value,\n",
                "# replace it with a higher value (1.2) to see how the output changes\n",
                "temperatures = [1.2]\n",
                "\n",
                "for temp in temperatures:\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "        temperature=temp,\n",
                "        max_tokens=150\n",
                "    )\n",
                "    reply = response.choices[0].message.content.strip()\n",
                "    print(f\"\\nTemperature: {temp}\")\n",
                "    print(f\"Response: {reply}\\n\")\n",
                "```\n",
                "\n",
                "**Step 3: Run the code again and observe the differences**\n",
                "When you run the modified script, you'll see a very different kind of output. The model is now more willing to \"take risks\" and choose less probable words, resulting in a more imaginative and less predictable response.\n",
                "\n",
                "**Example Output with temperature 1.2:**\n",
                "\n",
                "```\n",
                "Temperature: 1.2\n",
                "Response: Deep within the Whisperwood glades lives the Gleamwing, a feline-like being with transparent, moth-like wings that hum with starlight. Its body is a constellation of moss-green and opalescent scales, and it communicates by chirping echoes that ripple through the forest, revealing lost memories.\n",
                "```\n",
                "\n",
                "**Step 4: Explain what you notice**\n",
                "Now, add a print statement to your code to explain your observations.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "prompt = \"Describe a completely fictional animal found in a magical forest.\"\n",
                "\n",
                "temperatures = [1.2]\n",
                "\n",
                "for temp in temperatures:\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "        temperature=temp,\n",
                "        max_tokens=150\n",
                "    )\n",
                "    reply = response.choices[0].message.content.strip()\n",
                "    print(f\"\\nTemperature: {temp}\")\n",
                "    print(f\"Response: {reply}\\n\")\n",
                "    print(\"Observation: The output with the higher temperature is far more creative and unpredictable, inventing a new type of creature with unique characteristics like a 'Gleamwing' and 'chirping echoes that ripple through the forest.' The low-temperature output was more predictable and descriptive.\")\n",
                "```\n",
                "\n",
                "By completing this exercise, you'll see firsthand how the `temperature` parameter is a powerful tool for controlling the balance between predictability and creativity in LLM outputs.\n",
                "\n",
                "[ChatGPT in Sheets: how to use OpenAI temperature parameter](https://www.youtube.com/watch?v=51HjX7eLAik)\n",
                "This video explains what the temperature parameter is and provides a demonstration of how it can be used to control the creativity of AI-generated content.\n",
                "http://googleusercontent.com/youtube_content/1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exploring the Temperature Creativity Spectrum\n",
                "\n",
                "Excellent work comparing low and high temperature settings! Now, let's take your understanding a step further by exploring the full spectrum of temperature values.\n",
                "\n",
                "In this exercise, you'll expand your experiment to observe how LLM outputs change gradually across multiple temperature settings.\n",
                "\n",
                "You'll start with code that uses just one temperature value (0.0). Your task is to modify the temperature list to include several values (0.0, 0.3, 0.7, and 1.2), allowing you to see the progressive shift from predictable to creative responses.\n",
                "\n",
                "When you run your updated code, pay attention to how:\n",
                "\n",
                "Responses at 0.0 remain focused and consistent\n",
                "Middle values (0.3 and 0.7) show increasing creativity\n",
                "The highest value (1.9) produces the most varied and imaginative output\n",
                "This side-by-side comparison will give you practical insight into selecting the right temperature setting for different applications — whether you need factual consistency or creative exploration.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "prompt = \"Describe a completely fictional animal found in a magical forest.\"\n",
                "\n",
                "# TODO: Expand this list to include multiple temperature values (0.0, 0.3, 0.7, 1.2)\n",
                "# This will let you observe how responses gradually change from predictable to creative\n",
                "temperatures = [0.0]\n",
                "\n",
                "for temp in temperatures:\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "        temperature=temp,\n",
                "        max_tokens=150\n",
                "    )\n",
                "    reply = response.choices[0].message.content.strip()\n",
                "    print(f\"\\nTemperature: {temp}\")\n",
                "    print(f\"Response: {reply}\\n\")\n",
                "```\n",
                "\n",
                "To complete this exercise, you need to modify the `temperatures` list in the provided code to include the values `0.0`, `0.3`, `0.7`, and `1.2`. This will allow you to see the progressive change in the model's output as the `temperature` increases.\n",
                "\n",
                "### Modified Code\n",
                "\n",
                "Here is the updated code with the expanded `temperatures` list:\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "prompt = \"Describe a completely fictional animal found in a magical forest.\"\n",
                "\n",
                "# TODO: Expand this list to include multiple temperature values (0.0, 0.3, 0.7, 1.2)\n",
                "# This will let you observe how responses gradually change from predictable to creative\n",
                "temperatures = [0.0, 0.3, 0.7, 1.2]\n",
                "\n",
                "for temp in temperatures:\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "        temperature=temp,\n",
                "        max_tokens=150\n",
                "    )\n",
                "    reply = response.choices[0].message.content.strip()\n",
                "    print(f\"\\nTemperature: {temp}\")\n",
                "    print(f\"Response: {reply}\\n\")\n",
                "```\n",
                "\n",
                "### Running the Code\n",
                "\n",
                "When you run this script, the loop will execute four times, once for each temperature value. The output you see will demonstrate the \"creativity spectrum\" in action. Your exact results may vary, but the general pattern will be consistent:\n",
                "\n",
                "  * **Temperature: 0.0**\n",
                "\n",
                "      * The output will be the most predictable and straightforward. It will likely describe a logical, almost \"standard\" fantastical creature. This is the setting for reliable, consistent responses.\n",
                "\n",
                "  * **Temperature: 0.3**\n",
                "\n",
                "      * The response will be slightly more creative than at 0.0. It might add some unique details or a more imaginative name, but it will still be grounded and coherent.\n",
                "\n",
                "  * **Temperature: 0.7**\n",
                "\n",
                "      * Here, the creativity will become more pronounced. The model will start to introduce more surprising elements or behaviors for the creature, moving further away from a \"safe\" response. This is a good setting for a balance of creativity and coherence.\n",
                "\n",
                "  * **Temperature: 1.2**\n",
                "\n",
                "      * This is where the model is most random. The output will likely be highly imaginative, inventing a creature with unusual, whimsical, or even surreal characteristics. This setting is ideal for brainstorming and generating highly unique content.\n",
                "\n",
                "By running this code, you will get a clear, side-by-side view of how a simple change to the `temperature` parameter can dramatically alter the tone and creativity of a large language model's output."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Models at Same Temperature\n",
                "\n",
                "Cosmo\n",
                "Just now\n",
                "Read message aloud\n",
                "You've done a fantastic job exploring how temperature affects a single model's outputs! Now, let's add another dimension to your understanding: comparing how different models respond to the same temperature setting.\n",
                "\n",
                "In this exercise, you'll modify the code to compare GPT-3.5 Turbo and GPT-4 using the same prompt and temperature value, and you'll also print out the token usage for each response.\n",
                "\n",
                "Your tasks are:\n",
                "\n",
                "Add GPT-4 to the models list\n",
                "Update the code to loop through both models\n",
                "Use a consistent mid-range temperature (0.8)\n",
                "For each response, print the model name, temperature, response, and token usage (prompt, completion, and total tokens)\n",
                "Run the code and carefully observe the differences between models\n",
                "When comparing the outputs, look for variations in the level of detail, creativity, writing style, and how closely each model follows the prompt. Also, pay attention to the token usage for each model—this can give you insight into how efficiently each model generates its responses. This comparison will help you understand that temperature sensitivity isn't just about the parameter itself — the model architecture also plays a crucial role in determining how \"creative\" the output becomes.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "prompt =  \"Invent a completely new holiday and describe how people celebrate it.\"\n",
                "\n",
                "\n",
                "# TODO: Add a second model (gpt-4) to this list to compare responses\n",
                "models = [\"gpt-3.5-turbo\"]\n",
                "\n",
                "# Using a mid-range temperature\n",
                "temperature = 0.8\n",
                "\n",
                "# TODO: Update this code to loop through both models and print token usage\n",
                "response = client.chat.completions.create(\n",
                "    model=models[0],\n",
                "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "    temperature=temperature\n",
                ")\n",
                "reply = response.choices[0].message.content.strip()\n",
                "usage = response.usage\n",
                "print(f\"\\nModel: {models[0]}\")\n",
                "print(f\"Temperature: {temperature}\")\n",
                "print(f\"Response: {reply}\\n\")\n",
                "print(f\"Token Usage: prompt={usage.prompt_tokens}, completion={usage.completion_tokens}, total={usage.total_tokens}\")\n",
                "```\n",
                "\n",
                "Excellent\\! This is a great next step to see how the underlying model architecture impacts the output, even with the same temperature setting.\n",
                "\n",
                "Here is the updated code that will loop through both `gpt-3.5-turbo` and `gpt-4`, using a consistent temperature of `0.8` and printing the token usage for each.\n",
                "\n",
                "### Modified Code\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "prompt = \"Invent a completely new holiday and describe how people celebrate it.\"\n",
                "\n",
                "# TODO: Add a second model (gpt-4) to this list to compare responses\n",
                "models = [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
                "\n",
                "# Using a consistent mid-range temperature\n",
                "temperature = 0.8\n",
                "\n",
                "# TODO: Update this code to loop through both models and print token usage\n",
                "for model_name in models:\n",
                "    response = client.chat.completions.create(\n",
                "        model=model_name,\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "        temperature=temperature\n",
                "    )\n",
                "    reply = response.choices[0].message.content.strip()\n",
                "    usage = response.usage\n",
                "    \n",
                "    print(f\"\\nModel: {model_name}\")\n",
                "    print(f\"Temperature: {temperature}\")\n",
                "    print(f\"Response: {reply}\\n\")\n",
                "    print(f\"Token Usage: prompt={usage.prompt_tokens}, completion={usage.completion_tokens}, total={usage.total_tokens}\")\n",
                "\n",
                "```\n",
                "\n",
                "### Running the Code\n",
                "\n",
                "When you run this script, you will see the output for both models. Pay close attention to the differences in their responses. Here is an example of the kind of output you might get (your results may vary):\n",
                "\n",
                "**Example Output with GPT-3.5 Turbo:**\n",
                "\n",
                "```\n",
                "Model: gpt-3.5-turbo\n",
                "Temperature: 0.8\n",
                "Response: \"The Festival of Aetherbloom\" is a holiday celebrated on the last day of spring, when the air is filled with the ethereal pollen of the Aetherbloom flower. People celebrate by creating \"Aether Lanterns\" out of transparent paper and dried Aetherbloom petals. They release these glowing lanterns into the night sky, symbolizing the release of old energies and welcoming new growth. Families gather in open fields to share stories and a sweet nectar called \"Aether-Dew,\" believing it imbues them with creativity for the coming season. The night concludes with a communal dance under the light of the Aether Lanterns, celebrating the beauty and renewal of nature.\n",
                "\n",
                "Token Usage: prompt=21, completion=99, total=120\n",
                "```\n",
                "\n",
                "**Example Output with GPT-4:**\n",
                "\n",
                "```\n",
                "Model: gpt-4\n",
                "Temperature: 0.8\n",
                "Response: \"The Festival of Lumen\" is a new holiday that marks the summer solstice, a day dedicated to celebrating the inner light and potential within every individual. On this day, communities gather at sunrise, each person carrying a small, unlit candle. As the sun crests the horizon, a designated \"Lumen-bearer\" lights their candle from a central flame, and this light is passed from person to person until every candle is aglow. This ritual, called the \"Chain of Illumination,\" symbolizes how one person's light can inspire and brighten the path of others. The day is spent in reflection and community service, where people perform acts of kindness to \"share their light.\" In the evening, the candles are arranged in intricate patterns on the ground, creating a glowing mural of shared intentions. The celebration culminates in a feast of light-themed foods like citrus salads and sun-baked breads, followed by a communal bonfire where people share stories of personal growth and inspiration. The Festival of Lumen serves as a reminder that even in the darkest times, we all carry a light within us that can be shared to illuminate the world.\n",
                "\n",
                "Token Usage: prompt=21, completion=175, total=196\n",
                "```\n",
                "\n",
                "### Observation\n",
                "\n",
                "When you compare the outputs, you'll likely notice several key differences:\n",
                "\n",
                "  * **Creativity and Detail:** While both models follow the prompt, the GPT-4 response tends to be more detailed, nuanced, and structurally complex. It often invents more sophisticated rituals and deeper symbolic meanings for the holiday.\n",
                "  * **Token Usage:** The token usage for GPT-4 is often higher for a similar task. This is because GPT-4's responses are typically longer and more detailed, which requires more tokens to generate. This highlights the trade-off between model power, response quality, and computational cost.\n",
                "  * **Style and Cohesion:** GPT-4's narrative often feels more polished and coherent, creating a richer and more fully realized concept for the holiday.\n",
                "\n",
                "This exercise demonstrates that while `temperature` controls the randomness, the model's inherent capabilities and size (which influence factors like context management and vocabulary) are equally important in determining the quality and character of the final output."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
