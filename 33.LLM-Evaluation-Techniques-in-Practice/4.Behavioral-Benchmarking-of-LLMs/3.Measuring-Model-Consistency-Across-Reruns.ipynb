{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Measuring Model Consistency Across Reruns\n",
                "\n",
                "# Introduction: Why Consistency Matters in LLMs\n",
                "\n",
                "Welcome back\\! In the last lesson, you explored how the **temperature** parameter affects the creativity and randomness of large language model (LLM) outputs. You saw that higher temperature values make responses more varied, while lower values make them more predictable. In this lesson, we will focus on a related but distinct concept: **model consistency**.\n",
                "\n",
                "Model consistency refers to whether an LLM gives the same answer every time you ask it the same question, using the same settings. This is important for benchmarking because, in many applications, you want to know if the model is reliable and repeatable. If a model gives different answers to the same prompt under the same conditions, it can be hard to trust or evaluate its performance. By the end of this lesson, you will know how to check for consistency in LLM outputs and understand why this matters for your projects.\n",
                "\n",
                "-----\n",
                "\n",
                "## Temperature and Consistency: The Connection\n",
                "\n",
                "As a quick reminder from the previous lesson, the **temperature** parameter controls how much randomness the model uses when generating text. When you set **temperature=0**, you are telling the model to always pick the most likely next word at each step. This setting is used when you want the model to be as deterministic as possible, which means it should give the same answer every time for the same prompt.\n",
                "\n",
                "For consistency testing, we use **temperature=0** because it removes randomness from the model’s output. If the model still gives different answers with this setting, it means there is some underlying non-determinism in the model or the API. This is a key part of behavioral benchmarking, as it helps you understand the limits of model reliability.\n",
                "\n",
                "-----\n",
                "\n",
                "## Example: Testing Consistency with Repeated Prompts\n",
                "\n",
                "Let’s look at a practical example to see how you can measure model consistency. In this example, you will use the OpenAI Python client to send the same prompt to the model five times, always with **temperature=0**. The prompt asks the model to name three planets in our solar system.\n",
                "\n",
                "Here is the code you will use:\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "client = OpenAI()\n",
                "prompt = \"Name three planets in our solar system.\"\n",
                "responses = []\n",
                "\n",
                "# Run the same prompt 5 times at temperature=0\n",
                "for i in range(5):\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        temperature=0.0,\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    )\n",
                "    answer = response.choices[0].message.content.strip()\n",
                "    responses.append(answer)\n",
                "\n",
                "# Display all responses\n",
                "print(\"Model responses at temperature=0:\")\n",
                "for idx, r in enumerate(responses, 1):\n",
                "    print(f\"{idx}: {r}\")\n",
                "\n",
                "# Check if all responses are identical\n",
                "unique_responses = set(responses)\n",
                "if len(unique_responses) == 1:\n",
                "    print(\"\\n✅ The model was fully consistent.\")\n",
                "else:\n",
                "    print(\"\\n⚠️ The model produced different outputs.\")\n",
                "```\n",
                "\n",
                "In this code, you first define your **prompt** and set up an empty list to store the responses. You then run a loop five times, each time sending the same prompt to the model with **temperature=0.0**. After each response, you extract the answer and add it to your list. Once all responses are collected, you print them out to see if they are the same. Finally, you check if all responses are identical by converting the list to a set and checking its length. If the set has only one item, the model was fully consistent; otherwise, it produced different outputs.\n",
                "\n",
                "### Sample Output\n",
                "\n",
                "When you run this code, you might see output like the following:\n",
                "\n",
                "```\n",
                "Model responses at temperature=0:\n",
                "1: Mercury, Venus, Earth\n",
                "2: Mercury, Venus, Earth\n",
                "3: Mercury, Venus, Earth\n",
                "4: Mercury, Venus, Earth\n",
                "5: Mercury, Venus, Earth\n",
                "\n",
                "✅ The model was fully consistent.\n",
                "```\n",
                "\n",
                "Or, in rare cases, you might see something like:\n",
                "\n",
                "```\n",
                "Model responses at temperature=0:\n",
                "1: Mercury, Venus, Earth\n",
                "2: Mercury, Venus, Earth\n",
                "3: Mercury, Earth, Mars\n",
                "4: Mercury, Venus, Earth\n",
                "5: Mercury, Venus, Earth\n",
                "\n",
                "⚠️ The model produced different outputs.\n",
                "```\n",
                "\n",
                "This output helps you quickly see whether the model is consistent or not.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding the Results\n",
                "\n",
                "If all the responses are the same, it means the model is fully consistent for that prompt and setting. This is what you would expect when using **temperature=0**, since the model should always pick the most likely answer. Consistency is important for tasks where you need reliable, repeatable results, such as automated grading, data extraction, or any application where you want to avoid surprises.\n",
                "\n",
                "If you see different outputs, even with **temperature=0**, it suggests that there may be some randomness or instability in the model or the API. This is useful to know, as it can affect how much you trust the model’s outputs in critical applications. Measuring consistency in this way is a simple but powerful tool for understanding model behavior.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and What’s Next\n",
                "\n",
                "In this lesson, you learned how to measure the consistency of LLM outputs by sending the same prompt multiple times at **temperature=0**. You saw how to collect and compare the responses and how to interpret the results. Consistency is a key part of benchmarking, especially when you need reliable answers from your model.\n",
                "\n",
                "Next, you will get a chance to practice running and modifying this code yourself. Try using different prompts or running the loop more times to see if the model remains consistent. This hands-on practice will help you build confidence in evaluating LLM behavior for your own projects."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Refactoring for Cleaner Consistency Checks\n",
                "\n",
                "Now that you've simplified the consistency-checking code, let's improve its structure! Good software design involves breaking down code into reusable functions that each do one thing well.\n",
                "\n",
                "In this exercise, you'll refactor the code by creating a dedicated check_consistency function for checking consistency. This approach makes your code more organized and easier to maintain.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Create a new check_consistency function that takes a list of responses as input\n",
                "Make the function use the set conversion technique to check whether all responses are identical\n",
                "Have the function return a boolean value (True if consistent, False if not)\n",
                "Update the main code to use your new function for the consistency check\n",
                "This refactoring skill is valuable beyond just this example — it's a fundamental practice that will help you write cleaner, more professional code throughout your programming journey.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# TODO: Create a function called check_consistency that:\n",
                "# - Takes a list of responses as input\n",
                "# - Converts the list to a set to find unique responses\n",
                "# - Returns True if all responses are identical (set has only 1 item)\n",
                "# - Returns False otherwise\n",
                "\n",
                "prompt = \"Name three planets in our solar system.\"\n",
                "\n",
                "responses = []\n",
                "\n",
                "# Run the same prompt 5 times at temperature=0\n",
                "for i in range(5):\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        temperature=0.0,\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    )\n",
                "    answer = response.choices[0].message.content.strip()\n",
                "    responses.append(answer)\n",
                "\n",
                "# Display all responses\n",
                "print(\"Model responses at temperature=0:\")\n",
                "for idx, r in enumerate(responses, 1):\n",
                "    print(f\"{idx}: \\n{r}\", end=f\"\\n {'-'*50} \\n\")\n",
                "\n",
                "# TODO: Replace the code below with a call to your check_consistency function\n",
                "# and use its return value to determine which message to print\n",
                "# (Do not check set here; just call your function)\n",
                "if check_consistency(responses):\n",
                "    print(\"\\n✅ The model was fully consistent.\")\n",
                "else:\n",
                "    print(\"\\n⚠️ The model produced different outputs.\")\n",
                "```\n",
                "\n",
                "To refactor your code for a cleaner consistency check, you need to create a `check_consistency` function and then call that function from the main part of your script. This will make your code more modular and easier to read.\n",
                "\n",
                "### Refactored Code\n",
                "\n",
                "Here is the updated code with the new `check_consistency` function and the modified main loop that uses it.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# TODO: Create a function called check_consistency that:\n",
                "# - Takes a list of responses as input\n",
                "# - Converts the list to a set to find unique responses\n",
                "# - Returns True if all responses are identical (set has only 1 item)\n",
                "# - Returns False otherwise\n",
                "def check_consistency(responses):\n",
                "    \"\"\"\n",
                "    Checks if all responses in a list are identical.\n",
                "\n",
                "    Args:\n",
                "        responses (list): A list of strings, where each string is an LLM response.\n",
                "\n",
                "    Returns:\n",
                "        bool: True if all responses are the same, False otherwise.\n",
                "    \"\"\"\n",
                "    unique_responses = set(responses)\n",
                "    return len(unique_responses) == 1\n",
                "\n",
                "prompt = \"Name three planets in our solar system.\"\n",
                "\n",
                "responses = []\n",
                "\n",
                "# Run the same prompt 5 times at temperature=0\n",
                "for i in range(5):\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        temperature=0.0,\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    )\n",
                "    answer = response.choices[0].message.content.strip()\n",
                "    responses.append(answer)\n",
                "\n",
                "# Display all responses\n",
                "print(\"Model responses at temperature=0:\")\n",
                "for idx, r in enumerate(responses, 1):\n",
                "    print(f\"{idx}: \\n{r}\", end=f\"\\n {'-'*50} \\n\")\n",
                "\n",
                "# TODO: Replace the code below with a call to your check_consistency function\n",
                "# and use its return value to determine which message to print\n",
                "# (Do not check set here; just call your function)\n",
                "if check_consistency(responses):\n",
                "    print(\"\\n✅ The model was fully consistent.\")\n",
                "else:\n",
                "    print(\"\\n⚠️ The model produced different outputs.\")\n",
                "```\n",
                "\n",
                "### Key Changes\n",
                "\n",
                "The new `check_consistency` function encapsulates the logic for determining consistency. It takes a list as an argument, performs the set conversion, and returns a simple `True` or `False`. This allows the main part of the script to be more readable. Instead of the logic being directly in the `if/else` block, the main script now simply says, \"if the responses are consistent, do this, otherwise, do that.\"\n",
                "\n",
                "This refactoring is a fundamental practice in software engineering, as it promotes **code reusability** and **modularity**, making it easier to read and debug your code in the future."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameterizing Consistency Test Runs\n",
                "\n",
                "Cosmo\n",
                "Just now\n",
                "Read message aloud\n",
                "After refactoring our code with functions, let's make it even more flexible! When testing model consistency, you might want to run different numbers of tests depending on how thorough you need to be.\n",
                "\n",
                "In this exercise, you'll make the consistency testing code configurable by:\n",
                "\n",
                "Adding a num_runs variable at the top of the file\n",
                "Updating the for-loop to use this variable instead of the hard-coded value 5\n",
                "Adding a temperature variable at the top of the file, and updating the code to use this variable instead of the hard-coded value 0.0\n",
                "Try setting temperature to a higher value, such as 0.7, and observe how this affects the consistency of the model's responses.\n",
                "This simple change will let you easily adjust how many times you test the model and how much randomness is in the responses, without editing code in multiple places. Try experimenting with different values for num_runs and temperature to see how the number of tests and the temperature affect your confidence in the consistency results. Being able to quickly adjust testing parameters is an essential skill when conducting thorough model evaluations.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# TODO: Add a variable called num_runs here to control how many times we run the prompt\n",
                "# TODO: Add a variable called temperature here to control the model's randomness\n",
                "\n",
                "prompt = \"Name three planets in our solar system.\"\n",
                "\n",
                "responses = []\n",
                "\n",
                "# TODO: Update this loop to use num_runs instead of the hard-coded value 5\n",
                "# TODO: Update the temperature parameter to use the temperature variable instead of 0.0\n",
                "for i in range(5):\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        temperature=0.0,\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    )\n",
                "    answer = response.choices[0].message.content.strip()\n",
                "    responses.append(answer)\n",
                "\n",
                "# Display all responses\n",
                "print(\"Model responses at temperature=0:\")\n",
                "for idx, r in enumerate(responses, 1):\n",
                "    print(f\"{idx}: \\n{r}\", end=f\"\\n {'-'*50} \\n\")\n",
                "\n",
                "# Check if all responses are identical\n",
                "unique_responses = set(responses)\n",
                "if len(unique_responses) == 1:\n",
                "    print(\"\\n✅ The model was fully consistent.\")\n",
                "else:\n",
                "    print(\"\\n⚠️ The model produced different outputs.\")\n",
                "\n",
                "```\n",
                "\n",
                "Making your code more flexible by **parameterizing** key variables is a fundamental practice in software engineering. By adding variables for `num_runs` and `temperature`, you can easily adjust your consistency tests without changing the core logic.\n",
                "\n",
                "Here is the updated code with the new variables.\n",
                "\n",
                "### Parameterized Consistency Test Code\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# TODO: Add a variable called num_runs here to control how many times we run the prompt\n",
                "# TODO: Add a variable called temperature here to control the model's randomness\n",
                "num_runs = 5\n",
                "temperature = 0.0\n",
                "\n",
                "prompt = \"Name three planets in our solar system.\"\n",
                "\n",
                "responses = []\n",
                "\n",
                "# TODO: Update this loop to use num_runs instead of the hard-coded value 5\n",
                "# TODO: Update the temperature parameter to use the temperature variable instead of 0.0\n",
                "for i in range(num_runs):\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        temperature=temperature,\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    )\n",
                "    answer = response.choices[0].message.content.strip()\n",
                "    responses.append(answer)\n",
                "\n",
                "# Display all responses\n",
                "print(f\"Model responses at temperature={temperature} for {num_runs} runs:\")\n",
                "for idx, r in enumerate(responses, 1):\n",
                "    print(f\"{idx}: \\n{r}\", end=f\"\\n {'-'*50} \\n\")\n",
                "\n",
                "# Check if all responses are identical\n",
                "unique_responses = set(responses)\n",
                "if len(unique_responses) == 1:\n",
                "    print(\"\\n✅ The model was fully consistent.\")\n",
                "else:\n",
                "    print(\"\\n⚠️ The model produced different outputs.\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### How to Experiment with the Parameters\n",
                "\n",
                "**1. Testing with a low temperature:**\n",
                "Keep `temperature` at `0.0`. This is the ideal setting for checking if a model is truly deterministic. You will almost always see a `fully consistent` result because the model is instructed to always choose the most probable token.\n",
                "\n",
                "**2. Testing with a higher temperature:**\n",
                "Change the `temperature` variable to a value like `0.7`.\n",
                "\n",
                "```python\n",
                "num_runs = 5\n",
                "temperature = 0.7\n",
                "```\n",
                "\n",
                "When you run the code with this setting, you will likely see **different outputs** in the response list. This is because the higher temperature introduces randomness, allowing the model to choose from a wider range of tokens at each step, making its output less predictable and therefore inconsistent.\n",
                "\n",
                "**3. Changing `num_runs`:**\n",
                "You can also increase the number of runs to get a more thorough test. For example, setting `num_runs = 10` will send the same prompt 10 times, providing a larger sample size to observe consistency.\n",
                "\n",
                "This approach gives you a flexible and powerful way to evaluate a model's behavior under various conditions."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tracking Response Patterns with Frequency Analysis\n",
                "\n",
                "Now that you've made your consistency testing code more flexible with parameters, let's take our analysis one step further! When working with higher temperature settings, it's helpful to know not just whether responses vary, but how often each unique response appears.\n",
                "\n",
                "In this exercise, you'll add response frequency tracking to your code:\n",
                "\n",
                "Create a dictionary to count how many times each unique response occurs.\n",
                "Update this counter each time you get a response from the model.\n",
                "Add code to display a summary showing each response and its frequency.\n",
                "This frequency analysis is particularly valuable when testing at higher temperatures, as it helps you identify which responses are most common even when the model isn't perfectly consistent. Understanding these patterns gives you deeper insights into model behavior and can help you make better decisions about reliability for your specific use case.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# Number of times to run the same prompt\n",
                "num_runs = 10\n",
                "\n",
                "# Temperature setting for the model\n",
                "temperature = 0.7  # Try changing this to see how it affects consistency\n",
                "\n",
                "prompt = \"Name three planets in our solar system.\"\n",
                "\n",
                "responses = []\n",
                "# TODO: Create a dictionary to track the frequency of each unique response\n",
                "\n",
                "# Run the same prompt num_runs times at the specified temperature\n",
                "for i in range(num_runs):\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        temperature=temperature,\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    )\n",
                "    answer = response.choices[0].message.content.strip()\n",
                "    responses.append(answer)\n",
                "    \n",
                "    # TODO: Update the frequency counter dictionary:\n",
                "    # - If the answer already exists as a key, increment its count\n",
                "    # - If it's a new answer, add it to the dictionary with count 1\n",
                "\n",
                "# Display all responses\n",
                "print(f\"Model responses at temperature={temperature}:\")\n",
                "for idx, r in enumerate(responses, 1):\n",
                "    print(f\"{idx}: \\n{r}\", end=f\"\\n {'-'*50} \\n\")\n",
                "\n",
                "# TODO: Add code to display a frequency summary of responses\n",
                "# The summary should show each unique response and how many times it occurred\n",
                "\n",
                "# Check if all responses are identical\n",
                "unique_responses = set(responses)\n",
                "if len(unique_responses) == 1:\n",
                "    print(\"\\n✅ The model was fully consistent.\")\n",
                "else:\n",
                "    print(f\"\\n⚠️ The model produced {len(unique_responses)} different outputs.\")\n",
                "```\n",
                "\n",
                "Frequency analysis is a great way to understand the **distribution** of a model's responses, especially with higher temperature settings. It shows you not only if the outputs vary, but which variations are most likely to occur.\n",
                "\n",
                "Here is the updated code that includes a dictionary to track and display the frequency of each unique response.\n",
                "\n",
                "### Code with Frequency Analysis\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "from collections import Counter\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "# Number of times to run the same prompt\n",
                "num_runs = 10\n",
                "\n",
                "# Temperature setting for the model\n",
                "temperature = 0.7  # Try changing this to see how it affects consistency\n",
                "\n",
                "prompt = \"Name three planets in our solar system.\"\n",
                "\n",
                "responses = []\n",
                "# TODO: Create a dictionary to track the frequency of each unique response\n",
                "response_counts = Counter()\n",
                "\n",
                "# Run the same prompt num_runs times at the specified temperature\n",
                "for i in range(num_runs):\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        temperature=temperature,\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    )\n",
                "    answer = response.choices[0].message.content.strip()\n",
                "    responses.append(answer)\n",
                "    \n",
                "    # TODO: Update the frequency counter dictionary:\n",
                "    # - If the answer already exists as a key, increment its count\n",
                "    # - If it's a new answer, add it to the dictionary with count 1\n",
                "    response_counts[answer] += 1\n",
                "\n",
                "# Display all responses\n",
                "print(f\"Model responses at temperature={temperature}:\")\n",
                "for idx, r in enumerate(responses, 1):\n",
                "    print(f\"{idx}: \\n{r}\", end=f\"\\n {'-'*50} \\n\")\n",
                "\n",
                "# TODO: Add code to display a frequency summary of responses\n",
                "# The summary should show each unique response and how many times it occurred\n",
                "print(\"\\n--- Response Frequency Summary ---\")\n",
                "for response, count in response_counts.items():\n",
                "    print(f\"Count: {count}\")\n",
                "    print(f\"Response: {response}\\n\")\n",
                "\n",
                "# Check if all responses are identical\n",
                "unique_responses = set(responses)\n",
                "if len(unique_responses) == 1:\n",
                "    print(\"\\n✅ The model was fully consistent.\")\n",
                "else:\n",
                "    print(f\"\\n⚠️ The model produced {len(unique_responses)} different outputs.\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### How it Works\n",
                "\n",
                "The key to this solution is the `collections.Counter` class, a specialized dictionary designed for this exact purpose. As the loop runs, `response_counts[answer] += 1` efficiently handles both cases:\n",
                "\n",
                "  * If `answer` is a new key, it's added to the dictionary with a value of `1`.\n",
                "  * If `answer` already exists, its current value is incremented by one.\n",
                "\n",
                "This new code provides a more informative view of the model's behavior. Instead of just seeing that the outputs are different, you can now see which ones are the **most common** or **most probable** at that specific temperature. This is essential for applications where you need to balance creativity with a degree of reliability."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
