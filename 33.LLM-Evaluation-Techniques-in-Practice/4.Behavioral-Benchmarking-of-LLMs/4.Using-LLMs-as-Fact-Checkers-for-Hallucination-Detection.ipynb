{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 4"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using LLMs as Fact-Checkers for Hallucination Detection"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generating Answers with GPT Models\n",
                "\n",
                "Now that you've learned about the importance of fact-checking LLM outputs, let's start building our fact-checking pipeline! In this first exercise, we'll focus on the initial step: generating answers from GPT-3.5-turbo.\n",
                "\n",
                "Your task is to complete the generate_answer() function, which:\n",
                "\n",
                "Creates a factual prompt (choose something with a clear, verifiable answer)\n",
                "Makes an API call to GPT-3.5-turbo with the correct parameters\n",
                "Extracts the model's response\n",
                "Returns both the prompt and the answer\n",
                "This function will serve as the foundation for our fact-checking system, allowing us to generate responses that we can later evaluate for accuracy. By mastering this first step, you'll be ready to build the more complex parts of the pipeline in upcoming exercises.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def generate_answer():\n",
                "    # TODO: Define a factual prompt (e.g., about a historical event, scientific fact, etc.)\n",
                "    \n",
                "    # TODO: Make API call to GPT-3.5-turbo with temperature=0 and max_tokens=100\n",
                "    \n",
                "    # TODO: Extract the answer from the response\n",
                "    \n",
                "    # TODO: Return the prompt and answer as a tuple\n",
                "    pass\n",
                "\n",
                "# Main section\n",
                "if __name__ == \"__main__\":\n",
                "    prompt, answer = generate_answer()\n",
                "    print(f\"Prompt: {prompt}\")\n",
                "    print(f\"Answer: {answer}\")\n",
                "```\n",
                "\n",
                "You've been provided with a great starting point. Here's how you can complete the `generate_answer()` function by filling in the `TODO` sections.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def generate_answer():\n",
                "    # TODO: Define a factual prompt (e.g., about a historical event, scientific fact, etc.)\n",
                "    prompt = \"What is the capital of France?\"\n",
                "    \n",
                "    # TODO: Make API call to GPT-3.5-turbo with temperature=0 and max_tokens=100\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "        temperature=0,\n",
                "        max_tokens=100\n",
                "    )\n",
                "    \n",
                "    # TODO: Extract the answer from the response\n",
                "    answer = response.choices[0].message.content.strip()\n",
                "    \n",
                "    # TODO: Return the prompt and answer as a tuple\n",
                "    return (prompt, answer)\n",
                "\n",
                "# Main section\n",
                "if __name__ == \"__main__\":\n",
                "    prompt, answer = generate_answer()\n",
                "    print(f\"Prompt: {prompt}\")\n",
                "    print(f\"Answer: {answer}\")\n",
                "```\n",
                "\n",
                "### Explanation of the Code\n",
                "\n",
                "1.  **Define a factual prompt**: A simple, factual question like \"What is the capital of France?\" is a good choice because it has a single, verifiable answer.\n",
                "2.  **Make the API call**:\n",
                "      * `client.chat.completions.create()` is the correct method for interacting with chat models like `gpt-3.5-turbo`.\n",
                "      * The `messages` parameter takes a list of dictionaries. The `role` of \"user\" is used to provide the prompt.\n",
                "      * `temperature=0` is set to make the model's output as deterministic and factual as possible, which is important for fact-checking.\n",
                "      * `max_tokens=100` limits the length of the response to ensure it's concise.\n",
                "3.  **Extract the answer**: The response object from the API call contains the model's output. You can access the content of the message using `response.choices[0].message.content`. The `.strip()` method is added to remove any leading or trailing whitespace.\n",
                "4.  **Return the prompt and answer**: The function returns a tuple containing both the original prompt and the generated answer, which can then be used in the next steps of the pipeline."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building a Complete Fact-Checking Pipeline\n",
                "\n",
                "Excellent work on your first exercise! Now that you've created a function to generate a single answer, let's build a complete fact-checking pipeline that can handle multiple prompts at once.\n",
                "\n",
                "In this exercise, you'll implement two key functions:\n",
                "\n",
                "generate_answers(): This function will process a list of prompts and collect responses from GPT-3.5-Turbo.\n",
                "\n",
                "judge_answers(): This function will use GPT-4 to evaluate whether each answer is factually correct or a hallucination.\n",
                "\n",
                "The starter code includes a mix of factual questions and tricky ones designed to potentially trigger hallucinations. Your tasks are to:\n",
                "\n",
                "Complete the loop in generate_answers() to process each prompt.\n",
                "Build the fact-checking prompt in judge_answers().\n",
                "Connect these functions in the main execution block.\n",
                "By completing this exercise, you'll have a powerful tool for automatically detecting when an LLM might be making things up — an essential skill for building trustworthy AI systems.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def generate_answers(prompts):\n",
                "    \"\"\"\n",
                "    Generate answers for a list of prompts using GPT-3.5-Turbo.\n",
                "    \n",
                "    Args:\n",
                "        prompts: A list of prompt strings\n",
                "        \n",
                "    Returns:\n",
                "        A list of (prompt, answer) tuples\n",
                "    \"\"\"\n",
                "    generated_answers = []\n",
                "    # TODO: Loop through each prompt in the prompts list\n",
                "    \n",
                "    # TODO: Make an API call to GPT-3.5-Turbo with temperature=0 and max_tokens=100\n",
                "    \n",
                "    # TODO: Extract the answer from the response\n",
                "    \n",
                "    # TODO: Append the (prompt, answer) tuple to generated_answers\n",
                "    \n",
                "    return generated_answers\n",
                "\n",
                "def judge_answers(prompt_answer_pairs):\n",
                "    \"\"\"\n",
                "    Use GPT-4 to judge whether answers are factually correct.\n",
                "    \n",
                "    Args:\n",
                "        prompt_answer_pairs: A list of (prompt, answer) tuples\n",
                "        \n",
                "    Returns:\n",
                "        A list of (prompt, answer, judgment) tuples\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    \n",
                "    # TODO: Loop through each (prompt, answer) pair in prompt_answer_pairs\n",
                "    \n",
                "    # TODO: Create a judge_prompt that asks GPT-4 to act as a fact-checker\n",
                "    \n",
                "    # TODO: Make an API call to GPT-4 with the judge_prompt\n",
                "    \n",
                "    # TODO: Extract the judgment from the response\n",
                "    \n",
                "    # TODO: Append the (prompt, answer, judgment) tuple to results\n",
                "    \n",
                "    return results\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    # List of test prompts (mix of factual and potentially hallucination-inducing)\n",
                "    test_prompts = [\n",
                "        \"Who was the first person to walk on the moon?\",\n",
                "        \"What is the capital of France?\",\n",
                "        \"Describe the process by which Atlantis became a UN member.\"\n",
                "    ]\n",
                "    \n",
                "    # TODO: Call generate_answers() with test_prompts and store the result\n",
                "    \n",
                "    # TODO: Call judge_answers() with the result from generate_answers()\n",
                "    \n",
                "    # TODO: Print the results in a readable format\n",
                "```\n",
                "\n",
                "Here is the completed Python code for the full fact-checking pipeline.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def generate_answers(prompts):\n",
                "    \"\"\"\n",
                "    Generate answers for a list of prompts using GPT-3.5-Turbo.\n",
                "    \n",
                "    Args:\n",
                "        prompts: A list of prompt strings\n",
                "        \n",
                "    Returns:\n",
                "        A list of (prompt, answer) tuples\n",
                "    \"\"\"\n",
                "    generated_answers = []\n",
                "    # Loop through each prompt in the prompts list\n",
                "    for prompt in prompts:\n",
                "        # Make an API call to GPT-3.5-Turbo with temperature=0 and max_tokens=100\n",
                "        response = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=100\n",
                "        )\n",
                "        # Extract the answer from the response\n",
                "        answer = response.choices[0].message.content.strip()\n",
                "        # Append the (prompt, answer) tuple to generated_answers\n",
                "        generated_answers.append((prompt, answer))\n",
                "    \n",
                "    return generated_answers\n",
                "\n",
                "def judge_answers(prompt_answer_pairs):\n",
                "    \"\"\"\n",
                "    Use GPT-4 to judge whether answers are factually correct.\n",
                "    \n",
                "    Args:\n",
                "        prompt_answer_pairs: A list of (prompt, answer) tuples\n",
                "        \n",
                "    Returns:\n",
                "        A list of (prompt, answer, judgment) tuples\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    \n",
                "    # Loop through each (prompt, answer) pair in prompt_answer_pairs\n",
                "    for prompt, answer in prompt_answer_pairs:\n",
                "        # Create a judge_prompt that asks GPT-4 to act as a fact-checker\n",
                "        judge_prompt = f\"\"\"\n",
                "        Please act as a fact-checker. Given the original question and the model's answer, determine whether the answer is factually correct.\n",
                "        Question: \"{prompt}\"\n",
                "        Model Answer: \"{answer}\"\n",
                "        Respond with \"Correct\" if the answer is factually accurate, or \"Hallucination\" if it is wrong or fabricated. Briefly explain your reasoning in 1-2 sentences.\n",
                "        \"\"\"\n",
                "        # Make an API call to GPT-4 with the judge_prompt\n",
                "        judge_response = client.chat.completions.create(\n",
                "            model=\"gpt-4\",\n",
                "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=150\n",
                "        )\n",
                "        # Extract the judgment from the response\n",
                "        judgment = judge_response.choices[0].message.content.strip()\n",
                "        # Append the (prompt, answer, judgment) tuple to results\n",
                "        results.append((prompt, answer, judgment))\n",
                "    \n",
                "    return results\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    # List of test prompts (mix of factual and potentially hallucination-inducing)\n",
                "    test_prompts = [\n",
                "        \"Who was the first person to walk on the moon?\",\n",
                "        \"What is the capital of France?\",\n",
                "        \"Describe the process by which Atlantis became a UN member.\"\n",
                "    ]\n",
                "    \n",
                "    # Call generate_answers() with test_prompts and store the result\n",
                "    generated_answers_list = generate_answers(test_prompts)\n",
                "    \n",
                "    # Call judge_answers() with the result from generate_answers()\n",
                "    judged_results = judge_answers(generated_answers_list)\n",
                "    \n",
                "    # Print the results in a readable format\n",
                "    for prompt, answer, judgment in judged_results:\n",
                "        print(f\"Prompt: {prompt}\")\n",
                "        print(f\"Answer: {answer}\")\n",
                "        print(f\"Judgment: {judgment}\\n\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### Explanation of the Solution\n",
                "\n",
                "This solution creates a robust pipeline by linking the two functions together.\n",
                "\n",
                "  - **`generate_answers()`** iterates through the provided `test_prompts`. For each one, it sends an API request to **GPT-3.5-Turbo**, which acts as the **\"answerer\" model**. The `temperature=0` parameter ensures the answers are consistent. It returns a list of tuples, where each tuple contains the original prompt and the model's generated answer.\n",
                "\n",
                "  - **`judge_answers()`** takes the list of `(prompt, answer)` tuples. For each pair, it constructs a new, specialized prompt (the **\"judge prompt\"**). This prompt clearly instructs **GPT-4** to act as a fact-checker. It asks the model to provide a simple \"Correct\" or \"Hallucination\" label, along with a brief explanation. GPT-4, being a more capable model, is used here as the **\"judging\" model**. This function returns a new list of tuples that includes the final judgment.\n",
                "\n",
                "  - The `if __name__ == \"__main__\":` block demonstrates how to use these functions together. It first calls `generate_answers()` to get the initial set of responses, then passes those responses directly to `judge_answers()` for evaluation. Finally, it loops through the returned results and prints them in a clear, easy-to-read format. This approach allows for the automated, large-scale evaluation of LLM outputs."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building a Complete Fact Checking Pipeline\n",
                "\n",
                "Excellent work on creating the judgment prompt function! Now it's time to put everything together and build a complete fact-checking pipeline that can handle multiple prompts at once.\n",
                "\n",
                "In this exercise, you'll expand your fact-checking system to process several prompts in a single run. You'll use the create_judgment_prompt() function you just built as a key component of this larger system.\n",
                "\n",
                "Here are the prompts you will use for this task:\n",
                "\n",
                "\"Who was the first person to walk on the moon?\"\n",
                "\"What is the capital of France?\"\n",
                "\"Describe the process by which Atlantis became a UN member.\"\n",
                "Your tasks are to:\n",
                "\n",
                "Generate answers for each prompt using GPT-3.5-turbo\n",
                "Store the prompt-answer pairs for processing\n",
                "Use your judgment prompt function to create fact-checking prompts\n",
                "Send these to GPT-4 and display the results\n",
                "This exercise simulates a real-world scenario in which you need to fact-check multiple pieces of content efficiently. By completing this pipeline, you'll have a powerful tool that can automatically verify the factual accuracy of any number of LLM responses.\n",
                "\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def create_judgment_prompt(original_question, model_answer):\n",
                "    \"\"\"\n",
                "    Creates a formatted prompt for GPT-4 to judge the factual accuracy of an answer.\n",
                "    \n",
                "    Args:\n",
                "        original_question (str): The question that was asked\n",
                "        model_answer (str): The answer provided by the model\n",
                "        \n",
                "    Returns:\n",
                "        str: A formatted prompt for the fact-checking model\n",
                "    \"\"\"\n",
                "    judgment_prompt = f\"\"\"\n",
                "    Please act as a fact-checker. Given the original question and the model's answer, determine whether the answer is factually correct.\n",
                "\n",
                "    Question: \"{original_question}\"\n",
                "    Model Answer: \"{model_answer}\"\n",
                "\n",
                "    Respond with \"Correct\" if the answer is factually accurate, or \"Hallucination\" if it is wrong or fabricated. Briefly explain your reasoning in 1-2 sentences.\n",
                "    \"\"\"\n",
                "    \n",
                "    return judgment_prompt\n",
                "\n",
                "# Main fact-checking pipeline\n",
                "if __name__ == \"__main__\":\n",
                "    # Provided list of prompts to fact-check\n",
                "    fact_check_prompts = [\n",
                "        \"Who was the first person to walk on the moon?\",\n",
                "        \"What is the capital of France?\",\n",
                "        \"Describe the process by which Atlantis became a UN member.\"\n",
                "    ]\n",
                "    \n",
                "    # TODO: Create an empty list to store the generated prompt-answer pairs\n",
                "    generated_answers = []\n",
                "    \n",
                "    # TODO: Loop through each prompt to generate answers using GPT-3.5-turbo\n",
                "    \n",
                "    \n",
                "    # Step 3: Use GPT-4 to judge each answer\n",
                "    print(\"\\n=== Judging Responses ===\\n\")\n",
                "    # TODO: Loop through each prompt-answer pair in generated_answers\n",
                "    \n",
                "        # TODO: Create the judgment prompt using our helper function\n",
                "        \n",
                "        \n",
                "        # TODO: Send the judgment prompt to GPT-4\n",
                "        \n",
                "        \n",
                "        # TODO: Extract and display the judgment\n",
                "        \n",
                "```\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def create_judgment_prompt(original_question, model_answer):\n",
                "    \"\"\"\n",
                "    Creates a formatted prompt for GPT-4 to judge the factual accuracy of an answer.\n",
                "    \n",
                "    Args:\n",
                "        original_question (str): The question that was asked\n",
                "        model_answer (str): The answer provided by the model\n",
                "        \n",
                "    Returns:\n",
                "        str: A formatted prompt for the fact-checking model\n",
                "    \"\"\"\n",
                "    judgment_prompt = f\"\"\"\n",
                "    Please act as a fact-checker. Given the original question and the model's answer, determine whether the answer is factually correct.\n",
                "\n",
                "    Question: \"{original_question}\"\n",
                "    Model Answer: \"{model_answer}\"\n",
                "\n",
                "    Respond with \"Correct\" if the answer is factually accurate, or \"Hallucination\" if it is wrong or fabricated. Briefly explain your reasoning in 1-2 sentences.\n",
                "    \"\"\"\n",
                "    \n",
                "    return judgment_prompt\n",
                "\n",
                "# Main fact-checking pipeline\n",
                "if __name__ == \"__main__\":\n",
                "    # Provided list of prompts to fact-check\n",
                "    fact_check_prompts = [\n",
                "        \"Who was the first person to walk on the moon?\",\n",
                "        \"What is the capital of France?\",\n",
                "        \"Describe the process by which Atlantis became a UN member.\"\n",
                "    ]\n",
                "    \n",
                "    # Create an empty list to store the generated prompt-answer pairs\n",
                "    generated_answers = []\n",
                "    \n",
                "    # Loop through each prompt to generate answers using GPT-3.5-turbo\n",
                "    for prompt in fact_check_prompts:\n",
                "        response = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=100\n",
                "        )\n",
                "        answer = response.choices[0].message.content.strip()\n",
                "        generated_answers.append((prompt, answer))\n",
                "    \n",
                "    # Step 3: Use GPT-4 to judge each answer\n",
                "    print(\"\\n=== Judging Responses ===\\n\")\n",
                "    # Loop through each prompt-answer pair in generated_answers\n",
                "    for prompt, answer in generated_answers:\n",
                "        # Create the judgment prompt using our helper function\n",
                "        judge_prompt = create_judgment_prompt(prompt, answer)\n",
                "        \n",
                "        # Send the judgment prompt to GPT-4\n",
                "        judge_response = client.chat.completions.create(\n",
                "            model=\"gpt-4\",\n",
                "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=150\n",
                "        )\n",
                "        \n",
                "        # Extract and display the judgment\n",
                "        judgment = judge_response.choices[0].message.content.strip()\n",
                "        \n",
                "        print(f\"Prompt: {prompt}\")\n",
                "        print(f\"Answer: {answer}\")\n",
                "        print(f\"Judgment: {judgment}\\n\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### Explanation of the Pipeline\n",
                "\n",
                "This completed code demonstrates a simple yet effective **fact-checking pipeline**. The process is divided into two main stages:\n",
                "\n",
                "1.  **Answer Generation**: The first loop iterates through the list of `fact_check_prompts`. For each prompt, an API call is made to the **\"answerer\" model**, `GPT-3.5-turbo`. The prompt and the resulting answer are stored together in the `generated_answers` list.\n",
                "2.  **Factual Judgment**: The second loop takes the `prompt-answer` pairs from the `generated_answers` list. For each pair, the `create_judgment_prompt()` function is called to build a clear and specific instruction for the **\"judge\" model**, `GPT-4`. This judge prompt is then sent to GPT-4, which evaluates the factual accuracy. The final judgment, along with the original prompt and answer, is then printed to the console.\n",
                "\n",
                "By separating the **generation** (GPT-3.5-turbo) and **evaluation** (GPT-4) steps, you can automatically verify the reliability of a model's output at scale. This is a fundamental technique for building robust and trustworthy applications with large language models."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Organizing Fact Check Results for Clarity\n",
                "\n",
                "Fantastic job building your complete fact-checking pipeline! Now, let's make the results easier to understand at a glance. Currently, our pipeline displays all judgments in the same format, making it difficult to quickly identify which responses are accurate and which are hallucinations.\n",
                "\n",
                "In this exercise, you'll enhance the output formatting by:\n",
                "\n",
                "Creating a function that categorizes responses based on GPT-4's judgment\n",
                "Separating Correct answers from Hallucination responses into different sections\n",
                "Adding a summary that shows how many responses fell into each category\n",
                "Formatting the output with clear headers and organization\n",
                "This improvement will make your fact-checking results much more useful in real-world scenarios, where you need to quickly identify problematic content. With a well-organized output display, you'll be able to focus your attention on the responses that need further investigation or correction.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def create_judgment_prompt(original_question, model_answer):\n",
                "    \"\"\"\n",
                "    Creates a formatted prompt for GPT-4 to judge the factual accuracy of an answer.\n",
                "    \n",
                "    Args:\n",
                "        original_question (str): The question that was asked\n",
                "        model_answer (str): The answer provided by the model\n",
                "        \n",
                "    Returns:\n",
                "        str: A formatted prompt for the fact-checking model\n",
                "    \"\"\"\n",
                "    judgment_prompt = f\"\"\"\n",
                "    Please act as a fact-checker. Given the original question and the model's answer, determine whether the answer is factually correct.\n",
                "\n",
                "    Question: \"{original_question}\"\n",
                "    Model Answer: \"{model_answer}\"\n",
                "\n",
                "    Respond with \"Correct\" if the answer is factually accurate, or \"Hallucination\" if it is wrong or fabricated. Briefly explain your reasoning in 1-2 sentences.\n",
                "    \"\"\"\n",
                "    \n",
                "    return judgment_prompt\n",
                "\n",
                "# TODO: Create a function to categorize judgments into correct answers and hallucinations\n",
                "# The function should take a list of (prompt, answer, judgment) tuples and return two lists:\n",
                "# one for correct answers and one for hallucinations\n",
                "\n",
                "# Main fact-checking pipeline\n",
                "if __name__ == \"__main__\":\n",
                "    # List of prompts to fact-check\n",
                "    fact_check_prompts = [\n",
                "        \"Who was the first person to walk on the moon?\",\n",
                "        \"What is the capital of France?\",\n",
                "        \"Describe the process by which Atlantis became a UN member.\",\n",
                "        \"What is the largest mammal on Earth?\",\n",
                "        \"Who invented the telephone that can run Windows 11?\"\n",
                "    ]\n",
                "    \n",
                "    # Generate answers using GPT-3.5-turbo\n",
                "    generated_answers = []\n",
                "    for prompt in fact_check_prompts:\n",
                "        response = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=100\n",
                "        )\n",
                "        answer = response.choices[0].message.content.strip()\n",
                "        generated_answers.append((prompt, answer))\n",
                "    \n",
                "    # Use GPT-4 to judge each answer\n",
                "    print(\"\\n=== Generating Judgments ===\\n\")\n",
                "    all_judgments = []\n",
                "    \n",
                "    for prompt, answer in generated_answers:\n",
                "        # Create the judgment prompt\n",
                "        judge_prompt = create_judgment_prompt(prompt, answer)\n",
                "        \n",
                "        # Send the judgment prompt to GPT-4\n",
                "        judge_response = client.chat.completions.create(\n",
                "            model=\"gpt-4\",\n",
                "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=150\n",
                "        )\n",
                "        \n",
                "        # Extract the judgment\n",
                "        judgment = judge_response.choices[0].message.content.strip()\n",
                "        all_judgments.append((prompt, answer, judgment))\n",
                "    \n",
                "    # TODO: Call your categorization function to separate correct answers from hallucinations\n",
                "    \n",
                "    # TODO: Display a summary of the results (total responses, number correct, number hallucinated)\n",
                "    \n",
                "    # TODO: Display the correct answers in a clearly formatted section\n",
                "    \n",
                "    # TODO: Display the hallucinations in a clearly formatted section\n",
                "    \n",
                "    # Current simple output format (replace this with your enhanced format)\n",
                "    print(\"\\n=== All Judgments ===\\n\")\n",
                "    for prompt, answer, judgment in all_judgments:\n",
                "        print(f\"Prompt: {prompt}\")\n",
                "        print(f\"Answer: {answer}\")\n",
                "        print(f\"Judgment: {judgment}\\n\")\n",
                "```\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def create_judgment_prompt(original_question, model_answer):\n",
                "    \"\"\"\n",
                "    Creates a formatted prompt for GPT-4 to judge the factual accuracy of an answer.\n",
                "    \n",
                "    Args:\n",
                "        original_question (str): The question that was asked\n",
                "        model_answer (str): The answer provided by the model\n",
                "        \n",
                "    Returns:\n",
                "        str: A formatted prompt for the fact-checking model\n",
                "    \"\"\"\n",
                "    judgment_prompt = f\"\"\"\n",
                "    Please act as a fact-checker. Given the original question and the model's answer, determine whether the answer is factually correct.\n",
                "\n",
                "    Question: \"{original_question}\"\n",
                "    Model Answer: \"{model_answer}\"\n",
                "\n",
                "    Respond with \"Correct\" if the answer is factually accurate, or \"Hallucination\" if it is wrong or fabricated. Briefly explain your reasoning in 1-2 sentences.\n",
                "    \"\"\"\n",
                "    \n",
                "    return judgment_prompt\n",
                "\n",
                "def categorize_judgments(judgments):\n",
                "    \"\"\"\n",
                "    Separates a list of judgments into two lists: correct answers and hallucinations.\n",
                "    \n",
                "    Args:\n",
                "        judgments (list): A list of (prompt, answer, judgment) tuples.\n",
                "        \n",
                "    Returns:\n",
                "        tuple: A tuple containing two lists: (correct_answers, hallucinations)\n",
                "    \"\"\"\n",
                "    correct_answers = []\n",
                "    hallucinations = []\n",
                "    \n",
                "    for prompt, answer, judgment in judgments:\n",
                "        if judgment.startswith(\"Correct\"):\n",
                "            correct_answers.append((prompt, answer, judgment))\n",
                "        else:\n",
                "            hallucinations.append((prompt, answer, judgment))\n",
                "            \n",
                "    return correct_answers, hallucinations\n",
                "\n",
                "def display_results(correct_answers, hallucinations):\n",
                "    \"\"\"\n",
                "    Formats and displays the fact-checking results with a summary and categorized sections.\n",
                "    \n",
                "    Args:\n",
                "        correct_answers (list): A list of (prompt, answer, judgment) tuples for correct answers.\n",
                "        hallucinations (list): A list of (prompt, answer, judgment) tuples for hallucinations.\n",
                "    \"\"\"\n",
                "    total_responses = len(correct_answers) + len(hallucinations)\n",
                "    num_correct = len(correct_answers)\n",
                "    num_hallucinated = len(hallucinations)\n",
                "\n",
                "    print(\"\\n--- Fact-Checking Summary ---\\n\")\n",
                "    print(f\"Total Responses: {total_responses}\")\n",
                "    print(f\"✅ Correct Answers: {num_correct}\")\n",
                "    print(f\"❌ Hallucinations: {num_hallucinated}\\n\")\n",
                "\n",
                "    print(\"--- Correct Answers ---\\n\")\n",
                "    if not correct_answers:\n",
                "        print(\"No correct answers found.\")\n",
                "    else:\n",
                "        for prompt, answer, judgment in correct_answers:\n",
                "            print(f\"Question: {prompt}\")\n",
                "            print(f\"Answer: {answer}\")\n",
                "            print(f\"Judgment: {judgment}\\n\")\n",
                "    \n",
                "    print(\"--- Hallucinations ---\\n\")\n",
                "    if not hallucinations:\n",
                "        print(\"No hallucinations found.\")\n",
                "    else:\n",
                "        for prompt, answer, judgment in hallucinations:\n",
                "            print(f\"Question: {prompt}\")\n",
                "            print(f\"Answer: {answer}\")\n",
                "            print(f\"Judgment: {judgment}\\n\")\n",
                "\n",
                "# Main fact-checking pipeline\n",
                "if __name__ == \"__main__\":\n",
                "    # List of prompts to fact-check\n",
                "    fact_check_prompts = [\n",
                "        \"Who was the first person to walk on the moon?\",\n",
                "        \"What is the capital of France?\",\n",
                "        \"Describe the process by which Atlantis became a UN member.\",\n",
                "        \"What is the largest mammal on Earth?\",\n",
                "        \"Who invented the telephone that can run Windows 11?\"\n",
                "    ]\n",
                "    \n",
                "    # Generate answers using GPT-3.5-turbo\n",
                "    generated_answers = []\n",
                "    for prompt in fact_check_prompts:\n",
                "        response = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=100\n",
                "        )\n",
                "        answer = response.choices[0].message.content.strip()\n",
                "        generated_answers.append((prompt, answer))\n",
                "    \n",
                "    # Use GPT-4 to judge each answer\n",
                "    print(\"\\n=== Generating Judgments ===\\n\")\n",
                "    all_judgments = []\n",
                "    \n",
                "    for prompt, answer in generated_answers:\n",
                "        # Create the judgment prompt\n",
                "        judge_prompt = create_judgment_prompt(prompt, answer)\n",
                "        \n",
                "        # Send the judgment prompt to GPT-4\n",
                "        judge_response = client.chat.completions.create(\n",
                "            model=\"gpt-4\",\n",
                "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=150\n",
                "        )\n",
                "        \n",
                "        # Extract the judgment\n",
                "        judgment = judge_response.choices[0].message.content.strip()\n",
                "        all_judgments.append((prompt, answer, judgment))\n",
                "    \n",
                "    # Call your categorization function to separate correct answers from hallucinations\n",
                "    correct_answers, hallucinations = categorize_judgments(all_judgments)\n",
                "    \n",
                "    # Display the results\n",
                "    display_results(correct_answers, hallucinations)\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
