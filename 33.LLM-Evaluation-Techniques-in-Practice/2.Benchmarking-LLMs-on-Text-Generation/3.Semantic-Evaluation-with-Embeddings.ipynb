{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Semantic Evaluation with Embeddings\n",
                "\n",
                "### Semantic Evaluation with Embeddings\n",
                "\n",
                "#### Introduction to Semantic Evaluation with Embeddings\n",
                "\n",
                "Welcome to the next step in your journey of benchmarking Large Language Models (LLMs) for text generation. In the previous lesson, you learned how to evaluate text generation models using the ROUGE metric, which focuses on string similarity. Now, we will explore **semantic evaluation**, which goes beyond surface-level text comparison to understand the meaning behind the words. This is where **embeddings** come into play.\n",
                "\n",
                "Embeddings are numerical representations of text that capture semantic meaning, allowing us to measure how similar two pieces of text are in terms of their underlying concepts. They transform words, phrases, or even entire documents into vectors in a continuous vector space. This transformation enables the comparison of texts based on their meanings rather than just their literal content.\n",
                "\n",
                "To evaluate semantic similarity, we use **cosine similarity** as a metric. Cosine similarity measures the cosine of the angle between two vectors, providing a value between -1 and 1. A value of 1 indicates that the vectors are identical in direction, meaning the texts are semantically similar. A value of 0 indicates orthogonality, meaning no similarity, and -1 indicates completely opposite meanings. This lesson will guide you through the process of using embeddings to assess the quality of generated summaries, providing a deeper understanding of model performance.\n",
                "\n",
                "-----\n",
                "\n",
                "#### Understanding Cosine Similarity: The Math Behind Semantic Evaluation\n",
                "\n",
                "Cosine similarity is a key metric for comparing the semantic similarity between two text embeddings. It measures the cosine of the angle between two vectors in a multi-dimensional space, providing a value between -1 and 1.\n",
                "\n",
                "The mathematical formula for cosine similarity between two vectors $A$ and $B$ is:\n",
                "\n",
                "$$cosine\\_similarity = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
                "\n",
                "  - **Dot product** ($A \\\\cdot B$): This is the sum of the products of the corresponding elements of the two vectors. If $A=[a\\_1, a\\_2, ..., a\\_n]$ and $B=[b\\_1, b\\_2, ..., b\\_n]$, then:\n",
                "\n",
                "$$A \\cdot B = a_1b_1 + a_2b_2 + \\dots + a_nb_n$$\n",
                "\n",
                "  - **Norm** ($|A|$): This is the length (or magnitude) of the vector, calculated as:\n",
                "\n",
                "$$\\|A\\| = \\sqrt{a_1^2 + a_2^2 + \\dots + a_n^2}$$\n",
                "\n",
                "The resulting cosine similarity value will be:\n",
                "\n",
                "  - **1** if the vectors are identical in direction (maximum similarity),\n",
                "  - **0** if the vectors are orthogonal (no similarity),\n",
                "  - **-1** if the vectors are diametrically opposed (opposite meaning).\n",
                "\n",
                "Understanding this formula helps clarify how embeddings are compared based on their meaning, not just their literal content.\n",
                "\n",
                "-----\n",
                "\n",
                "#### Setting Up the Environment\n",
                "\n",
                "Before we dive into the code, let's ensure your environment is ready. You will need the `openai`, `numpy`, and `csv` libraries. If you're working on your local machine, you can install these using `pip`:\n",
                "\n",
                "```bash\n",
                "pip install openai numpy\n",
                "```\n",
                "\n",
                "On CodeSignal, these libraries are pre-installed, so you can focus on the code without worrying about setup. This setup will allow us to interact with the OpenAI API, perform mathematical operations, and handle CSV files.\n",
                "\n",
                "-----\n",
                "\n",
                "#### Example: Calculating Semantic Similarity\n",
                "\n",
                "Now, let's walk through the code example to see how semantic similarity is calculated. We start by defining the `cosine_similarity` function, which uses the `dot` product and `norm` functions from `numpy` to compute the similarity between two vectors. This function is crucial for comparing the embeddings of the generated and reference summaries. Next, the `get_embedding` function interacts with the OpenAI API to obtain embeddings for a given text. This is done by calling the `embeddings.create` method with the appropriate model and input text. The main part of the code reads a CSV file containing articles and their summaries. For each article, a prompt is created to generate a summary using the GPT-4 model. The embeddings for both the generated summary and the reference summary are obtained using the `get_embedding` function. The cosine similarity between these embeddings is calculated and stored. Finally, the average semantic similarity score is printed, providing a quantitative measure of the model's performance.\n",
                "\n",
                "```python\n",
                "import csv\n",
                "from openai import OpenAI\n",
                "from numpy import dot\n",
                "from numpy.linalg import norm\n",
                "\n",
                "client = OpenAI()\n",
                "\n",
                "def cosine_similarity(a, b):\n",
                "    return dot(a, b) / (norm(a) * norm(b))\n",
                "\n",
                "def get_embedding(text):\n",
                "    return client.embeddings.create(\n",
                "        model=\"text-embedding-3-small\",\n",
                "        input=text\n",
                "    ).data[0].embedding\n",
                "\n",
                "with open(\"cnn_dailymail_subset.csv\") as f:\n",
                "    rows = list(csv.DictReader(f))\n",
                "\n",
                "scores = []\n",
                "for r in rows:\n",
                "    prompt = f\"Summarize the following article:\\n{r['article']}\"\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-4\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    ).choices[0].message.content.strip()\n",
                "    \n",
                "    ref_embed = get_embedding(r[\"summary\"])\n",
                "    resp_embed = get_embedding(response)\n",
                "\n",
                "    score = cosine_similarity(resp_embed, ref_embed)\n",
                "    scores.append(score)\n",
                "\n",
                "print(f\"Average Semantic Similarity Score: {sum(scores)/len(scores):.3f}\")\n",
                "```\n",
                "\n",
                "The output of this code will be an average semantic similarity score, which indicates how closely the generated summaries match the reference summaries in terms of meaning.\n",
                "\n",
                "-----\n",
                "\n",
                "#### Interpreting the Results and Troubleshooting\n",
                "\n",
                "The semantic similarity scores you obtain provide insight into the quality of the generated summaries. A higher score indicates a closer match to the reference summary, suggesting that the model has captured the essential meaning of the text. Conversely, a lower score may indicate that the generated summary is missing key concepts or includes irrelevant information. When interpreting these scores, consider the context and complexity of the text being summarized. If you encounter issues such as API errors or unexpected results, ensure that your API key is correctly configured and that the input text is formatted properly. Debugging these issues will help you achieve accurate and meaningful results.\n",
                "\n",
                "-----\n",
                "\n",
                "#### Summary and Preparation for Practice Exercises\n",
                "\n",
                "In this lesson, you learned how to use embeddings and cosine similarity to evaluate the semantic quality of text summaries. We covered the setup of the environment, the structure of the evaluation code, the mathematical foundation of cosine similarity, and how to interpret the results. This knowledge will be invaluable as you move on to the practice exercises, where you'll apply these concepts to assess the performance of text generation models. Remember, semantic evaluation provides a deeper understanding of model performance by focusing on meaning rather than just surface-level text similarity. Good luck with the exercises, and continue to explore the fascinating world of text generation\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementing Cosine Similarity for Vector Comparison\n",
                "\n",
                "Now that you've learned about semantic evaluation and how embeddings capture meaning, let's focus on one of the core components: the cosine similarity function. This mathematical operation is essential for comparing vector representations of text.\n",
                "\n",
                "In this exercise, you'll implement the cosine_similarity function, which calculates how similar two vectors are in terms of their direction. The function should:\n",
                "\n",
                "Take two vectors (a and b) as input\n",
                "Calculate the similarity using numpy's dot product and norm functions\n",
                "Handle edge cases properly, such as zero vectors\n",
                "Return a value between -1 and 1, where 1 means identical direction\n",
                "The test code is already set up with various vector pairs to verify that your implementation works correctly. By mastering this fundamental calculation, you'll build a solid foundation for evaluating semantic similarity in more complex text generation scenarios.\n",
                "\n",
                "```python\n",
                "from numpy import dot\n",
                "from numpy.linalg import norm\n",
                "import numpy as np\n",
                "\n",
                "def cosine_similarity(a, b):\n",
                "    # TODO: Implement the cosine similarity function\n",
                "    # Remember to handle edge cases like zero vectors\n",
                "    # The function should return a value between -1 and 1\n",
                "    pass\n",
                "\n",
                "# Identical vectors\n",
                "identical_vector_1 = np.array([0.5, 0.5, 0.5, 0.5])\n",
                "identical_vector_2 = np.array([0.5, 0.5, 0.5, 0.5])\n",
                "\n",
                "# Orthogonal vectors\n",
                "orthogonal_vector_1 = np.array([1, 0, 0, 0])\n",
                "orthogonal_vector_2 = np.array([0, 1, 0, 0])\n",
                "\n",
                "# Vectors with moderate similarity\n",
                "moderate_vector_1 = np.array([1, 0, 1, 0])\n",
                "moderate_vector_2 = np.array([0.5, 0.5, 0.5, 0.5])\n",
                "\n",
                "# Test with identical vectors (should be 1.0)\n",
                "similarity = cosine_similarity(identical_vector_1, identical_vector_2)\n",
                "print(f\"Similarity between identical vectors: {similarity:.4f}\")\n",
                "\n",
                "# Test with orthogonal vectors (should be 0.0)\n",
                "similarity = cosine_similarity(orthogonal_vector_1, orthogonal_vector_2)\n",
                "print(f\"Similarity between orthogonal vectors: {similarity:.4f}\")\n",
                "\n",
                "# Test with vectors with moderate similarity (should be around 0.5)\n",
                "similarity = cosine_similarity(moderate_vector_1, moderate_vector_2)\n",
                "print(f\"Similarity between moderately similar vectors: {similarity:.4f}\")\n",
                "```\n",
                "\n",
                "I can certainly help with that. Here is the completed code with the `cosine_similarity` function implemented as requested.\n",
                "\n",
                "```python\n",
                "from numpy import dot\n",
                "from numpy.linalg import norm\n",
                "import numpy as np\n",
                "\n",
                "def cosine_similarity(a, b):\n",
                "    # Calculate the dot product of the two vectors\n",
                "    dot_product = dot(a, b)\n",
                "    \n",
                "    # Calculate the norms (magnitudes) of the two vectors\n",
                "    norm_a = norm(a)\n",
                "    norm_b = norm(b)\n",
                "    \n",
                "    # Handle the edge case where one or both vectors are zero vectors.\n",
                "    # The dot product of a zero vector is 0, so the norm would also be 0.\n",
                "    # Division by zero would cause an error.\n",
                "    if norm_a == 0 or norm_b == 0:\n",
                "        return 0.0\n",
                "    \n",
                "    # Calculate the cosine similarity\n",
                "    similarity = dot_product / (norm_a * norm_b)\n",
                "    \n",
                "    return similarity\n",
                "\n",
                "# Identical vectors\n",
                "identical_vector_1 = np.array([0.5, 0.5, 0.5, 0.5])\n",
                "identical_vector_2 = np.array([0.5, 0.5, 0.5, 0.5])\n",
                "\n",
                "# Orthogonal vectors\n",
                "orthogonal_vector_1 = np.array([1, 0, 0, 0])\n",
                "orthogonal_vector_2 = np.array([0, 1, 0, 0])\n",
                "\n",
                "# Vectors with moderate similarity\n",
                "moderate_vector_1 = np.array([1, 0, 1, 0])\n",
                "moderate_vector_2 = np.array([0.5, 0.5, 0.5, 0.5])\n",
                "\n",
                "# Test with identical vectors (should be 1.0)\n",
                "similarity = cosine_similarity(identical_vector_1, identical_vector_2)\n",
                "print(f\"Similarity between identical vectors: {similarity:.4f}\")\n",
                "\n",
                "# Test with orthogonal vectors (should be 0.0)\n",
                "similarity = cosine_similarity(orthogonal_vector_1, orthogonal_vector_2)\n",
                "print(f\"Similarity between orthogonal vectors: {similarity:.4f}\")\n",
                "\n",
                "# Test with vectors with moderate similarity (should be around 0.5)\n",
                "similarity = cosine_similarity(moderate_vector_1, moderate_vector_2)\n",
                "print(f\"Similarity between moderately similar vectors: {similarity:.4f}\")\n",
                "```\n",
                "\n",
                "I have filled in the `cosine_similarity` function using the `dot` and `norm` functions from the NumPy library. I also added a check to handle the edge case of zero vectors to prevent a division-by-zero error."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Text Embeddings with OpenAI\n",
                "\n",
                "In this exercise, you'll create a function that transforms text into numerical vectors using OpenAI's embedding API. Your task is to complete the get_embedding() function that converts a text input into its vector representation. This is a crucial step in semantic evaluation, as these embeddings capture the meaning of text in a way that allows for mathematical comparison.\n",
                "\n",
                "To complete this exercise:\n",
                "\n",
                "Use the client.embeddings.create() method to call the OpenAI API and create an embedding using the \"text-embedding-3-small\" model.\n",
                "Extract the embedding vector from the response using response.data[0].embedding.\n",
                "Test your function with a simple input text.\n",
                "We've provided a setup to interact with the OpenAI API, so you can focus on the implementation. Once you've mastered this technique, you'll be able to compare the meanings of different texts by analyzing their embedding vectors.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "import numpy as np\n",
                "\n",
                "client = OpenAI()  # Initialize the OpenAI client\n",
                "\n",
                "def get_embedding(text):\n",
                "    \"\"\"\n",
                "    Get the embedding for a given text using OpenAI's API.\n",
                "    \n",
                "    Args:\n",
                "        text (str): The text to get an embedding for\n",
                "        \n",
                "    Returns:\n",
                "        list: The embedding vector\n",
                "    \"\"\"\n",
                "    # TODO: Call the client.embeddings.create method to get the embedding\n",
                "    # Use the \"text-embedding-3-small\" model and the input text\n",
                "    \n",
                "    # TODO: Extract and return the embedding from the response using response.data[0].embedding\n",
                "    pass\n",
                "\n",
                "# Test the function with a simple input\n",
                "test_text = \"The president addressed the nation, highlighting the importance of economic reforms.\"\n",
                "embedding = get_embedding(test_text)\n",
                "\n",
                "# Print the first 5 values of the embedding\n",
                "print(f\"First 5 values of the embedding: {embedding[:5]}\")\n",
                "\n",
                "# Print the length of the embedding vector\n",
                "print(f\"Embedding dimension: {len(embedding)}\")\n",
                "```\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "import numpy as np\n",
                "\n",
                "client = OpenAI()  # Initialize the OpenAI client\n",
                "\n",
                "def get_embedding(text):\n",
                "    \"\"\"\n",
                "    Get the embedding for a given text using OpenAI's API.\n",
                "    \n",
                "    Args:\n",
                "        text (str): The text to get an embedding for\n",
                "        \n",
                "    Returns:\n",
                "        list: The embedding vector\n",
                "    \"\"\"\n",
                "    # Call the client.embeddings.create method to get the embedding\n",
                "    # Use the \"text-embedding-3-small\" model and the input text\n",
                "    response = client.embeddings.create(\n",
                "        model=\"text-embedding-3-small\",\n",
                "        input=text\n",
                "    )\n",
                "    \n",
                "    # Extract and return the embedding from the response using response.data[0].embedding\n",
                "    return response.data[0].embedding\n",
                "\n",
                "# Test the function with a simple input\n",
                "test_text = \"The president addressed the nation, highlighting the importance of economic reforms.\"\n",
                "embedding = get_embedding(test_text)\n",
                "\n",
                "# Print the first 5 values of the embedding\n",
                "print(f\"First 5 values of the embedding: {embedding[:5]}\")\n",
                "\n",
                "# Print the length of the embedding vector\n",
                "print(f\"Embedding dimension: {len(embedding)}\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building a Semantic Comparison Pipeline\n",
                "\n",
                "Excellent work on implementing both the cosine similarity function and the embedding generator! Now it's time to bring these components together to create a complete semantic evaluation pipeline.\n",
                "\n",
                "In this exercise, you'll build a mini-system that compares two text summaries based on their meaning rather than just their words. You'll need to:\n",
                "\n",
                "Implement the cosine_similarity function that handles edge cases like zero vectors.\n",
                "Create the get_embedding function to transform text into vector representations.\n",
                "Apply both functions to compare a reference summary with a generated summary.\n",
                "The code includes example summaries for you to test your implementation. This exercise ties together everything you've learned about semantic evaluation, giving you a practical tool you can use to compare any two texts based on their meaning.\n",
                "\n",
                "By completing this pipeline, you'll have a solid understanding of how modern NLP systems evaluate text similarity beyond simple word matching — a key skill for anyone working with language models.\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "from numpy import dot\n",
                "from numpy.linalg import norm\n",
                "\n",
                "# Initialize the OpenAI client\n",
                "client = OpenAI()\n",
                "\n",
                "def cosine_similarity(a, b):\n",
                "    \"\"\"\n",
                "    Calculate the cosine similarity between two vectors.\n",
                "    \n",
                "    Args:\n",
                "        a (list): First vector\n",
                "        b (list): Second vector\n",
                "        \n",
                "    Returns:\n",
                "        float: Similarity score between -1 and 1\n",
                "    \"\"\"\n",
                "    # TODO: Handle edge case of zero vectors\n",
                "    \n",
                "    # TODO: Calculate and return the cosine similarity between vectors a and b\n",
                "    pass\n",
                "\n",
                "def get_embedding(text):\n",
                "    \"\"\"\n",
                "    Get the embedding for a given text using OpenAI's API.\n",
                "    \n",
                "    Args:\n",
                "        text (str): The text to get an embedding for\n",
                "        \n",
                "    Returns:\n",
                "        list: The embedding vector\n",
                "    \"\"\"\n",
                "    # TODO: Call the client.embeddings.create method to get the embedding\n",
                "    # Use the \"text-embedding-3-small\" model and the input text\n",
                "    \n",
                "    # TODO: Extract and return the embedding from the response\n",
                "    pass\n",
                "\n",
                "# Example text strings\n",
                "reference_summary = \"The company announced a new product that will revolutionize the market.\"\n",
                "generated_summary = \"A new revolutionary product was unveiled by the company.\"\n",
                "\n",
                "# Get embeddings for both summaries\n",
                "print(\"Getting embeddings for both summaries...\")\n",
                "# TODO: Get the embedding for the reference summary\n",
                "\n",
                "# TODO: Get the embedding for the generated summary\n",
                "\n",
                "# Calculate similarity between embeddings\n",
                "# TODO: Calculate the cosine similarity between the two embeddings\n",
                "\n",
                "# Print results\n",
                "print(f\"\\nReference summary: \\\"{reference_summary}\\\"\")\n",
                "print(f\"Generated summary: \\\"{generated_summary}\\\"\")\n",
                "print(f\"\\nSimilarity score: {similarity_score:.4f}\")\n",
                "\n",
                "```\n",
                "\n",
                "To complete the semantic comparison pipeline, you need to fill in the `cosine_similarity` and `get_embedding` functions, and then use them to compare the example summaries.\n",
                "\n",
                "Here's the completed code:\n",
                "\n",
                "```python\n",
                "from openai import OpenAI\n",
                "from numpy import dot\n",
                "from numpy.linalg import norm\n",
                "import numpy as np\n",
                "\n",
                "# Initialize the OpenAI client\n",
                "client = OpenAI()\n",
                "\n",
                "def cosine_similarity(a, b):\n",
                "    \"\"\"\n",
                "    Calculate the cosine similarity between two vectors.\n",
                "    \n",
                "    Args:\n",
                "        a (list): First vector\n",
                "        b (list): Second vector\n",
                "        \n",
                "    Returns:\n",
                "        float: Similarity score between -1 and 1\n",
                "    \"\"\"\n",
                "    # Handle edge case of zero vectors\n",
                "    if np.array_equal(a, np.zeros(len(a))) or np.array_equal(b, np.zeros(len(b))):\n",
                "        return 0.0\n",
                "\n",
                "    # Calculate and return the cosine similarity between vectors a and b\n",
                "    return dot(a, b) / (norm(a) * norm(b))\n",
                "\n",
                "def get_embedding(text):\n",
                "    \"\"\"\n",
                "    Get the embedding for a given text using OpenAI's API.\n",
                "    \n",
                "    Args:\n",
                "        text (str): The text to get an embedding for\n",
                "        \n",
                "    Returns:\n",
                "        list: The embedding vector\n",
                "    \"\"\"\n",
                "    # Call the client.embeddings.create method to get the embedding\n",
                "    response = client.embeddings.create(\n",
                "        model=\"text-embedding-3-small\",\n",
                "        input=text\n",
                "    )\n",
                "    \n",
                "    # Extract and return the embedding from the response\n",
                "    return response.data[0].embedding\n",
                "\n",
                "# Example text strings\n",
                "reference_summary = \"The company announced a new product that will revolutionize the market.\"\n",
                "generated_summary = \"A new revolutionary product was unveiled by the company.\"\n",
                "\n",
                "# Get embeddings for both summaries\n",
                "print(\"Getting embeddings for both summaries...\")\n",
                "reference_embedding = get_embedding(reference_summary)\n",
                "generated_embedding = get_embedding(generated_summary)\n",
                "\n",
                "# Calculate similarity between embeddings\n",
                "similarity_score = cosine_similarity(reference_embedding, generated_embedding)\n",
                "\n",
                "# Print results\n",
                "print(f\"\\nReference summary: \\\"{reference_summary}\\\"\")\n",
                "print(f\"Generated summary: \\\"{generated_summary}\\\"\")\n",
                "print(f\"\\nSimilarity score: {similarity_score:.4f}\")\n",
                "\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
