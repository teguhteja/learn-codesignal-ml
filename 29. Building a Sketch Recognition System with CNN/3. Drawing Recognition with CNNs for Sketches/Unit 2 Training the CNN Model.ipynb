{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 2 Training the CNN Model\n",
                                    "\n",
                                    "Welcome back\\! Now that you've built a **Convolutional Neural Network (CNN)** model for sketch recognition, it's time to bring it to life by training it. Training is a crucial step where the model learns to recognize patterns in the data. This lesson will guide you through the process of training your CNN model using a dataset of hand-drawn sketches. Whether you're familiar with the training process or this is your first time, this lesson will provide you with the knowledge and skills needed to train a CNN model effectively.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## What You'll Learn\n",
                                    "\n",
                                    "In this lesson, you will learn how to train your CNN model on a sketch dataset. We will cover the steps involved in preparing the data, setting up data augmentation, and training the model. You will also learn about the different parameters you can tune during training to improve the model's performance.\n",
                                    "\n",
                                    "Let's break down the code you'll be working with, step by step.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Part 1: Downloading and Preparing the Data\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Define the categories of sketches to recognize\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "# Download the .npy files for each category if not already present\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "for cat in categories:\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    if not os.path.exists(filepath):\n",
                                    "        print(f\"Downloading {cat} sketches...\")\n",
                                    "        urllib.request.urlretrieve(base_url + cat + '.npy', filepath)\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000  # Number of images per category\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]  # Load images for this category\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))  # Assign a label for each image\n",
                                    "\n",
                                    "# Combine all categories into a single dataset\n",
                                    "data = np.concatenate(data, axis=0).reshape(-1, 28, 28, 1).astype('float32') / 255.0  # Normalize pixel values\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Split the data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(\n",
                                    "    data, labels, test_size=0.2, random_state=42)\n",
                                    "```\n",
                                    "\n",
                                    "**Explanation of Part 1:**\n",
                                    "\n",
                                    "  * **Downloading the data:** The code checks if the `.npy` files for each category exist locally. If not, it downloads them from the QuickDraw dataset.\n",
                                    "  * **Loading and labeling:** Each category's images are loaded and assigned a numeric label.\n",
                                    "  * **Combining and normalizing:** All images are combined into a single array, reshaped for the CNN, and normalized to values between 0 and 1.\n",
                                    "  * **Splitting:** The dataset is split into training and testing sets to evaluate model performance.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Part 2: Data Augmentation and Training the Model\n",
                                    "\n",
                                    "```python\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "\n",
                                    "# Set up data augmentation to improve model generalization\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest')\n",
                                    "\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "# Build and train the model\n",
                                    "# Assume build_simple_cnn() function is defined elsewhere and builds your CNN model\n",
                                    "# Example placeholder for build_simple_cnn() if not already defined:\n",
                                    "# def build_simple_cnn():\n",
                                    "#     model = tf.keras.Sequential([\n",
                                    "#         tf.keras.layers.Input(shape=(28, 28, 1)),\n",
                                    "#         tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "#         tf.keras.layers.MaxPooling2D(),\n",
                                    "#         tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "#         tf.keras.layers.MaxPooling2D(),\n",
                                    "#         tf.keras.layers.Flatten(),\n",
                                    "#         tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "#         tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "#     ])\n",
                                    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
                                    "#     return model\n",
                                    "\n",
                                    "model = build_simple_cnn()  # Use your previously defined build_simple_cnn() function\n",
                                    "\n",
                                    "history = model.fit(\n",
                                    "    datagen.flow(x_train, y_train, batch_size=32),  # Use augmented data\n",
                                    "    epochs=1,\n",
                                    "    validation_data=(x_test, y_test),\n",
                                    "    steps_per_epoch=len(x_train) // 32)\n",
                                    "```\n",
                                    "\n",
                                    "**Explanation of Part 2:**\n",
                                    "\n",
                                    "  * **Data augmentation:** The `ImageDataGenerator` is set up to randomly transform images during training, helping the model generalize better.\n",
                                    "  * **Training:** The model is trained using the augmented data, and its performance is validated on the test set.\n",
                                    "\n",
                                    "You will understand each step and how it contributes to the model's learning process.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Exploring Training Information\n",
                                    "\n",
                                    "When you train your model using the `model.fit()` function, it returns a **history** object. This object contains valuable information about the training process, such as how the model's accuracy and loss changed over each epoch. Specifically, the `history.history` dictionary includes lists of metrics recorded at the end of each epoch, such as:\n",
                                    "\n",
                                    "  * **loss:** The training loss for each epoch.\n",
                                    "  * **accuracy:** The training accuracy for each epoch (if accuracy is being tracked).\n",
                                    "  * **val\\_loss:** The validation loss for each epoch.\n",
                                    "  * **val\\_accuracy:** The validation accuracy for each epoch.\n",
                                    "\n",
                                    "You can use this information to monitor the model's learning progress, detect **overfitting** or **underfitting**, and compare different training runs. For example, you can print the training and validation accuracy after training:\n",
                                    "\n",
                                    "```python\n",
                                    "print(\"Training Accuracy:\", history.history['accuracy'])\n",
                                    "print(\"Validation Accuracy:\", history.history['val_accuracy'])\n",
                                    "```\n",
                                    "\n",
                                    "By analyzing the `history` attribute, you gain insights into how your model is learning and can make informed decisions about adjusting hyperparameters or training strategies.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Why It Matters\n",
                                    "\n",
                                    "Training a CNN model is a vital skill in machine learning, especially for image recognition tasks. It allows the model to learn from data and improve its accuracy over time. By mastering the training process, you will be able to create models that can recognize and classify images with high precision. This skill is essential for various applications, from developing intelligent systems to advancing research in AI."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Compiling and Training Your Sketch Recognizer\n",
                                    "\n",
                                    "Great job building your CNN architecture! Now it's time to bring your model to life by training it on our sketch dataset.\n",
                                    "\n",
                                    "Training is when your model learns to recognize patterns by adjusting its weights based on the examples it processes. Before committing to a full training run, it's good practice to verify that your training pipeline works correctly with a short initial training session.\n",
                                    "\n",
                                    "In this task, you'll compile your model with the appropriate optimizer and loss function, then run a brief training session to make sure everything is working properly. The data loading and augmentation have already been set up for you, and your model architecture from the previous task is included.\n",
                                    "\n",
                                    "You'll need to fill in the missing parts to compile the model, train it for a few epochs, and evaluate its performance on the test data.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# TODO: Compile the model with the \"adam\" optimizer, \"sparse_categorical_crossentropy\" loss, and \"accuracy\" metric\n",
                                    "model.compile(optimizer=________,\n",
                                    "              loss=________,\n",
                                    "              metrics=[________])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# TODO: Train the model for 3 epochs using the data augmentation generator\n",
                                    "# Use batch_size=32 and include validation_data=(x_test, y_test)\n",
                                    "history = model.fit(________,\n",
                                    "                   epochs=________,\n",
                                    "                   validation_data=________,\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# TODO: Evaluate the model on the test data and print the accuracy\n",
                                    "loss, accuracy = model.________\n",
                                    "print(f\"Test accuracy: {________}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model with the \"adam\" optimizer, \"sparse_categorical_crossentropy\" loss, and \"accuracy\" metric\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for 3 epochs using the data augmentation generator\n",
                                    "# Use batch_size=32 and include validation_data=(x_test, y_test)\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the model on the test data and print the accuracy\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
                                    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Evaluating Model Performance and Generalization\n",
                                    "\n",
                                    "Great job training your sketch recognition model! Now that your model has learned from the training data, it's essential to evaluate how well it performs on unseen examples.\n",
                                    "\n",
                                    "Evaluation helps you understand whether your model is actually learning useful patterns or just memorizing the training data. By comparing training and validation metrics, you can identify issues like overfitting and make informed decisions about how to improve your model.\n",
                                    "\n",
                                    "In this task, you'll analyze your model's performance by evaluating it on the test dataset and comparing the results with training metrics. You'll also learn to interpret these metrics to determine whether your model is generalizing well to new sketches.\n",
                                    "\n",
                                    "This analysis will help you decide what steps to take next — whether to adjust your model architecture, train for more epochs, or consider your model ready for deployment.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# TODO: Evaluate the model on test data using model.evaluate()\n",
                                    "test_loss, test_accuracy = model.________\n",
                                    "\n",
                                    "# TODO: Print the test loss and accuracy with proper formatting\n",
                                    "print(f\"Test loss: {________}\")\n",
                                    "print(f\"Test accuracy: {________}\")\n",
                                    "\n",
                                    "# TODO: Get the final training and validation accuracy from the history object\n",
                                    "final_train_accuracy = history.history['________'][-1]\n",
                                    "final_val_accuracy = history.history['________'][-1]\n",
                                    "\n",
                                    "# TODO: Print comparison of accuracies\n",
                                    "print(f\"Final training accuracy: {________}\")\n",
                                    "print(f\"Final validation accuracy: {________}\")\n",
                                    "\n",
                                    "# TODO: Calculate the difference between training and validation accuracy\n",
                                    "accuracy_difference = ________\n",
                                    "\n",
                                    "# TODO: Analyze model performance based on the accuracy difference\n",
                                    "if accuracy_difference > 0.1:\n",
                                    "    print(\"Model shows signs of overfitting (training accuracy much higher than validation).\")\n",
                                    "    print(\"Possible improvements: Add dropout layers, use more data, or apply stronger regularization.\")\n",
                                    "elif test_accuracy < 0.7:\n",
                                    "    print(\"Model accuracy is relatively low.\")\n",
                                    "    print(\"Possible improvements: Train for more epochs, use a more complex architecture, or gather more training data.\")\n",
                                    "else:\n",
                                    "    print(\"Model is generalizing well to unseen data!\")\n",
                                    "    print(\"To further improve: Fine-tune hyperparameters or try transfer learning with a pre-trained model.\")\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the model on test data using model.evaluate()\n",
                                    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
                                    "\n",
                                    "# Print the test loss and accuracy with proper formatting\n",
                                    "print(f\"Test loss: {test_loss:.4f}\")\n",
                                    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Get the final training and validation accuracy from the history object\n",
                                    "final_train_accuracy = history.history['accuracy'][-1]\n",
                                    "final_val_accuracy = history.history['val_accuracy'][-1]\n",
                                    "\n",
                                    "# Print comparison of accuracies\n",
                                    "print(f\"Final training accuracy: {final_train_accuracy:.4f}\")\n",
                                    "print(f\"Final validation accuracy: {final_val_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Calculate the difference between training and validation accuracy\n",
                                    "accuracy_difference = abs(final_train_accuracy - final_val_accuracy)\n",
                                    "\n",
                                    "# Analyze model performance based on the accuracy difference\n",
                                    "if accuracy_difference > 0.1:\n",
                                    "    print(\"Model shows signs of overfitting (training accuracy much higher than validation).\")\n",
                                    "    print(\"Possible improvements: Add dropout layers, use more data, or apply stronger regularization.\")\n",
                                    "elif test_accuracy < 0.7:\n",
                                    "    print(\"Model accuracy is relatively low.\")\n",
                                    "    print(\"Possible improvements: Train for more epochs, use a more complex architecture, or gather more training data.\")\n",
                                    "else:\n",
                                    "    print(\"Model is generalizing well to unseen data!\")\n",
                                    "    print(\"To further improve: Fine-tune hyperparameters or try transfer learning with a pre-trained model.\")\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Saving and Loading Your Sketch Recognizer\n",
                                    "\n",
                                    "Now that you've evaluated your model's performance, let's take the next important step: saving your trained model for future use. This is a critical skill for any machine learning project.\n",
                                    "\n",
                                    "When you save a model, you're preserving all its learned weights and architecture, allowing you to use it later without retraining. This saves time and computational resources, especially for complex models that take hours or days to train.\n",
                                    "\n",
                                    "In this task, you'll learn how to save your trained CNN model to disk and then load it back. You'll verify that the loaded model performs identically to the original by comparing their test accuracies.\n",
                                    "\n",
                                    "This workflow is essential for deploying models to production or sharing them with others. It's also good practice to save checkpoints during training in case of unexpected interruptions.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the original model\n",
                                    "original_loss, original_accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Original model - Test accuracy: {original_accuracy:.4f}\")\n",
                                    "\n",
                                    "# TODO: Save the model to disk with the filename 'sketch_recognition_model.keras'\n",
                                    "________\n",
                                    "print(\"Model saved successfully!\")\n",
                                    "\n",
                                    "# TODO: Load the model from disk using tf.keras.models.load_model()\n",
                                    "loaded_model = ________\n",
                                    "print(\"Model loaded successfully!\")\n",
                                    "\n",
                                    "# TODO: Evaluate the loaded model on the test data\n",
                                    "loaded_loss, loaded_accuracy = ________\n",
                                    "print(f\"Loaded model - Test accuracy: {________}\")\n",
                                    "\n",
                                    "# TODO: Calculate and print the absolute difference between original and loaded model accuracies\n",
                                    "print(f\"Accuracy difference: {________}\")\n",
                                    "\n",
                                    "# TODO: Check if the models perform identically (difference less than 1e-6)\n",
                                    "if ________:\n",
                                    "    print(\"Success! The loaded model performs identically to the original model.\")\n",
                                    "else:\n",
                                    "    print(\"There might be small differences between the original and loaded models.\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the original model\n",
                                    "original_loss, original_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
                                    "print(f\"Original model - Test accuracy: {original_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Save the model to disk with the filename 'sketch_recognition_model.keras'\n",
                                    "model.save('sketch_recognition_model.keras')\n",
                                    "print(\"Model saved successfully!\")\n",
                                    "\n",
                                    "# Load the model from disk using tf.keras.models.load_model()\n",
                                    "loaded_model = tf.keras.models.load_model('sketch_recognition_model.keras')\n",
                                    "print(\"Model loaded successfully!\")\n",
                                    "\n",
                                    "# Evaluate the loaded model on the test data\n",
                                    "loaded_loss, loaded_accuracy = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
                                    "print(f\"Loaded model - Test accuracy: {loaded_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Calculate and print the absolute difference between original and loaded model accuracies\n",
                                    "accuracy_difference = abs(original_accuracy - loaded_accuracy)\n",
                                    "print(f\"Accuracy difference: {accuracy_difference:.6f}\")\n",
                                    "\n",
                                    "# Check if the models perform identically (difference less than 1e-6)\n",
                                    "if accuracy_difference < 1e-6:\n",
                                    "    print(\"Success! The loaded model performs identically to the original model.\")\n",
                                    "else:\n",
                                    "    print(\"There might be small differences between the original and loaded models.\")\n",
                                    "\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
