{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 4 Improving the Model Performance\n",
                                    "\n",
                                    "Welcome back\\! In the previous lesson, you learned how to evaluate your CNN model’s performance and visualize its predictions. You also explored important metrics like accuracy, precision, recall, and the F1 score, and saw how to interpret a classification report and confusion matrix. Now that you know how to measure your model’s strengths and weaknesses, it’s time to take the next step: making your model even better.\n",
                                    "\n",
                                    "In this lesson, you will learn practical techniques to improve your CNN’s performance on sketch recognition tasks. These methods are widely used in real-world deep learning projects to help models generalize better and avoid common pitfalls like overfitting. By the end of this lesson, you’ll be able to apply these improvements to your own models and see the difference in results.\n",
                                    "\n",
                                    "### What You'll Learn\n",
                                    "\n",
                                    "You will focus on two powerful techniques for boosting your model’s performance:\n",
                                    "\n",
                                    "  * **Dropout Layers:** Dropout is a simple but effective way to prevent overfitting. It works by randomly “dropping out” (ignoring) a fraction of the neurons during training, which forces the model to learn more robust features. You’ll see how to add a `Dropout` layer to your CNN and understand why it helps.\n",
                                    "  * **Early Stopping:** Sometimes, training a model for too many epochs can actually make it perform worse on new data. Early stopping is a technique that monitors your model’s performance on the validation set and automatically stops training when the model stops improving. This saves time and helps you get the best version of your model.\n",
                                    "\n",
                                    "Here’s a quick look at how these improvements appear in code:\n",
                                    "\n",
                                    "```python\n",
                                    "from tensorflow.keras.layers import Dropout\n",
                                    "from tensorflow.keras.callbacks import EarlyStopping\n",
                                    "\n",
                                    "# Add a Dropout layer after a Dense layer\n",
                                    "model.add(Dense(512, activation='relu'))\n",
                                    "model.add(Dropout(0.5))  # 50% of neurons will be dropped during training\n",
                                    "\n",
                                    "# Set up EarlyStopping\n",
                                    "early_stopping = EarlyStopping(\n",
                                    "    patience=10,\n",
                                    "    monitor='val_accuracy',\n",
                                    "    restore_best_weights=True)\n",
                                    "```\n",
                                    "\n",
                                    "You’ll also notice some other helpful changes in the code, such as using more training data, adding batch normalization, and making the model a bit deeper. These changes help the model learn better and make the training process more stable.\n",
                                    "\n",
                                    "### Why It Matters\n",
                                    "\n",
                                    "Improving your model’s performance is a key part of building reliable AI systems. Overfitting is a common problem where a model does well on training data but fails on new, unseen data. Dropout and early stopping are two of the most effective tools to fight overfitting and make your model more robust.\n",
                                    "\n",
                                    "By mastering these techniques, you’ll be able to:\n",
                                    "\n",
                                    "  * Build models that generalize well to new sketches and drawings.\n",
                                    "  * Train faster and avoid wasting time on unnecessary epochs.\n",
                                    "  * Understand how to tune your model for the best possible results.\n",
                                    "\n",
                                    "Ready to see these improvements in action? Let’s move on to the practice section and apply these techniques to your own sketch recognition model\\!\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Preventing Overfitting with Dropout and EarlyStopping\n",
                                    "\n",
                                    "Great job analyzing those misclassified sketches! Now, let's improve your model to reduce these errors. In this task, you'll add a `Dropout` layer after the Dense layer in your CNN.\n",
                                    "\n",
                                    "Dropout works by randomly disabling neurons during training, which prevents your network from becoming too dependent on any single neuron. This forces the model to learn more robust features and reduces overfitting.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "from tensorflow.keras.callbacks import EarlyStopping\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_improved_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        # TODO: Add a Dropout layer with a rate of 0.5\n",
                                    "        ________,\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "\n",
                                    "    model.compile(optimizer='adam',\n",
                                    "                 loss='sparse_categorical_crossentropy',\n",
                                    "                 metrics=['accuracy'])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the improved model\n",
                                    "model = build_improved_cnn()\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model with early stopping\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=1,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "# Get predictions\n",
                                    "y_pred = model.predict(x_test)\n",
                                    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                                    "\n",
                                    "# Find misclassified images\n",
                                    "misclassified_indices = np.where(y_pred_classes != y_test)[0]\n",
                                    "print(f\"Found {len(misclassified_indices)} misclassified images\")\n",
                                    "\n",
                                    "# Visualize misclassified images\n",
                                    "plt.figure(figsize=(10, 10))\n",
                                    "for i in range(min(9, len(misclassified_indices))):\n",
                                    "    idx = misclassified_indices[i]\n",
                                    "    plt.subplot(3, 3, i + 1)\n",
                                    "    plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
                                    "    plt.title(f\"True: {categories[y_test[idx]]}\\nPred: {categories[y_pred_classes[idx]]}\")\n",
                                    "    plt.axis('off')\n",
                                    "plt.tight_layout()\n",
                                    "plt.savefig('static/images/misclassified.png')\n",
                                    "print(\"Misclassified images visualization saved to 'static/images/misclassified.png'\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To add a `Dropout` layer with a rate of 0.5, you should replace the `________` placeholder with `tf.keras.layers.Dropout(0.5)`.\n",
                                    "\n",
                                    "Here's the corrected `build_improved_cnn` function:\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "from tensorflow.keras.callbacks import EarlyStopping\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_improved_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dropout(0.5), # Added Dropout layer\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "\n",
                                    "    model.compile(optimizer='adam',\n",
                                    "                 loss='sparse_categorical_crossentropy',\n",
                                    "                 metrics=['accuracy'])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the improved model\n",
                                    "model = build_improved_cnn()\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model with early stopping\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=1,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "# Get predictions\n",
                                    "y_pred = model.predict(x_test)\n",
                                    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                                    "\n",
                                    "# Find misclassified images\n",
                                    "misclassified_indices = np.where(y_pred_classes != y_test)[0]\n",
                                    "print(f\"Found {len(misclassified_indices)} misclassified images\")\n",
                                    "\n",
                                    "# Visualize misclassified images\n",
                                    "plt.figure(figsize=(10, 10))\n",
                                    "for i in range(min(9, len(misclassified_indices))):\n",
                                    "    idx = misclassified_indices[i]\n",
                                    "    plt.subplot(3, 3, i + 1)\n",
                                    "    plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
                                    "    plt.title(f\"True: {categories[y_test[idx]]}\\nPred: {categories[y_pred_classes[idx]]}\")\n",
                                    "    plt.axis('off')\n",
                                    "plt.tight_layout()\n",
                                    "plt.savefig('static/images/misclassified.png')\n",
                                    "print(\"Misclassified images visualization saved to 'static/images/misclassified.png'\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Automatic Training Termination with EarlyStopping\n",
                                    "\n",
                                    "Now that you've analyzed your model's performance with detailed metrics, let's implement another key technique to prevent overfitting: Early Stopping.\n",
                                    "\n",
                                    "When training neural networks, we often face a challenge in which training for too many epochs can lead to the model memorizing the training data instead of learning general patterns. This is called overfitting and hurts performance on new data.\n",
                                    "\n",
                                    "Early Stopping automatically monitors your model's performance on validation data and stops training when it detects that performance has stopped improving. This saves training time and helps you capture your model at its best point.\n",
                                    "\n",
                                    "In this task, you'll add an EarlyStopping callback to your sketch recognition model. This will monitor validation accuracy and automatically stop training when the model stops improving for a specified number of epochs.\n",
                                    "\n",
                                    "Note, in this practice EarlyStopping won't make a significant difference because the model is already set to train for only one epoch. We set the number of epochs to one to keep the training time short. However, in a real-world scenario, you would typically train for many epochs (e.g., 50 or more) and use early stopping to prevent overfitting.\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "from sklearn.metrics import classification_report\n",
                                    "from tensorflow.keras.callbacks import EarlyStopping\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "\n",
                                    "    model.compile(optimizer='adam',\n",
                                    "                 loss='sparse_categorical_crossentropy',\n",
                                    "                 metrics=['accuracy'])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# TODO: Create an EarlyStopping callback that:\n",
                                    "#  - Monitors validation accuracy\n",
                                    "#  - Has a patience of 5 epochs\n",
                                    "#  - Restores the best weights\n",
                                    "#  - Provides verbose output\n",
                                    "early_stopping = ________\n",
                                    "\n",
                                    "# Train the model with early stopping\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=1,  # We can set a higher number of epochs since early stopping will prevent overfitting\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   # TODO: Add the early stopping callback to the training process\n",
                                    "                   callbacks=[________],\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "# Get predictions\n",
                                    "y_pred = model.predict(x_test)\n",
                                    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                                    "\n",
                                    "# Generate classification report\n",
                                    "report = classification_report(y_test, y_pred_classes, target_names=categories)\n",
                                    "print(\"\\nClassification Report:\")\n",
                                    "print(report)\n",
                                    "\n",
                                    "# Print per-class analysis\n",
                                    "print(\"\\nPer-class Analysis:\")\n",
                                    "print(\"------------------\")\n",
                                    "for i, category in enumerate(categories):\n",
                                    "    # Find indices for this class\n",
                                    "    class_indices = np.where(y_test == i)[0]\n",
                                    "    class_pred = y_pred_classes[class_indices]\n",
                                    "    class_true = y_test[class_indices]\n",
                                    "\n",
                                    "    # Calculate accuracy for this class\n",
                                    "    class_accuracy = np.mean(class_pred == class_true)\n",
                                    "\n",
                                    "    # Count total samples for this class\n",
                                    "    total_samples = len(class_indices)\n",
                                    "\n",
                                    "    print(f\"{category}: {class_accuracy:.2%} accuracy ({total_samples} samples)\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "The error `AttributeError: module 'warnings' has no attribute 'warnings'` indicates that you are trying to access `warnings.warnings.filterwarnings`, but the `warnings` module itself does not have an attribute named `warnings`. You should directly call `filterwarnings` on the `warnings` module.\n",
                                    "\n",
                                    "Here's the corrected line:\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "from sklearn.metrics import classification_report\n",
                                    "from tensorflow.keras.callbacks import EarlyStopping\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "\n",
                                    "    model.compile(optimizer='adam',\n",
                                    "                 loss='sparse_categorical_crossentropy',\n",
                                    "                 metrics=['accuracy'])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Create an EarlyStopping callback that:\n",
                                    "#  - Monitors validation accuracy\n",
                                    "#  - Has a patience of 5 epochs\n",
                                    "#  - Restores the best weights\n",
                                    "#  - Provides verbose output\n",
                                    "early_stopping = EarlyStopping(\n",
                                    "    monitor='val_accuracy',\n",
                                    "    patience=5,\n",
                                    "    restore_best_weights=True,\n",
                                    "    verbose=1\n",
                                    ")\n",
                                    "\n",
                                    "# Train the model with early stopping\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=1,  # We can set a higher number of epochs since early stopping will prevent overfitting\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   # Add the early stopping callback to the training process\n",
                                    "                   callbacks=[early_stopping],\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "# Get predictions\n",
                                    "y_pred = model.predict(x_test)\n",
                                    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                                    "\n",
                                    "# Generate classification report\n",
                                    "report = classification_report(y_test, y_pred_classes, target_names=categories)\n",
                                    "print(\"\\nClassification Report:\")\n",
                                    "print(report)\n",
                                    "\n",
                                    "# Print per-class analysis\n",
                                    "print(\"\\nPer-class Analysis:\")\n",
                                    "print(\"------------------\")\n",
                                    "for i, category in enumerate(categories):\n",
                                    "    # Find indices for this class\n",
                                    "    class_indices = np.where(y_test == i)[0]\n",
                                    "    class_pred = y_pred_classes[class_indices]\n",
                                    "    class_true = y_test[class_indices]\n",
                                    "\n",
                                    "    # Calculate accuracy for this class\n",
                                    "    class_accuracy = np.mean(class_pred == class_true)\n",
                                    "\n",
                                    "    # Count total samples for this class\n",
                                    "    total_samples = len(class_indices)\n",
                                    "\n",
                                    "    print(f\"{category}: {class_accuracy:.2%} accuracy ({total_samples} samples)\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "eb4dcbd6",
                           "metadata": {},
                           "source": [
                                    "Now that you've learned about techniques to improve model performance, simply analyze the final code which we will use for building the model for the upcoming course.\n",
                                    "\n",
                                    "Here are the main steps of the code explained:\n",
                                    "\n",
                                    "Data Loading and Preprocessing: Downloads the QuickDraw .npy files for each category, loads them, reshapes, and normalizes the images. The data is shuffled and split into training and test sets.\n",
                                    "\n",
                                    "Data Augmentation: Uses ImageDataGenerator to apply random transformations (rotation, shift, zoom, shear) to the training images, helping the model generalize better.\n",
                                    "\n",
                                    "Model Definition: Builds a CNN with three convolutional blocks, each followed by batch normalization and max pooling. A dense layer with batch normalization and a Dropout layer is added before the output. This helps prevent overfitting.\n",
                                    "\n",
                                    "Early Stopping: Sets up an EarlyStopping callback to monitor validation accuracy and stop training when the model stops improving, restoring the best weights.\n",
                                    "\n",
                                    "Label Preparation: Converts integer labels to one-hot encoded vectors for training with categorical cross-entropy loss.\n",
                                    "\n",
                                    "Training: Trains the model using the augmented data, with early stopping enabled.\n",
                                    "\n",
                                    "Evaluation: Evaluates the model on the test set and prints a classification report to show precision, recall, and F1-score for each class.\n",
                                    "\n",
                                    "Saving the Model: Saves the trained model to disk for future use.\n",
                                    "\n",
                                    "Note, running the code won't do anything, since the model training takes more than 30 minutes.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "from sklearn.metrics import classification_report\n",
                                    "from tensorflow.keras.callbacks import EarlyStopping\n",
                                    "\n",
                                    "# 1. Data loading and preprocessing\n",
                                    "categories = [\n",
                                    "    'house', 'apple', 'car', 'cat',\n",
                                    "    'dog', 'flower', 'star', 'tree',\n",
                                    "    'bowtie', 'eyeglasses', 'door', 'umbrella'\n",
                                    "]\n",
                                    "NUM_CLASSES = len(categories)\n",
                                    "DATA_DIR = 'quickdraw_data'\n",
                                    "os.makedirs(DATA_DIR, exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = os.path.join(DATA_DIR, f'{category}.npy')\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(\n",
                                    "            f'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{category}.npy',\n",
                                    "            file_path\n",
                                    "        )\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 20000\n",
                                    "IMAGE_SIZE = 28\n",
                                    "IMAGE_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 1)\n",
                                    "\n",
                                    "print(\"Loading and preparing data...\")\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = os.path.join(DATA_DIR, f'{cat}.npy')\n",
                                    "    try:\n",
                                    "        imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "        if imgs.shape[1] != IMAGE_SIZE * IMAGE_SIZE:\n",
                                    "            print(f\"Warning: Unexpected data shape in {cat}.npy. Skipping.\")\n",
                                    "            continue\n",
                                    "        if imgs.dtype != np.uint8:\n",
                                    "            imgs = imgs.astype(np.uint8)\n",
                                    "        imgs_reshaped = imgs.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
                                    "        data.append(imgs_reshaped)\n",
                                    "        labels.append(np.full(imgs_reshaped.shape[0], idx))\n",
                                    "    except Exception as e:\n",
                                    "        print(f\"Error loading or processing {cat}.npy: {e}\")\n",
                                    "\n",
                                    "if not data:\n",
                                    "    print(\"Error: No data loaded. Exiting.\")\n",
                                    "    exit()\n",
                                    "\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "print(f\"Total data shape: {data.shape}, Total labels shape: {labels.shape}\")\n",
                                    "\n",
                                    "# Shuffle and normalize data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "data = data.astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data\n",
                                    "x_train, x_test, y_train_indices, y_test_indices = train_test_split(\n",
                                    "    data, labels, test_size=0.2, random_state=42, stratify=labels\n",
                                    ")\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# 2. Data augmentation\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=20,\n",
                                    "    width_shift_range=0.15,\n",
                                    "    height_shift_range=0.15,\n",
                                    "    zoom_range=0.15,\n",
                                    "    shear_range=0.15,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# 3. Model definition with Dropout and Batch Normalization\n",
                                    "def build_simple_cnn_with_dropout():\n",
                                    "    model = tf.keras.models.Sequential(name=\"SketchCNN_with_Dropout\")\n",
                                    "    model.add(tf.keras.layers.Conv2D(32, 5, padding='same', activation='relu', input_shape=IMAGE_SHAPE))\n",
                                    "    model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
                                    "    model.add(tf.keras.layers.BatchNormalization())\n",
                                    "    model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
                                    "    model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
                                    "    model.add(tf.keras.layers.BatchNormalization())\n",
                                    "    model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
                                    "    model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
                                    "    model.add(tf.keras.layers.BatchNormalization())\n",
                                    "    model.add(tf.keras.layers.Flatten())\n",
                                    "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
                                    "    model.add(tf.keras.layers.BatchNormalization())\n",
                                    "    model.add(tf.keras.layers.Dropout(0.5))\n",
                                    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
                                    "    model.compile(\n",
                                    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
                                    "        loss='categorical_crossentropy',\n",
                                    "        metrics=['accuracy']\n",
                                    "    )\n",
                                    "    model.summary()\n",
                                    "    return model\n",
                                    "\n",
                                    "model = build_simple_cnn_with_dropout()\n",
                                    "\n",
                                    "# 4. Early stopping callback\n",
                                    "early_stopping_callback = EarlyStopping(\n",
                                    "    patience=10,\n",
                                    "    monitor='val_accuracy',\n",
                                    "    restore_best_weights=True,\n",
                                    "    verbose=1\n",
                                    ")\n",
                                    "\n",
                                    "# 5. Prepare labels for training\n",
                                    "y_train_cat = tf.keras.utils.to_categorical(y_train_indices, num_classes=NUM_CLASSES)\n",
                                    "y_test_cat = tf.keras.utils.to_categorical(y_test_indices, num_classes=NUM_CLASSES)\n",
                                    "\n",
                                    "# 6. Training\n",
                                    "BATCH_SIZE = 64\n",
                                    "EPOCHS = 50\n",
                                    "print(\"\\n--- Starting Training ---\")\n",
                                    "history = model.fit(\n",
                                    "    datagen.flow(x_train, y_train_cat, batch_size=BATCH_SIZE),\n",
                                    "    epochs=EPOCHS,\n",
                                    "    validation_data=(x_test, y_test_cat),\n",
                                    "    callbacks=[early_stopping_callback],\n",
                                    "    verbose=1\n",
                                    ")\n",
                                    "print(\"\\n--- Training Finished ---\")\n",
                                    "\n",
                                    "# 7. Evaluation\n",
                                    "print(\"\\n--- Evaluating Model on Test Set ---\")\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test_cat, verbose=0)\n",
                                    "print(f\"Test Loss: {loss:.4f}\")\n",
                                    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "print(\"\\n--- Classification Report ---\")\n",
                                    "y_pred_probs = model.predict(x_test)\n",
                                    "y_pred_indices = np.argmax(y_pred_probs, axis=1)\n",
                                    "print(classification_report(y_test_indices, y_pred_indices, target_names=categories))\n",
                                    "\n",
                                    "# 8. Save the model\n",
                                    "MODEL_SAVE_PATH = 'models/drawing_cnn.keras'\n",
                                    "model.save(MODEL_SAVE_PATH)\n",
                                    "print(f\"\\nModel saved successfully to {MODEL_SAVE_PATH}!\")\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "The provided code outlines a comprehensive approach to building and training a Convolutional Neural Network (CNN) for sketch recognition, incorporating several best practices for deep learning. Let's break down the analysis of this final code.\n",
                                    "\n",
                                    "### Overall Structure and Purpose\n",
                                    "\n",
                                    "The code's primary goal is to train a robust CNN capable of classifying hand-drawn sketches into 12 distinct categories from the QuickDraw dataset. It emphasizes techniques to prevent overfitting and ensure the model generalizes well to new, unseen sketches.\n",
                                    "\n",
                                    "### Key Components and Their Significance\n",
                                    "\n",
                                    "1.  **Data Loading and Preprocessing:**\n",
                                    "    * **Increased Categories:** The model is now set up to classify 12 categories, which is a more realistic scenario compared to just 3.\n",
                                    "    * **Larger `IMAGE_COUNT`:** Using `20000` images per category significantly expands the training data, which is crucial for deep learning models to learn complex patterns.\n",
                                    "    * **Dynamic `NUM_CLASSES`:** `len(categories)` correctly sets the output layer size, making the model adaptable to changes in the number of categories.\n",
                                    "    * **Robust Data Loading:** Includes error handling with `try-except` for file loading and a check for `imgs.shape[1]` to ensure data consistency, making the script more resilient.\n",
                                    "    * **Stratified Splitting:** `stratify=labels` in `train_test_split` ensures that the proportion of each class is maintained in both the training and testing sets. This is vital for balanced datasets, especially with multiple classes, to avoid a situation where some classes are underrepresented in the test set.\n",
                                    "    * **Normalization:** Images are normalized to `[0, 1]` by dividing by `255.0`, a standard practice that helps neural networks converge faster.\n",
                                    "\n",
                                    "2.  **Data Augmentation (`ImageDataGenerator`):**\n",
                                    "    * **Expanded Augmentation:** The `ImageDataGenerator` now uses wider ranges for `rotation_range`, `width_shift_range`, `height_shift_range`, `zoom_range`, and `shear_range` (e.g., 20 degrees for rotation, 0.15 for shifts/zooms/shears). This creates a much richer and more diverse training set from the existing images, making the model more robust to variations in drawing styles and minor distortions. This is a powerful technique to combat overfitting.\n",
                                    "\n",
                                    "3.  **Model Definition (`build_simple_cnn_with_dropout`):**\n",
                                    "    * **Deeper Architecture:** The model now features three convolutional blocks instead of two. More layers allow the model to learn more hierarchical and abstract features.\n",
                                    "    * **`padding='same'`:** This ensures the output feature map has the same spatial dimensions as the input, which can be beneficial for preserving information at the borders of the images and making it easier to stack more layers.\n",
                                    "    * **Batch Normalization (`BatchNormalization()`):** This is a critical addition after each convolutional block and before the final `Dense` layer. Batch Normalization helps:\n",
                                    "        * Stabilize and speed up the training process.\n",
                                    "        * Reduce internal covariate shift.\n",
                                    "        * Act as a regularization technique, further reducing overfitting.\n",
                                    "    * **`Dropout(0.5)`:** Applied to the `Dense` layer, this explicitly regularizes the network by randomly setting 50% of the inputs to zero during each training step. This prevents co-adaptation of neurons and makes the network more robust.\n",
                                    "    * **Adam Optimizer with Learning Rate:** Explicitly defining `tf.keras.optimizers.Adam(learning_rate=0.001)` gives fine-grained control over the optimization process. A learning rate of 0.001 is a common and often effective starting point.\n",
                                    "    * **`categorical_crossentropy` Loss:** This is the correct loss function for multi-class classification with one-hot encoded labels, which `tf.keras.utils.to_categorical` will provide.\n",
                                    "\n",
                                    "4.  **Early Stopping Callback (`early_stopping_callback`):**\n",
                                    "    * **`patience=10`:** This is a good value, allowing the model to continue training for 10 epochs after the `val_accuracy` stops improving before stopping. This accounts for minor fluctuations in validation performance.\n",
                                    "    * **`restore_best_weights=True`:** Crucially, this ensures that once training stops, the model's weights are reverted to the epoch where `val_accuracy` was highest, providing the best performing model on unseen data.\n",
                                    "    * **`verbose=1`:** Provides useful output during training about when early stopping is triggered.\n",
                                    "\n",
                                    "5.  **Label Preparation (`to_categorical`):**\n",
                                    "    * **One-Hot Encoding:** `tf.keras.utils.to_categorical` converts the integer labels (`y_train_indices`, `y_test_indices`) into one-hot encoded vectors (`y_train_cat`, `y_test_cat`). This is necessary because the `softmax` activation in the output layer and `categorical_crossentropy` loss expect this format for multi-class classification.\n",
                                    "\n",
                                    "6.  **Training:**\n",
                                    "    * **Increased Epochs (`EPOCHS = 50`):** While the previous example used `epochs=1` for quick demonstration, here `epochs=50` is set. This is a more realistic number for a deep learning model to converge, relying on `EarlyStopping` to prevent excessive training.\n",
                                    "    * **Larger `BATCH_SIZE = 64`:** A batch size of 64 is common and provides a good balance between training speed and stable gradient updates.\n",
                                    "\n",
                                    "7.  **Evaluation:**\n",
                                    "    * **Standard Metrics:** `model.evaluate` provides overall loss and accuracy.\n",
                                    "    * **Classification Report:** `sklearn.metrics.classification_report` is used to get detailed per-class metrics (precision, recall, F1-score), which are essential for understanding the model's performance on individual categories, especially if the dataset is imbalanced.\n",
                                    "\n",
                                    "8.  **Saving the Model:**\n",
                                    "    * **`model.save(MODEL_SAVE_PATH)`:** This is a vital step for production, allowing the trained model to be loaded later without retraining. The `.keras` extension is the recommended format for Keras models.\n",
                                    "\n",
                                    "### Improvements over Previous Iterations\n",
                                    "\n",
                                    "* **Scalability:** Increased number of categories and `IMAGE_COUNT` makes the solution more robust and applicable to larger datasets.\n",
                                    "* **Robustness:** Incorporates `BatchNormalization` for more stable and faster training, and `Dropout` for stronger regularization.\n",
                                    "* **Generalization:** Enhanced data augmentation and `EarlyStopping` are critical for building a model that performs well on unseen data, mitigating overfitting effectively.\n",
                                    "* **Comprehensive Evaluation:** Detailed `classification_report` provides deeper insights into per-class performance, which is more informative than just overall accuracy.\n",
                                    "* **Production Readiness:** Model saving is a crucial step towards deploying the model.\n",
                                    "\n",
                                    "### Potential Considerations (Beyond the Scope of this Analysis)\n",
                                    "\n",
                                    "* **Hyperparameter Tuning:** While good default values are used, optimizing learning rate, dropout rate, number of filters, and patience for early stopping could further improve performance.\n",
                                    "* **Cross-validation:** For more robust evaluation, k-fold cross-validation could be used.\n",
                                    "* **Class Imbalance Handling:** If the actual QuickDraw dataset for these 12 categories has significant class imbalance, techniques like class weighting or oversampling/undersampling might be considered, though `stratify` helps with the split.\n",
                                    "* **Transfer Learning:** For very large and complex image datasets, using a pre-trained model (transfer learning) is often more efficient and effective.\n",
                                    "\n",
                                    "In summary, this final code represents a well-structured and thoughtfully designed CNN for image classification, incorporating industry-standard techniques to achieve better performance and generalization.\n"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
