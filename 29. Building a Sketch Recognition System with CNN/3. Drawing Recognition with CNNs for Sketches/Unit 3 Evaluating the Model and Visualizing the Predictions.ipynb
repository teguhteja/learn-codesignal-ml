{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 3 Evaluating the Model and Visualizing the Predictions\n",
                                    "\n",
                                    "Welcome back\\! Now that you've trained your Convolutional Neural Network (CNN) model, it's time to evaluate its performance and visualize its predictions. This lesson will guide you through the process of assessing how well your model performs on unseen data and how to interpret its predictions. Whether you're familiar with model evaluation or this is your first time, this lesson will provide you with the knowledge and skills needed to evaluate a CNN model effectively.\n",
                                    "\n",
                                    "## What You'll Learn\n",
                                    "\n",
                                    "In this lesson, you will learn how to evaluate the performance of your CNN model using various metrics. We will cover the steps involved in assessing the model's accuracy and visualizing its predictions. You will also learn how to interpret the results and understand the model's strengths and weaknesses.\n",
                                    "\n",
                                    "Here's a glimpse of the code you'll be working with:\n",
                                    "\n",
                                    "```python\n",
                                    "# Evaluate the model\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "# Visualize predictions\n",
                                    "def show_predictions(model, x_test, y_test, categories):\n",
                                    "    predictions = model.predict(x_test)\n",
                                    "    predicted_labels = np.argmax(predictions, axis=1)\n",
                                    "    plt.figure(figsize=(10, 10))\n",
                                    "    for i in range(9):\n",
                                    "        plt.subplot(3, 3, i + 1)\n",
                                    "        plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
                                    "        plt.title(f\"True: {categories[y_test[i]]}\\nPred: {categories[predicted_labels[i]]}\")\n",
                                    "        plt.axis('off')\n",
                                    "    plt.tight_layout()\n",
                                    "    plt.savefig('static/images/predictions.png')\n",
                                    "\n",
                                    "show_predictions(model, x_test, y_test, categories)\n",
                                    "```\n",
                                    "\n",
                                    "This code snippet demonstrates how to evaluate your model's accuracy and visualize its predictions. You will understand each step and how it contributes to assessing the model's performance.\n",
                                    "\n",
                                    "## Understanding Precision and Recall\n",
                                    "\n",
                                    "Precision and recall are two important metrics for evaluating classification models, especially when dealing with imbalanced datasets.\n",
                                    "\n",
                                    "**Precision** measures how many of the items predicted as a certain class are actually of that class. High precision means that when the model predicts a class, it is usually correct.\n",
                                    "\n",
                                    "$$Precision=\\frac{True Positives}{True Positives+False Positives}$$\n",
                                    "\n",
                                    "**Recall** (also known as sensitivity) measures how many of the actual items of a certain class the model correctly identified. High recall means the model finds most of the items of that class.\n",
                                    "\n",
                                    "$$Recall=\\frac{True Positives}{True Positives+False Negatives}$$\n",
                                    "\n",
                                    "Precision and recall are often in tension: increasing one can sometimes decrease the other. That’s why the F1 score, which combines both, is also useful.\n",
                                    "\n",
                                    "You can compute precision and recall for your model as follows:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import precision_score, recall_score\n",
                                    "\n",
                                    "# Compute precision and recall (macro average for multi-class)\n",
                                    "precision = precision_score(y_test, predicted_labels, average='macro')\n",
                                    "recall = recall_score(y_test, predicted_labels, average='macro')\n",
                                    "print(f\"Precision (macro): {precision:.4f}\")\n",
                                    "print(f\"Recall (macro): {recall:.4f}\")\n",
                                    "```\n",
                                    "\n",
                                    "## Evaluating with the F1 Score\n",
                                    "\n",
                                    "In addition to accuracy, another important metric for evaluating your CNN model is the **F1 score**. The F1 score is especially useful when your data is imbalanced, meaning some classes have more samples than others. It combines both **precision** (how many of the predicted positives are actually positive) and **recall** (how many of the actual positives were correctly predicted) into a single metric.\n",
                                    "\n",
                                    "The F1 score is calculated as:\n",
                                    "\n",
                                    "$$F1 Score=2\\times\\frac{Precision\\times Recall}{Precision+Recall}$$\n",
                                    "\n",
                                    "A higher F1 score indicates better model performance, especially when you care equally about precision and recall.\n",
                                    "\n",
                                    "Here's how you can compute the F1 score for your model's predictions:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import f1_score\n",
                                    "\n",
                                    "# Get predicted labels\n",
                                    "predictions = model.predict(x_test)\n",
                                    "predicted_labels = np.argmax(predictions, axis=1)\n",
                                    "\n",
                                    "# Compute F1 score (macro average for multi-class)\n",
                                    "f1 = f1_score(y_test, predicted_labels, average='macro')\n",
                                    "print(f\"F1 Score (macro): {f1:.4f}\")\n",
                                    "```\n",
                                    "\n",
                                    "By including the F1 score in your evaluation, you gain a more comprehensive understanding of your model's performance, especially in cases where accuracy alone might be misleading.\n",
                                    "\n",
                                    "## The Classification Report\n",
                                    "\n",
                                    "The **classification report** is a comprehensive summary of key evaluation metrics for each class in your dataset. It includes precision, recall, F1 score, and support (the number of true instances for each class). This report helps you quickly assess how well your model is performing for each individual class, not just overall.\n",
                                    "\n",
                                    "Here’s how you can generate and display a classification report:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import classification_report\n",
                                    "\n",
                                    "# Generate classification report\n",
                                    "report = classification_report(y_test, predicted_labels, target_names=categories)\n",
                                    "print(report)\n",
                                    "```\n",
                                    "\n",
                                    "The output will look something like this:\n",
                                    "\n",
                                    "```\n",
                                    "              precision    recall  f1-score   support\n",
                                    "\n",
                                    "      apple       0.85      0.80      0.82        50\n",
                                    "     banana       0.78      0.84      0.81        50\n",
                                    "      chair       0.90      0.88      0.89        50\n",
                                    "\n",
                                    "   accuracy                           0.84       150\n",
                                    "  macro avg       0.84      0.84      0.84       150\n",
                                    "weighted avg       0.84      0.84      0.84       150\n",
                                    "```\n",
                                    "\n",
                                    "Each row shows the metrics for a specific class, while the averages at the bottom summarize overall performance. This detailed breakdown helps you identify which classes your model predicts well and which may need more attention.\n",
                                    "\n",
                                    "## Understanding the Confusion Matrix and Heatmap\n",
                                    "\n",
                                    "Another valuable tool for evaluating your CNN model is the **confusion matrix**. A confusion matrix provides a detailed breakdown of your model’s predictions by showing how many times each class was correctly or incorrectly predicted. Each row of the matrix represents the actual class, while each column represents the predicted class.\n",
                                    "\n",
                                    "  * **Diagonal elements** (from top-left to bottom-right) show the number of correct predictions for each class.\n",
                                    "  * **Off-diagonal elements** indicate misclassifications, showing where the model confused one class for another.\n",
                                    "\n",
                                    "This matrix helps you identify specific classes your model struggles with, which is especially useful for multi-class problems like sketch recognition.\n",
                                    "\n",
                                    "To make the confusion matrix easier to interpret, you can visualize it as a **heatmap**. A heatmap uses color intensity to represent the values in the confusion matrix, making patterns and problem areas stand out visually.\n",
                                    "\n",
                                    "Here’s how you can compute and visualize the confusion matrix as a heatmap:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import confusion_matrix\n",
                                    "import seaborn as sns\n",
                                    "\n",
                                    "# Compute confusion matrix\n",
                                    "cm = confusion_matrix(y_test, predicted_labels)\n",
                                    "\n",
                                    "# Plot confusion matrix as a heatmap\n",
                                    "plt.figure(figsize=(8, 6))\n",
                                    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                                    "            xticklabels=categories, yticklabels=categories)\n",
                                    "plt.xlabel('Predicted Label')\n",
                                    "plt.ylabel('True Label')\n",
                                    "plt.title('Confusion Matrix')\n",
                                    "plt.tight_layout()\n",
                                    "plt.savefig('static/images/confusion_matrix.png')\n",
                                    "```\n",
                                    "\n",
                                    "By analyzing the confusion matrix and its heatmap, you can quickly spot which classes are most often confused with each other. This insight can guide you in refining your model or dataset to improve overall performance.\n",
                                    "\n",
                                    "Here is an example of how the heatmap might look like:\n",
                                    "\n",
                                    "## Why It Matters\n",
                                    "\n",
                                    "Evaluating a CNN model is a crucial step in the machine learning process. It allows you to measure the model's accuracy and identify areas for improvement. By mastering the evaluation process, you will be able to create models that can reliably recognize and classify images. This skill is essential for developing robust AI systems and advancing research in machine learning.\n",
                                    "\n",
                                    "Are you excited to see how well your model performs? Let's dive into the practice section and start evaluating your CNN model for sketch recognition\\!\n",
                                    "\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Visualizing Loss and Accuracy Together\n",
                                    "\n",
                                    "Great job saving and loading your sketch recognition model! Now, let's enhance your evaluation toolkit by visualizing both accuracy and loss metrics. This dual visualization will give you deeper insights into how your model learns over time and help you identify potential issues like overfitting.\n",
                                    "\n",
                                    "The history object returned by model.fit() contains all metrics tracked during training. You've already seen how to plot accuracy — now you'll expand this to include loss metrics in a side-by-side comparison.\n",
                                    "\n",
                                    "This comprehensive visualization approach is standard practice among machine learning engineers to fully understand model performance. By tracking both metrics together, you can make better decisions about when to stop training or how to improve your model architecture.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the original model\n",
                                    "original_loss, original_accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Original model - Test accuracy: {original_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Save the model to disk\n",
                                    "model.save('sketch_recognition_model.keras')\n",
                                    "print(\"Model saved successfully!\")\n",
                                    "\n",
                                    "# Load the model from disk\n",
                                    "loaded_model = tf.keras.models.load_model('sketch_recognition_model.keras')\n",
                                    "print(\"Model loaded successfully!\")\n",
                                    "\n",
                                    "# Evaluate the loaded model\n",
                                    "loaded_loss, loaded_accuracy = loaded_model.evaluate(x_test, y_test)\n",
                                    "print(f\"Loaded model - Test accuracy: {loaded_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Compare the performance\n",
                                    "print(f\"Accuracy difference: {abs(original_accuracy - loaded_accuracy):.6f}\")\n",
                                    "\n",
                                    "# Check if the models perform identically\n",
                                    "if abs(original_accuracy - loaded_accuracy) < 1e-6:\n",
                                    "    print(\"Success! The loaded model performs identically to the original model.\")\n",
                                    "else:\n",
                                    "    print(\"There might be small differences between the original and loaded models.\")\n",
                                    "\n",
                                    "# Create a figure with two subplots\n",
                                    "plt.figure(figsize=(12, 5))\n",
                                    "\n",
                                    "# Plot training & validation accuracy values\n",
                                    "plt.subplot(1, 2, 1)\n",
                                    "plt.plot(history.history['accuracy'])\n",
                                    "plt.plot(history.history['val_accuracy'])\n",
                                    "plt.title('Model accuracy')\n",
                                    "plt.ylabel('Accuracy')\n",
                                    "plt.xlabel('Epoch')\n",
                                    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
                                    "\n",
                                    "# TODO: Create the second subplot (1, 2, 2) for loss values\n",
                                    "# TODO: Plot the training loss from history.history['loss']\n",
                                    "# TODO: Plot the validation loss from history.history['val_loss']\n",
                                    "# TODO: Add appropriate title, labels, and legend to the loss plot\n",
                                    "plt.subplot(________) \n",
                                    "plt.plot(________) \n",
                                    "plt.plot(________) \n",
                                    "plt.title(________) \n",
                                    "plt.ylabel(________) \n",
                                    "plt.xlabel(________) \n",
                                    "plt.legend(________, loc=________) \n",
                                    "\n",
                                    "plt.tight_layout()\n",
                                    "plt.savefig('static/images/plot.png')\n",
                                    "print(\"Training metrics visualization saved to 'static/images/plot.png'\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "## Visualizing Loss and Accuracy Together\n",
                                    "\n",
                                    "Great job saving and loading your sketch recognition model\\! Now, let's enhance your evaluation toolkit by visualizing both accuracy and loss metrics. This dual visualization will give you deeper insights into how your model learns over time and help you identify potential issues like overfitting.\n",
                                    "\n",
                                    "The `history` object returned by `model.fit()` contains all metrics tracked during training. You've already seen how to plot accuracy — now you'll expand this to include loss metrics in a side-by-side comparison.\n",
                                    "\n",
                                    "This comprehensive visualization approach is standard practice among machine learning engineers to fully understand model performance. By tracking both metrics together, you can make better decisions about when to stop training or how to improve your model architecture.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the original model\n",
                                    "original_loss, original_accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Original model - Test accuracy: {original_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Save the model to disk\n",
                                    "model.save('sketch_recognition_model.keras')\n",
                                    "print(\"Model saved successfully!\")\n",
                                    "\n",
                                    "# Load the model from disk\n",
                                    "loaded_model = tf.keras.models.load_model('sketch_recognition_model.keras')\n",
                                    "print(\"Model loaded successfully!\")\n",
                                    "\n",
                                    "# Evaluate the loaded model\n",
                                    "loaded_loss, loaded_accuracy = loaded_model.evaluate(x_test, y_test)\n",
                                    "print(f\"Loaded model - Test accuracy: {loaded_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Compare the performance\n",
                                    "print(f\"Accuracy difference: {abs(original_accuracy - loaded_accuracy):.6f}\")\n",
                                    "\n",
                                    "# Check if the models perform identically\n",
                                    "if abs(original_accuracy - loaded_accuracy) < 1e-6:\n",
                                    "    print(\"Success! The loaded model performs identically to the original model.\")\n",
                                    "else:\n",
                                    "    print(\"There might be small differences between the original and loaded models.\")\n",
                                    "\n",
                                    "# Create a figure with two subplots\n",
                                    "plt.figure(figsize=(12, 5))\n",
                                    "\n",
                                    "# Plot training & validation accuracy values\n",
                                    "plt.subplot(1, 2, 1)\n",
                                    "plt.plot(history.history['accuracy'])\n",
                                    "plt.plot(history.history['val_accuracy'])\n",
                                    "plt.title('Model accuracy')\n",
                                    "plt.ylabel('Accuracy')\n",
                                    "plt.xlabel('Epoch')\n",
                                    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
                                    "\n",
                                    "# Create the second subplot (1, 2, 2) for loss values\n",
                                    "plt.subplot(1, 2, 2) \n",
                                    "# Plot the training loss from history.history['loss']\n",
                                    "plt.plot(history.history['loss']) \n",
                                    "# Plot the validation loss from history.history['val_loss']\n",
                                    "plt.plot(history.history['val_loss']) \n",
                                    "# Add appropriate title, labels, and legend to the loss plot\n",
                                    "plt.title('Model loss') \n",
                                    "plt.ylabel('Loss') \n",
                                    "plt.xlabel('Epoch') \n",
                                    "plt.legend(['Train', 'Validation'], loc='upper right') \n",
                                    "\n",
                                    "plt.tight_layout()\n",
                                    "plt.savefig('static/images/plot.png')\n",
                                    "print(\"Training metrics visualization saved to 'static/images/plot.png'\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Creating Confusion Matrix for Sketch Recognition\n",
                                    "\n",
                                    "Now that you've visualized your model's training metrics, let's create a confusion matrix to gain deeper insights into your model's performance. A confusion matrix shows how many predictions were correct for each class and reveals patterns in misclassifications.\n",
                                    "\n",
                                    "In this task, you'll create and visualize a confusion matrix for your sketch recognition model, complete with proper category labels. This visualization will help you identify which sketch types are most challenging for your model to distinguish.\n",
                                    "\n",
                                    "The confusion matrix is a standard evaluation tool in machine learning that complements accuracy metrics by showing exactly where your model makes mistakes. This information is invaluable for improving your model's performance on specific classes.\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "from sklearn.metrics import confusion_matrix\n",
                                    "import seaborn as sns\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the original model\n",
                                    "original_loss, original_accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Original model - Test accuracy: {original_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Save the model to disk\n",
                                    "model.save('sketch_recognition_model.keras')\n",
                                    "print(\"Model saved successfully!\")\n",
                                    "\n",
                                    "# TODO: Get model predictions on test data (convert from probabilities to class indices)\n",
                                    "y_pred = model.predict(________) \n",
                                    "y_pred_classes = np.argmax(________, axis=________) \n",
                                    "\n",
                                    "# TODO: Create the confusion matrix using sklearn's confusion_matrix function\n",
                                    "cm = confusion_matrix(________, ________) \n",
                                    "\n",
                                    "# TODO: Plot the confusion matrix as a heatmap using seaborn's heatmap function.\n",
                                    "#       - Set annot=True to show numbers in each cell.\n",
                                    "#       - Set fmt='d' for integer formatting.\n",
                                    "#       - Use cmap='Blues' for color.\n",
                                    "#       - Set xticklabels and yticklabels to the category names.\n",
                                    "#       - Add axis labels and a title.\n",
                                    "\n",
                                    "# TODO: For each category, print the accuracy for that class.\n",
                                    "#       - For each row in the confusion matrix, calculate the number of correct predictions (diagonal value).\n",
                                    "#       - Calculate the total number of true samples for that class (sum of the row).\n",
                                    "#       - Print the accuracy as a percentage and the count of correct predictions.\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "## Creating Confusion Matrix for Sketch Recognition\n",
                                    "\n",
                                    "Now that you've visualized your model's training metrics, let's create a confusion matrix to gain deeper insights into your model's performance. A confusion matrix shows how many predictions were correct for each class and reveals patterns in misclassifications.\n",
                                    "\n",
                                    "In this task, you'll create and visualize a confusion matrix for your sketch recognition model, complete with proper category labels. This visualization will help you identify which sketch types are most challenging for your model to distinguish.\n",
                                    "\n",
                                    "The confusion matrix is a standard evaluation tool in machine learning that complements accuracy metrics by showing exactly where your model makes mistakes. This information is invaluable for improving your model's performance on specific classes.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "from sklearn.metrics import confusion_matrix\n",
                                    "import seaborn as sns\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the original model\n",
                                    "original_loss, original_accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Original model - Test accuracy: {original_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Save the model to disk\n",
                                    "model.save('sketch_recognition_model.keras')\n",
                                    "print(\"Model saved successfully!\")\n",
                                    "\n",
                                    "# Get model predictions on test data (convert from probabilities to class indices)\n",
                                    "y_pred = model.predict(x_test) \n",
                                    "y_pred_classes = np.argmax(y_pred, axis=1) \n",
                                    "\n",
                                    "# Create the confusion matrix using sklearn's confusion_matrix function\n",
                                    "cm = confusion_matrix(y_test, y_pred_classes) \n",
                                    "\n",
                                    "# Plot the confusion matrix as a heatmap using seaborn's heatmap function.\n",
                                    "plt.figure(figsize=(8, 6))\n",
                                    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                                    "            xticklabels=categories, yticklabels=categories)\n",
                                    "plt.xlabel('Predicted Label')\n",
                                    "plt.ylabel('True Label')\n",
                                    "plt.title('Confusion Matrix')\n",
                                    "plt.tight_layout()\n",
                                    "plt.savefig('static/images/confusion_matrix.png')\n",
                                    "print(\"Confusion matrix visualization saved to 'static/images/confusion_matrix.png'\")\n",
                                    "\n",
                                    "# For each category, print the accuracy for that class.\n",
                                    "print(\"\\nAccuracy per class:\")\n",
                                    "for i, category in enumerate(categories):\n",
                                    "    correct_predictions = cm[i, i]\n",
                                    "    total_true_samples = np.sum(cm[i, :])\n",
                                    "    if total_true_samples > 0:\n",
                                    "        accuracy = (correct_predictions / total_true_samples) * 100\n",
                                    "        print(f\"  {category}: {accuracy:.2f}% ({correct_predictions}/{total_true_samples} correct)\")\n",
                                    "    else:\n",
                                    "        print(f\"  {category}: No true samples for this class in the test set.\")\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Finding Most Confused Category Pairs\n",
                                    "\n",
                                    "Great job creating the confusion matrix! Now, let's take your analysis one step further by identifying the most confused pairs of categories. This is a crucial skill for model improvement — knowing exactly which classes your model struggles to distinguish helps you focus your efforts on the right areas.\n",
                                    "\n",
                                    "In this task, you'll implement a function that analyzes the confusion matrix to find and rank pairs of categories that are most frequently confused with each other. For example, your model might often mistake cat sketches for house sketches, or vice versa.\n",
                                    "\n",
                                    "Understanding these confusion patterns is valuable for targeting specific weaknesses in your model, potentially collecting more training data for problematic categories, or considering feature engineering to better distinguish similar classes.\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "from sklearn.metrics import confusion_matrix\n",
                                    "import seaborn as sns\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the original model\n",
                                    "original_loss, original_accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Original model - Test accuracy: {original_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Get predictions on test data\n",
                                    "y_pred = model.predict(x_test)\n",
                                    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                                    "\n",
                                    "# Create confusion matrix\n",
                                    "cm = confusion_matrix(y_test, y_pred_classes)\n",
                                    "\n",
                                    "# TODO: Define a function to find the most confused pairs\n",
                                    "# The function should:\n",
                                    "# 1. Take the confusion matrix and category list as input\n",
                                    "# 2. Create a list to store tuples of (true_category, predicted_category, count)\n",
                                    "# 3. Loop through each cell in the confusion matrix (except diagonal)\n",
                                    "# 4. Sort the list by count in descending order\n",
                                    "# 5. Return the sorted list\n",
                                    "def find_confused_pairs(cm, categories):\n",
                                    "    n_classes = len(________) \n",
                                    "    confused_pairs = []\n",
                                    "    \n",
                                    "    for i in range(________): \n",
                                    "        for j in range(________): \n",
                                    "            if ________:  # Skip diagonal (correct predictions) \n",
                                    "                confused_pairs.append((________, ________, ________)) \n",
                                    "    \n",
                                    "    # Sort by confusion count (descending)\n",
                                    "    confused_pairs.sort(key=lambda x: ________, reverse=________) \n",
                                    "    return confused_pairs\n",
                                    "\n",
                                    "# TODO: Call the function and print the results\n",
                                    "confused_pairs = find_confused_pairs(________, ________) \n",
                                    "print(\"\\nMost Confused Category Pairs:\")\n",
                                    "print(\"-----------------------------\")\n",
                                    "for true_cat, pred_cat, count in confused_pairs:\n",
                                    "    print(f\"True: {________}, Predicted: {________}, Count: {________}\") \n",
                                    "\n",
                                    "# TODO: Calculate and print confusion as percentage of true class\n",
                                    "print(\"\\nConfusion as Percentage of True Class:\")\n",
                                    "print(\"------------------------------------\")\n",
                                    "for true_cat, pred_cat, count in confused_pairs:\n",
                                    "    true_idx = categories.index(________) \n",
                                    "    total = np.sum(cm[________, :]) \n",
                                    "    percentage = (________ / ________) * 100 \n",
                                    "    print(f\"True: {true_cat}, Predicted: {pred_cat}: {percentage:.2f}%\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "## Finding Most Confused Category Pairs\n",
                                    "\n",
                                    "Great job creating the confusion matrix\\! Now, let's take your analysis one step further by identifying the most confused pairs of categories. This is a crucial skill for model improvement — knowing exactly which classes your model struggles to distinguish helps you focus your efforts on the right areas.\n",
                                    "\n",
                                    "In this task, you'll implement a function that analyzes the confusion matrix to find and rank pairs of categories that are most frequently confused with each other. For example, your model might often mistake cat sketches for house sketches, or vice versa.\n",
                                    "\n",
                                    "Understanding these confusion patterns is valuable for targeting specific weaknesses in your model, potentially collecting more training data for problematic categories, or considering feature engineering to better distinguish similar classes.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "from sklearn.metrics import confusion_matrix\n",
                                    "import seaborn as sns\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam',\n",
                                    "              loss='sparse_categorical_crossentropy',\n",
                                    "              metrics=['accuracy'])\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the original model\n",
                                    "original_loss, original_accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Original model - Test accuracy: {original_accuracy:.4f}\")\n",
                                    "\n",
                                    "# Get predictions on test data\n",
                                    "y_pred = model.predict(x_test)\n",
                                    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                                    "\n",
                                    "# Create confusion matrix\n",
                                    "cm = confusion_matrix(y_test, y_pred_classes)\n",
                                    "\n",
                                    "# Define a function to find the most confused pairs\n",
                                    "def find_confused_pairs(cm, categories):\n",
                                    "    n_classes = len(categories) \n",
                                    "    confused_pairs = []\n",
                                    "    \n",
                                    "    for i in range(n_classes): \n",
                                    "        for j in range(n_classes): \n",
                                    "            if i != j:  # Skip diagonal (correct predictions) \n",
                                    "                confused_pairs.append((categories[i], categories[j], cm[i, j])) \n",
                                    "    \n",
                                    "    # Sort by confusion count (descending)\n",
                                    "    confused_pairs.sort(key=lambda x: x[2], reverse=True) \n",
                                    "    return confused_pairs\n",
                                    "\n",
                                    "# Call the function and print the results\n",
                                    "confused_pairs = find_confused_pairs(cm, categories) \n",
                                    "print(\"\\nMost Confused Category Pairs:\")\n",
                                    "print(\"-----------------------------\")\n",
                                    "for true_cat, pred_cat, count in confused_pairs:\n",
                                    "    print(f\"True: {true_cat}, Predicted: {pred_cat}, Count: {count}\") \n",
                                    "\n",
                                    "# Calculate and print confusion as percentage of true class\n",
                                    "print(\"\\nConfusion as Percentage of True Class:\")\n",
                                    "print(\"------------------------------------\")\n",
                                    "for true_cat, pred_cat, count in confused_pairs:\n",
                                    "    true_idx = categories.index(true_cat) \n",
                                    "    total = np.sum(cm[true_idx, :]) \n",
                                    "    if total > 0:\n",
                                    "        percentage = (count / total) * 100 \n",
                                    "        print(f\"True: {true_cat}, Predicted: {pred_cat}: {percentage:.2f}%\")\n",
                                    "    else:\n",
                                    "        print(f\"True: {true_cat}, Predicted: {pred_cat}: No true samples for this class.\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Detailed Metrics for Sketch Recognition Performance\n",
                                    "\n",
                                    "After analyzing confusion patterns between categories, let's generate a more comprehensive evaluation using a classification report. This powerful tool provides detailed metrics for each sketch category that go beyond simple accuracy.\n",
                                    "\n",
                                    "A classification report includes three key metrics:\n",
                                    "\n",
                                    "Precision: How many of the sketches identified as a certain category are actually that category\n",
                                    "Recall: How many of the actual sketches of a category were correctly identified\n",
                                    "F1-score: A balanced measure combining precision and recall\n",
                                    "These metrics reveal nuanced insights about your model's performance on each category. For instance, your model might correctly identify most apple sketches (high recall) but sometimes mistakenly classify other sketches as apples (lower precision).\n",
                                    "\n",
                                    "In this task, you'll generate a classification report and add a custom per-class analysis to better understand your model's strengths and weaknesses with each sketch type.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "from sklearn.metrics import classification_report\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "\n",
                                    "    model.compile(optimizer='adam',\n",
                                    "                 loss='sparse_categorical_crossentropy',\n",
                                    "                 metrics=['accuracy'])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "# TODO: Get model predictions on the test data\n",
                                    "y_pred = model.predict(________) \n",
                                    "\n",
                                    "# TODO: Convert the prediction probabilities to class indices\n",
                                    "y_pred_classes = np.argmax(________, axis=________) \n",
                                    "\n",
                                    "# TODO: Generate a classification report using sklearn's classification_report function\n",
                                    "report = classification_report(________, ________, target_names=________) \n",
                                    "\n",
                                    "# TODO: Print the classification report with a descriptive header\n",
                                    "print(\"\\nClassification Report:\")\n",
                                    "print(________) \n",
                                    "\n",
                                    "# TODO: Add a per-class analysis section that:\n",
                                    "print(\"\\nPer-class Analysis:\")\n",
                                    "print(\"------------------\")\n",
                                    "for i, category in enumerate(categories):\n",
                                    "    # TODO: Find indices for this class\n",
                                    "    class_indices = np.where(________ == i)[0] \n",
                                    "    \n",
                                    "    # TODO: Get predictions and true labels for this class only\n",
                                    "    class_pred = y_pred_classes[________] \n",
                                    "    class_true = y_test[________] \n",
                                    "    \n",
                                    "    # TODO: Calculate accuracy for this class\n",
                                    "    class_accuracy = np.mean(________ == ________) \n",
                                    "    \n",
                                    "    # TODO: Count total samples for this class\n",
                                    "    total_samples = len(________) \n",
                                    "    \n",
                                    "    # TODO: Print the results for this class\n",
                                    "    print(f\"{category}: {________:.2%} accuracy ({________} samples)\") \n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "## Detailed Metrics for Sketch Recognition Performance\n",
                                    "\n",
                                    "After analyzing confusion patterns between categories, let's generate a more comprehensive evaluation using a classification report. This powerful tool provides detailed metrics for each sketch category that go beyond simple accuracy.\n",
                                    "\n",
                                    "A classification report includes three key metrics:\n",
                                    "\n",
                                    "  * **Precision**: How many of the sketches identified as a certain category are actually that category\n",
                                    "  * **Recall**: How many of the actual sketches of a category were correctly identified\n",
                                    "  * **F1-score**: A balanced measure combining precision and recall\n",
                                    "\n",
                                    "These metrics reveal nuanced insights about your model's performance on each category. For instance, your model might correctly identify most apple sketches (high recall) but sometimes mistakenly classify other sketches as apples (lower precision).\n",
                                    "\n",
                                    "In this task, you'll generate a classification report and add a custom per-class analysis to better understand your model's strengths and weaknesses with each sketch type.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "from sklearn.metrics import classification_report\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "\n",
                                    "    model.compile(optimizer='adam',\n",
                                    "                 loss='sparse_categorical_crossentropy',\n",
                                    "                 metrics=['accuracy'])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "# Get model predictions on the test data\n",
                                    "y_pred = model.predict(x_test) \n",
                                    "\n",
                                    "# Convert the prediction probabilities to class indices\n",
                                    "y_pred_classes = np.argmax(y_pred, axis=1) \n",
                                    "\n",
                                    "# Generate a classification report using sklearn's classification_report function\n",
                                    "report = classification_report(y_test, y_pred_classes, target_names=categories) \n",
                                    "\n",
                                    "# Print the classification report with a descriptive header\n",
                                    "print(\"\\nClassification Report:\")\n",
                                    "print(report) \n",
                                    "\n",
                                    "# Add a per-class analysis section that:\n",
                                    "print(\"\\nPer-class Analysis:\")\n",
                                    "print(\"------------------\")\n",
                                    "for i, category in enumerate(categories):\n",
                                    "    # Find indices for this class\n",
                                    "    class_indices = np.where(y_test == i)[0] \n",
                                    "    \n",
                                    "    # Get predictions and true labels for this class only\n",
                                    "    class_pred = y_pred_classes[class_indices] \n",
                                    "    class_true = y_test[class_indices] \n",
                                    "    \n",
                                    "    # Calculate accuracy for this class\n",
                                    "    class_accuracy = np.mean(class_pred == class_true) \n",
                                    "    \n",
                                    "    # Count total samples for this class\n",
                                    "    total_samples = len(class_indices) \n",
                                    "    \n",
                                    "    # Print the results for this class\n",
                                    "    print(f\"{category}: {class_accuracy:.2%} accuracy ({total_samples} samples)\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Visualizing Misclassified Sketches in Action\n",
                                    "\n",
                                    "After analyzing your model's performance with metrics and confusion patterns, let's examine what is actually happening with misclassified sketches. Seeing examples of where your model makes mistakes provides valuable insights that numbers alone cannot capture.\n",
                                    "\n",
                                    "In this task, you will identify sketches that your model classified incorrectly and display them in a grid. This visual inspection will help you spot patterns in your model's errors — perhaps certain sketches are consistently misinterpreted due to similar visual features.\n",
                                    "\n",
                                    "By examining misclassified examples, you can better understand your model's weaknesses and potentially improve its architecture or training data to address these specific issues.\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "\n",
                                    "    model.compile(optimizer='adam',\n",
                                    "                 loss='sparse_categorical_crossentropy',\n",
                                    "                 metrics=['accuracy'])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "# Get predictions\n",
                                    "y_pred = model.predict(x_test)\n",
                                    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                                    "\n",
                                    "# TODO: Find indices of misclassified images (where predicted class != true class)\n",
                                    "misclassified_indices = np.where(________ != ________)[0]\n",
                                    "print(f\"Found {len(misclassified_indices)} misclassified images\")\n",
                                    "\n",
                                    "# TODO: Create a figure to display misclassified images\n",
                                    "plt.figure(figsize=(________, ________))\n",
                                    "\n",
                                    "# TODO: Loop through the first 9 misclassified images (or fewer if there aren't 9)\n",
                                    "for i in range(min(________, len(________))): \n",
                                    "    # TODO: Get the index of the misclassified image\n",
                                    "    idx = ________[i]\n",
                                    "    \n",
                                    "    # TODO: Create a subplot in a 3x3 grid\n",
                                    "    plt.subplot(________, ________, ________)\n",
                                    "    \n",
                                    "    # TODO: Display the image (remember to reshape from (28,28,1) to (28,28))\n",
                                    "    plt.imshow(________.reshape(________, ________), cmap='gray')\n",
                                    "    \n",
                                    "    # TODO: Set the title to show both true and predicted categories\n",
                                    "    plt.title(f\"True: {________[________[idx]]}\\nPred: {________[________[idx]]}\")\n",
                                    "    \n",
                                    "    # TODO: Turn off axis labels\n",
                                    "    plt.axis(________)\n",
                                    "\n",
                                    "# TODO: Adjust layout and save the figure\n",
                                    "plt.tight_layout()\n",
                                    "plt.savefig('static/images/misclassified.png')\n",
                                    "print(\"Misclassified images visualization saved to 'static/images/misclassified.png'\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "## Visualizing Misclassified Sketches in Action\n",
                                    "\n",
                                    "After analyzing your model's performance with metrics and confusion patterns, let's examine what is actually happening with misclassified sketches. Seeing examples of where your model makes mistakes provides valuable insights that numbers alone cannot capture.\n",
                                    "\n",
                                    "In this task, you will identify sketches that your model classified incorrectly and display them in a grid. This visual inspection will help you spot patterns in your model's errors — perhaps certain sketches are consistently misinterpreted due to similar visual features.\n",
                                    "\n",
                                    "By examining misclassified examples, you can better understand your model's weaknesses and potentially improve its architecture or training data to address these specific issues.\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import warnings\n",
                                    "\n",
                                    "# Suppress warnings\n",
                                    "warnings.filterwarnings(\"ignore\", message=\"Your `PyDataset` class should call\", category=UserWarning)\n",
                                    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
                                    "\n",
                                    "import urllib.request\n",
                                    "import numpy as np\n",
                                    "import os\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                                    "import tensorflow as tf\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Data loading and preprocessing (already done for you)\n",
                                    "categories = ['cat', 'house', 'apple']\n",
                                    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
                                    "\n",
                                    "os.makedirs('quickdraw_data', exist_ok=True)\n",
                                    "\n",
                                    "for category in categories:\n",
                                    "    file_path = f'quickdraw_data/{category}.npy'\n",
                                    "    if not os.path.exists(file_path):\n",
                                    "        print(f\"Downloading {category}...\")\n",
                                    "        urllib.request.urlretrieve(base_url + category + '.npy', file_path)\n",
                                    "    else:\n",
                                    "        print(f\"{category}.npy already exists.\")\n",
                                    "\n",
                                    "# Load and prepare data\n",
                                    "data = []\n",
                                    "labels = []\n",
                                    "IMAGE_COUNT = 3000\n",
                                    "\n",
                                    "for idx, cat in enumerate(categories):\n",
                                    "    filepath = f'quickdraw_data/{cat}.npy'\n",
                                    "    imgs = np.load(filepath)[:IMAGE_COUNT]\n",
                                    "    if imgs.dtype != np.uint8:\n",
                                    "        imgs = imgs.astype(np.uint8)\n",
                                    "    data.append(imgs)\n",
                                    "    labels.append(np.full(imgs.shape[0], idx))\n",
                                    "\n",
                                    "# Combine all data and labels\n",
                                    "data = np.concatenate(data, axis=0)\n",
                                    "labels = np.concatenate(labels, axis=0)\n",
                                    "\n",
                                    "# Shuffle data\n",
                                    "indices = np.arange(len(data))\n",
                                    "np.random.shuffle(indices)\n",
                                    "data, labels = data[indices], labels[indices]\n",
                                    "\n",
                                    "# Reshape and normalize\n",
                                    "data = data.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
                                    "print(f\"Training data shape: {x_train.shape}, Testing data shape: {x_test.shape}\")\n",
                                    "\n",
                                    "# Create data augmentation generator\n",
                                    "datagen = ImageDataGenerator(\n",
                                    "    rotation_range=15,\n",
                                    "    width_shift_range=0.1,\n",
                                    "    height_shift_range=0.1,\n",
                                    "    zoom_range=0.1,\n",
                                    "    shear_range=0.1,\n",
                                    "    fill_mode='nearest'\n",
                                    ")\n",
                                    "\n",
                                    "# Fit the generator to the training data\n",
                                    "datagen.fit(x_train)\n",
                                    "\n",
                                    "def build_simple_cnn():\n",
                                    "    model = tf.keras.Sequential([\n",
                                    "        tf.keras.layers.Input(shape=(28,28,1)),\n",
                                    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
                                    "        tf.keras.layers.MaxPooling2D(),\n",
                                    "        tf.keras.layers.Flatten(),\n",
                                    "        tf.keras.layers.Dense(128, activation='relu'),\n",
                                    "        tf.keras.layers.Dense(len(categories), activation='softmax')\n",
                                    "    ])\n",
                                    "\n",
                                    "    model.compile(optimizer='adam',\n",
                                    "                 loss='sparse_categorical_crossentropy',\n",
                                    "                 metrics=['accuracy'])\n",
                                    "    return model\n",
                                    "\n",
                                    "# Build the model\n",
                                    "model = build_simple_cnn()\n",
                                    "\n",
                                    "# Print model summary\n",
                                    "model.summary()\n",
                                    "\n",
                                    "# Train the model for a few epochs\n",
                                    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
                                    "                   epochs=3,\n",
                                    "                   validation_data=(x_test, y_test),\n",
                                    "                   steps_per_epoch=len(x_train) // 32)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "loss, accuracy = model.evaluate(x_test, y_test)\n",
                                    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
                                    "\n",
                                    "# Get predictions\n",
                                    "y_pred = model.predict(x_test)\n",
                                    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                                    "\n",
                                    "# Find indices of misclassified images (where predicted class != true class)\n",
                                    "misclassified_indices = np.where(y_pred_classes != y_test)[0]\n",
                                    "print(f\"Found {len(misclassified_indices)} misclassified images\")\n",
                                    "\n",
                                    "# Create a figure to display misclassified images\n",
                                    "plt.figure(figsize=(10, 10))\n",
                                    "\n",
                                    "# Loop through the first 9 misclassified images (or fewer if there aren't 9)\n",
                                    "for i in range(min(9, len(misclassified_indices))): \n",
                                    "    # Get the index of the misclassified image\n",
                                    "    idx = misclassified_indices[i]\n",
                                    "    \n",
                                    "    # Create a subplot in a 3x3 grid\n",
                                    "    plt.subplot(3, 3, i + 1)\n",
                                    "    \n",
                                    "    # Display the image (remember to reshape from (28,28,1) to (28,28))\n",
                                    "    plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
                                    "    \n",
                                    "    # Set the title to show both true and predicted categories\n",
                                    "    plt.title(f\"True: {categories[y_test[idx]]}\\nPred: {categories[y_pred_classes[idx]]}\")\n",
                                    "    \n",
                                    "    # Turn off axis labels\n",
                                    "    plt.axis('off')\n",
                                    "\n",
                                    "# Adjust layout and save the figure\n",
                                    "plt.tight_layout()\n",
                                    "plt.savefig('static/images/misclassified.png')\n",
                                    "print(\"Misclassified images visualization saved to 'static/images/misclassified.png'\")\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
