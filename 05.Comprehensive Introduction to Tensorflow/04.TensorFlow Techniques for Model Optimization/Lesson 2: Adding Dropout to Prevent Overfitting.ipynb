{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to today's lesson! The topic of our discussion is the regularization technique Dropout. In machine learning, dropout is a powerful tool to help your models avoid overfitting. Overfitting happens when your model performs well on training data but poorly on new, unseen data. To address this problem, dropout randomly turns off a fraction of neurons during each training iteration which helps to improve the generalization of the model. By the end of this lesson, you will understand the dropout technique, and you'll be able to integrate it in TensorFlow models to increase their robustness and generalization. Let's dive in!\n",
    "\n",
    "## Understanding Dropout\n",
    "The concept of Dropout is a simple yet powerful regularization technique. In the training phase, dropout randomly 'drops' or 'switches off' a fraction of neurons in the hidden layers. By 'drop', it means these neurons are not considered during a particular forward or backward pass. This reduced model is used for training in that iteration. The fraction of neurons to be dropped is controlled by a hyperparameter which is typically set between 0.2 and 0.5.\n",
    "\n",
    "The random turning-off of neurons ensures that no one set of neurons always works with each other to give outputs, thereby reducing their 'co-adaptation'. It also means multiple networks with different architectures (due to different neurons being active or 'on') are trained. The final result is as if we have taken an ensemble of all these different networks.\n",
    "\n",
    "Hence, dropout acts as a regularization method to prevent model's over-reliance on any specific neuron(s), leading to a more robust model that generalizes better.\n",
    "\n",
    "## Defining a Dropout Layer in TensorFlow\n",
    "In TensorFlow, adding a dropout layer to your model is straightforward. Here's a simple example of how to define a dropout layer in a TensorFlow model:\n",
    "\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a dropout layer with a 50% drop rate\n",
    "dropout_layer = tf.keras.layers.Dropout(0.5)\n",
    "```\n",
    "In this example, 0.5 is the dropout rate, meaning that during each training iteration, 50% of the neurons in this layer will be randomly turned off.\n",
    "\n",
    "## Building a TensorFlow Model with Dropout\n",
    "Now, let's see how to integrate this dropout layer into a TensorFlow model and print the summary of the model. Typically, a dropout layer is used after a Dense layer, where the model is learning most of the parameters. It influences the subsequent layers by randomly turning off neurons during each training iteration, which helps in preventing overfitting.\n",
    "\n",
    "Here's an example model where the dropout layer is placed right after a Dense layer:\n",
    "\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a model with dropout layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Adding dropout with a 50% rate after a Dense layer\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate a summary of the model's layers and parameters\n",
    "model.summary()\n",
    "```\n",
    "The output of the above code will be:\n",
    "\n",
    "```sh\n",
    "Copy\n",
    "Model: \"sequential\"\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
    "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
    "│ dense (Dense)                   │ (None, 64)             │           256 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dropout (Dropout)               │ (None, 64)             │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense_1 (Dense)                 │ (None, 1)              │            65 │\n",
    "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
    " Total params: 321 (1.25 KB)\n",
    " Trainable params: 321 (1.25 KB)\n",
    " Non-trainable params: 0 (0.00 B)\n",
    "```\n",
    "\n",
    "This output summarizes the architecture of the TensorFlow model with a dropout layer. It shows the number of parameters each layer has, including the dropout layer, which doesn't add any trainable parameters to the model. This architecture helps in preventing overfitting by randomly dropping out neurons during the training phase.\n",
    "\n",
    "## Best Practices and Pitfalls\n",
    "So when should you use dropout? In general, dropout is a good go-to when you're dealing with large neural networks that may be prone to overfitting. However, be wary when applying it to smaller networks as it may actually increase the likelihood of underfitting.\n",
    "\n",
    "Here are some common pitfalls when implementing dropout:\n",
    "\n",
    "Placement of Dropout Layers: Dropout layers should be used after Dense or Convolutional layers where the model is learning the most parameters.\n",
    "Incorrect Dropout Rate: As mentioned, the typical drop rate is between 0.2 and 0.5. A drop rate that is too high results in underfitting the data, while too low a drop rate doesn't have the intended effect of regularization.\n",
    "To sum up, dropout is a simple, yet powerful regularization technique that can help us to prevent overfitting in large neural networks. Experimenting with its positioning, along with setting an appropriate dropout rate, play a big role in getting its benefits.\n",
    "\n",
    "## Lesson Summary and Practice\n",
    "Great work! We've just gone over a powerful regularization technique called Dropout. After today, you should have a clearer understanding of what dropout is, when to use it, and how to integrate it in TensorFlow to improve the robustness and generalization of your models.\n",
    "\n",
    "In our next hands-on practice session, you'll be faced with tasks that require you to implement dropout layers. This will allow you to practically apply what you've learned today and better understand how Dropout can be used with TensorFlow. Practice is the key to mastering these concepts!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary with Dropout Layers\n",
    "You've learned how to integrate dropout layers to prevent overfitting in your models. Now, let's put that knowledge into practice by running some code.\n",
    "\n",
    "Simply execute the provided code to see how a TensorFlow model with dropout layers is constructed and summarized.\n",
    "\n",
    "Observe the architecture and layer details in the output.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a model with dropout layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Adding dropout with a 50% rate\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate a summary of the model's layers and parameters\n",
    "model.summary()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Dropout Rate in Model\n",
    "\n",
    "\n",
    "Great job on the previous task! Now, let's deepen your understanding by modifying the code.\n",
    "\n",
    "In this task, change the dropout rate from 0.5 to 0.3. This will help you understand how you can adjust the dropout rate of a model to deal with underfitting or overfitting .\n",
    "\n",
    "```py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a model with dropout layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # TODO: Change dropout rate to 30%\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate a summary of the model's layers and parameters\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### answer\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a model with dropout layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),  # Changed dropout rate to 30%\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate a summary of the model's layers and parameters\n",
    "model.summary()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Model Dropout Issues\n",
    "\n",
    "You've understood TensorFlow models built with dropout layers. Now, it's time to test your debugging skills. The code below builds a model containing an implementation mistake related to dropout layer placement.\n",
    "\n",
    "Your task is to identify and fix the issue. Debugging is a crucial skill in programming, and this exercise will help reinforce your understanding of using dropout layers in TensorFlow.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a model with dropout layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dropout(0.5),  \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate a summary of the model's layers and parameters\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### answer\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a model with dropout layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Moved dropout layer after the dense layer\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate a summary of the model's layers and parameters\n",
    "model.summary()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Dropout Layer to Model\n",
    "\n",
    "Great progress so far! Now, let's see if you can implement part of the code by yourself. In this exercise, you will add a dropout layer to our model. Specifically, you need to fill in the missing parts to add the dropout layer after the first dense layer.\n",
    "\n",
    "Fill in the missing parts denoted by TODO comments.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a model with dropout layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    # TODO: Add a Dropout layer with a 50% drop rate here\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate a summary of the model's layers and parameters\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### answer\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model with Dropout\n",
    "\n",
    "You've practiced implementing dropout layers in the previous exercises. This final task will consolidate your understanding by having you write the code from scratch. Define a binary classification model, integrate dropout to handle overfitting, compile it, and summarize it.\n",
    "\n",
    "This task will help solidify your understanding of integrating dropout layers in TensorFlow.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Define a sequential model with the following layers and properties:\n",
    "# - An input layer with shape of 3\n",
    "# - A dense layer with 64 neurons and 'relu' activation function\n",
    "# - A dropout layer with a 50% rate\n",
    "# - A dense output layer with 1 neuron and 'sigmoid' activation function\n",
    "\n",
    "# TODO: Compile the model with the following configuration:\n",
    "# - Use the 'adam' optimizer\n",
    "# - Set loss to 'binary_crossentropy'\n",
    "# - Set metrics to ['accuracy']\n",
    "\n",
    "# TODO: Generate a summary of the model's layers and parameters\n",
    "```\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    # Input layer with shape of 3\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    \n",
    "    # Dense layer with 64 neurons and 'relu' activation function\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    # Dropout layer with a 50% rate\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Dense output layer with 1 neuron and 'sigmoid' activation function\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with the specified configuration\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Generate and print a summary of the model's layers and parameters\n",
    "model.summary()\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
