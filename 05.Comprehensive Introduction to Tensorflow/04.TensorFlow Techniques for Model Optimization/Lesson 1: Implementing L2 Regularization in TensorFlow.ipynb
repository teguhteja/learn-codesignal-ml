{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Overview\n",
    "Hello and welcome to this lesson on implementing regularization in TensorFlow. Regularization can be an important tool when you're building a machine learning model, especially when you need to manage overfitting. In this lesson, we will focus on both L1 and L2 regularization and how to implement them in your TensorFlow model.\n",
    "\n",
    "## Regularization Basics\n",
    "Regularization is a crucial technique in machine learning for avoiding overfitting. Overfitting happens when a model performs exceptionally well on training data but poorly on new, unseen data. Regularization adds a penalty to the loss function to keep the model from learning the noise in the training data, encouraging it to focus on the most important patterns.\n",
    "\n",
    "L1 regularization, also known as Lasso regularization, adds the absolute values of the weights to the loss function. By doing so, it can drive some weights to exactly zero. This is useful for feature selection because it can eliminate less important features, simplifying the model.\n",
    "\n",
    "L2 regularization, also referred to as ridge regression or Tikhonov regularization, penalizes the squared values of the weights. This approach shrinks the weights but doesn't eliminate them entirely. As a result, all features are retained but with reduced importance, lowering the complexity of the model and helping prevent overfitting.\n",
    "\n",
    "In summary, both L1 and L2 regularization help in managing overfitting, but they do so in slightly different ways. L1 regularization can zero out weights, effectively performing feature selection, while L2 regularization reduces the magnitude of weights, keeping all features but with less impact.\n",
    "\n",
    "## Implementing L1 Regularization in a Layer\n",
    "Implementing L1 regularization in a TensorFlow model is straightforward. You utilize the kernel_regularizer argument within the layer's constructor and specify tf.keras.regularizers.l1 along with the desired strength of regularization. Below is an example:\n",
    "\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a dense layer with L1 regularization\n",
    "dense_layer_l1 = tf.keras.layers.Dense(\n",
    "    10, \n",
    "    activation='relu',\n",
    "    kernel_regularizer=tf.keras.regularizers.l1(0.01)\n",
    ")\n",
    "```\n",
    "In this snippet, we're creating a dense layer that applies L1 regularization on its weights. The regularization strength for L1 is set to 0.01, meaning larger weights will incur a heavier penalty, which helps to prevent overfitting.\n",
    "\n",
    "## Employing L2 Regularization in a Layer\n",
    "Just as adding L1 regularization is simple, incorporating L2 regularization follows a similar process but with tf.keras.regularizers.l2. Instead of minimizing the sum of the absolute values of the weights, L2 minimizes the sum of the squared values.\n",
    "\n",
    "Here is how you can implement it:\n",
    "\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a dense layer with L2 regularization\n",
    "dense_layer_l2 = tf.keras.layers.Dense(\n",
    "    10, \n",
    "    activation='relu',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    ")\n",
    "```\n",
    "This example demonstrates the addition of L2 regularization to a dense layer. The regularization strength is also set to 0.01, which will penalize larger weights, reducing their magnitude and helping to generalize the model better.\n",
    "\n",
    "## Combining L1 and L2 Regularization in a Model\n",
    "After familiarizing yourself with both types of regularization, you can apply them together within a single model. Let's illustrate this by applying L1 regularization to one layer and L2 regularization to another:\n",
    "\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Input layer\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    # First dense layer with L1 regularization\n",
    "    tf.keras.layers.Dense(\n",
    "        10, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(0.01)\n",
    "    ),\n",
    "    # Second dense layer with L2 regularization\n",
    "    tf.keras.layers.Dense(\n",
    "        10, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "    ),\n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "```\n",
    "In this code, the first dense layer uses L1 regularization, penalizing larger weights by adding the absolute value of the weights to the loss function. The second dense layer, however, uses L2 regularization, minimizing the sum of the squared values of the weights. This combination aims to leverage the strengths of both regularization techniques to manage overfitting effectively.\n",
    "\n",
    "## Verifying the Model\n",
    "To ensure that regularization is properly applied to our model layers, we can check the kernel_regularizer attribute of each layer. Note that the Input layer doesn't count as a layer in the model.layers list, so the indexes start from 0 for the first actual layer.\n",
    "\n",
    "Here's how to do it:\n",
    "\n",
    "```Python\n",
    "# Verify regularization in the first dense layer\n",
    "print(\"First layer kernel_regularizer:\", model.layers[0].kernel_regularizer)\n",
    "\n",
    "# Verify regularization in the second dense layer\n",
    "print(\"Second layer kernel_regularizer:\", model.layers[1].kernel_regularizer)\n",
    "```\n",
    "The output of the above code will be similar to this:\n",
    "\n",
    "```sh\n",
    "\n",
    "First layer kernel_regularizer: <keras.src.regularizers.regularizers.L1 object at 0x177c9b6e0>\n",
    "Second layer kernel_regularizer: <keras.src.regularizers.regularizers.L2 object at 0x177684380>\n",
    "```\n",
    "The output confirms that the desired regularization techniques (L1 and L2) have been correctly applied to the layers. This verification step is helpful because it visually assures you that the regularization mechanisms are part of the layer attributes. Having regularized layers is crucial for managing overfitting, as it ensures that larger weights are appropriately penalized, which helps the model generalize better on new, unseen data.\n",
    "\n",
    "## Lesson Summary and Practice\n",
    "That wraps up our lesson on implementing regularization in TensorFlow. You should now understand what L1 and L2 regularization are, why they might be useful for your models, and most importantly, how to add them to your models using TensorFlow.\n",
    "\n",
    "Next up, why don't you try practicing what you've learnt by implementing regularization in some of your own models? Remember, while we've focused on L1 and L2 regularization here, TensorFlow also supports other types of regularization, so feel free to explore those as well. Happy modeling!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization: See it in Action\n",
    "In this task, you'll gain hands-on experience with how to add L1 and L2 regularization to a TensorFlow model. Regularization helps reduce overfitting, thus enhancing the model's generalization capability.\n",
    "\n",
    "All you need to do is run the code to see how L1 and L2 regularization are implemented and verified in a neural network layer.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Input layer\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    # First dense layer with L1 regularization\n",
    "    tf.keras.layers.Dense(\n",
    "        10, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(0.01)\n",
    "    ),\n",
    "    # Second dense layer with L2 regularization\n",
    "    tf.keras.layers.Dense(\n",
    "        10, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "    ),\n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Verify regularization in the first dense layer\n",
    "print(\"First layer kernel_regularizer:\", model.layers[0].kernel_regularizer)\n",
    "\n",
    "# Verify regularization in the second dense layer\n",
    "print(\"Second layer kernel_regularizer:\", model.layers[1].kernel_regularizer)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Regularization Techniques in Model\n",
    "Now that you've learned about L1 and L2 regularization, it's time to put that knowledge into practice.\n",
    "\n",
    "Your task is to swap the regularization techniques in the code.\n",
    "\n",
    "Change the first layer to use L2 regularization and the second layer to use L1 regularization. This change will help you see how different regularization techniques can be applied to different layers.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Input layer\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    # First dense layer with L2 regularization\n",
    "    tf.keras.layers.Dense(\n",
    "        10, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "    ),\n",
    "    # Second dense layer with L1 regularization\n",
    "    tf.keras.layers.Dense(\n",
    "        10, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(0.01)\n",
    "    ),\n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Verify regularization in the first dense layer\n",
    "print(\"First layer kernel_regularizer:\", model.layers[0].kernel_regularizer)\n",
    "\n",
    "# Verify regularization in the second dense layer\n",
    "print(\"Second layer kernel_regularizer:\", model.layers[1].kernel_regularizer)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the Regularization Mistakes\n",
    "\n",
    "Great job so far! You've learned how to implement regularization in TensorFlow to help prevent overfitting in your models.\n",
    "\n",
    "In the following task, you will debug a TensorFlow model. There is a mistake in the verification of the regularization in the layers. Your task is to find and fix the error.\n",
    "\n",
    "```py\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Regularization in TensorFlow"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
