{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In today's lesson, our focus is on preprocessing the Iris dataset for TensorFlow. We will explore various techniques, such as data splitting, feature scaling, and one-hot encoding. This foundation is invaluable in the field of machine learning as it aids in understanding the intricacies of data transformation before we feed it to a neural network. Let's get into it!\n",
    "\n",
    "## Overview of the Iris Dataset\n",
    "Before we delve into data preprocessing, it is imperative to understand the data we are processing. The Iris dataset comprises measurements from 150 Iris flowers coming from three different species. Each sample includes the following 4 features:\n",
    "\n",
    "- Sepal length (cm): e.g., 5.1, 4.9, 4.7, etc.\n",
    "- Sepal width (cm): e.g., 3.5, 3.0, 3.2, etc.\n",
    "- Petal length (cm): e.g., 1.4, 1.4, 1.3, etc.\n",
    "- Petal width (cm): e.g., 0.2, 0.2, 0.2, etc.\n",
    "Additionally, each sample has a class label representing the Iris species. The targets in the dataset are represented as one of the following options:\n",
    "\n",
    "- Iris setosa: 0\n",
    "- Iris versicolor: 1\n",
    "- Iris virginica: 2\n",
    "With these measurements and labels, the Iris dataset becomes a multivariate dataset often used for machine learning introductions.\n",
    "\n",
    "## Insight into Data Preprocessing\n",
    "Data preprocessing is a crucial step in machine learning. It is the process of converting or mapping data from the initial form to another format to prepare the data for the next processing phase. This converted data could be easier for the algorithms to extract information, hence improving their ability to predict. The steps involved in preprocessing we will cover in today's lesson include data load, split, scale, and encode.\n",
    "\n",
    "## Step 1: Loading the Dataset\n",
    "Before diving into preprocessing, let's start by loading the Iris dataset. We use the load_iris function from scikit-learn for this purpose. It returns the feature matrix X and the target vector y.\n",
    "\n",
    "```Python\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Displaying shapes\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')\n",
    "```\n",
    "The output will be:\n",
    "\n",
    "```sh\n",
    "\n",
    "X shape: (150, 4)\n",
    "y shape: (150,)\n",
    "```\n",
    "\n",
    "Here, X contains 150 samples, each with 4 features (sepal length, sepal width, petal length, and petal width). The y vector contains 150 class labels, with each label representing one of the three Iris species. This initial step helps us understand the dimensions of our dataset before we proceed with further processing.\n",
    "\n",
    "## Step 2: Splitting into Training and Testing Sets\n",
    "The initial step in preprocessing itself is data splitting. We divide the dataset into two parts: a training set and a testing set. The training set is used to train the model, while the testing set validates its performance. Typically, we use scikit-learn's train_test_split function for this purpose. By splitting the data, we ensure that our model can generalize well to new, unseen data. The stratify parameter ensures that the proportion of different classes in the split datasets is the same as in the original dataset. For our specific example, we will use 70% of the data for training and 30% for testing.\n",
    "\n",
    "```Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "```\n",
    "\n",
    "## Step 3: Feature Scaling\n",
    "After splitting the data, we perform feature scaling to normalize the range of independent variables or features. This step is crucial because it ensures all input features have the same scale, preventing features with larger scales from dominating those with smaller scales. We achieve this normalization using the StandardScaler from scikit-learn, which standardizes features by centering the data to have a mean of 0 and scaling to unit variance. The fit method calculates the mean and standard deviation for scaling based on the training data.\n",
    "\n",
    "```Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "## Step 4: Target Encoding\n",
    "The final preprocessing step is data encoding. The target variables in the Iris dataset are categorical and must be converted into a format that our machine learning model can utilize. This is done using one-hot encoding, which transforms categorical data into a binary (0 or 1) format. For example, a variable labeled as 1 (Iris versicolor) would be represented as [0, 1, 0] after one-hot encoding. We use the OneHotEncoder from scikit-learn to perform this action, ensuring our target variables are ready for input into the model. The fit method learns the unique categories present in the training data, which will be used for encoding.\n",
    "\n",
    "```Python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-hot encode the targets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(y_train.reshape(-1, 1))\n",
    "y_train_encoded = encoder.transform(y_train.reshape(-1, 1))\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "```\n",
    "\n",
    "## Data Preprocessing in Practice\n",
    "Below are the summarized preprocessing steps, including data loading, splitting, scaling, and encoding in one section encapsulated in a single function. This function facilitates modularization, allowing us to use the processed data imported in another file where we develop our model.\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # One-hot encode the targets\n",
    "    encoder = OneHotEncoder(sparse_output=False).fit(y_train.reshape(-1, 1))\n",
    "    y_train_encoded = encoder.transform(y_train.reshape(-1, 1))\n",
    "    y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded\n",
    "```\n",
    "\n",
    "## Loading and Printing Preprocessed Data\n",
    "After defining the function that preprocesses the data, we can load the preprocessed data and print a sample of the training input and target.\n",
    "\n",
    "```Python\n",
    "# Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Print a sample of one training input and target\n",
    "print(f'Sample of preprocessed X_train: {X_train[0]}')\n",
    "print(f'Sample of preprocessed y_train: {y_train[0]}\\n')\n",
    "\n",
    "# Print the shape of scaled and encoded data\n",
    "print(f'Shape of preprocessed X_train: {X_train.shape}')\n",
    "print(f'Shape of preprocessed X_test: {X_test.shape}')\n",
    "print(f'Shape of preprocessed y_train: {y_train.shape}')\n",
    "print(f'Shape of preprocessed y_test: {y_test.shape}')\n",
    "```\n",
    "The output of the above code will be:\n",
    "\n",
    "```sh\n",
    "Sample of preprocessed X_train: [-0.90045861 -1.22024754 -0.4419858  -0.13661044]\n",
    "Sample of preprocessed y_train: [0. 1. 0.]\n",
    "\n",
    "Shape of preprocessed X_train: (105, 4)\n",
    "Shape of preprocessed X_test: (45, 4)\n",
    "Shape of preprocessed y_train: (105, 3)\n",
    "Shape of preprocessed y_test: (45, 3)\n",
    "```\n",
    "\n",
    "This output illustrates the results of our preprocessing steps â€” scaling of feature data to ensure a standardized dataset and one-hot encoding of target variables to prepare them for machine learning models.\n",
    "\n",
    "## Lesson Summary and Practice\n",
    "In conclusion, we have successfully preprocessed the Iris dataset and made it ready for machine learning modeling with TensorFlow. We've loaded, split, scaled, and encoded the data using Python. This fundamental knowledge is essential to you as a Machine Learning Engineer to improve accuracy and build efficient models using TensorFlow.\n",
    "\n",
    "Next, we will have exercises to consolidate these preprocessing steps. The exercises aim to enhance your understanding and application of data preprocessing and prepare you for more challenging tasks in the future. Happy learning!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and Preprocessing the Iris Dataset\n",
    "We have covered important concepts for preprocessing the Iris dataset. Now, let's put that knowledge into practice.\n",
    "\n",
    "In this task, you will see how to preprocess the Iris dataset for TensorFlow by running the provided code. This includes splitting the dataset, scaling the features, and one-hot encoding the targets.\n",
    "\n",
    "Simply execute the code to observe how the data is preprocessed and the shapes of the resulting arrays.\n",
    "\n",
    "```py\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # One-hot encode the targets\n",
    "    encoder = OneHotEncoder(sparse_output=False).fit(y_train.reshape(-1, 1))\n",
    "    y_train_encoded = encoder.transform(y_train.reshape(-1, 1))\n",
    "    y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Print a sample of one training input and target\n",
    "print(f'Sample of preprocessed X_train: {X_train[0]}')\n",
    "print(f'Sample of preprocessed y_train: {y_train[0]}\\n')\n",
    "\n",
    "# Print the shape of scaled and encoded data\n",
    "print(f'Shape of preprocessed X_train: {X_train.shape}')\n",
    "print(f'Shape of preprocessed X_test: {X_test.shape}')\n",
    "print(f'Shape of preprocessed y_train: {y_train.shape}')\n",
    "print(f'Shape of preprocessed y_test: {y_test.shape}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Train-Test Split Ratio\n",
    "\n",
    "Great job understanding the basics of preprocessing the Iris dataset. Now, let's explore how changing the train-test split ratio impacts data preparation.\n",
    "\n",
    "Change the train-test split ratio from 70/30 to 80/20 by modifying the test_size parameter from 0.3 to 0.2. This will help you see how the data split affects model training and evaluation.\n",
    "\n",
    "```py\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Create an 80/20 train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # One-hot encode the targets\n",
    "    encoder = OneHotEncoder(sparse_output=False).fit(y_train.reshape(-1, 1))\n",
    "    y_train_encoded = encoder.transform(y_train.reshape(-1, 1))\n",
    "    y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Print a sample of one training input and target\n",
    "print(f'Sample of preprocessed X_train: {X_train[0]}')\n",
    "print(f'Sample of preprocessed y_train: {y_train[0]}\\n')\n",
    "\n",
    "# Print the shape of scaled and encoded data\n",
    "print(f'Shape of preprocessed X_train: {X_train.shape}')\n",
    "print(f'Shape of preprocessed X_test: {X_test.shape}')\n",
    "print(f'Shape of preprocessed y_train: {y_train.shape}')\n",
    "print(f'Shape of preprocessed y_test: {y_test.shape}')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the Data Preprocessing Bugs\n",
    "\n",
    "So far, you have learned how to preprocess the Iris dataset. Now, it's time to practice and ensure you can implement it correctly.\n",
    "\n",
    "In this exercise, you need to fix a few bugs in the code that preprocesses the Iris dataset for TensorFlow. There are mistakes that prevent the code from running correctly.\n",
    "\n",
    "Your task is to find and fix these errors. This will help you understand common mistakes and ensure you can preprocess datasets accurately.\n",
    "\n",
    "```py\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Split the dataset into training and testing sets (80/20 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # One-hot encode the targets\n",
    "    encoder = OneHotEncoder(sparse_output=False).fit(y_train.reshape(-1, 1))\n",
    "    y_train_encoded = encoder.transform(y_train.reshape(-1, 1))\n",
    "    y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Print a sample of one training input and target\n",
    "print(f'Sample of preprocessed X_train: {X_train[0]}')\n",
    "print(f'Sample of preprocessed y_train: {y_train[0]}\\n')\n",
    "\n",
    "# Print the shape of scaled and encoded data\n",
    "print(f'Shape of preprocessed X_train: {X_train.shape}')\n",
    "print(f'Shape of preprocessed X_test: {X_test.shape}')\n",
    "print(f'Shape of preprocessed y_train: {y_train.shape}')\n",
    "print(f'Shape of preprocessed y_test: {y_test.shape}')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Data Preprocessing\n",
    "\n",
    "In the previous tasks, you practiced various preprocessing techniques for the Iris dataset.\n",
    "\n",
    "Now, in this exercise, you will complete the code to preprocess the dataset. This includes scaling the features, and one-hot encoding the targets.\n",
    "\n",
    "Fill in the missing parts denoted by TODO comments.\n",
    "\n",
    "```py\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "    # TODO: Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # TODO: One-hot encode the targets\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Print a sample of one training input and target\n",
    "print(f'Sample of preprocessed X_train: {X_train[0]}')\n",
    "print(f'Sample of preprocessed y_train: {y_train[0]}\\n')\n",
    "\n",
    "# Print the shape of scaled and encoded data\n",
    "print(f'Shape of preprocessed X_train: {X_train.shape}')\n",
    "print(f'Shape of preprocessed X_test: {X_test.shape}')\n",
    "print(f'Shape of preprocessed y_train: {y_train.shape}')\n",
    "print(f'Shape of preprocessed y_test: {y_test.shape}')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end Preprocessing the Iris Dataset\n",
    "\n",
    "You've done a great job understanding the concepts of preprocessing the Iris dataset.\n",
    "\n",
    "In this final task, you will implement the complete process of preprocessing the Iris dataset for TensorFlow from scratch. Follow the steps to load the dataset, split it into training and testing sets, scale the features, and one-hot encode the target labels.\n",
    "\n",
    "Create the function load_preprocessed_data() to achieve the desired outcomes.\n",
    "\n",
    "```py\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "    \n",
    "    # Assign the dataset attributes: data to X and target to y\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # One-hot encode the targets\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Print a sample of one training input and target\n",
    "print(f'Sample of preprocessed X_train: {X_train[0]}')\n",
    "print(f'Sample of preprocessed y_train: {y_train[0]}\\n')\n",
    "\n",
    "# Print the shape of scaled and encoded data\n",
    "print(f'Shape of preprocessed X_train: {X_train.shape}')\n",
    "print(f'Shape of preprocessed X_test: {X_test.shape}')\n",
    "print(f'Shape of preprocessed y_train: {y_train.shape}')\n",
    "print(f'Shape of preprocessed y_test: {y_test.shape}')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
