{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Overview\n",
    "Hello, there! You have done a fantastic job with preprocessing the Iris dataset. Now, it's time to apply what you've learned by building a multi-class classification model using TensorFlow. We will guide you through each step of constructing and training this model specifically for our problem. We'll also unpack the history object returned by the model's training process. You're in good hands. Let's get started.\n",
    "\n",
    "## Loading the Preprocessed Dataset\n",
    "Before we start building our multi-class classification model, we need to load our preprocessed dataset. To maintain modular code, we use the load_preprocessed_data function from our previous lesson, stored in data_preprocessing.py. This function handles loading, splitting, scaling, and one-hot encoding the Iris dataset, providing the data in a format that is ready to train our model.\n",
    "\n",
    "Load the preprocessed dataset:\n",
    "\n",
    "```Python\n",
    "from data_preprocessing import load_preprocessed_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "```\n",
    "Here's a brief recap of the data_preprocessing.py:\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # One-hot encode the targets\n",
    "    encoder = OneHotEncoder(sparse_output=False).fit(y_train.reshape(-1, 1))\n",
    "    y_train_encoded = encoder.transform(y_train.reshape(-1, 1))\n",
    "    y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded\n",
    "```\n",
    "With the dataset loaded, we're ready to build our multi-class classification model.\n",
    "\n",
    "## Building the Model\n",
    "The next step is building our multi-class classification model. TensorFlow makes it easy for us to construct models using the Sequential API which allows us to create models layer-by-layer.\n",
    "\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "```\n",
    "Our input shape aligns with the four features (sepal length, sepal width, petal length, petal width) in our Iris dataset. The model includes two dense layers, each having 10 neurons and ReLU (Rectified Linear Unit) as the activation function, which will help us introduce non-linearity into our model. Finally, we have an output layer with three neurons representing our three Iris species.\n",
    "\n",
    "Our output layer uses the softmax activation function, which helps in multi-class classification problems by converting the raw output scores (logits) into probabilities. Softmax takes the output of each neuron and turns it into a probability between 0 and 1, with all probabilities adding up to 100%. Essentially, it tells us the likelihood that the input data point belongs to each of the three classes (Iris species) we have in our dataset. The class with the highest probability is selected as the model's prediction.\n",
    "\n",
    "## Compiling the Model\n",
    "Before we can train our model, we need to compile it. Compiling the model signifies that we're set with the architecture and ready to smooth the weights to better predict our data.\n",
    "\n",
    "Let's compile our model:\n",
    "\n",
    "```Python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "We use the adam optimizer — a combination of RMSprop and Stochastic Gradient Descent with momentum. It adapts the parameter learning rates based on the average of recent weight updates. For the loss function, we're using categorical_crossentropy, which suits multi-class classification problems. And finally, we're using accuracy as our evaluation metric.\n",
    "\n",
    "## Training the Model\n",
    "Now comes the moment of truth — training our model on our preprocessed Iris dataset. Training involves the iterative process of feeding our data through the model (forward propagation), calculating an error (loss), and back-propagating through the model to adjust the weights.\n",
    "\n",
    "We will use our test set as a validation set to monitor the performance of the model on unseen data during training. The validation set is not used to train the model; it is only used to evaluate the model's performance each epoch to ensure it generalizes well to new data.\n",
    "\n",
    "Let's train our model:\n",
    "\n",
    "```Python\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=5, validation_data=(X_test, y_test))\n",
    "```\n",
    "\n",
    "We use the fit method for training. We're training for 150 epochs, meaning the entire dataset will pass through the model 150 times. We've set a batch size of 5, which means that after the model has seen 5 samples, it will update the weights. We also pass in our test data for validation.\n",
    "\n",
    "## Understanding History\n",
    "You might have noticed that model's fit method returned an object called history. But what exactly is it? history is an object that TensorFlow provides out of the box and contains a record of training loss values and metrics values at successive epochs, along with validation loss values and validation metrics values (if applicable). Analyzing its values can help us understand the training process and possibly diagnose issues (such as overfitting). Let's see how to extract the data from this object:\n",
    "\n",
    "```Python\n",
    "print(history.history)\n",
    "```\n",
    "\n",
    "The output of the above code will have following format:\n",
    "\n",
    "```sh\n",
    "{'accuracy': [0.3523809611797333, ...], 'loss': [1.6941570043563843, ...], 'val_accuracy': [0.2888889014720917, ...], 'val_loss': [1.6415398120880127, ...]}\n",
    "```\n",
    "\n",
    "This output summarises the model's performance over 150 epochs, showing accuracy and loss for both training and validation data sets. This kind of data is crucial for understanding how well our model is learning and generalizing to new data.\n",
    "\n",
    "## Lesson Summary and Practice\n",
    "Great job! You've learned how to build, compile, and train a TensorFlow multi-class classification model and how to interpret training results through the history object. Continue practicing these steps with the following coding exercises to solidify what you learned. Stay tuned for more lessons that will expand your TensorFlow and machine learning capabilities. Keep up the great work!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Model Training Basics\n",
    "\n",
    "Well done on preprocessing the Iris dataset and understanding its importance! Now, let's put it all together by building and training a multi-class classification model using TensorFlow.\n",
    "\n",
    "Simply run the code to see how we load the preprocessed data, define the model, compile it, and then train it. This exercise will solidify your understanding of the model’s construction and training process in TensorFlow.\n",
    "\n",
    "```py\n",
    "import tensorflow as tf\n",
    "from data_preprocessing import load_preprocessed_data\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=5, validation_data=(X_test, y_test))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Training Parameters in TensorFlow\n",
    "You've done a great job so far! Let's challenge ourselves by making a change. Your task is to modify the code to use the sgd optimizer instead of adam and to change the number of epochs to 50.\n",
    "\n",
    "Then, run the code to load the preprocessed data, define the model, compile it, and train it. This will help you understand how to customize the training with different optimizers and durations.\n",
    "\n",
    "```py\n",
    "import tensorflow as tf\n",
    "from data_preprocessing import load_preprocessed_data\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with SGD optimizer\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model with 50 epochs\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=5, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing TensorFlow Model Training\n",
    "\n",
    "You've made great progress in learning how to build and train TensorFlow models. Now, let's put your skills to the test with a debugging task. Below is code designed to build and train a multi-class classification model, but something isn't right.\n",
    "\n",
    "A common mistake has been introduced, and your task is to find and correct it. Debugging is a critical skill that will help you understand TensorFlow better.\n",
    "\n",
    "```py\n",
    "import tensorflow as tf\n",
    "from data_preprocessing import load_preprocessed_data\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with incorrect loss function\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model without validation data\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a TensorFlow Model\n",
    "\n",
    "Great job on building and training your multi-class classification model! Now, let's reinforce your understanding by completing the steps to construct, compile, and train the model using TensorFlow.\n",
    "\n",
    "Follow the TODOs in the starter code to complete the implementation.\n",
    "\n",
    "```py\n",
    "import tensorflow as tf\n",
    "from data_preprocessing import load_preprocessed_data\n",
    "\n",
    "# TODO: Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# TODO: Define the model with layers here\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),               # Input layer with 4 features\n",
    "    tf.keras.layers.Dense(10, activation='relu'),    # First hidden layer with 10 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(10, activation='relu'),    # Second hidden layer with 10 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(3, activation='softmax')   # Output layer with 3 neurons for multi-class classification\n",
    "])\n",
    "\n",
    "# TODO: Compile the model\n",
    "model.compile(optimizer='adam',                      # Use Adam optimizer\n",
    "              loss='categorical_crossentropy',       # Categorical crossentropy loss function\n",
    "              metrics=['accuracy'])                  # Track accuracy during training\n",
    "\n",
    "# TODO: Train the model\n",
    "history = model.fit(X_train, y_train,                # Fit the model to training data\n",
    "                    epochs=150,                      # Set number of epochs to 150\n",
    "                    batch_size=5,                    # Set batch size to 5\n",
    "                    validation_data=(X_test, y_test)) # Provide validation data\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a TensorFlow Model\n",
    "So far, you've learned how to build and train a TensorFlow model by following structured steps. Now it’s time to put all of that into practice.\n",
    "\n",
    "In this task, you will define a multi-class classification model, compile it, and train it from scratch.\n",
    "\n",
    "Remember to store the training history in a variable named history. Good luck!\n",
    "\n",
    "```py\n",
    "\n",
    "import tensorflow as tf\n",
    "from data_preprocessing import load_preprocessed_data\n",
    "\n",
    "# TODO: Load preprocessed data\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "\n",
    "# TODO: Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),               # Input layer with 4 features\n",
    "    tf.keras.layers.Dense(10, activation='relu'),    # First hidden layer with 10 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(10, activation='relu'),    # Second hidden layer with 10 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(3, activation='softmax')   # Output layer with 3 neurons and softmax activation\n",
    "])\n",
    "\n",
    "# TODO: Compile the model\n",
    "model.compile(optimizer='adam',                      # Use Adam optimizer\n",
    "              loss='categorical_crossentropy',       # Categorical crossentropy loss function\n",
    "              metrics=['accuracy'])                  # Track accuracy during training\n",
    "\n",
    "# TODO: Train the model\n",
    "history = model.fit(X_train, y_train,                # Fit the model to training data\n",
    "                    epochs=150,                      # Set number of epochs to 150\n",
    "                    batch_size=5,                    # Set batch size to 5\n",
    "                    validation_data=(X_test, y_test)) # Provide validation data\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
