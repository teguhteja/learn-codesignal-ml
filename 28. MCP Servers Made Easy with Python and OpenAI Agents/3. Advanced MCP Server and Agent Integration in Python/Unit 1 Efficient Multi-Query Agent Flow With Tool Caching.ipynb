{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 1 Efficient Multi-Query Agent Flow With Tool Caching\n",
                                    "\n",
                                    "# Advanced MCP Server and Agent Integration in Python: Lesson 1 - Tool Caching\n",
                                    "\n",
                                    "Welcome to the first lesson of this course on **advanced MCP server and agent integration in Python**. In previous courses, you learned how to build an MCP server and connect it to an agent, giving your agent the ability to use external tools. Now, we will take your skills further by focusing on how to make your agent more efficient and responsive, especially when handling a sequence of user queries. In this lesson, you will learn how to use tool caching to reduce latency and improve performance when running an agent across multiple queries. These techniques are essential for building agents that feel fast and natural in real-world applications.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Recap Of MCP Tools And Agent Integration\n",
                                    "\n",
                                    "Before we dive into advanced topics, let's quickly review how **MCP servers and agents** work together. An MCP server provides a set of tools — these are actions or functions the agent can use to help answer user queries. In earlier lessons, you learned how to launch an MCP server and connect it to an agent using the OpenAI Agents SDK. The agent gathers tools from the MCP server (or servers) and can use them alongside any built-in tools you define. This integration allows your agent to perform a wide range of tasks, from fetching data to managing files, depending on the tools available.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## The Importance And Benefits Of Tool Caching\n",
                                    "\n",
                                    "Every time an agent starts a new session or receives a query, it needs to know what tools are available. By default, the agent asks the MCP server for the list of tools each time. If the MCP server is running locally, this might be fast, but if it's remote or the tool list is large, this can slow things down. **Caching the tool list** means the agent remembers the tools after the first request, so it doesn't have to ask the server again for every query. This reduces latency, speeds up responses, and saves resources.\n",
                                    "\n",
                                    "To see the impact of not using tool caching, here's an example of what you might see in the logs of an MCP server running with Server-Sent Events (SSE) when an agent processes multiple queries without caching enabled. Notice how the server receives a `ListToolsRequest` for each new query, even though the list of tools hasn't changed:\n",
                                    "\n",
                                    "```\n",
                                    "INFO:     127.0.0.1:37682 - \"POST /messages/?session_id=... HTTP/1.1\" 202 Accepted\n",
                                    "[05/05/25 15:30:03] INFO     Processing request of type ListToolsRequest\n",
                                    "\n",
                                    "INFO:     127.0.0.1:37682 - \"POST /messages/?session_id=... HTTP/1.1\" 202 Accepted\n",
                                    "[05/05/25 15:30:07] INFO     Processing request of type CallToolRequest\n",
                                    "\n",
                                    "INFO:     127.0.0.1:37682 - \"POST /messages/?session_id=... HTTP/1.1\" 202 Accepted\n",
                                    "[05/05/25 15:30:19] INFO     Processing request of type ListToolsRequest\n",
                                    "\n",
                                    "INFO:     127.0.0.1:37682 - \"POST /messages/?session_id=... HTTP/1.1\" 202 Accepted\n",
                                    "[05/05/25 15:30:22] INFO     Processing request of type CallToolRequest\n",
                                    "\n",
                                    "INFO:     127.0.0.1:37682 - \"POST /messages/?session_id=... HTTP/1.1\" 202 Accepted\n",
                                    "[05/05/25 15:30:28] INFO     Processing request of type ListToolsRequest\n",
                                    "\n",
                                    "INFO:     127.0.0.1:37682 - \"POST /messages/?session_id=... HTTP/1.1\" 202 Accepted\n",
                                    "[05/05/25 15:30:32] INFO     Processing request of type CallToolRequest\n",
                                    "```\n",
                                    "\n",
                                    "Each time the agent receives a new query, it sends a `ListToolsRequest` to the MCP server to fetch the available tools, even if the tool list hasn't changed. This repeated fetching adds unnecessary latency and load to the server, especially in multi-query conversations. By enabling tool caching, you can avoid these redundant requests and make your agent much more efficient.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## When Not To Cache\n",
                                    "\n",
                                    "Tool caching is most effective when your tool list is stable and does not change often. However, if your tool list is dynamic and changes frequently—such as when tools are added, removed, or updated at runtime—caching can cause the agent to miss new or updated tools. In these cases, the agent may continue to use an outdated tool list, leading to incorrect or incomplete responses.\n",
                                    "\n",
                                    "If you expect frequent changes to your tool list, consider disabling caching or make sure to manually invalidate the cache whenever the tool set changes. This ensures your agent always has access to the latest tools and can respond accurately to user queries.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Implementing Tool Caching With The OpenAI Agents SDK\n",
                                    "\n",
                                    "Let's look at how to enable tool caching in your agent setup. In the **OpenAI Agents SDK**, you can set the `cache_tools_list` parameter to `True` when you create your MCP server connection. This tells the agent to fetch the tool list once and reuse it for future queries. Here's how it looks in practice:\n",
                                    "\n",
                                    "```python\n",
                                    "# Connect to the MCP server via stdio\n",
                                    "async with MCPServerStdio(\n",
                                    "    params=server_params,\n",
                                    "    cache_tools_list=True  # Cache the tools list to avoid re-fetching it\n",
                                    ") as mcp_server:\n",
                                    "```\n",
                                    "\n",
                                    "When you use this `mcp_server` in your agent, the agent will only fetch the tool list from the server the first time.\n",
                                    "\n",
                                    "### Clearing the Tools List Cache\n",
                                    "\n",
                                    "Tool caching works best when your tool list is stable, but there are situations where you need to update the cache to reflect changes. Only the metadata about the tools (such as names, input/output schema, and descriptions) is cached—not the actual tool logic or state. The logic itself always runs live on the server. If you update a tool's implementation on the server but keep its metadata the same, you do not need to invalidate the cache. However, if you add, remove, or modify tools (for example, by changing their schema or description), you should clear the cache so the agent can fetch the latest tool list.\n",
                                    "\n",
                                    "You can manually clear the cached tool list by calling:\n",
                                    "\n",
                                    "```python\n",
                                    "# Manually clear the cached tool list if tools have changed\n",
                                    "mcp_server.invalidate_tools_cache()\n",
                                    "```\n",
                                    "\n",
                                    "A common use case for this is when tools are dynamically registered based on user roles or runtime data. For example, if administrative tools are only available when a user logs in as an admin, you should call `invalidate_tools_cache()` immediately after a login event or role change. This ensures the agent fetches the correct tool list for the new user context and always has access to the appropriate tools.\n",
                                    "\n",
                                    "If your tool list changes frequently, consider when and how to invalidate the cache so your agent always operates with the most up-to-date information.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Running An Agent Across Multiple Queries\n",
                                    "\n",
                                    "A key part of building a helpful agent is maintaining the conversation context across multiple user queries. This means the agent remembers what was said before and can respond in a way that makes sense for the ongoing conversation. In the example below, you will see how to run an agent through a series of queries, updating the conversation history each time.\n",
                                    "\n",
                                    "Here is a complete example that brings together everything we've discussed:\n",
                                    "\n",
                                    "```python\n",
                                    "import asyncio\n",
                                    "from agents import Agent, Runner\n",
                                    "from agents.mcp import MCPServerSse\n",
                                    "\n",
                                    "async def main():\n",
                                    "    server_params = {\"url\": \"http://localhost:3000/sse\"}\n",
                                    "    \n",
                                    "    queries = [\n",
                                    "        \"Show me the whole list\",\n",
                                    "        \"Add 2 bananas\",\n",
                                    "        \"Mark milk as purchased\"\n",
                                    "    ]\n",
                                    "\n",
                                    "    async with MCPServerSse(\n",
                                    "        params=server_params,\n",
                                    "        cache_tools_list=True  # Enable tool list caching\n",
                                    "    ) as mcp_server:\n",
                                    "        \n",
                                    "        agent = Agent(\n",
                                    "            name=\"Shopping Assistant\",\n",
                                    "            instructions=\"You are an assistant that uses shopping list tools to help manage a shopping list.\",\n",
                                    "            mcp_servers=[mcp_server],\n",
                                    "            model=\"gpt-4.1\"\n",
                                    "        )\n",
                                    "        \n",
                                    "        conversation_history = []\n",
                                    "        for query in queries:\n",
                                    "            print(f\"Query: {query}\")\n",
                                    "            conversation_history.append({\"role\": \"user\", \"content\": query})\n",
                                    "            \n",
                                    "            result = await Runner.run(\n",
                                    "                starting_agent=agent,\n",
                                    "                input=conversation_history\n",
                                    "            )\n",
                                    "            \n",
                                    "            print(f\"Assistant: {result.final_output}\\n\")\n",
                                    "            conversation_history = result.to_input_list()\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    asyncio.run(main())\n",
                                    "```\n",
                                    "\n",
                                    "In this code, the agent is connected to an MCP server with tool caching enabled. Because the tool list is cached, the agent only fetches the list of available tools from the server once, instead of making a separate request for each query. This significantly reduces latency and speeds up the agent's responses across the entire sequence of user queries. Tool caching is especially beneficial in multi-query scenarios like this, where the agent can reuse the same tool list for each turn in the conversation, resulting in a smoother and more responsive user experience.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Summary, Best Practices, And Next Steps\n",
                                    "\n",
                                    "In this lesson, you learned how to make your agent more efficient by **caching the list of tools** from the MCP server. Caching reduces latency and improves performance, especially when the tool list is stable. As a best practice, enable tool caching when your tool list does not change often, and remember to invalidate the cache if you update the tools.\n",
                                    "\n",
                                    "You are now ready to practice these concepts in hands-on exercises. Keep up the great work — these skills will help you build faster and smarter agents\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Watching Tool Requests in Action\n",
                                    "\n",
                                    "In this first task, you’ll see for yourself how an agent interacts with an MCP server when tool caching is not enabled. This will help you understand the performance impact of repeatedly fetching the tool list for every query.\n",
                                    "\n",
                                    "The terminal tab is already open and running an MCP server. You do not need to change any code. Simply click the Run button to execute the agent code, and watch the terminal tab to observe how the agent interacts with the MCP server as it processes each query.\n",
                                    "\n",
                                    "Pay close attention to how often the server receives a ListToolsRequest. This hands-on step will help you see the impact of not using tool caching before you move on to making your agent more efficient.\n",
                                    "\n",
                                    "```python\n",
                                    "import asyncio\n",
                                    "from agents import Agent, Runner\n",
                                    "from agents.mcp import MCPServerSse\n",
                                    "\n",
                                    "\n",
                                    "async def main():\n",
                                    "    # Configure the MCP server parameters\n",
                                    "    server_params = {\"url\": \"http://localhost:3000/sse\"}\n",
                                    "    \n",
                                    "    # List of queries to run\n",
                                    "    queries = [\n",
                                    "        \"Show me the whole list\",\n",
                                    "        \"Add 2 bananas\",\n",
                                    "        \"Mark milk as purchased\",\n",
                                    "        \"Show me only unpurchased items\",\n",
                                    "        \"Remove bread from the list\",\n",
                                    "        \"Show me the whole list\"\n",
                                    "    ]\n",
                                    "    \n",
                                    "    # Connect to the MCP server\n",
                                    "    async with MCPServerSse(params=server_params) as mcp_server:\n",
                                    "        \n",
                                    "        # Create an agent with conversation-aware instructions.\n",
                                    "        agent = Agent(\n",
                                    "            name=\"Shopping Assistant\",\n",
                                    "            instructions=(\n",
                                    "                \"You are an assistant that uses shopping list tools to help manage a shopping list.\"\n",
                                    "            ),\n",
                                    "            mcp_servers=[mcp_server],\n",
                                    "            model=\"gpt-4.1\"\n",
                                    "        )\n",
                                    "        \n",
                                    "        # Initialize conversation history as a list of message dictionaries.\n",
                                    "        conversation_history = []\n",
                                    "\n",
                                    "        # Iterate over the queries\n",
                                    "        for query in queries:\n",
                                    "            # Print the query\n",
                                    "            print(f\"Query: {query}\")\n",
                                    "\n",
                                    "            # Append the query to the conversation history\n",
                                    "            conversation_history.append({\"role\": \"user\", \"content\": query})\n",
                                    "            \n",
                                    "            # Run the agent with the conversation history\n",
                                    "            result = await Runner.run(\n",
                                    "                starting_agent=agent,\n",
                                    "                input=conversation_history\n",
                                    "            )\n",
                                    "            \n",
                                    "            # Print the agent's response\n",
                                    "            print(f\"Assistant: {result.final_output}\\n\")\n",
                                    "            \n",
                                    "            # Update the conversation history with the agent's output\n",
                                    "            conversation_history = result.to_input_list()\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    asyncio.run(main())\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Speeding Up Agent Conversations"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Refreshing the Agent Tool Cache"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
