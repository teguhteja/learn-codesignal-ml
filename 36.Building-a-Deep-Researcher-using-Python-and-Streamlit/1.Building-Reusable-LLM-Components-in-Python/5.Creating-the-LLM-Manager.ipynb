{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating the LLM Manager\n",
                "\n",
                "Here is the content converted into Markdown format.\n",
                "\n",
                "# Welcome to the Lesson on Creating the LLM Manager\n",
                "\n",
                "Welcome to the lesson on creating the **LLM Manager**, a crucial component of the **DeepResearcher** project. In previous lessons, you learned about the design of DeepResearcher, the prompts module, and how to make basic LLM calls. Now, we will focus on the LLM Manager, which facilitates interactions with language models like OpenAI's GPT. This manager is responsible for rendering prompts, sending them to the language model, and handling the responses. By the end of this lesson, you will understand how to set up and use the LLM Manager effectively.\n",
                "\n",
                "-----\n",
                "\n",
                "## Setting Up the OpenAI Client\n",
                "\n",
                "To interact with OpenAI's language models, we need to set up an **OpenAI client**. This client requires an **API key** and a **base URL**, which are typically stored in environment variables for security reasons. Let's start by initializing the client.\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "```\n",
                "\n",
                "In this code snippet:\n",
                "\n",
                "  * We import the `os` module to access environment variables.\n",
                "  * We import the `OpenAI` class from the `openai` package.\n",
                "  * We initialize the **`client`** by reading the API key and base URL from environment variables using `os.getenv()`. This approach keeps sensitive information secure and separate from your code.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding the `generate_response` Function\n",
                "\n",
                "The **`generate_response`** function is central to the LLM Manager. It renders system and user prompts, sends them to the language model, and returns the response. Let's break it down step-by-step.\n",
                "\n",
                "### 1\\. Rendering Prompts\n",
                "\n",
                "First, we need to render the system and user prompts using the **`render_prompt_from_file`** function, which was covered in a previous lesson.\n",
                "\n",
                "```python\n",
                "system_prompt = render_prompt_from_file(\"path/to/system_prompt.txt\", variables)\n",
                "user_prompt = render_prompt_from_file(\"path/to/user_prompt.txt\", variables)\n",
                "```\n",
                "\n",
                "**`system_prompt`** and **`user_prompt`** are generated by calling `render_prompt_from_file` with the respective prompt names and variables. This function replaces placeholders in the prompt templates with actual values.\n",
                "\n",
                "### 2\\. Sending the Prompts\n",
                "\n",
                "Next, we send the rendered prompts to the language model using the **`client`**.\n",
                "\n",
                "```python\n",
                "completion = client.chat.completions.create(\n",
                "    model=model,\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": user_prompt},\n",
                "    ],\n",
                "    temperature=temperature\n",
                ")\n",
                "```\n",
                "\n",
                "  * We use the **`client.chat.completions.create`** method to send the prompts.\n",
                "  * The **`model`** parameter specifies which language model to use, such as `\"gpt-4o-mini\"`.\n",
                "  * The **`messages`** parameter contains the system and user prompts.\n",
                "  * The **`temperature`** parameter controls the randomness of the response. A higher temperature results in more creative responses.\n",
                "\n",
                "### 3\\. Returning the Result\n",
                "\n",
                "Finally, we extract and return the response from the language model.\n",
                "\n",
                "```python\n",
                "return completion.choices[0].message.content.strip()\n",
                "```\n",
                "\n",
                "  * We access the first choice in the **`completion`** object and retrieve the message content.\n",
                "  * The **`strip()`** method removes any leading or trailing whitespace from the response.\n",
                "\n",
                "-----\n",
                "\n",
                "## Exploring the `generate_boolean` Function\n",
                "\n",
                "The **`generate_boolean`** function interprets LLM responses as boolean values. It builds on the `generate_response` function.\n",
                "\n",
                "### 1\\. Getting the Response\n",
                "\n",
                "First, we call `generate_response` to get the LLM's response.\n",
                "\n",
                "```python\n",
                "response = generate_response(\"path/to/other/system_prompt.txt\", \"path/to/other/user_prompt.txt\", variables, model, temperature)\n",
                "```\n",
                "\n",
                "This line calls **`generate_response`** with a system prompt, a user prompt, the variables to enrich the prompts, the model, and the temperature, to obtain a response from the LLM.\n",
                "\n",
                "### 2\\. Interpreting the Response\n",
                "\n",
                "Next, we interpret the response as a boolean value using truthy keywords.\n",
                "\n",
                "```python\n",
                "truthy_keywords = [\"yes\", \"true\", \"correct\", \"affirmative\", \"certainly\", \"absolutely\"]\n",
                "return any(keyword in response.lower() for keyword in truthy_keywords)\n",
                "```\n",
                "\n",
                "  * We define a list of **`truthy_keywords`** that represent affirmative responses.\n",
                "  * We use a generator expression to check if any of these keywords are present in the response (converted to lowercase).\n",
                "  * The function returns **`True`** if any keyword is found; otherwise, **`False`**.\n",
                "\n",
                "-----\n",
                "\n",
                "## Error Handling and Logging\n",
                "\n",
                "Error handling is crucial when interacting with APIs. The LLM Manager includes error handling and logging to manage unexpected issues.\n",
                "\n",
                "### 1\\. Logging Configuration\n",
                "\n",
                "First, logging must be configured at the beginning of the script.\n",
                "\n",
                "```python\n",
                "import logging\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "```\n",
                "\n",
                "We import the **`logging`** module and configure it to display messages at the **INFO** level or higher. The format specifies how log messages are displayed, including the log level and message.\n",
                "\n",
                "### 2\\. Handling API Errors\n",
                "\n",
                "To handle API Errors, we will use a **`try-except`** block.\n",
                "\n",
                "```python\n",
                "try:\n",
                "    # Code to generate response\n",
                "except APIError as e:\n",
                "    logging.error(f\"LLM API Error: {e}\")\n",
                "    return None\n",
                "except Exception as e:\n",
                "    logging.error(f\"Unexpected error: {e}\")\n",
                "    return None\n",
                "```\n",
                "\n",
                "  * We catch **`APIError`** (from the OpenAI SDK) to handle specific errors from the API.\n",
                "  * We log the error message using **`logging.error`**.\n",
                "  * We also catch any other exceptions to handle unexpected errors gracefully, returning `None` to indicate failure.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, you learned how to create the **LLM Manager**, a key component of the DeepResearcher tool. We covered setting up the **OpenAI client**, understanding the **`generate_response`** and **`generate_boolean`** functions, and implementing **error handling** and **logging**. These skills are essential for managing interactions with language models effectively.\n",
                "\n",
                "As you move on to the practice exercises, you'll have the opportunity to apply what you've learned. Experiment with different prompt inputs and model parameters to see how they affect the responses. Congratulations on reaching this point in the course, and keep up the great work as you continue to build your DeepResearcher tool\\! 🎉"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Adding Prompt Logging for Debugging\n",
                "\n",
                "Now that you understand how the LLM Manager works with its error-handling capabilities, let's enhance its debugging features. In this exercise, you'll add a logging statement to the generate_response function that displays both system and user prompts after they've been rendered.\n",
                "\n",
                "This addition is valuable because it lets you see exactly what's being sent to the language model before the API call is made. When working with template-based prompts, it's important to verify that variable substitution is working correctly.\n",
                "\n",
                "Simply add a logging.info statement between the prompt rendering and the API call. This small improvement will make a big difference when troubleshooting prompt-related issues and will save you time when developing more complex LLM applications.\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "    user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "    \n",
                "    # TODO: Add a logging.info statement that logs both system_prompt and user_prompt\n",
                "    # to help with debugging and verify prompt substitution is working correctly\n",
                "\n",
                "    completion = client.chat.completions.create(\n",
                "        model=\"gpt-4o-mini\",\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": system_prompt},\n",
                "            {\"role\": \"user\", \"content\": user_prompt},\n",
                "        ],\n",
                "        temperature=0.7\n",
                "    )\n",
                "\n",
                "    return completion.choices[0].message.content.strip()\n",
                "\n",
                "```\n",
                "\n",
                "I'll add the necessary `logging.info` statement to the `generate_response` function to display the rendered prompts before the API call.\n",
                "\n",
                "This is a crucial step for debugging complex, dynamic prompts.\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "    user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "    \n",
                "    # TODO: Add a logging.info statement that logs both system_prompt and user_prompt\n",
                "    # to help with debugging and verify prompt substitution is working correctly\n",
                "    logging.info(f\"--- Sending Prompts to LLM ---\\nSystem: {system_prompt}\\nUser: {user_prompt}\\n----------------------------\")\n",
                "\n",
                "    completion = client.chat.completions.create(\n",
                "        model=\"gpt-4o-mini\",\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": system_prompt},\n",
                "            {\"role\": \"user\", \"content\": user_prompt},\n",
                "        ],\n",
                "        temperature=0.7\n",
                "    )\n",
                "\n",
                "    return completion.choices[0].message.content.strip()\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Enhancing API Error Handling\n",
                "\n",
                "You've added logging for debugging, which is excellent! Now let's strengthen our error handling to make our LLM Manager more robust. In this exercise, you'll add error handling in the generate_response function to catch specific types of OpenAI API errors.\n",
                "\n",
                "When working with external APIs, different errors require different responses. For example, rate limit errors might need a retry strategy, while authentication errors indicate configuration problems.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Import error types from the OpenAI package\n",
                "Add specific exception handlers for rate limit and authentication errors\n",
                "Ensure each error type has appropriate logging messages\n",
                "This enhanced error handling will make your application more resilient and easier to troubleshoot when interacting with language models in production environments.\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI\n",
                "# TODO: Import additional error types from the openai package that we should handle\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "        user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "\n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=0.7\n",
                "        )\n",
                "\n",
                "        return completion.choices[0].message.content.strip()\n",
                "\n",
                "    # TODO: Add specific exception handling for rate limit errors with appropriate logging\n",
                "    \n",
                "    # TODO: Add specific exception handling for authentication errors with appropriate logging\n",
                "    \n",
                "    # TODO: Add specific exception handling for API errors\n",
                "    \n",
                "    # TODO: Add specific exception handling for general exceptions\n",
                "```\n",
                "\n",
                "To enhance error handling for the OpenAI API, we need to import specific error classes, primarily **`APIError`** (the base class for all API-related exceptions), **`RateLimitError`**, and **`AuthenticationError`**. We'll then implement a structured `try...except` block in `generate_response` to catch and log these specific errors gracefully.\n",
                "\n",
                "Here is the completed code:\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI\n",
                "# TODO: Import additional error types from the openai package that we should handle\n",
                "from openai import APIError, RateLimitError, AuthenticationError\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "# Define a logger instance for clear messaging\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    Handles specific OpenAI API errors.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "        user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "\n",
                "        # Logging the rendered prompts for debugging (as per the previous exercise)\n",
                "        logger.info(f\"--- Sending Prompts to LLM ---\\nSystem: {system_prompt}\\nUser: {user_prompt}\\n----------------------------\")\n",
                "\n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=0.7\n",
                "        )\n",
                "\n",
                "        return completion.choices[0].message.content.strip()\n",
                "\n",
                "    # TODO: Add specific exception handling for rate limit errors with appropriate logging\n",
                "    except RateLimitError as e:\n",
                "        logger.error(f\"OpenAI Rate Limit Error: Request failed due to too many requests. Consider a retry or throttling. Error: {e}\")\n",
                "        return None\n",
                "    \n",
                "    # TODO: Add specific exception handling for authentication errors with appropriate logging\n",
                "    except AuthenticationError as e:\n",
                "        logger.critical(f\"OpenAI Authentication Error: Invalid API key or organization. Check environment variables. Error: {e}\")\n",
                "        # Critical errors should typically halt or be escalated\n",
                "        return None\n",
                "    \n",
                "    # TODO: Add specific exception handling for API errors (catches all other API-related errors like bad requests, server errors, etc.)\n",
                "    except APIError as e:\n",
                "        logger.error(f\"OpenAI API Error: A general API-related error occurred. Error: {e}\")\n",
                "        return None\n",
                "    \n",
                "    # TODO: Add specific exception handling for general exceptions (e.g., network issues, file rendering issues)\n",
                "    except Exception as e:\n",
                "        logger.error(f\"Unexpected Error during prompt generation or API call: {e}\")\n",
                "        return None\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optimizing Boolean Response Detection"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Validating Environment Variables for Security"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating a Flexible LLM Wrapper Function"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
