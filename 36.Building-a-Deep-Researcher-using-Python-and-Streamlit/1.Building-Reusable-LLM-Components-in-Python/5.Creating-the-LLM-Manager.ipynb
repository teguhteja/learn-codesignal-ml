{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating the LLM Manager\n",
                "\n",
                "Here is the content converted into Markdown format.\n",
                "\n",
                "# Welcome to the Lesson on Creating the LLM Manager\n",
                "\n",
                "Welcome to the lesson on creating the **LLM Manager**, a crucial component of the **DeepResearcher** project. In previous lessons, you learned about the design of DeepResearcher, the prompts module, and how to make basic LLM calls. Now, we will focus on the LLM Manager, which facilitates interactions with language models like OpenAI's GPT. This manager is responsible for rendering prompts, sending them to the language model, and handling the responses. By the end of this lesson, you will understand how to set up and use the LLM Manager effectively.\n",
                "\n",
                "-----\n",
                "\n",
                "## Setting Up the OpenAI Client\n",
                "\n",
                "To interact with OpenAI's language models, we need to set up an **OpenAI client**. This client requires an **API key** and a **base URL**, which are typically stored in environment variables for security reasons. Let's start by initializing the client.\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "```\n",
                "\n",
                "In this code snippet:\n",
                "\n",
                "  * We import the `os` module to access environment variables.\n",
                "  * We import the `OpenAI` class from the `openai` package.\n",
                "  * We initialize the **`client`** by reading the API key and base URL from environment variables using `os.getenv()`. This approach keeps sensitive information secure and separate from your code.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding the `generate_response` Function\n",
                "\n",
                "The **`generate_response`** function is central to the LLM Manager. It renders system and user prompts, sends them to the language model, and returns the response. Let's break it down step-by-step.\n",
                "\n",
                "### 1\\. Rendering Prompts\n",
                "\n",
                "First, we need to render the system and user prompts using the **`render_prompt_from_file`** function, which was covered in a previous lesson.\n",
                "\n",
                "```python\n",
                "system_prompt = render_prompt_from_file(\"path/to/system_prompt.txt\", variables)\n",
                "user_prompt = render_prompt_from_file(\"path/to/user_prompt.txt\", variables)\n",
                "```\n",
                "\n",
                "**`system_prompt`** and **`user_prompt`** are generated by calling `render_prompt_from_file` with the respective prompt names and variables. This function replaces placeholders in the prompt templates with actual values.\n",
                "\n",
                "### 2\\. Sending the Prompts\n",
                "\n",
                "Next, we send the rendered prompts to the language model using the **`client`**.\n",
                "\n",
                "```python\n",
                "completion = client.chat.completions.create(\n",
                "    model=model,\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": user_prompt},\n",
                "    ],\n",
                "    temperature=temperature\n",
                ")\n",
                "```\n",
                "\n",
                "  * We use the **`client.chat.completions.create`** method to send the prompts.\n",
                "  * The **`model`** parameter specifies which language model to use, such as `\"gpt-4o-mini\"`.\n",
                "  * The **`messages`** parameter contains the system and user prompts.\n",
                "  * The **`temperature`** parameter controls the randomness of the response. A higher temperature results in more creative responses.\n",
                "\n",
                "### 3\\. Returning the Result\n",
                "\n",
                "Finally, we extract and return the response from the language model.\n",
                "\n",
                "```python\n",
                "return completion.choices[0].message.content.strip()\n",
                "```\n",
                "\n",
                "  * We access the first choice in the **`completion`** object and retrieve the message content.\n",
                "  * The **`strip()`** method removes any leading or trailing whitespace from the response.\n",
                "\n",
                "-----\n",
                "\n",
                "## Exploring the `generate_boolean` Function\n",
                "\n",
                "The **`generate_boolean`** function interprets LLM responses as boolean values. It builds on the `generate_response` function.\n",
                "\n",
                "### 1\\. Getting the Response\n",
                "\n",
                "First, we call `generate_response` to get the LLM's response.\n",
                "\n",
                "```python\n",
                "response = generate_response(\"path/to/other/system_prompt.txt\", \"path/to/other/user_prompt.txt\", variables, model, temperature)\n",
                "```\n",
                "\n",
                "This line calls **`generate_response`** with a system prompt, a user prompt, the variables to enrich the prompts, the model, and the temperature, to obtain a response from the LLM.\n",
                "\n",
                "### 2\\. Interpreting the Response\n",
                "\n",
                "Next, we interpret the response as a boolean value using truthy keywords.\n",
                "\n",
                "```python\n",
                "truthy_keywords = [\"yes\", \"true\", \"correct\", \"affirmative\", \"certainly\", \"absolutely\"]\n",
                "return any(keyword in response.lower() for keyword in truthy_keywords)\n",
                "```\n",
                "\n",
                "  * We define a list of **`truthy_keywords`** that represent affirmative responses.\n",
                "  * We use a generator expression to check if any of these keywords are present in the response (converted to lowercase).\n",
                "  * The function returns **`True`** if any keyword is found; otherwise, **`False`**.\n",
                "\n",
                "-----\n",
                "\n",
                "## Error Handling and Logging\n",
                "\n",
                "Error handling is crucial when interacting with APIs. The LLM Manager includes error handling and logging to manage unexpected issues.\n",
                "\n",
                "### 1\\. Logging Configuration\n",
                "\n",
                "First, logging must be configured at the beginning of the script.\n",
                "\n",
                "```python\n",
                "import logging\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "```\n",
                "\n",
                "We import the **`logging`** module and configure it to display messages at the **INFO** level or higher. The format specifies how log messages are displayed, including the log level and message.\n",
                "\n",
                "### 2\\. Handling API Errors\n",
                "\n",
                "To handle API Errors, we will use a **`try-except`** block.\n",
                "\n",
                "```python\n",
                "try:\n",
                "    # Code to generate response\n",
                "except APIError as e:\n",
                "    logging.error(f\"LLM API Error: {e}\")\n",
                "    return None\n",
                "except Exception as e:\n",
                "    logging.error(f\"Unexpected error: {e}\")\n",
                "    return None\n",
                "```\n",
                "\n",
                "  * We catch **`APIError`** (from the OpenAI SDK) to handle specific errors from the API.\n",
                "  * We log the error message using **`logging.error`**.\n",
                "  * We also catch any other exceptions to handle unexpected errors gracefully, returning `None` to indicate failure.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, you learned how to create the **LLM Manager**, a key component of the DeepResearcher tool. We covered setting up the **OpenAI client**, understanding the **`generate_response`** and **`generate_boolean`** functions, and implementing **error handling** and **logging**. These skills are essential for managing interactions with language models effectively.\n",
                "\n",
                "As you move on to the practice exercises, you'll have the opportunity to apply what you've learned. Experiment with different prompt inputs and model parameters to see how they affect the responses. Congratulations on reaching this point in the course, and keep up the great work as you continue to build your DeepResearcher tool\\! 🎉"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Adding Prompt Logging for Debugging\n",
                "\n",
                "Now that you understand how the LLM Manager works with its error-handling capabilities, let's enhance its debugging features. In this exercise, you'll add a logging statement to the generate_response function that displays both system and user prompts after they've been rendered.\n",
                "\n",
                "This addition is valuable because it lets you see exactly what's being sent to the language model before the API call is made. When working with template-based prompts, it's important to verify that variable substitution is working correctly.\n",
                "\n",
                "Simply add a logging.info statement between the prompt rendering and the API call. This small improvement will make a big difference when troubleshooting prompt-related issues and will save you time when developing more complex LLM applications.\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "    user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "    \n",
                "    # TODO: Add a logging.info statement that logs both system_prompt and user_prompt\n",
                "    # to help with debugging and verify prompt substitution is working correctly\n",
                "\n",
                "    completion = client.chat.completions.create(\n",
                "        model=\"gpt-4o-mini\",\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": system_prompt},\n",
                "            {\"role\": \"user\", \"content\": user_prompt},\n",
                "        ],\n",
                "        temperature=0.7\n",
                "    )\n",
                "\n",
                "    return completion.choices[0].message.content.strip()\n",
                "\n",
                "```\n",
                "\n",
                "I'll add the necessary `logging.info` statement to the `generate_response` function to display the rendered prompts before the API call.\n",
                "\n",
                "This is a crucial step for debugging complex, dynamic prompts.\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "    user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "    \n",
                "    # TODO: Add a logging.info statement that logs both system_prompt and user_prompt\n",
                "    # to help with debugging and verify prompt substitution is working correctly\n",
                "    logging.info(f\"--- Sending Prompts to LLM ---\\nSystem: {system_prompt}\\nUser: {user_prompt}\\n----------------------------\")\n",
                "\n",
                "    completion = client.chat.completions.create(\n",
                "        model=\"gpt-4o-mini\",\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": system_prompt},\n",
                "            {\"role\": \"user\", \"content\": user_prompt},\n",
                "        ],\n",
                "        temperature=0.7\n",
                "    )\n",
                "\n",
                "    return completion.choices[0].message.content.strip()\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Enhancing API Error Handling\n",
                "\n",
                "You've added logging for debugging, which is excellent! Now let's strengthen our error handling to make our LLM Manager more robust. In this exercise, you'll add error handling in the generate_response function to catch specific types of OpenAI API errors.\n",
                "\n",
                "When working with external APIs, different errors require different responses. For example, rate limit errors might need a retry strategy, while authentication errors indicate configuration problems.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Import error types from the OpenAI package\n",
                "Add specific exception handlers for rate limit and authentication errors\n",
                "Ensure each error type has appropriate logging messages\n",
                "This enhanced error handling will make your application more resilient and easier to troubleshoot when interacting with language models in production environments.\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI\n",
                "# TODO: Import additional error types from the openai package that we should handle\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "        user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "\n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=0.7\n",
                "        )\n",
                "\n",
                "        return completion.choices[0].message.content.strip()\n",
                "\n",
                "    # TODO: Add specific exception handling for rate limit errors with appropriate logging\n",
                "    \n",
                "    # TODO: Add specific exception handling for authentication errors with appropriate logging\n",
                "    \n",
                "    # TODO: Add specific exception handling for API errors\n",
                "    \n",
                "    # TODO: Add specific exception handling for general exceptions\n",
                "```\n",
                "\n",
                "To enhance error handling for the OpenAI API, we need to import specific error classes, primarily **`APIError`** (the base class for all API-related exceptions), **`RateLimitError`**, and **`AuthenticationError`**. We'll then implement a structured `try...except` block in `generate_response` to catch and log these specific errors gracefully.\n",
                "\n",
                "Here is the completed code:\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI\n",
                "# TODO: Import additional error types from the openai package that we should handle\n",
                "from openai import APIError, RateLimitError, AuthenticationError\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "# Define a logger instance for clear messaging\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    Handles specific OpenAI API errors.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "        user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "\n",
                "        # Logging the rendered prompts for debugging (as per the previous exercise)\n",
                "        logger.info(f\"--- Sending Prompts to LLM ---\\nSystem: {system_prompt}\\nUser: {user_prompt}\\n----------------------------\")\n",
                "\n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=0.7\n",
                "        )\n",
                "\n",
                "        return completion.choices[0].message.content.strip()\n",
                "\n",
                "    # TODO: Add specific exception handling for rate limit errors with appropriate logging\n",
                "    except RateLimitError as e:\n",
                "        logger.error(f\"OpenAI Rate Limit Error: Request failed due to too many requests. Consider a retry or throttling. Error: {e}\")\n",
                "        return None\n",
                "    \n",
                "    # TODO: Add specific exception handling for authentication errors with appropriate logging\n",
                "    except AuthenticationError as e:\n",
                "        logger.critical(f\"OpenAI Authentication Error: Invalid API key or organization. Check environment variables. Error: {e}\")\n",
                "        # Critical errors should typically halt or be escalated\n",
                "        return None\n",
                "    \n",
                "    # TODO: Add specific exception handling for API errors (catches all other API-related errors like bad requests, server errors, etc.)\n",
                "    except APIError as e:\n",
                "        logger.error(f\"OpenAI API Error: A general API-related error occurred. Error: {e}\")\n",
                "        return None\n",
                "    \n",
                "    # TODO: Add specific exception handling for general exceptions (e.g., network issues, file rendering issues)\n",
                "    except Exception as e:\n",
                "        logger.error(f\"Unexpected Error during prompt generation or API call: {e}\")\n",
                "        return None\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optimizing Boolean Response Detection\n",
                "\n",
                "After improving our error handling, let's now focus on the generate_boolean function, which converts LLM responses into boolean values. This function is crucial for decision-making in our DeepResearcher tool.\n",
                "\n",
                "In this exercise, you'll create this function by detecting affirmative responses by:\n",
                "\n",
                "Creating a list of truthy keywords to capture variations of \"yes\"\n",
                "Making a case-insensitive check by converting the response to lowercase once\n",
                "Returning the correct boolean value\n",
                "You can find the prompts that we will be using inside the deepresearcher/prompts/data/ folder.\n",
                "\n",
                "This improvement will make our tool more reliable when interpreting LLM responses as boolean values, especially when the model uses different phrasing for affirmative answers. The skills you'll practice here — working with strings and optimizing code — are valuable for many other aspects of Python development.\n",
                "\n",
                "Suggestions\n",
                "Give me an example\n",
                "Hint, please\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI, APIError\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "        user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "\n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=0.7\n",
                "        )\n",
                "\n",
                "        return completion.choices[0].message.content.strip()\n",
                "\n",
                "    except APIError as e:\n",
                "        logging.error(f\"LLM API Error: {e}\")\n",
                "        return None\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Unexpected error: {e}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "def generate_boolean(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render prompts, call LLM, and interpret the result as a boolean.\n",
                "    \"\"\"\n",
                "    response = generate_response(system_prompt_name, user_prompt_name, variables)\n",
                "\n",
                "    if response is None:\n",
                "        return False  # Assume failure as negative\n",
                "\n",
                "    # TODO: Expand this list with more truthy keywords to catch more affirmative responses\n",
                "    truthy_keywords = []\n",
                "    \n",
                "    # TODO: Convert the response to lowercase before checking\n",
                "    \n",
                "    # TODO: Return the correct boolean value\n",
                "```\n",
                "\n",
                "This is a great task for improving the robustness of your `DeepResearcher` tool\\! Detecting boolean intent from natural language is crucial.\n",
                "\n",
                "Here is the completed `generate_boolean` function, incorporating the truthy keyword list and the case-insensitive check.\n",
                "\n",
                "## Completed `generate_boolean` Function\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI, APIError\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "        user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "\n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=0.7\n",
                "        )\n",
                "\n",
                "        return completion.choices[0].message.content.strip()\n",
                "\n",
                "    except APIError as e:\n",
                "        logging.error(f\"LLM API Error: {e}\")\n",
                "        return None\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Unexpected error: {e}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "def generate_boolean(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render prompts, call LLM, and interpret the result as a boolean.\n",
                "    \"\"\"\n",
                "    response = generate_response(system_prompt_name, user_prompt_name, variables)\n",
                "\n",
                "    if response is None:\n",
                "        return False  # Assume failure as negative\n",
                "\n",
                "    # TODO: Expand this list with more truthy keywords to catch more affirmative responses\n",
                "    # Expanded list of common affirmative keywords\n",
                "    truthy_keywords = [\"yes\", \"true\", \"affirmative\", \"correct\", \"yep\", \"sure\", \"ok\", \"of course\", \"definitely\", \"i agree\", \"that is correct\"]\n",
                "    \n",
                "    # TODO: Convert the response to lowercase before checking\n",
                "    # Perform the conversion once for efficiency\n",
                "    response_lower = response.lower()\n",
                "    \n",
                "    # TODO: Return the correct boolean value\n",
                "    # Check if any truthy keyword is present in the lowercase response\n",
                "    for keyword in truthy_keywords:\n",
                "        if keyword in response_lower:\n",
                "            return True\n",
                "            \n",
                "    # If no truthy keyword is found, return False\n",
                "    return False\n",
                "\n",
                "# You can now test this function with various LLM responses!\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Explanation of Changes\n",
                "\n",
                "1.  **Truthy Keywords List:**\n",
                "\n",
                "    ```python\n",
                "    truthy_keywords = [\"yes\", \"true\", \"affirmative\", \"correct\", \"yep\", \"sure\", \"ok\", \"of course\", \"definitely\", \"i agree\", \"that is correct\"]\n",
                "    ```\n",
                "\n",
                "    This list has been expanded to cover common ways an LLM might express agreement or an affirmative answer, beyond just \"yes\" or \"true.\"\n",
                "\n",
                "2.  **Case-Insensitive Check Optimization:**\n",
                "\n",
                "    ```python\n",
                "    response_lower = response.lower()\n",
                "    ```\n",
                "\n",
                "    The original response is converted to **lowercase** *once* and stored in a new variable. This is more efficient than calling `.lower()` repeatedly inside the loop.\n",
                "\n",
                "3.  **Boolean Interpretation Logic:**\n",
                "\n",
                "    ```python\n",
                "    for keyword in truthy_keywords:\n",
                "        if keyword in response_lower:\n",
                "            return True\n",
                "            \n",
                "    return False\n",
                "    ```\n",
                "\n",
                "    The function iterates through the `truthy_keywords`. If any keyword is found within the lowercase LLM response, the function immediately returns `True`. If the loop completes without finding any match, it returns `False`, indicating a negative or neutral response. This structure effectively implements the **case-insensitive** check you aimed for."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Validating Environment Variables for Security\n",
                "\n",
                "With our error handling and response detection improvements in place, let's now focus on making our LLM Manager more secure and robust. In this exercise, you'll add validation for the environment variables that are essential for the OpenAI client to function properly.\n",
                "\n",
                "Currently, our code assumes the environment variables will be available, which could lead to runtime errors if they're missing. This is a common issue in production environments where configuration might not be set up correctly.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Extract the API key and base URL into variables before using them.\n",
                "Check if these variables exist and log appropriate warnings if they don't.\n",
                "Only initialize the OpenAI client if both required variables are present.\n",
                "Add a check in the generate_response function to handle cases when the client isn't initialized.\n",
                "This validation will prevent cryptic errors when API credentials are missing and provide clear feedback about what needs to be fixed. These defensive programming techniques are essential when building reliable software that interacts with external services.\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI, APIError\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# TODO: Get API key and base URL from environment variables and store in variables\n",
                "# TODO: Add validation to check if the API key exists and log a warning if it doesn't\n",
                "# TODO: Add validation to check if the base URL exists and log a warning if it doesn't\n",
                "\n",
                "# TODO: Modify the client initialization to only happen if both environment variables are present\n",
                "# TODO: Add a fallback (set client to None) if environment variables are missing\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # TODO: Add a check to see if the client is initialized before proceeding\n",
                "        \n",
                "        system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "        user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "\n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=0.7\n",
                "        )\n",
                "\n",
                "        return completion.choices[0].message.content.strip()\n",
                "\n",
                "    except APIError as e:\n",
                "        logging.error(f\"LLM API Error: {e}\")\n",
                "        return None\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Unexpected error: {e}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "def generate_boolean(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render prompts, call LLM, and interpret the result as a boolean.\n",
                "    \"\"\"\n",
                "    response = generate_response(system_prompt_name, user_prompt_name, variables)\n",
                "\n",
                "    if response is None:\n",
                "        return False  # Assume failure as negative\n",
                "\n",
                "    truthy_keywords = [\"yes\", \"true\", \"correct\", \"affirmative\", \"certainly\", \"absolutely\"]\n",
                "    return any(keyword in response.lower() for keyword in truthy_keywords)\n",
                "\n",
                "```\n",
                "\n",
                "Securing your application by validating external dependencies like environment variables is essential for **robustness** and **security**\\! Here is the completed code with the environment variable extraction, validation, and conditional client initialization implemented.\n",
                "\n",
                "## Validated LLM Manager\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI, APIError\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# --- Environment Variable Extraction and Validation ---\n",
                "\n",
                "# Get API key and base URL from environment variables and store in variables\n",
                "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
                "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
                "\n",
                "# Flags for configuration status\n",
                "config_ok = True\n",
                "\n",
                "# Add validation to check if the API key exists and log a warning if it doesn't\n",
                "if not api_key:\n",
                "    logging.warning(\"Environment variable 'OPENAI_API_KEY' is missing. LLM calls will be disabled.\")\n",
                "    config_ok = False\n",
                "\n",
                "# Add validation to check if the base URL exists and log a warning if it doesn't\n",
                "if not base_url:\n",
                "    logging.warning(\"Environment variable 'OPENAI_BASE_URL' is missing. LLM calls will be disabled.\")\n",
                "    config_ok = False\n",
                "\n",
                "# Modify the client initialization to only happen if both environment variables are present\n",
                "# Add a fallback (set client to None) if environment variables are missing\n",
                "client = None\n",
                "if config_ok:\n",
                "    try:\n",
                "        client = OpenAI(api_key=api_key, base_url=base_url)\n",
                "        logging.info(\"OpenAI client initialized successfully.\")\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Failed to initialize OpenAI client: {e}\")\n",
                "        client = None\n",
                "\n",
                "\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Add a check to see if the client is initialized before proceeding\n",
                "        if client is None:\n",
                "            logging.error(\"LLM client is not initialized due to missing configuration. Aborting response generation.\")\n",
                "            return None\n",
                "        \n",
                "        system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "        user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "\n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=0.7\n",
                "        )\n",
                "\n",
                "        return completion.choices[0].message.content.strip()\n",
                "\n",
                "    except APIError as e:\n",
                "        logging.error(f\"LLM API Error: {e}\")\n",
                "        return None\n",
                "    except Exception as e:\n",
                "        # Catch unexpected errors, which might include issues with prompt rendering or file access\n",
                "        logging.error(f\"Unexpected error: {e}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "def generate_boolean(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render prompts, call LLM, and interpret the result as a boolean.\n",
                "    \"\"\"\n",
                "    response = generate_response(system_prompt_name, user_prompt_name, variables)\n",
                "\n",
                "    if response is None:\n",
                "        return False  # Assume failure as negative, especially if API call failed\n",
                "\n",
                "    truthy_keywords = [\"yes\", \"true\", \"correct\", \"affirmative\", \"certainly\", \"absolutely\"]\n",
                "    \n",
                "    # Efficiently check for truthy keywords in the lowercase response\n",
                "    response_lower = response.lower()\n",
                "    return any(keyword in response_lower for keyword in truthy_keywords)\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary of Defensive Improvements\n",
                "\n",
                "1.  **Variable Extraction:** The environment variables `OPENAI_API_KEY` and `OPENAI_BASE_URL` are now extracted into local variables (`api_key` and `base_url`).\n",
                "2.  **Configuration Check:** A `config_ok` flag tracks the status of the required variables. Clear **warnings** are logged if either variable is missing, providing explicit feedback to the developer or operator.\n",
                "3.  **Conditional Initialization:** The `client` variable is initialized to `None` by default. The `OpenAI` client is *only* instantiated if `config_ok` is `True`, preventing runtime errors from bad or missing credentials.\n",
                "4.  **Runtime Guard:** The `generate_response` function now includes an immediate check:\n",
                "    ```python\n",
                "    if client is None:\n",
                "        logging.error(\"LLM client is not initialized due to missing configuration. Aborting response generation.\")\n",
                "        return None\n",
                "    ```\n",
                "    This **defensive programming** pattern ensures that the function fails gracefully and logs a clear error instead of raising a cryptic `NameError` or `AttributeError` if the client was never successfully created."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating a Flexible LLM Wrapper Function\n",
                "\n",
                "Now that you've improved the security and reliability of our LLM Manager, let's make it more flexible for users. In this exercise, you'll modify the functions of the LLM Manager to be able to customize the OpenAI call.\n",
                "\n",
                "This will make it easier for users to customize the LLM calls without having to remember all the default parameters. It will accept the same base parameters, plus optional parameters for model and temperature that override the defaults when provided.\n",
                "\n",
                "Your tasks are to:\n",
                "\n",
                "Add logic to use custom values when provided or fall back to defaults.\n",
                "Log the chosen model and temperature settings for transparency.\n",
                "Modify as well the function generate_boolean to ensure you can also use this parameters.\n",
                "Modify the main.py file to call the new function with temperature 0.5 and model gpt-4o.\n",
                "This addition will make our LLM Manager more user-friendly while maintaining all the robust error handling we've built. The ability to create well-designed wrapper functions is a valuable skill that will help you build more intuitive APIs for your users.\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI, APIError\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Initialize OpenAI client (API key read from environment variable)\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "\n",
                "# TODO: Add a model and temperature parameter to the function with a default value\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "        user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "        \n",
                "        # TODO: Log the model and temperature values used\n",
                "        \n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=0.7\n",
                "        )\n",
                "\n",
                "        return completion.choices[0].message.content.strip()\n",
                "\n",
                "    except APIError as e:\n",
                "        logging.error(f\"LLM API Error: {e}\")\n",
                "        return None\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Unexpected error: {e}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "def generate_boolean(system_prompt_name, user_prompt_name, variables):\n",
                "    \"\"\"\n",
                "    Render prompts, call LLM, and interpret the result as a boolean.\n",
                "    \"\"\"\n",
                "    # TODO: Modify this function to use the model and temperature parameters\n",
                "    response = generate_response(system_prompt_name, user_prompt_name, variables)\n",
                "\n",
                "    if response is None:\n",
                "        return False  # Assume failure as negative\n",
                "\n",
                "    truthy_keywords = [\"yes\", \"true\", \"correct\", \"affirmative\", \"certainly\", \"absolutely\"]\n",
                "    return any(keyword in response.lower() for keyword in truthy_keywords)\n",
                "\n",
                "# app.py\n",
                "from deepresearcher.llm.llm_manager import generate_response\n",
                "\n",
                "# TODO: Use the temperature and model parameters\n",
                "response = generate_response(\"system_test\", \"user_test\", {})\n",
                "\n",
                "print(response)\n",
                "\n",
                "```\n",
                "\n",
                "Enhancing your LLM Manager with flexible parameters is a great way to improve its **user experience** and **versatility**\\! This update maintains the robust foundation you've built while making the core functions much more adaptable.\n",
                "\n",
                "Here are the modifications for `llm_manager.py` and the example `main.py` usage.\n",
                "\n",
                "## Modified `llm_manager.py` 🛠️\n",
                "\n",
                "I've integrated the environment variable checking logic from the previous exercise to ensure the code remains robust, and then implemented the flexible model and temperature parameters.\n",
                "\n",
                "```python\n",
                "import os\n",
                "import logging\n",
                "from openai import OpenAI, APIError\n",
                "# Assuming prompt_manager is available relative to this file's location\n",
                "from ..prompts.prompt_manager import render_prompt_from_file\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# --- Environment Variable Check (for robustness) ---\n",
                "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
                "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
                "config_ok = api_key and base_url\n",
                "\n",
                "client = None\n",
                "if config_ok:\n",
                "    try:\n",
                "        # Initialize OpenAI client only if keys are present\n",
                "        client = OpenAI(api_key=api_key, base_url=base_url)\n",
                "        logging.info(\"OpenAI client initialized successfully.\")\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Failed to initialize OpenAI client: {e}\")\n",
                "        client = None\n",
                "elif not api_key:\n",
                "    logging.warning(\"Environment variable 'OPENAI_API_KEY' is missing. LLM calls will be disabled.\")\n",
                "elif not base_url:\n",
                "    logging.warning(\"Environment variable 'OPENAI_BASE_URL' is missing. LLM calls will be disabled.\")\n",
                "# ----------------------------------------------------\n",
                "\n",
                "\n",
                "# TODO: Add a model and temperature parameter to the function with a default value\n",
                "def generate_response(system_prompt_name, user_prompt_name, variables, \n",
                "                      model: str = \"gpt-4o-mini\", temperature: float = 0.7):\n",
                "    \"\"\"\n",
                "    Render system and user prompts, send to LLM, and return the response text.\n",
                "    Uses flexible model and temperature parameters with defaults.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        if client is None:\n",
                "            logging.error(\"LLM client is not initialized. Aborting response generation.\")\n",
                "            return None\n",
                "        \n",
                "        system_prompt = render_prompt_from_file(system_prompt_name, variables)\n",
                "        user_prompt = render_prompt_from_file(user_prompt_name, variables)\n",
                "        \n",
                "        # TODO: Log the model and temperature values used\n",
                "        logging.info(f\"LLM Call: Model='{model}', Temperature={temperature}\")\n",
                "        \n",
                "        completion = client.chat.completions.create(\n",
                "            # Use the provided/default parameters\n",
                "            model=model,\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=temperature\n",
                "        )\n",
                "\n",
                "        return completion.choices[0].message.content.strip()\n",
                "\n",
                "    except APIError as e:\n",
                "        logging.error(f\"LLM API Error: {e}\")\n",
                "        return None\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Unexpected error: {e}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "def generate_boolean(system_prompt_name, user_prompt_name, variables, \n",
                "                     model: str = \"gpt-4o-mini\", temperature: float = 0.7):\n",
                "    \"\"\"\n",
                "    Render prompts, call LLM, and interpret the result as a boolean.\n",
                "    Passes optional parameters to generate_response.\n",
                "    \"\"\"\n",
                "    # TODO: Modify this function to use the model and temperature parameters\n",
                "    # Pass the optional parameters directly to generate_response\n",
                "    response = generate_response(\n",
                "        system_prompt_name, \n",
                "        user_prompt_name, \n",
                "        variables,\n",
                "        model=model,\n",
                "        temperature=temperature\n",
                "    )\n",
                "\n",
                "    if response is None:\n",
                "        return False  # Assume failure as negative\n",
                "\n",
                "    truthy_keywords = [\"yes\", \"true\", \"correct\", \"affirmative\", \"certainly\", \"absolutely\"]\n",
                "    return any(keyword in response.lower() for keyword in truthy_keywords)\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Modified `main.py` Example 🚀\n",
                "\n",
                "This example demonstrates how a user calls the function, overriding the defaults to use **`gpt-4o`** with a **`temperature` of `0.5`** (a more deterministic setting).\n",
                "\n",
                "```python\n",
                "# app.py (Modified)\n",
                "# Assuming 'deepresearcher.llm.llm_manager' refers to the file above\n",
                "from deepresearcher.llm.llm_manager import generate_response, generate_boolean \n",
                "\n",
                "# Example: Using the new parameters in generate_response\n",
                "# (Assuming \"system_test\" and \"user_test\" are valid prompt names)\n",
                "response = generate_response(\n",
                "    \"system_test\", \n",
                "    \"user_test\", \n",
                "    {},\n",
                "    # Custom parameters for the LLM call\n",
                "    model=\"gpt-4o\",\n",
                "    temperature=0.5\n",
                ")\n",
                "\n",
                "print(f\"Response using gpt-4o (temp 0.5): {response}\")\n",
                "\n",
                "# Example: Using the new parameters in generate_boolean\n",
                "# This also uses the custom model and temperature\n",
                "boolean_result = generate_boolean(\n",
                "    \"system_test\", \n",
                "    \"user_test\", \n",
                "    {},\n",
                "    model=\"gpt-4o\",\n",
                "    temperature=0.5\n",
                ")\n",
                "\n",
                "print(f\"Boolean result using gpt-4o (temp 0.5): {boolean_result}\")\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
