{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Making Basic LLM Calls\n",
                "\n",
                "## Welcome to Lesson 2: Making Basic LLM Calls\n",
                "\n",
                "Welcome to the second lesson of our course on building your own **Deep Researcher**. In this lesson, we will explore the concept of making basic **LLM** (Large Language Model) calls. LLMs, such as OpenAI's models, are powerful tools that can generate human-like text responses. They are integral to AI applications, enabling them to understand and respond to user inputs naturally. By the end of this lesson, you will understand how to make a basic LLM call and interpret its output.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding the Code Structure\n",
                "\n",
                "Let's start by understanding the structure of the code used to make an LLM call. We'll build this step-by-step.\n",
                "\n",
                "### 1\\. Setting Up the OpenAI Client\n",
                "\n",
                "First, we need to import the necessary libraries and set up the **OpenAI client**. This client will allow us to interact with the OpenAI API.\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "```\n",
                "\n",
                "Here, we import the `os` module to access environment variables and the `OpenAI` class from the `openai` library. We then create an `OpenAI` client using the **API key** and **base URL** stored in environment variables. This client is essential for making requests to the OpenAI API.\n",
                "\n",
                "### 2\\. Creating System and User Prompts\n",
                "\n",
                "Next, we need to define the prompts that will guide the model's behavior. There are two types of prompts: **system prompts** and **user prompts**.\n",
                "\n",
                "```python\n",
                "system_prompt = \"You are a coding assistant that talks like a pirate.\"\n",
                "user_prompt = \"How do I check if a Python object is an instance of a class?\"\n",
                "```\n",
                "\n",
                "  * **System Prompt:** This sets the **context** for the model. In our example, the system prompt instructs the model to respond like a pirate.\n",
                "  * **User Prompt:** This is the input from the user. Here, the user is asking how to check if a Python object is an instance of a class.\n",
                "\n",
                "These prompts are crucial as they shape the model's responses, ensuring they are relevant and contextually appropriate.\n",
                "\n",
                "### 3\\. Configuring the Model Parameters\n",
                "\n",
                "To control the model's output, we configure certain parameters like **temperature** and the used **model**.\n",
                "\n",
                "```python\n",
                "temperature = 0.7\n",
                "model = \"gpt-4o-mini\"\n",
                "```\n",
                "\n",
                "  * **Temperature:** This parameter controls the **randomness** of the model's output. A lower temperature (e.g., 0.2) makes the output more deterministic, while a higher temperature (e.g., 0.8) introduces more randomness and creativity. In our example, a temperature of **0.7** strikes a balance between creativity and coherence.\n",
                "  * **Model:** This parameter specifies the model that will be used to generate the response. For this course, we will use **`gpt-4o-mini`**, but you are free to change this parameter to your preferred model.\n",
                "\n",
                "### 4\\. Executing the LLM Call\n",
                "\n",
                "Now, let's execute the LLM call using the `client.chat.completions.create` method.\n",
                "\n",
                "```python\n",
                "completion = client.chat.completions.create(\n",
                "    model=model,\n",
                "    messages=[\n",
                "       {\"role\": \"system\", \"content\": system_prompt},\n",
                "       {\"role\": \"user\", \"content\": user_prompt},\n",
                "    ],\n",
                "    temperature=temperature\n",
                ")\n",
                "print(completion.choices[0].message.content.strip())\n",
                "```\n",
                "\n",
                "  * **Model:** We specify the model to use, in this case, `\"gpt-4o-mini\"`.\n",
                "  * **Messages:** This is a list of messages that includes both the system and user prompts.\n",
                "  * **Temperature:** We pass the temperature parameter to control the output's randomness.\n",
                "\n",
                "The `create` method sends the request to the OpenAI API, and the response is stored in the `completion` variable. We then print the model's response, which is accessed through `completion.choices[0].message.content.strip()`.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, we explored how to make basic LLM calls using OpenAI's API. We covered the setup of the OpenAI client, the creation of system and user prompts, the configuration of model parameters, and the execution of the LLM call. Understanding these components is essential for leveraging LLMs in your projects.\n",
                "\n",
                "As you move on to the practice exercises, experiment with different prompts and temperature settings to see how they affect the model's output. This hands-on experience will deepen your understanding and prepare you for more advanced applications in future lessons."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setting Up Your OpenAI Client\n",
                "\n",
                "Now that you've learned about the components needed for making LLM calls, let's put that knowledge into practice! Your first task is to set up the OpenAI client, which serves as the gateway for communicating with language models.\n",
                "\n",
                "Look for the TODO comment in the code and add the line that creates the client variable using your API key and base URL from environment variables.\n",
                "\n",
                "This client initialization is the foundation for all the LLM interactions we'll build throughout this course, so mastering this step will set you up for success in the more complex tasks ahead.\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "# TODO: Create the OpenAI client using the API key and base URL from environment variables\n",
                "client = None\n",
                "\n",
                "print(client)\n",
                "```\n",
                "\n",
                "Here is the corrected Python code with the line added to create the OpenAI client.\n",
                "\n",
                "The instruction from the previous lesson was: `client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))`.\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "# TODO: Create the OpenAI client using the API key and base URL from environment variables\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "print(client)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Changing Personas with System Prompts\n",
                "\n",
                "Excellent job setting up your OpenAI client! Now let's explore how system prompts influence an LLM's behavior. In this exercise, you'll modify the system prompt to transform the AI's persona from a pirate to a friendly tutor.\n",
                "\n",
                "First, run the code as-is to see the \"pirate\" response to the coding question. Then, change the system prompt line to create a helpful tutor persona instead. When you run the code again with your new prompt, notice how dramatically the tone and style of the response change, even though the user's question remains identical.\n",
                "\n",
                "This hands-on experiment will help you understand the powerful role system prompts play in shaping AI interactions — a key skill for creating exactly the right kind of assistant for different tasks in your projects.\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "# TODO: Change this system prompt to make the AI act as a friendly tutor instead of a pirate\n",
                "system_prompt = \"You are a coding assistant that talks like a pirate.\"\n",
                "user_prompt = \"How do I check if a Python object is an instance of a class?\"\n",
                "temperature = 0.7\n",
                "\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "       {\"role\": \"system\", \"content\": system_prompt},\n",
                "       {\"role\": \"user\", \"content\": user_prompt},\n",
                "    ],\n",
                "    temperature=temperature\n",
                ")\n",
                "\n",
                "print(completion.choices[0].message.content.strip())\n",
                "\n",
                "```\n",
                "\n",
                "That's a fantastic next step\\! Understanding how to control the **persona** of your AI is crucial for any application.\n",
                "\n",
                "To change the AI's persona from a \"pirate\" to a \"friendly tutor,\" simply replace the original `system_prompt` line with a new one that clearly defines the new role and tone.\n",
                "\n",
                "Here is the modified code with the **friendly tutor** persona implemented:\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "# TODO: Change this system prompt to make the AI act as a friendly tutor instead of a pirate\n",
                "system_prompt = \"You are a friendly, patient, and knowledgeable programming tutor. Your goal is to provide clear, easy-to-understand, and encouraging explanations.\"\n",
                "user_prompt = \"How do I check if a Python object is an instance of a class?\"\n",
                "temperature = 0.7\n",
                "\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "       {\"role\": \"system\", \"content\": system_prompt},\n",
                "       {\"role\": \"user\", \"content\": user_prompt},\n",
                "    ],\n",
                "    temperature=temperature\n",
                ")\n",
                "\n",
                "print(completion.choices[0].message.content.strip())\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## The Power of the System Prompt ✨\n",
                "\n",
                "When you run this updated code, you'll see a dramatic shift in the response:\n",
                "\n",
                "1.  **Original (Pirate) Response:** Likely uses nautical slang (\"Ahoy,\" \"matey,\" \"shiver me timbers\") and a dramatic tone.\n",
                "2.  **New (Tutor) Response:** Will use helpful language (\"That's a great question,\" \"Let's walk through this\") and provide a clear, structured explanation of the Python method, which is usually the **`isinstance()`** function.\n",
                "\n",
                "This simple change demonstrates the core function of the system prompt: it acts as a **directive** that controls the language, style, and attitude of the LLM, making it an indispensable tool for tailoring your AI for specific tasks like research, coding, or creative writing.\n",
                "\n",
                "What's a different kind of persona you could try for a marketing research task?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Crafting Effective User Prompts\n",
                "\n",
                "Wonderful work with changing the system prompt! Now that you've seen how the AI's persona can change, let's explore the other half of the conversation — the user prompt.\n",
                "\n",
                "In this exercise, you'll modify what you're asking the AI while keeping the friendly tutor persona. Your task is to change the user prompt from asking about Python instance checking to asking about list comprehensions instead.\n",
                "\n",
                "Run the code first with the original question, then update the user prompt as indicated by the TODO comment and run it again. Pay attention to how the same AI persona provides completely different technical content based solely on what you ask.\n",
                "\n",
                "This practice will help you develop the skill of crafting clear, specific questions — a crucial ability when working with LLMs in your projects.\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "system_prompt = \"You are a friendly and helpful tutor who explains coding concepts clearly and patiently.\"\n",
                "# TODO: Change this user prompt to ask about list comprehensions in Python\n",
                "user_prompt = \"How do I check if a Python object is an instance of a class?\"\n",
                "temperature = 0.7\n",
                "\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "       {\"role\": \"system\", \"content\": system_prompt},\n",
                "       {\"role\": \"user\", \"content\": user_prompt},\n",
                "    ],\n",
                "    temperature=temperature\n",
                ")\n",
                "\n",
                "print(completion.choices[0].message.content.strip())\n",
                "```\n",
                "\n",
                "That's a fantastic idea\\! You're absolutely right; mastering the art of the **user prompt** is just as crucial as setting the **system prompt** when working with LLMs. A clear, well-crafted question gets you a precise, useful answer.\n",
                "\n",
                "Here is the updated code where I've changed the `user_prompt` from asking about Python instance checking to asking about **list comprehensions** instead.\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "# NOTE: The actual API call will not run here, but this shows the change.\n",
                "# For the purpose of this exercise, the environment variables are assumed to be set \n",
                "# in the actual execution environment.\n",
                "\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "system_prompt = \"You are a friendly and helpful tutor who explains coding concepts clearly and patiently.\"\n",
                "# TODO: Change this user prompt to ask about list comprehensions in Python\n",
                "user_prompt = \"What are Python list comprehensions and how are they better than a regular 'for' loop for creating lists?\"\n",
                "temperature = 0.7\n",
                "\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "       {\"role\": \"system\", \"content\": system_prompt},\n",
                "       {\"role\": \"user\", \"content\": user_prompt},\n",
                "    ],\n",
                "    temperature=temperature\n",
                ")\n",
                "\n",
                "print(completion.choices[0].message.content.strip())\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Explanation of the Change\n",
                "\n",
                "| Original `user_prompt` | New `user_prompt` |\n",
                "| :--- | :--- |\n",
                "| `\"How do I check if a Python object is an instance of a class?\"` | `\"What are Python list comprehensions and how are they better than a regular 'for' loop for creating lists?\"` |\n",
                "\n",
                "By changing the prompt, we're asking the same friendly tutor persona for a completely different piece of technical information. The initial prompt focused on object-oriented programming concepts (`isinstance()`), while the new prompt focuses on a specific technique for **efficient list creation and manipulation** in Python.\n",
                "\n",
                "Running the code with the new prompt will demonstrate how the AI's response shifts entirely in **content** (discussing syntax, efficiency, and readability of list comprehensions) while maintaining the same helpful and patient **tone** established by the `system_prompt`. This perfectly illustrates the power of combining a strong persona with a focused question\\! 👍"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Controlling Randomness with Temperature Settings\n",
                "\n",
                "Now that you've mastered changing both system and user prompts, let's explore another key parameter that affects LLM responses — the temperature setting!\n",
                "\n",
                "Temperature controls how random or predictable the AI's responses are. In this exercise, you'll experiment with different temperature values while keeping the same prompts to see how this single parameter changes the output.\n",
                "\n",
                "Follow these steps:\n",
                "\n",
                "Create an array with different temperature values like 0.7, 0.2, 0.9\n",
                "Loop this array and run the code with different temperature values\n",
                "Print the output and inspect the differences\n",
                "Notice how lower temperatures produce more focused, consistent responses, while higher temperatures create more varied and creative outputs. This parameter gives you fine-grained control over AI behavior — a valuable tool when you need either reliable, predictable answers or more diverse, imaginative responses in your applications.\n",
                "\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "system_prompt = \"You are a friendly and helpful tutor who explains coding concepts clearly and patiently.\"\n",
                "user_prompt = \"Can you explain list comprehensions in Python?\"\n",
                "\n",
                "# TODO: Create an array with different temperature values\n",
                "temperature = 0.7\n",
                "\n",
                "# TODO: Loop the different values executing the competition code\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "       {\"role\": \"system\", \"content\": system_prompt},\n",
                "       {\"role\": \"user\", \"content\": user_prompt},\n",
                "    ],\n",
                "    temperature=temperature\n",
                ")\n",
                "\n",
                "print(completion.choices[0].message.content.strip())\n",
                "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
                "```\n",
                "\n",
                "That's an excellent next step\\! **Temperature** is indeed a critical parameter for fine-tuning the balance between creativity and consistency in an LLM's output. It gives you direct control over the **randomness** of the token selection.\n",
                "\n",
                "Here is the updated Python code that implements the loop to test different temperature values and demonstrates how to print the output for each run:\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "# NOTE: This code assumes that OPENAI_API_KEY and OPENAI_BASE_URL \n",
                "# are correctly set in the environment where the code is run.\n",
                "\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "system_prompt = \"You are a friendly and helpful tutor who explains coding concepts clearly and patiently.\"\n",
                "user_prompt = \"Can you explain list comprehensions in Python?\"\n",
                "\n",
                "# TODO: Create an array with different temperature values\n",
                "temperatures = [0.2, 0.7, 0.9]\n",
                "\n",
                "print(f\"System Prompt: '{system_prompt}'\")\n",
                "print(f\"User Prompt: '{user_prompt}'\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# TODO: Loop the different values executing the competition code\n",
                "for temp in temperatures:\n",
                "    print(f\"\\n--- Running with Temperature: {temp} ---\")\n",
                "    \n",
                "    try:\n",
                "        completion = client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[\n",
                "               {\"role\": \"system\", \"content\": system_prompt},\n",
                "               {\"role\": \"user\", \"content\": user_prompt},\n",
                "            ],\n",
                "            temperature=temp\n",
                "        )\n",
                "        # Printing a separator and the output for inspection\n",
                "        print(completion.choices[0].message.content.strip())\n",
                "        print(\"-\" * 50)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"An error occurred while calling the API: {e}\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Observing the Temperature Effect\n",
                "\n",
                "When you run the code above, you'll see three different responses to the same question, all from the same \"friendly tutor\" persona:\n",
                "\n",
                "1.  **Temperature 0.2 (Low Randomness):**\n",
                "\n",
                "      * **Output:** The response will likely be very **direct, focused, and predictable**. The AI will choose the most statistically probable words, leading to a highly consistent explanation of list comprehensions (e.g., standard syntax, a simple example). If you ran it multiple times, the output would be nearly identical each time. This is best for reliable, fact-based answers.\n",
                "\n",
                "2.  **Temperature 0.7 (Moderate Randomness):**\n",
                "\n",
                "      * **Output:** This is often the **default setting** for general-purpose tasks. The explanation will be clear and helpful, but the phrasing, analogies, or the complexity of the example provided might vary slightly compared to the 0.2 run. It strikes a good balance between coherence and variety.\n",
                "\n",
                "3.  **Temperature 0.9 (High Randomness):**\n",
                "\n",
                "      * **Output:** The response will be the **most creative and varied**. The AI might use a more elaborate analogy, provide a less common or more complex example of a list comprehension (perhaps one with filtering or nested loops), or use a slightly different conversational flow. If you ran it repeatedly, the outputs could be noticeably different, potentially leading to more imaginative but occasionally less focused results. This is ideal for brainstorming or creative writing tasks.   "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Selecting the Right LLM Model\n",
                "\n",
                "Impressive work with temperature settings! Now let's complete our exploration of key LLM parameters by examining model selection itself.\n",
                "\n",
                "Different models have unique capabilities and performance characteristics that can significantly impact your results. In this exercise, you'll experiment with changing the model while keeping all other parameters the same.\n",
                "\n",
                "Your task is simple:\n",
                "\n",
                "First, run the code with the default gpt-4o-mini model.\n",
                "Change the model parameter to a different OpenAI model (such as \"gpt-4o\" or \"gpt-3.5-turbo\").\n",
                "Run the code again and compare the responses.\n",
                "Pay attention to differences in response quality, detail level, and style between models. This knowledge will help you make smart model selections for different tasks, balancing performance needs with cost considerations in your future AI applications.\n",
                "\n",
                "\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "system_prompt = \"You are a friendly and helpful tutor who explains coding concepts clearly and patiently.\"\n",
                "user_prompt = \"Can you explain list comprehensions in Python?\"\n",
                "temperature = 0.7\n",
                "\n",
                "# TODO: Change the model parameter below to a different OpenAI model (like \"gpt-4o\" or \"gpt-3.5-turbo\")\n",
                "# Run the code with both models and compare the differences in responses\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "       {\"role\": \"system\", \"content\": system_prompt},\n",
                "       {\"role\": \"user\", \"content\": user_prompt},\n",
                "    ],\n",
                "    temperature=temperature\n",
                ")\n",
                "\n",
                "print(completion.choices[0].message.content.strip())\n",
                "```\n",
                "\n",
                "That's a perfect final step in understanding LLM configuration\\! **Model selection** is the main lever for controlling the **intelligence, cost, and speed** of your AI application.\n",
                "\n",
                "Here is the updated code, where the model has been changed from the default `gpt-4o-mini` to a more powerful, higher-cost model: `gpt-4o`.\n",
                "\n",
                "```python\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "# NOTE: This code assumes that OPENAI_API_KEY and OPENAI_BASE_URL \n",
                "# are correctly set in the environment where the code is run.\n",
                "\n",
                "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
                "\n",
                "system_prompt = \"You are a friendly and helpful tutor who explains coding concepts clearly and patiently.\"\n",
                "user_prompt = \"Can you explain list comprehensions in Python?\"\n",
                "temperature = 0.7\n",
                "\n",
                "# TODO: Change the model parameter below to a different OpenAI model (like \"gpt-4o\" or \"gpt-3.5-turbo\")\n",
                "# Run the code with both models and compare the differences in responses\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"gpt-4o\",  # Changed from \"gpt-4o-mini\" to \"gpt-4o\"\n",
                "    messages=[\n",
                "       {\"role\": \"system\", \"content\": system_prompt},\n",
                "       {\"role\": \"user\", \"content\": user_prompt},\n",
                "    ],\n",
                "    temperature=temperature\n",
                ")\n",
                "\n",
                "print(completion.choices[0].message.content.strip())\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Comparing the Models: gpt-4o-mini vs. gpt-4o\n",
                "\n",
                "When you run the code with `gpt-4o-mini` and then with `gpt-4o`, here are the general differences you'd likely observe in the output and performance, even with the same prompts and temperature:\n",
                "\n",
                "| Characteristic | `gpt-4o-mini` (Original) | `gpt-4o` (Changed) |\n",
                "| :--- | :--- | :--- |\n",
                "| **Intelligence/Reasoning** | High performance for a small model; excels over `gpt-3.5-turbo` in benchmarks. | **Top-tier performance**. Stronger in complex reasoning, nuanced technical tasks, and following intricate instructions. |\n",
                "| **Response Quality** | Clear, concise explanation; may use simpler code examples or analogies. | More **detailed, polished, and comprehensive** explanation. Code examples might be more advanced or include best practices. |\n",
                "| **Speed** | Very fast inference speed. | Very fast, generally matching the speed of the mini model, but with higher intelligence. |\n",
                "| **Cost** | Very **cost-efficient** (designed for affordability). | Significantly **higher cost** (designed for maximum capability). |\n",
                "| **Use Case** | Ideal for high-volume, cost-sensitive tasks like summarizing, basic customer support, or quick code generation. | Best for high-stakes tasks, complex problem-solving, creative writing, and scenarios where *quality* is more important than cost. |\n",
                "\n",
                "In this specific exercise, the `gpt-4o` response might be a few sentences longer, provide a more robust comparison to a `for` loop, or include more sophisticated syntax examples (like nested list comprehensions or set comprehensions) compared to the `gpt-4o-mini` version. **The style and tone** will remain consistent thanks to the system prompt, but the **depth and quality of the technical content** will reflect the difference in the model's underlying intelligence.\n",
                "\n",
                "This comparison highlights the **performance-vs-cost tradeoff** that is central to model selection in real-world applications\\!\n",
                "\n",
                "-----\n",
                "\n",
                "If you're interested in a more in-depth comparison of these different models and their intended uses, you might find this video helpful: [OpenAI Models explained GPT-4.1, O3, O4 Mini & More](https://www.youtube.com/watch?v=y7tjLZnYjBM).\n",
                "http://googleusercontent.com/youtube_content/2"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
