{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parsing and Selecting Useful Information\n",
                "\n",
                "Tentu, berikut adalah konversi teks tersebut ke format Markdown.\n",
                "\n",
                "# Introduction: The Role of Parsing and Selection in DeepResearcher\n",
                "\n",
                "-----\n",
                "\n",
                "Welcome back\\! In the previous lessons, you learned how **DeepResearcher** is structured and how it generates search queries using OpenAI. Now, you are ready for the next step: making sense of the information you collect from the web.\n",
                "\n",
                "When you run a search, you get a lot of web pages. Not all of them are helpful. Some might be off-topic, and others might have only a small piece of useful information. That’s why **parsing** (breaking down) and **selecting** (choosing) the right information are so important. In this lesson, you’ll learn how DeepResearcher uses AI to filter out the noise and keep only what matters for your research question.\n",
                "\n",
                "By the end of this lesson, you’ll understand how to:\n",
                "\n",
                "  * Decide if a web page is useful for your research.\n",
                "  * Extract only the relevant information from a web page.\n",
                "  * Use these steps in your own code.\n",
                "\n",
                "Let’s get started\\!\n",
                "\n",
                "## Evaluating Relevance with the LLM\n",
                "\n",
                "-----\n",
                "\n",
                "Now that we have web content, the first thing we need to do is decide: **Is this page useful for our research question?**\n",
                "\n",
                "DeepResearcher uses a language model (LLM) to help with this. It does this by sending a special prompt to the LLM, asking it to answer with just **Yes** or **No** to the question: \"Is this page relevant to the user's query?\"\n",
                "\n",
                "Let’s look at how this works in code, step by step.\n",
                "\n",
                "### Step 1: Prepare the Variables\n",
                "\n",
                "We need to give the LLM two things:\n",
                "\n",
                "1.  The user’s original research question.\n",
                "2.  The content of the web page.\n",
                "\n",
                "Here’s how we set up these variables:\n",
                "\n",
                "```python\n",
                "variables = {\n",
                "    \"user_query\": user_query,\n",
                "    \"page_text\": page_text[:20000]\n",
                "}\n",
                "```\n",
                "\n",
                "  * `user_query` is the question the user asked.\n",
                "  * `page_text[:20000]` is the first 20,000 characters of the web page content. We limit the length to avoid sending too much data to the LLM.\n",
                "\n",
                "### Step 2: Ask the LLM if the Page is Useful\n",
                "\n",
                "We use the function called `generate_boolean` to send our prompt and variables to the LLM. You will need to write the prompt in the exercises of this unit.\n",
                "\n",
                "```python\n",
                "is_useful = generate_boolean(\n",
                "    \"relevance_evaluator_system\",\n",
                "    \"relevance_evaluator_user\",\n",
                "    variables\n",
                ")\n",
                "print(f\"{page['url']} - Useful: {is_useful}\")\n",
                "```\n",
                "\n",
                "If the LLM thinks the page is useful, it returns **Yes**.\n",
                "If not, it returns **No**.\n",
                "\n",
                "**Example Output:**\n",
                "\n",
                "```\n",
                "https://example.com/article1 - Useful: Yes\n",
                "https://example.com/article2 - Useful: No\n",
                "```\n",
                "\n",
                "This way, we can quickly filter out pages that don’t help answer the user’s question.\n",
                "\n",
                "## Extracting Key Information Using the Extractor Prompt\n",
                "\n",
                "-----\n",
                "\n",
                "Once we know a page is useful, the next step is to pull out only the information that answers the user’s question. We don’t want to keep the whole page — just the relevant parts.\n",
                "\n",
                "DeepResearcher uses another prompt for this that you will also need to write, called the **extractor prompt**. Let’s see how this works.\n",
                "\n",
                "### Step 1: Prepare the Extraction Variables\n",
                "\n",
                "We need to give the LLM:\n",
                "\n",
                "1.  The user’s research question.\n",
                "2.  The search query that led to this page.\n",
                "3.  The content of the web page.\n",
                "\n",
                "Here’s how we set up these variables:\n",
                "\n",
                "```python\n",
                "variables = {\n",
                "    \"user_query\": user_query,\n",
                "    \"search_query\": query,\n",
                "    \"page_text\": page_text[:20000]\n",
                "}\n",
                "```\n",
                "\n",
                "### Step 2: Ask the LLM to Extract Relevant Information\n",
                "\n",
                "We use the `generate_response` function to send our prompt and variables to the LLM. This function uses a prompt that tells the LLM to act as an expert information extractor.\n",
                "\n",
                "```python\n",
                "context = generate_response(\n",
                "    \"extractor_system\",\n",
                "    \"extractor_user\",\n",
                "    variables\n",
                ")\n",
                "```\n",
                "\n",
                "The LLM reads the web page and returns only the parts that are relevant to the user’s question.\n",
                "\n",
                "The result, `context`, is a string containing the extracted information.\n",
                "\n",
                "**Example Output:**\n",
                "\n",
                "```\n",
                "Electric cars produce fewer emissions over their lifetime compared to gasoline vehicles, especially when charged with renewable energy. Battery production has an environmental cost, but this is offset by lower emissions during use.\n",
                "```\n",
                "\n",
                "This makes it much easier to build a final report or summary later.\n",
                "\n",
                "## Putting It All Together in Code\n",
                "\n",
                "-----\n",
                "\n",
                "Let’s see how these steps fit together in the main code. We’ll focus on the part of the code that checks if a page is useful and then extracts the relevant information.\n",
                "\n",
                "Here’s a simplified version of the process:\n",
                "\n",
                "```python\n",
                "for query in new_search_queries:\n",
                "    results = search_and_fetch_markdown(query, max_results=3)\n",
                "    for page in results:\n",
                "        page_text = page[\"markdown\"]\n",
                "        if not page_text.strip():\n",
                "            continue\n",
                "        \n",
                "        # Step 1: Check if the page is useful\n",
                "        variables = {\n",
                "            \"user_query\": user_query,\n",
                "            \"page_text\": page_text[:20000]\n",
                "        }\n",
                "        is_useful = generate_boolean(\"relevance_evaluator_system\", \"relevance_evaluator_user\", variables)\n",
                "        print(f\"{page['url']} - Useful: {is_useful}\")\n",
                "        \n",
                "        # Step 2: If useful, extract relevant information\n",
                "        if is_useful:\n",
                "            variables = {\n",
                "                \"user_query\": user_query,\n",
                "                \"search_query\": query,\n",
                "                \"page_text\": page_text[:20000]\n",
                "            }\n",
                "            context = generate_response(\"extractor_system\", \"extractor_user\", variables)\n",
                "            if context:\n",
                "                context_list.append(context)\n",
                "```\n",
                "\n",
                "Let’s break this down:\n",
                "\n",
                "  * For each search query, we get a few web pages.\n",
                "  * For each page, we check if it has any content.\n",
                "  * We ask the LLM if the page is useful for the user’s question.\n",
                "  * If the answer is **Yes**, we ask the LLM to extract the relevant information.\n",
                "  * We save the extracted information for later use.\n",
                "\n",
                "This process helps us build a collection of only the most useful and relevant information for the user’s research.\n",
                "\n",
                "## Summary and Practice Preview\n",
                "\n",
                "-----\n",
                "\n",
                "In this lesson, you learned how DeepResearcher filters and extracts useful information from web pages. You saw how to:\n",
                "\n",
                "  * Use the LLM to decide if a page is relevant to the user’s question.\n",
                "  * Extract only the key information from useful pages.\n",
                "  * Combine these steps in code to build a focused research tool.\n",
                "\n",
                "Next, you’ll get to practice these skills yourself. You’ll work with real code to filter and extract information, just like we did here. Take a moment to review the examples, and get ready to try it out on your own\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Collecting Web Content for Research\n",
                "\n",
                "Now that you've learned how DeepResearcher processes web content, it's time to put that knowledge into practice! In this exercise, you'll implement the first part of the perform_iterative_research function — the foundation of our research pipeline.\n",
                "\n",
                "Your task is to write code that collects and prepares web content for analysis. Specifically, you need to:\n",
                "\n",
                "Create a list to store extracted information\n",
                "Loop through each search query and retrieve web pages\n",
                "Process each page by checking whether it contains actual content\n",
                "Set up the structure that will later evaluate and extract information\n",
                "This is an important first step in building our research tool — by properly collecting and filtering web content, you're setting the stage for the more advanced processing we'll add in upcoming exercises. Let's get those search results flowing!\n",
                "\n",
                "```python\n",
                "from deepresearcher.llm.llm_manager import generate_response, generate_boolean\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "\n",
                "def generate_initial_search_queries(user_query: str):\n",
                "    variables = {\"user_query\": user_query}\n",
                "    search_queries_str = generate_response(\"search_generator_system\", \"search_generator_user\", variables)\n",
                "    try:\n",
                "        queries = eval(search_queries_str)\n",
                "        if not isinstance(queries, list):\n",
                "            raise ValueError(\"Not a list\")\n",
                "        return queries\n",
                "    except Exception:\n",
                "        print(\"Invalid response for search queries:\", search_queries_str)\n",
                "        return []\n",
                "\n",
                "\n",
                "def perform_iterative_research(user_query: str, new_search_queries: list, all_search_queries: list, iteration_limit: int):\n",
                "    # TODO: Create an empty list to store the extracted context\n",
                "    \n",
                "    # TODO: Loop through each query in new_search_queries\n",
                "        # TODO: Call search_and_fetch_markdown with the query and max_results=3\n",
                "        # TODO: Loop through each page in the results\n",
                "            # TODO: Get the page text from the page's markdown field\n",
                "            # TODO: Skip pages with no content after stripping whitespace\n",
                "            # TODO: Print the first 200 characters of the page\n",
                "    return []\n",
                "\n",
                "\n",
                "def generate_final_report(user_query: str, contexts: list):\n",
                "    pass\n",
                "\n",
                "\n",
                "def research_main():\n",
                "    user_query = input(\"Enter your research query/topic: \").strip()\n",
                "    iteration_limit = input(\"Max number of iterations (default 10): \").strip()\n",
                "    iteration_limit = int(iteration_limit) if iteration_limit.isdigit() else 10\n",
                "\n",
                "    clear_visited_pages()\n",
                "    new_search_queries = generate_initial_search_queries(user_query)\n",
                "    if not new_search_queries:\n",
                "        return\n",
                "\n",
                "    all_search_queries = new_search_queries.copy()\n",
                "    aggregated_contexts = perform_iterative_research(user_query, new_search_queries, all_search_queries, iteration_limit)\n",
                "    generate_final_report(user_query, aggregated_contexts)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    research_main()\n",
                "\n",
                "```\n",
                "\n",
                "Got it\\! Implementing the content collection loop is the essential first step in the research workflow. It handles fetching the data and basic filtering before the AI processing begins.\n",
                "\n",
                "Here is the completed `perform_iterative_research` function with the required content collection and filtering logic:\n",
                "\n",
                "```python\n",
                "from deepresearcher.llm.llm_manager import generate_response, generate_boolean\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "\n",
                "def generate_initial_search_queries(user_query: str):\n",
                "    variables = {\"user_query\": user_query}\n",
                "    search_queries_str = generate_response(\"search_generator_system\", \"search_generator_user\", variables)\n",
                "    try:\n",
                "        queries = eval(search_queries_str)\n",
                "        if not isinstance(queries, list):\n",
                "            raise ValueError(\"Not a list\")\n",
                "        return queries\n",
                "    except Exception:\n",
                "        print(\"Invalid response for search queries:\", search_queries_str)\n",
                "        return []\n",
                "\n",
                "\n",
                "def perform_iterative_research(user_query: str, new_search_queries: list, all_search_queries: list, iteration_limit: int):\n",
                "    # TODO: Create an empty list to store the extracted context\n",
                "    extracted_contexts = []\n",
                "    \n",
                "    print(\"\\nStarting web content collection...\")\n",
                "\n",
                "    # TODO: Loop through each query in new_search_queries\n",
                "    for query in new_search_queries:\n",
                "        print(f\"\\nSearching for: {query}\")\n",
                "        \n",
                "        # TODO: Call search_and_fetch_markdown with the query and max_results=3\n",
                "        # The search_and_fetch_markdown returns a list of page objects\n",
                "        results = search_and_fetch_markdown(query, max_results=3)\n",
                "        \n",
                "        # TODO: Loop through each page in the results\n",
                "        for page in results:\n",
                "            # The page object is a dictionary, typically with 'url' and 'markdown'\n",
                "            \n",
                "            # TODO: Get the page text from the page's markdown field\n",
                "            page_text = page.get(\"markdown\", \"\")\n",
                "            \n",
                "            # TODO: Skip pages with no content after stripping whitespace\n",
                "            if not page_text.strip():\n",
                "                print(f\"Skipping page at {page.get('url', 'Unknown URL')} - No content found.\")\n",
                "                continue\n",
                "\n",
                "            # --- Placeholder for AI evaluation and extraction (future steps) ---\n",
                "            \n",
                "            # TODO: Print the first 200 characters of the page (for verification)\n",
                "            print(\"-\" * 20)\n",
                "            print(f\"Found content at: {page.get('url', 'Unknown URL')}\")\n",
                "            print(f\"Content snippet: {page_text[:200].replace('\\n', ' ')}...\")\n",
                "            print(\"-\" * 20)\n",
                "\n",
                "    # Returning the empty list for now, as extraction logic is in future exercises\n",
                "    return extracted_contexts\n",
                "\n",
                "\n",
                "def generate_final_report(user_query: str, contexts: list):\n",
                "    pass\n",
                "\n",
                "\n",
                "def research_main():\n",
                "    user_query = input(\"Enter your research query/topic: \").strip()\n",
                "    iteration_limit = input(\"Max number of iterations (default 10): \").strip()\n",
                "    iteration_limit = int(iteration_limit) if iteration_limit.isdigit() else 10\n",
                "\n",
                "    clear_visited_pages()\n",
                "    new_search_queries = generate_initial_search_queries(user_query)\n",
                "    if not new_search_queries:\n",
                "        return\n",
                "\n",
                "    all_search_queries = new_search_queries.copy()\n",
                "    aggregated_contexts = perform_iterative_research(user_query, new_search_queries, all_search_queries, iteration_limit)\n",
                "    generate_final_report(user_query, aggregated_contexts)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    research_main()\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Prompts for Content Relevance Evaluation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating Content Relevance with AI"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Designing Prompts for Information Extraction"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Gold from Web Content"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Gold from Web Content"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
