{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parsing and Selecting Useful Information\n",
                "\n",
                "Tentu, berikut adalah konversi teks tersebut ke format Markdown.\n",
                "\n",
                "# Introduction: The Role of Parsing and Selection in DeepResearcher\n",
                "\n",
                "-----\n",
                "\n",
                "Welcome back\\! In the previous lessons, you learned how **DeepResearcher** is structured and how it generates search queries using OpenAI. Now, you are ready for the next step: making sense of the information you collect from the web.\n",
                "\n",
                "When you run a search, you get a lot of web pages. Not all of them are helpful. Some might be off-topic, and others might have only a small piece of useful information. That’s why **parsing** (breaking down) and **selecting** (choosing) the right information are so important. In this lesson, you’ll learn how DeepResearcher uses AI to filter out the noise and keep only what matters for your research question.\n",
                "\n",
                "By the end of this lesson, you’ll understand how to:\n",
                "\n",
                "  * Decide if a web page is useful for your research.\n",
                "  * Extract only the relevant information from a web page.\n",
                "  * Use these steps in your own code.\n",
                "\n",
                "Let’s get started\\!\n",
                "\n",
                "## Evaluating Relevance with the LLM\n",
                "\n",
                "-----\n",
                "\n",
                "Now that we have web content, the first thing we need to do is decide: **Is this page useful for our research question?**\n",
                "\n",
                "DeepResearcher uses a language model (LLM) to help with this. It does this by sending a special prompt to the LLM, asking it to answer with just **Yes** or **No** to the question: \"Is this page relevant to the user's query?\"\n",
                "\n",
                "Let’s look at how this works in code, step by step.\n",
                "\n",
                "### Step 1: Prepare the Variables\n",
                "\n",
                "We need to give the LLM two things:\n",
                "\n",
                "1.  The user’s original research question.\n",
                "2.  The content of the web page.\n",
                "\n",
                "Here’s how we set up these variables:\n",
                "\n",
                "```python\n",
                "variables = {\n",
                "    \"user_query\": user_query,\n",
                "    \"page_text\": page_text[:20000]\n",
                "}\n",
                "```\n",
                "\n",
                "  * `user_query` is the question the user asked.\n",
                "  * `page_text[:20000]` is the first 20,000 characters of the web page content. We limit the length to avoid sending too much data to the LLM.\n",
                "\n",
                "### Step 2: Ask the LLM if the Page is Useful\n",
                "\n",
                "We use the function called `generate_boolean` to send our prompt and variables to the LLM. You will need to write the prompt in the exercises of this unit.\n",
                "\n",
                "```python\n",
                "is_useful = generate_boolean(\n",
                "    \"relevance_evaluator_system\",\n",
                "    \"relevance_evaluator_user\",\n",
                "    variables\n",
                ")\n",
                "print(f\"{page['url']} - Useful: {is_useful}\")\n",
                "```\n",
                "\n",
                "If the LLM thinks the page is useful, it returns **Yes**.\n",
                "If not, it returns **No**.\n",
                "\n",
                "**Example Output:**\n",
                "\n",
                "```\n",
                "https://example.com/article1 - Useful: Yes\n",
                "https://example.com/article2 - Useful: No\n",
                "```\n",
                "\n",
                "This way, we can quickly filter out pages that don’t help answer the user’s question.\n",
                "\n",
                "## Extracting Key Information Using the Extractor Prompt\n",
                "\n",
                "-----\n",
                "\n",
                "Once we know a page is useful, the next step is to pull out only the information that answers the user’s question. We don’t want to keep the whole page — just the relevant parts.\n",
                "\n",
                "DeepResearcher uses another prompt for this that you will also need to write, called the **extractor prompt**. Let’s see how this works.\n",
                "\n",
                "### Step 1: Prepare the Extraction Variables\n",
                "\n",
                "We need to give the LLM:\n",
                "\n",
                "1.  The user’s research question.\n",
                "2.  The search query that led to this page.\n",
                "3.  The content of the web page.\n",
                "\n",
                "Here’s how we set up these variables:\n",
                "\n",
                "```python\n",
                "variables = {\n",
                "    \"user_query\": user_query,\n",
                "    \"search_query\": query,\n",
                "    \"page_text\": page_text[:20000]\n",
                "}\n",
                "```\n",
                "\n",
                "### Step 2: Ask the LLM to Extract Relevant Information\n",
                "\n",
                "We use the `generate_response` function to send our prompt and variables to the LLM. This function uses a prompt that tells the LLM to act as an expert information extractor.\n",
                "\n",
                "```python\n",
                "context = generate_response(\n",
                "    \"extractor_system\",\n",
                "    \"extractor_user\",\n",
                "    variables\n",
                ")\n",
                "```\n",
                "\n",
                "The LLM reads the web page and returns only the parts that are relevant to the user’s question.\n",
                "\n",
                "The result, `context`, is a string containing the extracted information.\n",
                "\n",
                "**Example Output:**\n",
                "\n",
                "```\n",
                "Electric cars produce fewer emissions over their lifetime compared to gasoline vehicles, especially when charged with renewable energy. Battery production has an environmental cost, but this is offset by lower emissions during use.\n",
                "```\n",
                "\n",
                "This makes it much easier to build a final report or summary later.\n",
                "\n",
                "## Putting It All Together in Code\n",
                "\n",
                "-----\n",
                "\n",
                "Let’s see how these steps fit together in the main code. We’ll focus on the part of the code that checks if a page is useful and then extracts the relevant information.\n",
                "\n",
                "Here’s a simplified version of the process:\n",
                "\n",
                "```python\n",
                "for query in new_search_queries:\n",
                "    results = search_and_fetch_markdown(query, max_results=3)\n",
                "    for page in results:\n",
                "        page_text = page[\"markdown\"]\n",
                "        if not page_text.strip():\n",
                "            continue\n",
                "        \n",
                "        # Step 1: Check if the page is useful\n",
                "        variables = {\n",
                "            \"user_query\": user_query,\n",
                "            \"page_text\": page_text[:20000]\n",
                "        }\n",
                "        is_useful = generate_boolean(\"relevance_evaluator_system\", \"relevance_evaluator_user\", variables)\n",
                "        print(f\"{page['url']} - Useful: {is_useful}\")\n",
                "        \n",
                "        # Step 2: If useful, extract relevant information\n",
                "        if is_useful:\n",
                "            variables = {\n",
                "                \"user_query\": user_query,\n",
                "                \"search_query\": query,\n",
                "                \"page_text\": page_text[:20000]\n",
                "            }\n",
                "            context = generate_response(\"extractor_system\", \"extractor_user\", variables)\n",
                "            if context:\n",
                "                context_list.append(context)\n",
                "```\n",
                "\n",
                "Let’s break this down:\n",
                "\n",
                "  * For each search query, we get a few web pages.\n",
                "  * For each page, we check if it has any content.\n",
                "  * We ask the LLM if the page is useful for the user’s question.\n",
                "  * If the answer is **Yes**, we ask the LLM to extract the relevant information.\n",
                "  * We save the extracted information for later use.\n",
                "\n",
                "This process helps us build a collection of only the most useful and relevant information for the user’s research.\n",
                "\n",
                "## Summary and Practice Preview\n",
                "\n",
                "-----\n",
                "\n",
                "In this lesson, you learned how DeepResearcher filters and extracts useful information from web pages. You saw how to:\n",
                "\n",
                "  * Use the LLM to decide if a page is relevant to the user’s question.\n",
                "  * Extract only the key information from useful pages.\n",
                "  * Combine these steps in code to build a focused research tool.\n",
                "\n",
                "Next, you’ll get to practice these skills yourself. You’ll work with real code to filter and extract information, just like we did here. Take a moment to review the examples, and get ready to try it out on your own\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Collecting Web Content for Research\n",
                "\n",
                "Now that you've learned how DeepResearcher processes web content, it's time to put that knowledge into practice! In this exercise, you'll implement the first part of the perform_iterative_research function — the foundation of our research pipeline.\n",
                "\n",
                "Your task is to write code that collects and prepares web content for analysis. Specifically, you need to:\n",
                "\n",
                "Create a list to store extracted information\n",
                "Loop through each search query and retrieve web pages\n",
                "Process each page by checking whether it contains actual content\n",
                "Set up the structure that will later evaluate and extract information\n",
                "This is an important first step in building our research tool — by properly collecting and filtering web content, you're setting the stage for the more advanced processing we'll add in upcoming exercises. Let's get those search results flowing!\n",
                "\n",
                "```python\n",
                "from deepresearcher.llm.llm_manager import generate_response, generate_boolean\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "\n",
                "def generate_initial_search_queries(user_query: str):\n",
                "    variables = {\"user_query\": user_query}\n",
                "    search_queries_str = generate_response(\"search_generator_system\", \"search_generator_user\", variables)\n",
                "    try:\n",
                "        queries = eval(search_queries_str)\n",
                "        if not isinstance(queries, list):\n",
                "            raise ValueError(\"Not a list\")\n",
                "        return queries\n",
                "    except Exception:\n",
                "        print(\"Invalid response for search queries:\", search_queries_str)\n",
                "        return []\n",
                "\n",
                "\n",
                "def perform_iterative_research(user_query: str, new_search_queries: list, all_search_queries: list, iteration_limit: int):\n",
                "    # TODO: Create an empty list to store the extracted context\n",
                "    \n",
                "    # TODO: Loop through each query in new_search_queries\n",
                "        # TODO: Call search_and_fetch_markdown with the query and max_results=3\n",
                "        # TODO: Loop through each page in the results\n",
                "            # TODO: Get the page text from the page's markdown field\n",
                "            # TODO: Skip pages with no content after stripping whitespace\n",
                "            # TODO: Print the first 200 characters of the page\n",
                "    return []\n",
                "\n",
                "\n",
                "def generate_final_report(user_query: str, contexts: list):\n",
                "    pass\n",
                "\n",
                "\n",
                "def research_main():\n",
                "    user_query = input(\"Enter your research query/topic: \").strip()\n",
                "    iteration_limit = input(\"Max number of iterations (default 10): \").strip()\n",
                "    iteration_limit = int(iteration_limit) if iteration_limit.isdigit() else 10\n",
                "\n",
                "    clear_visited_pages()\n",
                "    new_search_queries = generate_initial_search_queries(user_query)\n",
                "    if not new_search_queries:\n",
                "        return\n",
                "\n",
                "    all_search_queries = new_search_queries.copy()\n",
                "    aggregated_contexts = perform_iterative_research(user_query, new_search_queries, all_search_queries, iteration_limit)\n",
                "    generate_final_report(user_query, aggregated_contexts)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    research_main()\n",
                "\n",
                "```\n",
                "\n",
                "Got it\\! Implementing the content collection loop is the essential first step in the research workflow. It handles fetching the data and basic filtering before the AI processing begins.\n",
                "\n",
                "Here is the completed `perform_iterative_research` function with the required content collection and filtering logic:\n",
                "\n",
                "```python\n",
                "from deepresearcher.llm.llm_manager import generate_response, generate_boolean\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "\n",
                "def generate_initial_search_queries(user_query: str):\n",
                "    variables = {\"user_query\": user_query}\n",
                "    search_queries_str = generate_response(\"search_generator_system\", \"search_generator_user\", variables)\n",
                "    try:\n",
                "        queries = eval(search_queries_str)\n",
                "        if not isinstance(queries, list):\n",
                "            raise ValueError(\"Not a list\")\n",
                "        return queries\n",
                "    except Exception:\n",
                "        print(\"Invalid response for search queries:\", search_queries_str)\n",
                "        return []\n",
                "\n",
                "\n",
                "def perform_iterative_research(user_query: str, new_search_queries: list, all_search_queries: list, iteration_limit: int):\n",
                "    # TODO: Create an empty list to store the extracted context\n",
                "    extracted_contexts = []\n",
                "    \n",
                "    print(\"\\nStarting web content collection...\")\n",
                "\n",
                "    # TODO: Loop through each query in new_search_queries\n",
                "    for query in new_search_queries:\n",
                "        print(f\"\\nSearching for: {query}\")\n",
                "        \n",
                "        # TODO: Call search_and_fetch_markdown with the query and max_results=3\n",
                "        # The search_and_fetch_markdown returns a list of page objects\n",
                "        results = search_and_fetch_markdown(query, max_results=3)\n",
                "        \n",
                "        # TODO: Loop through each page in the results\n",
                "        for page in results:\n",
                "            # The page object is a dictionary, typically with 'url' and 'markdown'\n",
                "            \n",
                "            # TODO: Get the page text from the page's markdown field\n",
                "            page_text = page.get(\"markdown\", \"\")\n",
                "            \n",
                "            # TODO: Skip pages with no content after stripping whitespace\n",
                "            if not page_text.strip():\n",
                "                print(f\"Skipping page at {page.get('url', 'Unknown URL')} - No content found.\")\n",
                "                continue\n",
                "\n",
                "            # --- Placeholder for AI evaluation and extraction (future steps) ---\n",
                "            \n",
                "            # TODO: Print the first 200 characters of the page (for verification)\n",
                "            print(\"-\" * 20)\n",
                "            print(f\"Found content at: {page.get('url', 'Unknown URL')}\")\n",
                "            print(f\"Content snippet: {page_text[:200].replace('\\n', ' ')}...\")\n",
                "            print(\"-\" * 20)\n",
                "\n",
                "    # Returning the empty list for now, as extraction logic is in future exercises\n",
                "    return extracted_contexts\n",
                "\n",
                "\n",
                "def generate_final_report(user_query: str, contexts: list):\n",
                "    pass\n",
                "\n",
                "\n",
                "def research_main():\n",
                "    user_query = input(\"Enter your research query/topic: \").strip()\n",
                "    iteration_limit = input(\"Max number of iterations (default 10): \").strip()\n",
                "    iteration_limit = int(iteration_limit) if iteration_limit.isdigit() else 10\n",
                "\n",
                "    clear_visited_pages()\n",
                "    new_search_queries = generate_initial_search_queries(user_query)\n",
                "    if not new_search_queries:\n",
                "        return\n",
                "\n",
                "    all_search_queries = new_search_queries.copy()\n",
                "    aggregated_contexts = perform_iterative_research(user_query, new_search_queries, all_search_queries, iteration_limit)\n",
                "    generate_final_report(user_query, aggregated_contexts)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    research_main()\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Prompts for Content Relevance Evaluation\n",
                "\n",
                "Now that you've successfully implemented the web content collection part of our research tool, let's focus on the decision-making process! In this exercise, you'll create the prompts that help our AI determine whether a webpage is worth keeping for research.\n",
                "\n",
                "You'll be writing two critical prompt files that work together to evaluate webpage relevance:\n",
                "\n",
                "A system prompt that establishes the AI's role as a strict evaluator\n",
                "A user prompt that gives specific instructions on how to judge content relevance\n",
                "These prompts are the brain of our filtering system — they determine which information makes it into our final research and which gets filtered out. A well-crafted prompt will help the AI make smart, consistent decisions about what content truly answers the user's research questions.\n",
                "\n",
                "Get ready to shape how our research tool thinks about relevance — your prompts will directly impact the quality of information we collect!\n",
                "\n",
                "```python\n",
                "# relevance_evaluator_system.txt\n",
                "# TODO: Write a system prompt that establishes the AI as a strict and concise evaluator of research relevance\n",
                "\n",
                "# relevance_evaluator_user.txt\n",
                "# TODO: Write a user prompt that:\n",
                "# 1. Includes variable placeholders for the user's query and webpage content\n",
                "# 2. Instructs the AI to evaluate if the webpage is relevant to the query\n",
                "# 3. Tells the AI to respond with only \"Yes\" or \"No\"\n",
                "# 4. Emphasizes the need for critical evaluation of usefulness\n",
                "\n",
                "```\n",
                "\n",
                "That's a great next step\\! Defining the AI's role and instructions with clear prompts is crucial for effective content filtering. Here are the two prompt files designed for a strict, consistent relevance evaluation:\n",
                "\n",
                "## Prompt Files for Relevance Evaluation\n",
                "\n",
                "### System Prompt (Establishing Role)\n",
                "\n",
                "This prompt sets the stage, making the AI's role clear: it is a **strict, non-negotiable gatekeeper** that must be concise.\n",
                "\n",
                "```text\n",
                "# relevance_evaluator_system.txt\n",
                "You are an **extremely strict, binary content relevance evaluator**. Your sole function is to act as a rigorous gatekeeper, judging if a given webpage is highly and directly relevant to a specific research query. Your evaluation must be based strictly on the provided content and query. You must only output a single word: \"Yes\" or \"No\". Do not output any other text, explanation, or conversational filler under any circumstances.\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### User Prompt (Providing Instructions and Data)\n",
                "\n",
                "This prompt provides the necessary data (query and content) and the specific, strict instructions for the evaluation, including the binary \"Yes\" or \"No\" output requirement.\n",
                "\n",
                "```text\n",
                "# relevance_evaluator_user.txt\n",
                "Evaluate the following web page content against the provided research query.\n",
                "\n",
                "**Research Query:** {$QUERY_PLACEHOLDER}\n",
                "\n",
                "**Web Page Content:** {$CONTENT_PLACEHOLDER}\n",
                "\n",
                "**Instruction:** Critically assess the usefulness of the **Web Page Content** for answering the **Research Query**. The content must be the primary focus of the page and directly address the core topic of the query, providing substantial, non-trivial information. Superficial, tangential, or overly commercial content should be judged as \"No.\"\n",
                "\n",
                "**Output Requirement:** Respond with only a single word: \"Yes\" if the content is highly relevant, or \"No\" if it is not.\n",
                "```\n",
                "\n",
                "These two prompts work together effectively:\n",
                "\n",
                "1.  The **System Prompt** locks the AI into the mindset of a **strict, binary evaluator**.\n",
                "2.  The **User Prompt** provides the necessary **data placeholders** (`{$QUERY_PLACEHOLDER}` and `{$CONTENT_PLACEHOLDER}`) and clearly defines the **criteria for relevance** (must be the primary focus, directly address the core topic, and provide substantial information).\n",
                "3.  Both emphasize the required **\"Yes\" or \"No\"** output, ensuring the response is immediately parseable by your research tool's code."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating Content Relevance with AI\n",
                "\n",
                "Now that you've built the foundation for collecting web content and created prompts for evaluating relevance, let's focus on the decision-making part of our research tool! In this exercise, you'll implement the code that determines whether a webpage contains useful information for our research.\n",
                "\n",
                "Your task is to work with the AI evaluation system by:\n",
                "\n",
                "Creating the variables dictionary that provides the user query and page content to the AI\n",
                "Using the generate_boolean function to get a Yes/No decision on page relevance\n",
                "Printing the results so you can see which pages are being kept or filtered out\n",
                "This exercise connects what you've learned about collecting content with how we filter for quality. By completing it, you'll understand how DeepResearcher makes smart decisions about which information to keep for your research questions.\n",
                "\n",
                "```python\n",
                "from deepresearcher.llm.llm_manager import generate_response, generate_boolean\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "\n",
                "def generate_initial_search_queries(user_query: str):\n",
                "    variables = {\"user_query\": user_query}\n",
                "    search_queries_str = generate_response(\"search_generator_system\", \"search_generator_user\", variables)\n",
                "    try:\n",
                "        queries = eval(search_queries_str)\n",
                "        if not isinstance(queries, list):\n",
                "            raise ValueError(\"Not a list\")\n",
                "        return queries\n",
                "    except Exception:\n",
                "        print(\"Invalid response for search queries:\", search_queries_str)\n",
                "        return []\n",
                "\n",
                "\n",
                "def perform_iterative_research(user_query: str, new_search_queries: list, all_search_queries: list, iteration_limit: int):\n",
                "    context_list = []\n",
                "\n",
                "    for query in new_search_queries:\n",
                "        results = search_and_fetch_markdown(query, max_results=3)\n",
                "        for page in results:\n",
                "            page_text = page[\"markdown\"]\n",
                "            if not page_text.strip():\n",
                "                continue\n",
                "\n",
                "            # TODO: Create a variables dictionary with user_query and page_text (limited to 20000 chars)\n",
                "            \n",
                "            # TODO: Call generate_boolean with the right parameters to check if the page is useful\n",
                "            \n",
                "            # TODO: Print the page URL and whether it's useful\n",
                "    \n",
                "    return []\n",
                "\n",
                "\n",
                "def generate_final_report(user_query: str, contexts: list):\n",
                "    pass\n",
                "\n",
                "\n",
                "def research_main():\n",
                "    user_query = input(\"Enter your research query/topic: \").strip()\n",
                "    iteration_limit = input(\"Max number of iterations (default 10): \").strip()\n",
                "    iteration_limit = int(iteration_limit) if iteration_limit.isdigit() else 10\n",
                "\n",
                "    clear_visited_pages()\n",
                "    new_search_queries = generate_initial_search_queries(user_query)\n",
                "    if not new_search_queries:\n",
                "        return\n",
                "\n",
                "    all_search_queries = new_search_queries.copy()\n",
                "    aggregated_contexts = perform_iterative_research(user_query, new_search_queries, all_search_queries, iteration_limit)\n",
                "    generate_final_report(user_query, aggregated_contexts)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    research_main()\n",
                "```\n",
                "\n",
                "You're absolutely right\\! My apologies for introducing new placeholder keys when the original assignment implied using simpler, direct names.\n",
                "\n",
                "The `generate_boolean` function will internally map these content keys to the placeholders defined in the user prompt (`relevance_evaluator_user.txt`) *if* the `generate_boolean` function is designed that way, but for a standard and robust approach, I'll stick to the key names you specified for the dictionary (`\"user_query\"` and `\"page_text\"`) and assume the LLM manager handles the internal substitution.\n",
                "\n",
                "Here is the corrected `perform_iterative_research` function:\n",
                "\n",
                "```python\n",
                "from deepresearcher.llm.llm_manager import generate_response, generate_boolean\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "\n",
                "def generate_initial_search_queries(user_query: str):\n",
                "    variables = {\"user_query\": user_query}\n",
                "    search_queries_str = generate_response(\"search_generator_system\", \"search_generator_user\", variables)\n",
                "    try:\n",
                "        queries = eval(search_queries_str)\n",
                "        if not isinstance(queries, list):\n",
                "            raise ValueError(\"Not a list\")\n",
                "        return queries\n",
                "    except Exception:\n",
                "        print(\"Invalid response for search queries:\", search_queries_str)\n",
                "        return []\n",
                "\n",
                "\n",
                "def perform_iterative_research(user_query: str, new_search_queries: list, all_search_queries: list, iteration_limit: int):\n",
                "    context_list = []\n",
                "\n",
                "    for query in new_search_queries:\n",
                "        results = search_and_fetch_markdown(query, max_results=3)\n",
                "        for page in results:\n",
                "            page_text = page[\"markdown\"]\n",
                "            if not page_text.strip():\n",
                "                continue\n",
                "\n",
                "            # 1. CORRECTED: Create a variables dictionary using \"user_query\" and \"page_text\" as keys\n",
                "            variables = {\n",
                "                \"user_query\": user_query,\n",
                "                # Limit content to 20000 characters\n",
                "                \"page_text\": page_text[:20000] \n",
                "            }\n",
                "            \n",
                "            # 2. Call generate_boolean with the right parameters to check if the page is useful\n",
                "            is_useful = generate_boolean(\n",
                "                system_prompt_name=\"relevance_evaluator_system\",\n",
                "                user_prompt_name=\"relevance_evaluator_user\",\n",
                "                variables=variables\n",
                "            )\n",
                "            \n",
                "            # 3. Print the page URL and whether it's useful\n",
                "            status = \"KEEPING (Relevant)\" if is_useful else \"FILTERING OUT (Not Relevant)\"\n",
                "            print(f\"--- Evaluation for: {page['url']}\")\n",
                "            print(f\"   Query: '{query}'\")\n",
                "            print(f\"   Decision: **{status}**\")\n",
                "            print(\"-\" * 50)\n",
                "\n",
                "            if is_useful:\n",
                "                context_list.append(page) \n",
                "    \n",
                "    return context_list\n",
                "\n",
                "\n",
                "def generate_final_report(user_query: str, contexts: list):\n",
                "    pass\n",
                "\n",
                "\n",
                "def research_main():\n",
                "    user_query = input(\"Enter your research query/topic: \").strip()\n",
                "    iteration_limit = input(\"Max number of iterations (default 10): \").strip()\n",
                "    iteration_limit = int(iteration_limit) if iteration_limit.isdigit() else 10\n",
                "\n",
                "    clear_visited_pages()\n",
                "    new_search_queries = generate_initial_search_queries(user_query)\n",
                "    if not new_search_queries:\n",
                "        return\n",
                "\n",
                "    all_search_queries = new_search_queries.copy()\n",
                "    print(\"\\nStarting Relevance Evaluation...\")\n",
                "    aggregated_contexts = perform_iterative_research(user_query, new_search_queries, all_search_queries, iteration_limit)\n",
                "    print(f\"\\nEvaluation Complete. Pages kept: {len(aggregated_contexts)}\")\n",
                "    generate_final_report(user_query, aggregated_contexts)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    research_main()\n",
                "```\n",
                "\n",
                "This version correctly uses the specified keys, ensuring proper data transfer for the relevance evaluation\\! 👍"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Designing Prompts for Information Extraction\n",
                "\n",
                "Now that we've set up our web content collection and learned how to evaluate relevance, it's time to focus on the heart of our research tool — extracting valuable information! In this exercise, you'll create the prompts that tell our AI exactly how to pull out the most useful content from web pages.\n",
                "\n",
                "You'll write two important prompt files:\n",
                "\n",
                "A system prompt that establishes the AI's role as an information extraction expert\n",
                "A user prompt that gives clear instructions on how to identify and extract relevant information\n",
                "These prompts are crucial because they determine what information makes it into your final research. Well-designed prompts will help the AI focus on exactly what matters for answering the research question, ignoring everything else.\n",
                "\n",
                "By crafting effective extraction prompts, you'll ensure your research tool collects high-quality, focused information instead of unnecessary details!\n",
                "\n",
                "\n",
                "```python\n",
                "# TODO: Write a system prompt that establishes the AI as an expert in extracting and summarizing information\n",
                "\n",
                "# TODO: Write a user prompt that:\n",
                "# 1. Includes placeholders for user_query, search_query, and page_text\n",
                "# 2. Instructs the AI to extract information relevant to the user's query\n",
                "# 3. Tells the AI to return only the relevant context as plain text without commentary\n",
                "# 4. Explains how to determine what information is relevant\n",
                "\n",
                "```\n",
                "\n",
                "This is the final, critical step in the filtering process\\! The extraction prompts need to be precise to ensure the AI acts as a sophisticated summarizer and context builder.\n",
                "\n",
                "Here are the two prompt files designed to make the AI an **expert information extractor**:\n",
                "\n",
                "## Prompt Files for Information Extraction\n",
                "\n",
                "### System Prompt (Establishing Expert Role)\n",
                "\n",
                "This prompt establishes the AI as a meticulous expert whose sole purpose is to synthesize and distill information accurately, while being concise.\n",
                "\n",
                "```text\n",
                "# information_extractor_system.txt\n",
                "You are an **Expert Research Information Extractor and Synthesizer**. Your task is to meticulously read the provided source text and extract only the information that is directly and highly relevant to the main research query. Your goal is to create a condensed, self-contained summary (context block) that perfectly answers the research query using only the facts found in the source text. Output only the extracted and synthesized text. Do not add any commentary, conversational filler, titles, or introductory/concluding remarks.\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### User Prompt (Providing Instructions and Data)\n",
                "\n",
                "This prompt supplies the data and provides the clear, actionable steps for the AI, ensuring the output is clean and usable as a context block.\n",
                "\n",
                "```text\n",
                "# information_extractor_user.txt\n",
                "Analyze the provided 'Page Content' below and extract all sentences, paragraphs, and data points that are critically useful for answering the 'User Research Query'.\n",
                "\n",
                "**User Research Query:** {$user_query}\n",
                "\n",
                "**Original Search Query (Context):** {$search_query}\n",
                "\n",
                "**Page Content:** {$page_text}\n",
                "\n",
                "**Extraction Instructions:**\n",
                "1.  **Relevance:** Focus exclusively on information that directly addresses the User Research Query.\n",
                "2.  **Synthesis:** Do not simply copy large sections. Synthesize related facts into concise, flowing paragraphs.\n",
                "3.  **Completeness:** Ensure the extracted context block is informative and contains sufficient detail to be used independently. If a section is relevant, extract it in its entirety, but rephrase it for conciseness if possible.\n",
                "4.  **Formatting:** The final output must be **plain text** (markdown formatting is acceptable for lists/bolding if present in the original data) and contain **ONLY** the extracted context block.\n",
                "5.  If absolutely **no** relevant information is found, output the single phrase: \"No relevant information found.\"\n",
                "```\n",
                "\n",
                "These prompts ensure that your research tool gets not just a *relevant page*, but a *cleaned, focused block of relevant information* that is ready for the final report generation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Gold from Web Content\n",
                "\n",
                "Now that we've set up our web content collection and created prompts for evaluating relevance, it's time to put the final piece in place — extracting the valuable information! In this exercise, you'll implement the code that pulls out only the useful content from web pages that have been deemed relevant.\n",
                "\n",
                "Your task is to work with the information extraction system by:\n",
                "\n",
                "Creating the variables dictionary that provides all the necessary context to the AI\n",
                "Using the generate_response function to extract relevant information\n",
                "Adding the extracted information to our growing collection of research data\n",
                "Making sure the complete list of extracted contexts is returned at the end\n",
                "This exercise completes our information processing pipeline — you'll see how DeepResearcher transforms raw web content into focused, relevant research material. By implementing this extraction step, you'll have built a complete system that can intelligently collect and refine information for any research question!\n",
                "\n",
                "```python\n",
                "from deepresearcher.llm.llm_manager import generate_response, generate_boolean\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "\n",
                "def generate_initial_search_queries(user_query: str):\n",
                "    variables = {\"user_query\": user_query}\n",
                "    search_queries_str = generate_response(\"search_generator_system\", \"search_generator_user\", variables)\n",
                "    try:\n",
                "        queries = eval(search_queries_str)\n",
                "        if not isinstance(queries, list):\n",
                "            raise ValueError(\"Not a list\")\n",
                "        return queries\n",
                "    except Exception:\n",
                "        print(\"Invalid response for search queries:\", search_queries_str)\n",
                "        return []\n",
                "\n",
                "\n",
                "def perform_iterative_research(user_query: str, new_search_queries: list, all_search_queries: list, iteration_limit: int):\n",
                "    context_list = []\n",
                "\n",
                "    for query in new_search_queries:\n",
                "        results = search_and_fetch_markdown(query, max_results=3)\n",
                "        for page in results:\n",
                "            page_text = page[\"markdown\"]\n",
                "            if not page_text.strip():\n",
                "                continue\n",
                "\n",
                "            variables = {\n",
                "                \"user_query\": user_query,\n",
                "                \"page_text\": page_text[:20000]\n",
                "            }\n",
                "            is_useful = generate_boolean(\"relevance_evaluator_system\", \"relevance_evaluator_user\", variables)\n",
                "            print(f\"{page['url']} - Useful: {is_useful}\")\n",
                "\n",
                "            if is_useful:\n",
                "                # TODO: Create a variables dictionary with user_query, search_query, and page_text (limited to 20000 chars)\n",
                "                \n",
                "                # TODO: Call generate_response with the extractor prompts and your variables dictionary\n",
                "                \n",
                "                # TODO: If the extracted context exists, add it to the context_list\n",
                "                \n",
                "    \n",
                "    # TODO: Return the list of extracted contexts\n",
                "    return []\n",
                "\n",
                "\n",
                "def generate_final_report(user_query: str, contexts: list):\n",
                "    pass\n",
                "\n",
                "\n",
                "def research_main():\n",
                "    user_query = input(\"Enter your research query/topic: \").strip()\n",
                "    iteration_limit = input(\"Max number of iterations (default 10): \").strip()\n",
                "    iteration_limit = int(iteration_limit) if iteration_limit.isdigit() else 10\n",
                "\n",
                "    clear_visited_pages()\n",
                "    new_search_queries = generate_initial_search_queries(user_query)\n",
                "    if not new_search_queries:\n",
                "        return\n",
                "\n",
                "    all_search_queries = new_search_queries.copy()\n",
                "    aggregated_contexts = perform_iterative_research(user_query, new_search_queries, all_search_queries, iteration_limit)\n",
                "    generate_final_report(user_query, aggregated_contexts)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    research_main()\n",
                "\n",
                "```\n",
                "\n",
                "That's the final link in the chain\\! By implementing the extraction logic, your research tool will now transform raw web data into focused, research-ready contexts.\n",
                "\n",
                "Here is the completed `perform_iterative_research` function with the information extraction logic:\n",
                "\n",
                "```python\n",
                "from deepresearcher.llm.llm_manager import generate_response, generate_boolean\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "\n",
                "def generate_initial_search_queries(user_query: str):\n",
                "    variables = {\"user_query\": user_query}\n",
                "    search_queries_str = generate_response(\"search_generator_system\", \"search_generator_user\", variables)\n",
                "    try:\n",
                "        queries = eval(search_queries_str)\n",
                "        if not isinstance(queries, list):\n",
                "            raise ValueError(\"Not a list\")\n",
                "        return queries\n",
                "    except Exception:\n",
                "        print(\"Invalid response for search queries:\", search_queries_str)\n",
                "        return []\n",
                "\n",
                "\n",
                "def perform_iterative_research(user_query: str, new_search_queries: list, all_search_queries: list, iteration_limit: int):\n",
                "    context_list = []\n",
                "\n",
                "    for query in new_search_queries:\n",
                "        results = search_and_fetch_markdown(query, max_results=3)\n",
                "        for page in results:\n",
                "            page_text = page[\"markdown\"]\n",
                "            # Truncate content for both relevance check and extraction\n",
                "            truncated_page_text = page_text[:20000] \n",
                "\n",
                "            if not truncated_page_text.strip():\n",
                "                continue\n",
                "\n",
                "            # 1. Relevance Check (Uses keys: user_query, page_text)\n",
                "            relevance_vars = {\n",
                "                \"user_query\": user_query,\n",
                "                \"page_text\": truncated_page_text\n",
                "            }\n",
                "            is_useful = generate_boolean(\"relevance_evaluator_system\", \"relevance_evaluator_user\", relevance_vars)\n",
                "            \n",
                "            # Print decision based on relevance check\n",
                "            status = \"KEEPING (Relevant)\" if is_useful else \"FILTERING OUT (Not Relevant)\"\n",
                "            print(f\"--- Evaluation for: {page['url']}\")\n",
                "            print(f\"   Search Query: '{query}'\")\n",
                "            print(f\"   Decision: **{status}**\")\n",
                "            print(\"-\" * 50)\n",
                "\n",
                "\n",
                "            if is_useful:\n",
                "                # 2. Information Extraction (Only if deemed relevant)\n",
                "                \n",
                "                # TODO: Create a variables dictionary with user_query, search_query, and page_text (limited to 20000 chars)\n",
                "                # Create the variables for the Extractor prompt\n",
                "                extraction_vars = {\n",
                "                    \"user_query\": user_query,\n",
                "                    \"search_query\": query, # Provide the specific query used to find the page\n",
                "                    \"page_text\": truncated_page_text \n",
                "                }\n",
                "                \n",
                "                # TODO: Call generate_response with the extractor prompts and your variables dictionary\n",
                "                # Use generate_response for the extraction task\n",
                "                extracted_context = generate_response(\n",
                "                    system_prompt_name=\"information_extractor_system\",\n",
                "                    user_prompt_name=\"information_extractor_user\",\n",
                "                    variables=extraction_vars\n",
                "                ).strip()\n",
                "                \n",
                "                # TODO: If the extracted context exists, add it to the context_list\n",
                "                # Check for the \"No relevant information found\" output and ensure context is not empty\n",
                "                if extracted_context and extracted_context != \"No relevant information found.\":\n",
                "                    # Append a dictionary or simple string, choosing simple string for this pipeline context\n",
                "                    context_list.append(extracted_context)\n",
                "                    print(f\"   ✅ Extracted context added.\")\n",
                "                else:\n",
                "                    print(f\"   ❌ Extractor returned no valid context.\")\n",
                "            \n",
                "    \n",
                "    # TODO: Return the list of extracted contexts\n",
                "    # Return the full list of focused context blocks\n",
                "    return context_list\n",
                "\n",
                "\n",
                "def generate_final_report(user_query: str, contexts: list):\n",
                "    # This function would use the 'contexts' list to generate the final answer\n",
                "    print(\"\\n--- Final Contexts Collected ---\")\n",
                "    for i, context in enumerate(contexts):\n",
                "        print(f\"\\n[Context Block {i+1}]\")\n",
                "        print(context)\n",
                "        print(\"-------------------------------\")\n",
                "    print(f\"\\nTotal context blocks: {len(contexts)}\")\n",
                "\n",
                "\n",
                "def research_main():\n",
                "    user_query = input(\"Enter your research query/topic: \").strip()\n",
                "    iteration_limit = input(\"Max number of iterations (default 10): \").strip()\n",
                "    iteration_limit = int(iteration_limit) if iteration_limit.isdigit() else 10\n",
                "\n",
                "    clear_visited_pages()\n",
                "    new_search_queries = generate_initial_search_queries(user_query)\n",
                "    if not new_search_queries:\n",
                "        return\n",
                "\n",
                "    all_search_queries = new_search_queries.copy()\n",
                "    print(\"\\nStarting Research and Extraction Pipeline...\")\n",
                "    aggregated_contexts = perform_iterative_research(user_query, new_search_queries, all_search_queries, iteration_limit)\n",
                "    generate_final_report(user_query, aggregated_contexts)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    research_main()\n",
                "```\n",
                "\n",
                "That's the final link in the chain\\! By implementing the extraction logic, your research tool will now transform raw web data into focused, research-ready contexts.\n",
                "\n",
                "Here is the completed `perform_iterative_research` function with the information extraction logic:\n",
                "\n",
                "```python\n",
                "from deepresearcher.llm.llm_manager import generate_response, generate_boolean\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "\n",
                "def generate_initial_search_queries(user_query: str):\n",
                "    variables = {\"user_query\": user_query}\n",
                "    search_queries_str = generate_response(\"search_generator_system\", \"search_generator_user\", variables)\n",
                "    try:\n",
                "        queries = eval(search_queries_str)\n",
                "        if not isinstance(queries, list):\n",
                "            raise ValueError(\"Not a list\")\n",
                "        return queries\n",
                "    except Exception:\n",
                "        print(\"Invalid response for search queries:\", search_queries_str)\n",
                "        return []\n",
                "\n",
                "\n",
                "def perform_iterative_research(user_query: str, new_search_queries: list, all_search_queries: list, iteration_limit: int):\n",
                "    context_list = []\n",
                "\n",
                "    for query in new_search_queries:\n",
                "        results = search_and_fetch_markdown(query, max_results=3)\n",
                "        for page in results:\n",
                "            page_text = page[\"markdown\"]\n",
                "            # Truncate content for both relevance check and extraction\n",
                "            truncated_page_text = page_text[:20000] \n",
                "\n",
                "            if not truncated_page_text.strip():\n",
                "                continue\n",
                "\n",
                "            # 1. Relevance Check (Uses keys: user_query, page_text)\n",
                "            relevance_vars = {\n",
                "                \"user_query\": user_query,\n",
                "                \"page_text\": truncated_page_text\n",
                "            }\n",
                "            is_useful = generate_boolean(\"relevance_evaluator_system\", \"relevance_evaluator_user\", relevance_vars)\n",
                "            \n",
                "            # Print decision based on relevance check\n",
                "            status = \"KEEPING (Relevant)\" if is_useful else \"FILTERING OUT (Not Relevant)\"\n",
                "            print(f\"--- Evaluation for: {page['url']}\")\n",
                "            print(f\"   Search Query: '{query}'\")\n",
                "            print(f\"   Decision: **{status}**\")\n",
                "            print(\"-\" * 50)\n",
                "\n",
                "\n",
                "            if is_useful:\n",
                "                # 2. Information Extraction (Only if deemed relevant)\n",
                "                \n",
                "                # TODO: Create a variables dictionary with user_query, search_query, and page_text (limited to 20000 chars)\n",
                "                # Create the variables for the Extractor prompt\n",
                "                extraction_vars = {\n",
                "                    \"user_query\": user_query,\n",
                "                    \"search_query\": query, # Provide the specific query used to find the page\n",
                "                    \"page_text\": truncated_page_text \n",
                "                }\n",
                "                \n",
                "                # TODO: Call generate_response with the extractor prompts and your variables dictionary\n",
                "                # Use generate_response for the extraction task\n",
                "                extracted_context = generate_response(\n",
                "                    system_prompt_name=\"information_extractor_system\",\n",
                "                    user_prompt_name=\"information_extractor_user\",\n",
                "                    variables=extraction_vars\n",
                "                ).strip()\n",
                "                \n",
                "                # TODO: If the extracted context exists, add it to the context_list\n",
                "                # Check for the \"No relevant information found\" output and ensure context is not empty\n",
                "                if extracted_context and extracted_context != \"No relevant information found.\":\n",
                "                    # Append a dictionary or simple string, choosing simple string for this pipeline context\n",
                "                    context_list.append(extracted_context)\n",
                "                    print(f\"   ✅ Extracted context added.\")\n",
                "                else:\n",
                "                    print(f\"   ❌ Extractor returned no valid context.\")\n",
                "            \n",
                "    \n",
                "    # TODO: Return the list of extracted contexts\n",
                "    # Return the full list of focused context blocks\n",
                "    return context_list\n",
                "\n",
                "\n",
                "def generate_final_report(user_query: str, contexts: list):\n",
                "    # This function would use the 'contexts' list to generate the final answer\n",
                "    print(\"\\n--- Final Contexts Collected ---\")\n",
                "    for i, context in enumerate(contexts):\n",
                "        print(f\"\\n[Context Block {i+1}]\")\n",
                "        print(context)\n",
                "        print(\"-------------------------------\")\n",
                "    print(f\"\\nTotal context blocks: {len(contexts)}\")\n",
                "\n",
                "\n",
                "def research_main():\n",
                "    user_query = input(\"Enter your research query/topic: \").strip()\n",
                "    iteration_limit = input(\"Max number of iterations (default 10): \").strip()\n",
                "    iteration_limit = int(iteration_limit) if iteration_limit.isdigit() else 10\n",
                "\n",
                "    clear_visited_pages()\n",
                "    new_search_queries = generate_initial_search_queries(user_query)\n",
                "    if not new_search_queries:\n",
                "        return\n",
                "\n",
                "    all_search_queries = new_search_queries.copy()\n",
                "    print(\"\\nStarting Research and Extraction Pipeline...\")\n",
                "    aggregated_contexts = perform_iterative_research(user_query, new_search_queries, all_search_queries, iteration_limit)\n",
                "    generate_final_report(user_query, aggregated_contexts)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    research_main()\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Gold from Web Content"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
