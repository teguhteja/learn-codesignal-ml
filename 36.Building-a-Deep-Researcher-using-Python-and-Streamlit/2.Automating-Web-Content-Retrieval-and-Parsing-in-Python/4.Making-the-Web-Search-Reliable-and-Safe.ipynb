{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 4"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Making the Web Search Reliable and Safe\n",
                "\n",
                "## Introduction: The Need for Reliable and Safe Web Search\n",
                "\n",
                "Welcome back\\! In the last lesson, you learned how to avoid common pitfalls in your web searcher, such as duplicate results and broken links. Now, we will take your skills a step further by making your web searcher more **reliable and safe**.\n",
                "\n",
                "When you automate web content retrieval, you will often face problems like **network errors**, slow responses, or even accidentally processing the same page more than once. If you don’t handle these issues, your research tool might miss important information or waste time and resources.\n",
                "\n",
                "In this lesson, you will learn how to:\n",
                "\n",
                "  * Automatically **retry failed web requests**,\n",
                "  * And use **logging** to monitor what happens during your web search.\n",
                "\n",
                "By the end of this lesson, you will have a web searcher that is much more **robust** and ready for real-world use.\n",
                "\n",
                "-----\n",
                "\n",
                "## Retrying Failed Requests with Tenacity\n",
                "\n",
                "When you fetch web pages, sometimes things go wrong. The website might be slow, your internet connection might drop, or the server might return an error. If you don’t handle these problems, your program could crash or miss important data.\n",
                "\n",
                "To solve this, you can use the **`tenacity`** library. This library lets you automatically retry a function if it fails due to certain errors, such as timeouts or connection problems.\n",
                "\n",
                "Let’s start by importing the necessary modules and setting up a simple retry mechanism.\n",
                "\n",
                "```python\n",
                "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
                "from httpx import TimeoutException, RequestError, HTTPStatusError\n",
                "import httpx\n",
                "```\n",
                "\n",
                "Here’s what each import does:\n",
                "\n",
                "  * **`tenacity`** provides decorators and tools for retrying functions.\n",
                "  * **`httpx`** is the library we use to make HTTP requests.\n",
                "  * The exception types (`TimeoutException`, `RequestError`, `HTTPStatusError`) help us specify which errors should trigger a retry.\n",
                "\n",
                "Now, let’s create a function that fetches a web page and automatically retries if it fails due to a network error or timeout.\n",
                "\n",
                "```python\n",
                "@retry(\n",
                "    stop=stop_after_attempt(3),\n",
                "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
                "    retry=(\n",
                "        retry_if_exception_type(TimeoutException) |\n",
                "        retry_if_exception_type(RequestError) |\n",
                "        retry_if_exception_type(HTTPStatusError)\n",
                "    ),\n",
                "    reraise=True\n",
                ")\n",
                "def fetch_page(url: str, timeout: int = 10) -> str:\n",
                "    with httpx.Client(timeout=timeout, follow_redirects=True) as client:\n",
                "        response = client.get(url)\n",
                "        response.raise_for_status()\n",
                "        return response.text\n",
                "```\n",
                "\n",
                "Let’s break down what’s happening here:\n",
                "\n",
                "  * The **`@retry`** decorator tells Python to retry the `fetch_page` function up to **3 times** if it fails due to a timeout, connection error, or HTTP error.\n",
                "  * **`wait_exponential`** means the wait time between retries will grow each time (2 seconds, then 4, then 8, up to 10 seconds).\n",
                "  * Inside the function, we use `httpx.Client` to fetch the web page. If the request fails, the function will automatically retry.\n",
                "\n",
                "### Example Output:\n",
                "\n",
                "If the first attempt fails due to a timeout, you might see:\n",
                "\n",
                "```\n",
                "Retrying fetch_page in 2 seconds as it raised TimeoutException...\n",
                "Retrying fetch_page in 4 seconds as it raised TimeoutException...\n",
                "```\n",
                "\n",
                "If the third attempt succeeds, the function returns the page content. If all attempts fail, the error is raised.\n",
                "\n",
                "-----\n",
                "\n",
                "## Logging: Monitoring Successes and Failures\n",
                "\n",
                "When your program runs, it’s helpful to know what’s happening — especially when things go wrong. **Logging** lets you record messages about what your program is doing, which can help you **debug problems** or understand how your code is working.\n",
                "\n",
                "Python’s built-in **`logging`** module makes this easy.\n",
                "\n",
                "Let’s set up basic logging:\n",
                "\n",
                "```python\n",
                "import logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "```\n",
                "\n",
                "  * This sets the logging level to **`INFO`**, so you’ll see informational messages and warnings.\n",
                "  * The format makes it clear what type of message is being logged.\n",
                "\n",
                "Now, let’s add logging to our web searcher. For example, you can log warnings when a request fails:\n",
                "\n",
                "```python\n",
                "try:\n",
                "    # Try to fetch and process the page\n",
                "    markdown = fetch_page(url)\n",
                "except TimeoutException:\n",
                "    logging.warning(f\"Timeout while fetching {url}\")\n",
                "except HTTPStatusError as e:\n",
                "    logging.warning(f\"HTTP error {e.response.status_code} for URL {url}\")\n",
                "except RequestError as e:\n",
                "    logging.warning(f\"Connection error for URL {url}: {str(e)}\")\n",
                "except Exception as e:\n",
                "    logging.warning(f\"Unexpected error for URL {url}: {str(e)}\")\n",
                "```\n",
                "\n",
                "  * If a timeout occurs, you log a warning with the URL.\n",
                "  * If there’s an HTTP error, you log the status code and URL.\n",
                "  * If there’s a connection error or any other unexpected error, you log those as well.\n",
                "\n",
                "### Example Output:\n",
                "\n",
                "```\n",
                "[WARNING] Timeout while fetching https://example.com/page1\n",
                "[WARNING] HTTP error 404 for URL https://example.com/page2\n",
                "[WARNING] Connection error for URL https://example.com/page3: Network unreachable\n",
                "```\n",
                "\n",
                "Logging helps you see what went wrong and where, making it much easier to fix problems.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and What’s Next\n",
                "\n",
                "In this lesson, you learned how to make your web searcher more reliable and safe by:\n",
                "\n",
                "  * Retrying failed requests automatically with the **`tenacity`** library,\n",
                "  * And using **logging** to monitor successes and failures.\n",
                "\n",
                "These improvements will help your automated research tool handle real-world problems and give you better control over what happens during web searches.\n",
                "\n",
                "Next, you’ll get a chance to practice these skills with hands-on exercises. This will help you reinforce what you’ve learned and prepare you for building even more advanced features. Keep up the great work\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Adding Logging to Your Web Searcher\n",
                "\n",
                "Now that you've learned about the importance of logging in web search applications, let's put this knowledge into practice! In this exercise, you'll implement basic logging in your web searcher module to help track what happens during web searches.\n",
                "\n",
                "Logging is essential for debugging and monitoring your program's behavior, especially when dealing with network operations that might fail.\n",
                "\n",
                "Your tasks are:\n",
                "\n",
                "Add a logging configuration at the top of the file that sets the log level to INFO and uses a format that clearly shows the log level and message.\n",
                "\n",
                "Add a log statement at the beginning of the fetch_and_convert_to_markdown function to indicate when a URL fetch starts.\n",
                "\n",
                "Add a warning when an Exception is caught inside search_and_fetch_markdown.\n",
                "\n",
                "When you run your code after these changes, you'll see helpful log messages in your console that show when URL fetches begin and any warnings about errors. This visibility into your program's execution will make troubleshooting much easier as your web searcher becomes more complex.\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "import logging\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "from httpx import TimeoutException, RequestError, HTTPStatusError\n",
                "\n",
                "# TODO: Add logging configuration here to set the log level to INFO and specify a format\n",
                "# that shows the log level and message\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def clear_visited_pages() -> None:\n",
                "    \"\"\"Clear the global set of visited pages.\"\"\"\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "def fetch_and_convert_to_markdown(url: str, timeout: int = 10) -> str:\n",
                "    \"\"\"Fetch the HTML content of a URL and convert it to Markdown. Retries on errors.\"\"\"\n",
                "    # TODO: Add a log statement here to indicate the start of a URL fetch\n",
                "    with httpx.Client(timeout=timeout, follow_redirects=True) as client:\n",
                "        response = client.get(url)\n",
                "        response.raise_for_status()\n",
                "        html = response.text\n",
                "        return convert_to_markdown(html)\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    region: str = \"wt-wt\",\n",
                "    safesearch: str = \"moderate\",\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Perform a DuckDuckGo search and return top results converted to Markdown,\n",
                "    skipping already-visited pages.\n",
                "\n",
                "    Returns a list of dicts: [{\"title\": ..., \"url\": ..., \"markdown\": ...}, ...]\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(\n",
                "        query,\n",
                "        region=region,\n",
                "        safesearch=safesearch,\n",
                "        max_results=max_results\n",
                "    )\n",
                "\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue\n",
                "\n",
                "        try:\n",
                "            markdown = fetch_and_convert_to_markdown(url, timeout=timeout)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "\n",
                "        except Exception as e:\n",
                "            # TODO: Add a warning here\n",
                "            pass\n",
                "\n",
                "        _visited_pages.add(url)\n",
                "\n",
                "    return markdown_pages\n",
                "\n",
                "```\n",
                "\n",
                "Hello\\! That's a great exercise. Implementing logging is a crucial step in building robust applications.\n",
                "\n",
                "Here is the updated Python code with the logging configuration and statements added according to your tasks:\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "import logging\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "# Assuming html_to_markdown is a local module\n",
                "from html_to_markdown import convert_to_markdown \n",
                "from httpx import TimeoutException, RequestError, HTTPStatusError\n",
                "\n",
                "# TODO: Add logging configuration here to set the log level to INFO and specify a format\n",
                "# that shows the log level and message\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def clear_visited_pages() -> None:\n",
                "    \"\"\"Clear the global set of visited pages.\"\"\"\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "def fetch_and_convert_to_markdown(url: str, timeout: int = 10) -> str:\n",
                "    \"\"\"Fetch the HTML content of a URL and convert it to Markdown. Retries on errors.\"\"\"\n",
                "    # TODO: Add a log statement here to indicate the start of a URL fetch\n",
                "    logging.info(f\"Starting fetch for URL: {url}\")\n",
                "    with httpx.Client(timeout=timeout, follow_redirects=True) as client:\n",
                "        response = client.get(url)\n",
                "        response.raise_for_status()\n",
                "        html = response.text\n",
                "        return convert_to_markdown(html)\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    region: str = \"wt-wt\",\n",
                "    safesearch: str = \"moderate\",\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Perform a DuckDuckGo search and return top results converted to Markdown,\n",
                "    skipping already-visited pages.\n",
                "\n",
                "    Returns a list of dicts: [{\"title\": ..., \"url\": ..., \"markdown\": ...}, ...]\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(\n",
                "        query,\n",
                "        region=region,\n",
                "        safesearch=safesearch,\n",
                "        max_results=max_results\n",
                "    )\n",
                "\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue\n",
                "\n",
                "        try:\n",
                "            markdown = fetch_and_convert_to_markdown(url, timeout=timeout)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "\n",
                "        except Exception as e:\n",
                "            # TODO: Add a warning here\n",
                "            logging.warning(f\"Failed to process URL: {url}. Error: {type(e).__name__} - {e}\")\n",
                "            pass\n",
                "\n",
                "        _visited_pages.add(url)\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "### Summary of Changes:\n",
                "\n",
                "1.  **Logging Configuration (Top of file):**\n",
                "\n",
                "    ```python\n",
                "    logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "    ```\n",
                "\n",
                "    This sets the base logging level to **INFO** and ensures the log output includes the **level name** (e.g., `[INFO]`) and the **message**.\n",
                "\n",
                "2.  **Start of URL Fetch (Inside `fetch_and_convert_to_markdown`):**\n",
                "\n",
                "    ```python\n",
                "    logging.info(f\"Starting fetch for URL: {url}\")\n",
                "    ```\n",
                "\n",
                "    This **`INFO`** level statement clearly indicates when the function begins processing a new URL.\n",
                "\n",
                "3.  **Error Warning (Inside `search_and_fetch_markdown`'s `except` block):**\n",
                "\n",
                "    ```python\n",
                "    logging.warning(f\"Failed to process URL: {url}. Error: {type(e).__name__} - {e}\")\n",
                "    ```\n",
                "\n",
                "    This **`WARNING`** level statement reports any failure during the fetch/conversion process, providing the **URL** that failed and the **type and message of the exception** caught."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Handling Web Errors Like a Pro\n",
                "\n",
                "Cosmo\n",
                "Just now\n",
                "Read message aloud\n",
                "Now that you've set up basic logging in your web searcher, let's improve the error handling to make your tool even more reliable! In this exercise, you'll add specific exception handlers to gracefully manage different types of errors that can occur during web searches.\n",
                "\n",
                "When fetching content from the web, many things can go wrong — servers might time out, return error codes, or network connections might fail. Proper error handling ensures that your program continues running despite these issues.\n",
                "\n",
                "Your tasks are:\n",
                "\n",
                "Add a specific exception handler for TimeoutException that logs a warning with the URL.\n",
                "Add a specific exception handler for HTTPStatusError that logs the status code and URL.\n",
                "Add a specific exception handler for RequestError that logs connection details and the URL.\n",
                "By implementing these specific error handlers, your web searcher will be more robust and provide clearer feedback about what went wrong during searches. This makes debugging easier and helps you understand which websites are causing problems.\n",
                "\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "import logging\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "from httpx import TimeoutException, RequestError, HTTPStatusError\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def clear_visited_pages() -> None:\n",
                "    \"\"\"Clear the global set of visited pages.\"\"\"\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "def fetch_and_convert_to_markdown(url: str, timeout: int = 10) -> str:\n",
                "    \"\"\"Fetch the HTML content of a URL and convert it to Markdown. Retries on errors.\"\"\"\n",
                "    logging.info(f\"Fetching URL: {url}\")\n",
                "    with httpx.Client(timeout=timeout, follow_redirects=True) as client:\n",
                "        response = client.get(url)\n",
                "        response.raise_for_status()\n",
                "        html = response.text\n",
                "        return convert_to_markdown(html)\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    region: str = \"wt-wt\",\n",
                "    safesearch: str = \"moderate\",\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Perform a DuckDuckGo search and return top results converted to Markdown,\n",
                "    skipping already-visited pages.\n",
                "\n",
                "    Returns a list of dicts: [{\"title\": ..., \"url\": ..., \"markdown\": ...}, ...]\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(\n",
                "        query,\n",
                "        region=region,\n",
                "        safesearch=safesearch,\n",
                "        max_results=max_results\n",
                "    )\n",
                "\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue\n",
                "\n",
                "        try:\n",
                "            markdown = fetch_and_convert_to_markdown(url, timeout=timeout)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "\n",
                "        # TODO: Add a specific except clause for TimeoutException that logs a warning with the URL\n",
                "        \n",
                "        # TODO: Add a specific except clause for HTTPStatusError that logs a warning with the status code and URL\n",
                "        \n",
                "        # TODO: Add a specific except clause for RequestError that logs a warning with the URL and error message\n",
                "        \n",
                "        except Exception as e:\n",
                "            logging.warning(f\"Unexpected error for URL {url}: {str(e)}\")\n",
                "\n",
                "        _visited_pages.add(url)\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "That's a smart move\\! Using specific exception handlers is the best way to handle web errors, as it allows for precise logging and recovery.\n",
                "\n",
                "Here is the updated Python code with the specific `httpx` exception handlers implemented in the `search_and_fetch_markdown` function.\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "import logging\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "from httpx import TimeoutException, RequestError, HTTPStatusError\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def clear_visited_pages() -> None:\n",
                "    \"\"\"Clear the global set of visited pages.\"\"\"\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "def fetch_and_convert_to_markdown(url: str, timeout: int = 10) -> str:\n",
                "    \"\"\"Fetch the HTML content of a URL and convert it to Markdown. Retries on errors.\"\"\"\n",
                "    logging.info(f\"Fetching URL: {url}\")\n",
                "    with httpx.Client(timeout=timeout, follow_redirects=True) as client:\n",
                "        response = client.get(url)\n",
                "        response.raise_for_status()\n",
                "        html = response.text\n",
                "        return convert_to_markdown(html)\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    region: str = \"wt-wt\",\n",
                "    safesearch: str = \"moderate\",\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Perform a DuckDuckGo search and return top results converted to Markdown,\n",
                "    skipping already-visited pages.\n",
                "\n",
                "    Returns a list of dicts: [{\"title\": ..., \"url\": ..., \"markdown\": ...}, ...]\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(\n",
                "        query,\n",
                "        region=region,\n",
                "        safesearch=safesearch,\n",
                "        max_results=max_results\n",
                "    )\n",
                "\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue\n",
                "\n",
                "        try:\n",
                "            markdown = fetch_and_convert_to_markdown(url, timeout=timeout)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "\n",
                "        # ✅ TODO: Add a specific except clause for TimeoutException that logs a warning with the URL\n",
                "        except TimeoutException:\n",
                "            logging.warning(f\"Timeout while fetching URL: {url}\")\n",
                "\n",
                "        # ✅ TODO: Add a specific except clause for HTTPStatusError that logs a warning with the status code and URL\n",
                "        except HTTPStatusError as e:\n",
                "            logging.warning(f\"HTTP Error {e.response.status_code} for URL: {url}\")\n",
                "        \n",
                "        # ✅ TODO: Add a specific except clause for RequestError that logs a warning with the URL and error message\n",
                "        except RequestError as e:\n",
                "            logging.warning(f\"Connection error for URL {url}: {str(e)}\")\n",
                "        \n",
                "        except Exception as e:\n",
                "            logging.warning(f\"Unexpected error for URL {url}: {str(e)}\")\n",
                "\n",
                "        _visited_pages.add(url)\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "### Explanation of Changes\n",
                "\n",
                "The `try...except` block is now ordered from most specific to least specific:\n",
                "\n",
                "1.  **`except TimeoutException:`** Handles cases where the server takes too long to respond.\n",
                "    ```python\n",
                "    logging.warning(f\"Timeout while fetching URL: {url}\")\n",
                "    ```\n",
                "2.  **`except HTTPStatusError as e:`** Handles non-successful HTTP status codes (e.g., 404 Not Found, 500 Server Error).\n",
                "    ```python\n",
                "    logging.warning(f\"HTTP Error {e.response.status_code} for URL: {url}\")\n",
                "    ```\n",
                "3.  **`except RequestError as e:`** Catches various network/request problems that aren't timeouts or bad status codes (e.g., DNS resolution failure, connection reset, SSL errors).\n",
                "    ```python\n",
                "    logging.warning(f\"Connection error for URL {url}: {str(e)}\")\n",
                "    ```\n",
                "4.  **`except Exception as e:`** Remains as a final catch-all for any other unforeseen issues, ensuring the program doesn't crash.\n",
                "\n",
                "This structure provides **clear, actionable feedback** in the logs, which is a hallmark of a robust web searcher. 👍"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Automatic Retries for Web Requests\n",
                "\n",
                "Cosmo\n",
                "Just now\n",
                "Read message aloud\n",
                "Now that you've set up logging and learned about error handling, let's focus on making your web searcher more resilient! In this exercise, you'll implement automatic retrying for failed web requests using the tenacity library.\n",
                "\n",
                "The internet is unpredictable — websites can be slow, connections can drop, and servers can return errors. Without proper retry mechanisms, your web searcher might miss important information when these issues occur.\n",
                "\n",
                "Your task is to add retry functionality to the fetch_and_convert_to_markdown function by:\n",
                "\n",
                "Applying the @retry decorator to the function\n",
                "Configuring it to retry up to 3 times when specific exceptions occur\n",
                "Setting up exponential wait times between retries\n",
                "To test your implementation, try setting a very low timeout value (such as 0.1 seconds) to simulate network problems. You should see retry attempts in the logs before the function either succeeds or gives up after 3 tries.\n",
                "\n",
                "This improvement will make your web searcher much more reliable when dealing with the unpredictable nature of the internet!\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "import logging\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
                "from httpx import TimeoutException, RequestError, HTTPStatusError\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def clear_visited_pages() -> None:\n",
                "    \"\"\"Clear the global set of visited pages.\"\"\"\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "# TODO: Add the @retry decorator here with the following configuration:\n",
                "# - stop after 3 attempts\n",
                "# - use exponential wait with multiplier=1, min=2, max=10\n",
                "# - set reraise=True to raise the last exception if all retries fail\n",
                "def fetch_and_convert_to_markdown(url: str, timeout: int = 10) -> str:\n",
                "    \"\"\"Fetch the HTML content of a URL and convert it to Markdown. Retries on errors.\"\"\"\n",
                "    logging.info(f\"Fetching URL: {url}\")\n",
                "    with httpx.Client(timeout=timeout, follow_redirects=True) as client:\n",
                "        response = client.get(url)\n",
                "        response.raise_for_status()\n",
                "        html = response.text\n",
                "        return convert_to_markdown(html)\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    region: str = \"wt-wt\",\n",
                "    safesearch: str = \"moderate\",\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Perform a DuckDuckGo search and return top results converted to Markdown,\n",
                "    skipping already-visited pages.\n",
                "\n",
                "    Returns a list of dicts: [{\"title\": ..., \"url\": ..., \"markdown\": ...}, ...]\n",
                "    \"\"\"\n",
                "    try:\n",
                "        ddgs = DDGS()\n",
                "        results = ddgs.text(\n",
                "            query,\n",
                "            region=region,\n",
                "            safesearch=safesearch,\n",
                "            max_results=max_results\n",
                "        )\n",
                "    except Exception as e:\n",
                "        logging.warning(f\"Unexpected error for DDGS: {str(e)}\")\n",
                "        results = []\n",
                "\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue\n",
                "\n",
                "        try:\n",
                "            markdown = fetch_and_convert_to_markdown(url, timeout=timeout)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "\n",
                "        except TimeoutException:\n",
                "            logging.warning(f\"Timeout while fetching {url}\")\n",
                "\n",
                "        except HTTPStatusError as e:\n",
                "            logging.warning(f\"HTTP error {e.response.status_code} for URL {url}\")\n",
                "\n",
                "        except RequestError as e:\n",
                "            logging.warning(f\"Connection error for URL {url}: {str(e)}\")\n",
                "\n",
                "        except Exception as e:\n",
                "            logging.warning(f\"Unexpected error for URL {url}: {str(e)}\")\n",
                "\n",
                "        _visited_pages.add(url)\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "That's an excellent next step\\! Implementing automatic retries using `tenacity` will significantly boost your web searcher's **resilience** against transient network issues.\n",
                "\n",
                "Here is the updated Python code with the `@retry` decorator applied to the `fetch_and_convert_to_markdown` function.\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "import logging\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
                "from httpx import TimeoutException, RequestError, HTTPStatusError\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def clear_visited_pages() -> None:\n",
                "    \"\"\"Clear the global set of visited pages.\"\"\"\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "# ✅ TODO: Add the @retry decorator here with the following configuration:\n",
                "# - stop after 3 attempts\n",
                "# - use exponential wait with multiplier=1, min=2, max=10\n",
                "# - set reraise=True to raise the last exception if all retries fail\n",
                "@retry(\n",
                "    stop=stop_after_attempt(3),\n",
                "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
                "    retry=(\n",
                "        retry_if_exception_type(TimeoutException) |\n",
                "        retry_if_exception_type(RequestError) |\n",
                "        retry_if_exception_type(HTTPStatusError)\n",
                "    ),\n",
                "    reraise=True\n",
                ")\n",
                "def fetch_and_convert_to_markdown(url: str, timeout: int = 10) -> str:\n",
                "    \"\"\"Fetch the HTML content of a URL and convert it to Markdown. Retries on errors.\"\"\"\n",
                "    logging.info(f\"Fetching URL: {url}\")\n",
                "    with httpx.Client(timeout=timeout, follow_redirects=True) as client:\n",
                "        response = client.get(url)\n",
                "        response.raise_for_status()\n",
                "        html = response.text\n",
                "        return convert_to_markdown(html)\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    region: str = \"wt-wt\",\n",
                "    safesearch: str = \"moderate\",\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Perform a DuckDuckGo search and return top results converted to Markdown,\n",
                "    skipping already-visited pages.\n",
                "\n",
                "    Returns a list of dicts: [{\"title\": ..., \"url\": ..., \"markdown\": ...}, ...]\n",
                "    \"\"\"\n",
                "    try:\n",
                "        ddgs = DDGS()\n",
                "        results = ddgs.text(\n",
                "            query,\n",
                "            region=region,\n",
                "            safesearch=safesearch,\n",
                "            max_results=max_results\n",
                "        )\n",
                "    except Exception as e:\n",
                "        logging.warning(f\"Unexpected error for DDGS: {str(e)}\")\n",
                "        results = []\n",
                "\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue\n",
                "\n",
                "        try:\n",
                "            markdown = fetch_and_convert_to_markdown(url, timeout=timeout)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "\n",
                "        # The specific exceptions are caught here AFTER the retries have been exhausted\n",
                "        except TimeoutException:\n",
                "            logging.warning(f\"Timeout while fetching {url}\")\n",
                "\n",
                "        except HTTPStatusError as e:\n",
                "            # Note: HTTPStatusError is included in the retry condition, so this only\n",
                "            # executes if the error persists after 3 attempts.\n",
                "            logging.warning(f\"HTTP error {e.response.status_code} for URL {url}\")\n",
                "\n",
                "        except RequestError as e:\n",
                "            logging.warning(f\"Connection error for URL {url}: {str(e)}\")\n",
                "\n",
                "        except Exception as e:\n",
                "            logging.warning(f\"Unexpected error for URL {url}: {str(e)}\")\n",
                "\n",
                "        _visited_pages.add(url)\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "### Key Retrying Configuration:\n",
                "\n",
                "1.  **`stop=stop_after_attempt(3)`**: Ensures the function will run a maximum of **3 times** (initial attempt + 2 retries).\n",
                "2.  **`wait=wait_exponential(multiplier=1, min=2, max=10)`**: Sets an increasing (exponential) backoff delay between retries: 2 seconds, then 4 seconds, etc., up to a 10-second maximum. This prevents overwhelming a slow server.\n",
                "3.  **`retry=(...)`**: Specifies that retries should only happen for the network-related errors (`TimeoutException`, `RequestError`, `HTTPStatusError`).\n",
                "4.  **`reraise=True`**: Crucially, this setting ensures that if all 3 attempts fail, the final exception is **re-raised** and then caught by your specific `except` blocks in `search_and_fetch_markdown` for appropriate logging."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Specify when to retry with Tenacity\n",
                "\n",
                "You've already set up the @retry decorator, configured the number of attempts, and established exponential wait times for your web searcher's fetch_and_convert_to_markdown function. Now, let's make your retry logic even smarter!\n",
                "\n",
                "Your task is to specify which types of exceptions should trigger a retry. Update the @retry decorator on fetch_and_convert_to_markdown so that it only retries when one of the following exceptions occurs:\n",
                "\n",
                "TimeoutException\n",
                "RequestError\n",
                "HTTPStatusError\n",
                "To test your implementation, try setting a very low timeout value (such as 0.1 seconds) to simulate network problems. You should see retry attempts in the logs before the function either succeeds or gives up after 3 tries.\n",
                "\n",
                "By fine-tuning which errors cause a retry, you'll make your web searcher more efficient and robust in handling real-world web interactions!\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "import logging\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
                "from httpx import TimeoutException, RequestError, HTTPStatusError\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def clear_visited_pages() -> None:\n",
                "    \"\"\"Clear the global set of visited pages.\"\"\"\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "# TODO: Modify the @retry decorator here with the following configuration:\n",
                "# - retry on TimeoutException, RequestError, and HTTPStatusError\n",
                "@retry(\n",
                "    stop=stop_after_attempt(3),\n",
                "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
                "    reraise=True\n",
                ")\n",
                "def fetch_and_convert_to_markdown(url: str, timeout: int = 10) -> str:\n",
                "    \"\"\"Fetch the HTML content of a URL and convert it to Markdown. Retries on errors.\"\"\"\n",
                "    logging.info(f\"Fetching URL: {url}\")\n",
                "    with httpx.Client(timeout=timeout, follow_redirects=True) as client:\n",
                "        response = client.get(url)\n",
                "        response.raise_for_status()\n",
                "        html = response.text\n",
                "        return convert_to_markdown(html)\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    region: str = \"wt-wt\",\n",
                "    safesearch: str = \"moderate\",\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Perform a DuckDuckGo search and return top results converted to Markdown,\n",
                "    skipping already-visited pages.\n",
                "\n",
                "    Returns a list of dicts: [{\"title\": ..., \"url\": ..., \"markdown\": ...}, ...]\n",
                "    \"\"\"\n",
                "    try:\n",
                "        ddgs = DDGS()\n",
                "        results = ddgs.text(\n",
                "            query,\n",
                "            region=region,\n",
                "            safesearch=safesearch,\n",
                "            max_results=max_results\n",
                "        )\n",
                "    except Exception as e:\n",
                "        logging.warning(f\"Unexpected error for DDGS: {str(e)}\")\n",
                "        results = []\n",
                "\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue\n",
                "\n",
                "        try:\n",
                "            markdown = fetch_and_convert_to_markdown(url, timeout=timeout)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "\n",
                "        except TimeoutException:\n",
                "            logging.warning(f\"Timeout while fetching {url}\")\n",
                "\n",
                "        except HTTPStatusError as e:\n",
                "            logging.warning(f\"HTTP error {e.response.status_code} for URL {url}\")\n",
                "\n",
                "        except RequestError as e:\n",
                "            logging.warning(f\"Connection error for URL {url}: {str(e)}\")\n",
                "\n",
                "        except Exception as e:\n",
                "            logging.warning(f\"Unexpected error for URL {url}: {str(e)}\")\n",
                "\n",
                "        _visited_pages.add(url)\n",
                "\n",
                "    return markdown_pages\n",
                "\n",
                "```\n",
                "\n",
                "You're absolutely right\\! Specifying which errors trigger a retry is essential for efficient resource usage. You only want to retry for **transient errors** (like timeouts or temporary server unavailability), not permanent ones (like a programming mistake).\n",
                "\n",
                "Here is the updated Python code with the `retry` argument added to the `@retry` decorator.\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "import logging\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
                "from httpx import TimeoutException, RequestError, HTTPStatusError\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def clear_visited_pages() -> None:\n",
                "    \"\"\"Clear the global set of visited pages.\"\"\"\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "# ✅ TODO: Modify the @retry decorator here with the following configuration:\n",
                "# - retry on TimeoutException, RequestError, and HTTPStatusError\n",
                "@retry(\n",
                "    stop=stop_after_attempt(3),\n",
                "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
                "    retry=(\n",
                "        retry_if_exception_type(TimeoutException) |\n",
                "        retry_if_exception_type(RequestError) |\n",
                "        retry_if_exception_type(HTTPStatusError)\n",
                "    ),\n",
                "    reraise=True\n",
                ")\n",
                "def fetch_and_convert_to_markdown(url: str, timeout: int = 10) -> str:\n",
                "    \"\"\"Fetch the HTML content of a URL and convert it to Markdown. Retries on errors.\"\"\"\n",
                "    logging.info(f\"Fetching URL: {url}\")\n",
                "    with httpx.Client(timeout=timeout, follow_redirects=True) as client:\n",
                "        response = client.get(url)\n",
                "        response.raise_for_status()\n",
                "        html = response.text\n",
                "        return convert_to_markdown(html)\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    region: str = \"wt-wt\",\n",
                "    safesearch: str = \"moderate\",\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Perform a DuckDuckGo search and return top results converted to Markdown,\n",
                "    skipping already-visited pages.\n",
                "\n",
                "    Returns a list of dicts: [{\"title\": ..., \"url\": ..., \"markdown\": ...}, ...]\n",
                "    \"\"\"\n",
                "    try:\n",
                "        ddgs = DDGS()\n",
                "        results = ddgs.text(\n",
                "            query,\n",
                "            region=region,\n",
                "            safesearch=safesearch,\n",
                "            max_results=max_results\n",
                "        )\n",
                "    except Exception as e:\n",
                "        logging.warning(f\"Unexpected error for DDGS: {str(e)}\")\n",
                "        results = []\n",
                "\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue\n",
                "\n",
                "        try:\n",
                "            markdown = fetch_and_convert_to_markdown(url, timeout=timeout)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "\n",
                "        except TimeoutException:\n",
                "            logging.warning(f\"Timeout while fetching {url}\")\n",
                "\n",
                "        except HTTPStatusError as e:\n",
                "            logging.warning(f\"HTTP error {e.response.status_code} for URL {url}\")\n",
                "\n",
                "        except RequestError as e:\n",
                "            logging.warning(f\"Connection error for URL {url}: {str(e)}\")\n",
                "\n",
                "        except Exception as e:\n",
                "            logging.warning(f\"Unexpected error for URL {url}: {str(e)}\")\n",
                "\n",
                "        _visited_pages.add(url)\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "### Key Update:\n",
                "\n",
                "The essential change is the addition of the **`retry`** argument to the decorator:\n",
                "\n",
                "```python\n",
                "    retry=(\n",
                "        retry_if_exception_type(TimeoutException) |\n",
                "        retry_if_exception_type(RequestError) |\n",
                "        retry_if_exception_type(HTTPStatusError)\n",
                "    ),\n",
                "```\n",
                "\n",
                "This uses the `tenacity` utility **`retry_if_exception_type`** combined with the bitwise OR operator (`|`) to specify that a retry should occur if the raised exception matches **any** of the three specified `httpx` error types. This makes the retry logic highly targeted and efficient."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
