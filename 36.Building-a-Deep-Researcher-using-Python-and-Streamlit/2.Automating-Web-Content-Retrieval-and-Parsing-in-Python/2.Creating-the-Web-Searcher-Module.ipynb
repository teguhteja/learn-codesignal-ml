{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating the Web Searcher Module\n",
                "\n",
                "Tentu, ini terjemahan teks ke dalam Bahasa Inggris dalam format Markdown:\n",
                "\n",
                "# Creating a Web Searcher Module: Combining Searching, Fetching, and Markdown Conversion\n",
                "\n",
                "Welcome back\\! In the previous lesson, you learned how to use the **DDGS** library to search the web, fetch a single web page, and convert its content from HTML to Markdown. These are essential skills for building an automated research tool.\n",
                "\n",
                "In this lesson, we will take the next step by creating a **Web Searcher module**. This module will allow you to search for a topic, fetch the top web pages, and convert their content to Markdownâ€”all in one go. This is a key part of building a tool that can gather and process information from the web automatically.\n",
                "\n",
                "By the end of this lesson, you will know how to combine searching, fetching, and converting web content into a single, reusable function. This will make your code cleaner and more powerful, and prepare you for more advanced automation tasks later in the course.\n",
                "\n",
                "-----\n",
                "\n",
                "## Main Tools Used\n",
                "\n",
                "Before we dive in, letâ€™s quickly remind ourselves of the main tools we have used so far:\n",
                "\n",
                "  * **DDGS**: This library lets us perform web searches in Python and get results as structured data.\n",
                "  * **httpx**: This is a library for making HTTP requests, which we use to fetch web pages.\n",
                "  * **html\\_to\\_markdown**: This tool converts HTML content into Markdown, making it easier to read and process.\n",
                "\n",
                "You have already seen how to use each of these tools separately. Now, we will see how to use them together to automate the process of searching for and retrieving useful web content.\n",
                "\n",
                "-----\n",
                "\n",
                "## Building the `search_and_fetch_markdown` Function\n",
                "\n",
                "Now, letâ€™s put everything we learned together into a single function. We want a function that:\n",
                "\n",
                "1.  Takes a search query.\n",
                "2.  Searches the web for the top results.\n",
                "3.  Fetches each result's web page.\n",
                "4.  Converts the HTML to Markdown.\n",
                "5.  Returns a list of dictionaries, each with the title, URL, and Markdown content.\n",
                "\n",
                "Letâ€™s build this step by step.\n",
                "\n",
                "### Step 1: Define the Function and Set Up the Search\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "def search_and_fetch_markdown(query, max_results=5, timeout=10):\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(query, max_results=max_results)\n",
                "    markdown_pages = []\n",
                "```\n",
                "\n",
                "  * We import the needed libraries.\n",
                "  * The function takes a `query`, and optional `max_results` and `timeout`.\n",
                "  * We create a `DDGS` object and perform the search.\n",
                "  * We prepare an empty list to store the results.\n",
                "\n",
                "### Step 2: Loop Through Results and Fetch Content\n",
                "\n",
                "```python\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "        \n",
                "        if not url:\n",
                "            continue\n",
                "            \n",
                "        try:\n",
                "            response = httpx.get(url, timeout=timeout, follow_redirects=True)\n",
                "            response.raise_for_status()\n",
                "            \n",
                "            markdown = convert_to_markdown(response.text)\n",
                "            \n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "            \n",
                "        except Exception as e:\n",
                "            # Optional: Print or log the error for debugging\n",
                "            print(f\"Error fetching page: {url}. Error: {e}\")\n",
                "            \n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "  * For each result, we get the **URL** (`href`) and **title**.\n",
                "  * If the URL is missing, we skip it (`continue`).\n",
                "  * We use a `try` block to handle errors. If fetching or converting fails, we continue to the next result.\n",
                "  * If successful, we convert the HTML to Markdown and add the result to our list.\n",
                "  * At the end, the function returns a list of dictionaries.\n",
                "\n",
                "-----\n",
                "\n",
                "## Example Output\n",
                "\n",
                "If you call the function like this:\n",
                "\n",
                "```python\n",
                "pages = search_and_fetch_markdown(\"python web scraping\", max_results=2)\n",
                "\n",
                "for page in pages:\n",
                "    print(f\"Title: {page['title']}\")\n",
                "    print(f\"URL: {page['url']}\")\n",
                "    print(f\"Markdown (first 100 chars):\\n{page['markdown'][:100]}\")\n",
                "    print(\"-\" * 40)\n",
                "```\n",
                "\n",
                "You might see output like:\n",
                "\n",
                "```\n",
                "Title: Web Scraping with Python - Real Python\n",
                "URL: https://realpython.com/python-web-scraping/\n",
                "Markdown (first 100 chars):\n",
                "# Web Scraping With Python: Collecting Data From the Modern WebWeb scraping is the process of program...\n",
                "----------------------------------------\n",
                "Title: Python Web Scraping Tutorial - GeeksforGeeks\n",
                "URL: https://www.geeksforgeeks.org/python-web-scraping-tutorial/\n",
                "Markdown (first 100 chars):\n",
                "# Python Web Scraping TutorialWeb scraping is a technique to extract data from websites. In this tutoria...\n",
                "----------------------------------------\n",
                "```\n",
                "\n",
                "This shows that your function is working: it searches, fetches, and converts web pages to Markdown.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and What's Next\n",
                "\n",
                "In this lesson, you learned how to build a **Web Searcher module** that can:\n",
                "\n",
                "  * Search the web for a topic.\n",
                "  * Fetch the top web pages from the results.\n",
                "  * Convert each page's HTML content to Markdown.\n",
                "  * Return all this information in a structured way.\n",
                "\n",
                "This function is a powerful building block for your automated research tool. In the next practice exercises, you will get hands-on experience using and modifying this function. You will practice searching for different topics, handling errors, and working with the Markdown output.\n",
                "\n",
                "Great job making it this farâ€”let's move on to the practice and solidify your skills\\! ðŸ’ª"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building the Web Searcher Function\n",
                "\n",
                "Now that you've learned about the individual components of web searching, it's time to bring everything together! In this exercise, you'll build a function called search_and_fetch_markdown that automates the process of searching the web and retrieving readable content.\n",
                "\n",
                "Your task:\n",
                "\n",
                "Write a function search_and_fetch_markdown(query: str) that:\n",
                "Uses the DDGS library to perform a web search for the given query.\n",
                "Gets the URL of the top search result.\n",
                "Fetches the HTML content of that URL using fetch_web_page.\n",
                "Converts the HTML content to Markdown using html_to_markdown_converter.\n",
                "Returns the resulting Markdown string.\n",
                "If any step fails (e.g., no results, fetch error), return a string describing the error.\n",
                "\n",
                "Follow the TODO comments in the starter code to implement each part of the function. When you're done, you'll have created a powerful tool that can gather information from multiple web sources with just one function call â€” an essential building block for your automated research toolkit!\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "\n",
                "def fetch_web_page(url):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        timeout (int): Maximum time to wait for response in seconds\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message\n",
                "    \"\"\"\n",
                "    try:\n",
                "        response = httpx.get(url)\n",
                "        response.raise_for_status()  # Raise an exception for 4XX/5XX responses\n",
                "        return response.text\n",
                "    except Exception as e:\n",
                "        return f\"Error fetching URL {url}: {e}\"\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    # Convert HTML to Markdown\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    \n",
                "    # Trim the Markdown content to the specified length\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    \n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(query: str):\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch the top result, and return its Markdown content.\n",
                "    \"\"\"\n",
                "    # TODO: Create a DDGS object\n",
                "    \n",
                "    # TODO: Perform a search and get the top result\n",
                "    \n",
                "    # TODO: If there are no results, return an error message\n",
                "    \n",
                "    # TODO: Get the URL from the top result\n",
                "    \n",
                "    # TODO: Fetch the HTML content using fetch_web_page\n",
                "    \n",
                "    # TODO: Convert the HTML to Markdown using html_to_markdown_converter\n",
                "    \n",
                "    # TODO: Return the Markdown string\n",
                "```\n",
                "\n",
                "Ini adalah implementasi dari fungsi `search_and_fetch_markdown` yang menggabungkan pencarian, pengambilan, dan konversi konten ke Markdown, berdasarkan *starter code* yang Anda berikan.\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "# Import the custom function from the previous context, assuming it exists\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "\n",
                "def fetch_web_page(url):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message prefixed with \"Error\"\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # A reasonable timeout is highly recommended in real-world use\n",
                "        response = httpx.get(url, timeout=10, follow_redirects=True) \n",
                "        response.raise_for_status()  # Raise an exception for 4XX/5XX responses\n",
                "        return response.text\n",
                "    except Exception as e:\n",
                "        return f\"Error fetching URL {url}: {e}\"\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    # Convert HTML to Markdown\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    \n",
                "    # Trim the Markdown content to the specified length\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    \n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(query: str):\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch the top result, and return its Markdown content.\n",
                "    \n",
                "    Args:\n",
                "        query (str): The search query to use.\n",
                "        \n",
                "    Returns:\n",
                "        str: The resulting Markdown string of the top page, or an error message.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # TODO: Create a DDGS object\n",
                "        ddgs = DDGS()\n",
                "        \n",
                "        # TODO: Perform a search and get the top result (using max_results=1)\n",
                "        # We use .text() which yields results as they are found, so we convert it to a list\n",
                "        results = list(ddgs.text(query, max_results=1))\n",
                "        \n",
                "        # TODO: If there are no results, return an error message\n",
                "        if not results:\n",
                "            return f\"Error: No search results found for query: '{query}'\"\n",
                "            \n",
                "        # Get the first (and only) result dictionary\n",
                "        top_result = results[0]\n",
                "        \n",
                "        # TODO: Get the URL from the top result\n",
                "        url = top_result.get(\"href\")\n",
                "        \n",
                "        if not url:\n",
                "            return \"Error: Top search result did not contain a valid URL.\"\n",
                "            \n",
                "        # TODO: Fetch the HTML content using fetch_web_page\n",
                "        html_content = fetch_web_page(url)\n",
                "        \n",
                "        # Check if fetch_web_page returned an error\n",
                "        if html_content.startswith(\"Error\"):\n",
                "            # The error message from fetch_web_page is the string we need to return\n",
                "            return html_content\n",
                "            \n",
                "        # TODO: Convert the HTML to Markdown using html_to_markdown_converter\n",
                "        markdown_content = html_to_markdown_converter(html_content)\n",
                "        \n",
                "        # TODO: Return the Markdown string\n",
                "        # Optionally, include the title and URL for context\n",
                "        title = top_result.get(\"title\", \"No Title\")\n",
                "        \n",
                "        return (f\"# Top Result for '{query}': {title}\\n\"\n",
                "                f\"URL: {url}\\n\\n\"\n",
                "                f\"---\\n\\n\"\n",
                "                f\"{markdown_content}...\") # Ellipsis to indicate content was trimmed\n",
                "                \n",
                "    except Exception as e:\n",
                "        # Catch any unexpected errors during the overall process (e.g., DDGS initialization)\n",
                "        return f\"Error during search_and_fetch process: {e}\"\n",
                "\n",
                "# --- Contoh Penggunaan (Opsional) ---\n",
                "\n",
                "# print(search_and_fetch_markdown(\"Linux compress pdf in terminal\"))\n",
                "# print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
                "# print(search_and_fetch_markdown(\"asdfghjkl random string that won't exist\"))\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Enhancing Web Searcher for Multiple Results\n",
                "\n",
                "Excellent work on building your first web searcher function! Now it's time to take it to the next level by enhancing it to handle multiple search results instead of just one.\n",
                "\n",
                "Your current function returns only the Markdown content from the top search result, but a truly useful research tool should gather information from multiple sources. In this exercise, you'll expand the function's capabilities.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Update the search to retrieve the top 3 results.\n",
                "Create a list to store information about each page.\n",
                "Loop through all search results instead of just processing the first one.\n",
                "For each result, fetch the web page and convert it to Markdown.\n",
                "Handle errors for individual pages without stopping the entire process.\n",
                "Return the list of Markdown strings.\n",
                "Follow the TODO comments in the starter code to implement these changes step by step.\n",
                "\n",
                "When you're done, you'll have a powerful tool that can gather information from multiple web sources with a single function call â€” a key component of any automated research system!\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "\n",
                "def fetch_web_page(url):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message\n",
                "    \"\"\"\n",
                "    try:\n",
                "        response = httpx.get(url)\n",
                "        response.raise_for_status()\n",
                "        return response.text\n",
                "    except Exception as e:\n",
                "        return f\"Error fetching URL {url}: {e}\"\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(query: str):\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch the top 3 results, and return their Markdown content as a list of strings.\n",
                "    \n",
                "    Args:\n",
                "        query (str): The search query\n",
                "        \n",
                "    Returns:\n",
                "        list: A list of Markdown strings, one for each result\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # TODO: Search for the top 3 results\n",
                "        ddgs = DDGS()\n",
                "        results = ddgs.text(query, max_results=1)\n",
                "        \n",
                "        # TODO: Create an empty list to store the markdown pages\n",
                "        \n",
                "        # TODO: Replace this single-result processing with a loop through all results\n",
                "        url = results[0].get(\"href\")\n",
                "        title = results[0].get(\"title\", \"\")\n",
                "        if not url:\n",
                "            return \"Top search result does not have a URL.\"\n",
                "            \n",
                "        html_content = fetch_web_page(url)\n",
                "        markdown = html_to_markdown_converter(html_content)\n",
                "        \n",
                "        return [markdown]\n",
                "    except Exception as e:\n",
                "        return [f\"Error: {e}\"]\n",
                "\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Adding a Parameter to control Multiple Results\n",
                "\n",
                "Excellent work on building the web searcher function! Now it's time to take it to the next level by enhancing it to handle multiple search results with a parameter.\n",
                "\n",
                "Your current function returns only the Markdown content from the top 3 search results, but a truly useful research tool should be flexible and handle different parameters for multiple sources. In this exercise, you'll expand the function's capabilities.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Add a new parameter to the function called max_results with a default of 5\n",
                "Use this new parameter in the function\n",
                "Update the docstring of the function to reflect the changes\n",
                "Follow the TODO comments in the starter code to implement these changes.\n",
                "\n",
                "When you're done, you'll have a much more powerful research tool that can gather information from multiple sources with just one function call!\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "\n",
                "def fetch_web_page(url):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message\n",
                "    \"\"\"\n",
                "    try:\n",
                "        response = httpx.get(url)\n",
                "        response.raise_for_status()\n",
                "        return response.text\n",
                "    except Exception as e:\n",
                "        return f\"Error fetching URL {url}: {e}\"\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "# TODO: Add a max_results parameter to the declaration of the function\n",
                "def search_and_fetch_markdown(query: str):\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch the top 3 results, and return their Markdown content.\n",
                "    \n",
                "    Args:\n",
                "        query (str): The search query\n",
                "        \n",
                "    Returns:\n",
                "        list: A list of Markdown strings, one for each result\n",
                "    \"\"\"\n",
                "    # TODO: Update the docstring of the function\n",
                "    try:\n",
                "        ddgs = DDGS()\n",
                "        # TODO: Use the new max_results parameter in the DDGS search\n",
                "        results = ddgs.text(query, max_results=3)\n",
                "        \n",
                "        markdown_pages = []\n",
                "        \n",
                "        for result in results:\n",
                "            url = result.get(\"href\")\n",
                "            if not url:\n",
                "                continue\n",
                "            try:\n",
                "                response = httpx.get(url, follow_redirects=True)\n",
                "                response.raise_for_status()\n",
                "                markdown = html_to_markdown_converter(response.text)\n",
                "                markdown_pages.append(markdown)\n",
                "            except Exception as e:\n",
                "                markdown_pages.append(f\"**Error fetching {url}:** {e}\")\n",
                "        return markdown_pages\n",
                "    except Exception as e:\n",
                "        return [f\"Error: {e}\"]\n",
                "```\n",
                "\n",
                "Tentu, berikut adalah fungsi `search_and_fetch_markdown` yang diperbarui dengan parameter `max_results` dan *docstring* yang disesuaikan.\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "\n",
                "def fetch_web_page(url):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Added a timeout for better robustness\n",
                "        response = httpx.get(url, timeout=10) \n",
                "        response.raise_for_status()\n",
                "        return response.text\n",
                "    except Exception as e:\n",
                "        return f\"Error fetching URL {url}: {e}\"\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "# TODO: Add a max_results parameter to the declaration of the function\n",
                "def search_and_fetch_markdown(query: str, max_results: int = 5):\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch the top results (up to max_results), \n",
                "    and return their Markdown content.\n",
                "    \n",
                "    Args:\n",
                "        query (str): The search query.\n",
                "        max_results (int, optional): The maximum number of search results to fetch. \n",
                "                                     Defaults to 5.\n",
                "        \n",
                "    Returns:\n",
                "        list: A list of Markdown strings (or error messages for failed fetches), \n",
                "              one for each result attempted.\n",
                "    \"\"\"\n",
                "    # TODO: Update the docstring of the function (Done above)\n",
                "    try:\n",
                "        ddgs = DDGS()\n",
                "        \n",
                "        # TODO: Use the new max_results parameter in the DDGS search\n",
                "        # Convert the generator to a list to ensure we only get up to max_results\n",
                "        results = list(ddgs.text(query, max_results=max_results))\n",
                "        \n",
                "        markdown_pages = []\n",
                "        \n",
                "        # Check if any results were returned\n",
                "        if not results:\n",
                "            return [f\"Notice: No search results found for query: '{query}'\"]\n",
                "            \n",
                "        for result in results:\n",
                "            url = result.get(\"href\")\n",
                "            title = result.get(\"title\", \"No Title\")\n",
                "            \n",
                "            if not url or not url.startswith('http'):\n",
                "                markdown_pages.append(f\"Notice: Skipped result '{title}' due to invalid URL.\")\n",
                "                continue\n",
                "                \n",
                "            try:\n",
                "                # Use a specific timeout for the request\n",
                "                response = httpx.get(url, follow_redirects=True, timeout=10) \n",
                "                response.raise_for_status()\n",
                "                \n",
                "                markdown = html_to_markdown_converter(response.text)\n",
                "                \n",
                "                # Add context (Title and URL) to the Markdown output\n",
                "                formatted_output = (\n",
                "                    f\"## Source: {title}\\n\"\n",
                "                    f\"**URL:** {url}\\n\\n\"\n",
                "                    f\"---\\n\\n\"\n",
                "                    f\"{markdown}...\"\n",
                "                )\n",
                "                markdown_pages.append(formatted_output)\n",
                "                \n",
                "            except Exception as e:\n",
                "                # Handle errors for individual page fetches gracefully\n",
                "                markdown_pages.append(f\"**Error fetching {url} ('{title}'):** {e}\")\n",
                "                \n",
                "        return markdown_pages\n",
                "        \n",
                "    except Exception as e:\n",
                "        # Catch critical errors during the DDGS search itself\n",
                "        return [f\"CRITICAL ERROR during DDGS search for '{query}': {e}\"]\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Adding Timeouts for Web Requests\n",
                "\n",
                "Now that you've enhanced your web searcher to handle multiple results, let's make it even more robust by adding timeout control! When working with web requests, timeouts are crucial â€” they prevent your program from hanging indefinitely when a website is slow to respond.\n",
                "\n",
                "In this exercise, you'll add timeout functionality to your web searcher module. This will give users control over how long the function waits for responses, making it more flexible for different network conditions.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Update the function signature to include a timeout parameter with a default value of 10 seconds.\n",
                "Pass this timeout parameter to the DDGS constructor when creating the search object.\n",
                "Use the same timeout parameter in the httpx.get() call when fetching web pages.\n",
                "Update the function's docstring to document the new parameter.\n",
                "Extend the code in app.py to test this functionality\n",
                "Follow the TODO comments in the starter code to implement these changes. When you're done, you'll have a more reliable web searcher that can adapt to different network speeds and connection qualities â€” an important feature for any tool that interacts with the web!\n",
                "\n",
                "Suggestions\n",
                "Give me an example\n",
                "Hint, please\n",
                "\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "\n",
                "def fetch_web_page(url):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message\n",
                "    \"\"\"\n",
                "    try:\n",
                "        response = httpx.get(url)\n",
                "        response.raise_for_status()\n",
                "        return response.text\n",
                "    except Exception as e:\n",
                "        return f\"Error fetching URL {url}: {e}\"\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "# TODO: Update the function signature to include a timeout parameter with default value of 10\n",
                "def search_and_fetch_markdown(query: str,  max_results: int = 5):\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    \n",
                "    Args:\n",
                "        query (str): The search query\n",
                "        max_results (int): Maximum number of results to fetch (default: 5)\n",
                "        \n",
                "    Returns:\n",
                "        list: A list of Markdown strings, one for each result\n",
                "    \"\"\"\n",
                "    # TODO: Update the docstring to reflect the changes\n",
                "    \n",
                "    try:\n",
                "        # TODO: Pass the timeout parameter to the DDGS constructor\n",
                "        ddgs = DDGS()\n",
                "        results = ddgs.text(query, max_results=max_results)\n",
                "        \n",
                "        markdown_pages = []\n",
                "        \n",
                "        for result in results:\n",
                "            url = result.get(\"href\")\n",
                "            if not url:\n",
                "                continue\n",
                "            try:\n",
                "                # TODO: Pass the timeout parameter to the fetch_web_page function\n",
                "                response = fetch_web_page(url)\n",
                "                markdown = html_to_markdown_converter(response.text)\n",
                "                markdown_pages.append(markdown)\n",
                "            except Exception as e:\n",
                "                markdown_pages.append(f\"**Error fetching {url}:** {e}\")\n",
                "        return markdown_pages\n",
                "    except Exception as e:\n",
                "        return [f\"Error: {e}\"]\n",
                "```\n",
                "\n",
                "That's an excellent step toward making your tool robust\\! Using timeouts is essential for reliable web scraping.\n",
                "\n",
                "Here is the implementation that incorporates the **`timeout`** parameter into the function signature, the **`DDGS`** constructor, and the **`httpx.get()`** call within `fetch_web_page`.\n",
                "\n",
                "-----\n",
                "\n",
                "## Updated Web Searcher Module with Timeouts\n",
                "\n",
                "### Hint\n",
                "\n",
                "You need to modify **three** places:\n",
                "\n",
                "1.  The function signature of **`search_and_fetch_markdown`** to accept `timeout: int = 10`.\n",
                "2.  The function signature of **`fetch_web_page`** to accept the `timeout` and pass it to **`httpx.get(url, timeout=timeout)`**.\n",
                "3.  The instantiation of **`DDGS()`** to include `DDGS(timeout=timeout)`.\n",
                "\n",
                "Since your `search_and_fetch_markdown` function already calls `fetch_web_page`, you'll need to update the definition and call of `fetch_web_page` as well.\n",
                "\n",
                "### Implementation\n",
                "\n",
                "Here is the complete, updated code:\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "\n",
                "# Updated fetch_web_page to accept and use the timeout parameter\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        timeout (int): Maximum time to wait for the response in seconds (default: 10).\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Pass the timeout parameter to httpx.get()\n",
                "        response = httpx.get(url, timeout=timeout, follow_redirects=True) \n",
                "        response.raise_for_status()\n",
                "        return response.text\n",
                "    except Exception as e:\n",
                "        return f\"Error fetching URL {url}: {e}\"\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "# TODO: Update the function signature to include a timeout parameter with default value of 10\n",
                "def search_and_fetch_markdown(query: str, max_results: int = 5, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    \n",
                "    Args:\n",
                "        query (str): The search query.\n",
                "        max_results (int): Maximum number of results to fetch (default: 5).\n",
                "        timeout (int): Maximum seconds to wait for connection/response (default: 10).\n",
                "        \n",
                "    Returns:\n",
                "        list: A list of Markdown strings (or error messages for failed fetches), \n",
                "              one for each result attempted.\n",
                "    \"\"\"\n",
                "    # TODO: Update the docstring to reflect the changes (Done above)\n",
                "    \n",
                "    try:\n",
                "        # TODO: Pass the timeout parameter to the DDGS constructor\n",
                "        ddgs = DDGS(timeout=timeout)\n",
                "        results = ddgs.text(query, max_results=max_results)\n",
                "        \n",
                "        markdown_pages = []\n",
                "        \n",
                "        for result in results:\n",
                "            url = result.get(\"href\")\n",
                "            title = result.get(\"title\", \"No Title\")\n",
                "            \n",
                "            if not url or not url.startswith('http'):\n",
                "                markdown_pages.append(f\"Notice: Skipped result '{title}' due to invalid URL.\")\n",
                "                continue\n",
                "            \n",
                "            try:\n",
                "                # TODO: Pass the timeout parameter to the fetch_web_page function\n",
                "                # Note: We are using the updated fetch_web_page that now accepts 'timeout'\n",
                "                html_content = fetch_web_page(url, timeout=timeout) \n",
                "                \n",
                "                # Check if fetch_web_page returned an error string\n",
                "                if html_content.startswith(\"Error fetching URL\"):\n",
                "                    markdown_pages.append(f\"**Error fetching {url} ('{title}'):** Timeout or other error.\")\n",
                "                    continue\n",
                "                    \n",
                "                markdown = html_to_markdown_converter(html_content)\n",
                "                \n",
                "                formatted_output = (\n",
                "                    f\"## Source: {title}\\n\"\n",
                "                    f\"**URL:** {url}\\n\\n\"\n",
                "                    f\"---\\n\\n\"\n",
                "                    f\"{markdown}...\"\n",
                "                )\n",
                "                markdown_pages.append(formatted_output)\n",
                "                \n",
                "            except Exception as e:\n",
                "                # This catch block is mostly for errors inside the loop body, but fetch_web_page handles most of them now\n",
                "                markdown_pages.append(f\"**Error processing {url} ('{title}'):** {e}\")\n",
                "                \n",
                "        return markdown_pages\n",
                "        \n",
                "    except Exception as e:\n",
                "        # Catch critical errors during the DDGS search itself\n",
                "        return [f\"CRITICAL ERROR during DDGS search for '{query}': {e}\"]\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Example Usage\n",
                "\n",
                "Here is an example of how you could test the new timeout functionality:\n",
                "\n",
                "```python\n",
                "# --- app.py (Testing/Example) ---\n",
                "\n",
                "# Example 1: Standard usage (uses default timeout=10)\n",
                "# results_standard = search_and_fetch_markdown(\"python asyncio tutorial\", max_results=2)\n",
                "# print(\"Standard Results:\")\n",
                "# for r in results_standard:\n",
                "#     print(r + \"\\n\" + \"=\"*40)\n",
                "\n",
                "# Example 2: Short timeout (more likely to fail if the site is slow)\n",
                "# results_short_timeout = search_and_fetch_markdown(\"slow website example\", max_results=1, timeout=1) \n",
                "# print(\"\\nShort Timeout Results (Likely to show a timeout error):\")\n",
                "# for r in results_short_timeout:\n",
                "#     print(r + \"\\n\" + \"=\"*40)\n",
                "\n",
                "# Example 3: Long timeout (less likely to fail, good for poor connections)\n",
                "# results_long_timeout = search_and_fetch_markdown(\"data science trends 2025\", max_results=2, timeout=30)\n",
                "# print(\"\\nLong Timeout Results:\")\n",
                "# for r in results_long_timeout:\n",
                "#     print(r + \"\\n\" + \"=\"*40)\n",
                "```\n",
                "\n",
                "The key changes were passing the `timeout` value down from the main function to both the `DDGS` initialization and the helper function `fetch_web_page`. This ensures consistent control over wait times throughout your web interactions."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Structuring Search Results for Better Context\n",
                "\n",
                "You've made excellent progress with your web searcher! Now that you can fetch multiple results, let's make your search results even more useful by improving how they're structured.\n",
                "\n",
                "Currently, your function returns just the Markdown content as a string, but this doesn't tell you anything about where the information came from. In this exercise, you'll enhance the function to return more complete information about each search result.\n",
                "\n",
                "Your task is to modify the search_and_fetch_markdown function to return a list of dictionaries instead of just Markdown strings. Each dictionary should contain:\n",
                "\n",
                "The title of the webpage\n",
                "The URL of the webpage\n",
                "The Markdown content from that page\n",
                "This structured format will make your search results much more valuable because you'll know the source of each piece of information. You'll also need to update the error handling to maintain this structure even when errors occur.\n",
                "\n",
                "Follow the TODO comments in the starter code to implement these changes. When you're done, you'll have a more powerful research tool that provides well-organized data, ready for further processing or display!\n",
                "\n",
                "````python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        timeout (int): Maximum time to wait for response in seconds (default: 10)\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message\n",
                "    \"\"\"\n",
                "    try:\n",
                "        response = httpx.get(url, timeout=timeout)\n",
                "        response.raise_for_status()\n",
                "        return response.text\n",
                "    except Exception as e:\n",
                "        return f\"Error fetching URL {url}: {e}\"\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(query: str, max_results: int = 5, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    \n",
                "    Args:\n",
                "        query (str): The search query\n",
                "        max_results (int): Maximum number of results to fetch (default: 5)\n",
                "        timeout (int): Maximum time to wait for response in seconds (default: 10)\n",
                "        \n",
                "    Returns:\n",
                "        list: A list of Markdown strings, one for each result\n",
                "    \"\"\"\n",
                "    # TODO: Update the docstring to indicate the function now returns a list of dictionaries\n",
                "    \n",
                "    try:\n",
                "        ddgs = DDGS(timeout=timeout)\n",
                "        results = ddgs.text(query, max_results=max_results)\n",
                "        \n",
                "        markdown_pages = []\n",
                "        \n",
                "        for result in results:\n",
                "            url = result.get(\"href\")\n",
                "            if not url:\n",
                "                continue\n",
                "            try:\n",
                "                response = fetch_web_page(url, timeout=timeout)\n",
                "                markdown = html_to_markdown_converter(response.text)\n",
                "                markdown_pages.append(markdown)\n",
                "                # TODO: Instead of returning the markdown directly, append a dictionary with title, url, and markdown to the list\n",
                "            except Exception as e:\n",
                "                markdown_pages.append(f\"**Error fetching {url}:** {e}\")\n",
                "        return markdown_pages\n",
                "    except Exception as e:\n",
                "        return [f\"Error: {e}\"]\n",
                "\n",
                "```\n",
                "\n",
                "That's a smart move\\! Returning structured data (dictionaries) is far more useful for downstream processing than simple strings.\n",
                "\n",
                "Here is the updated `search_and_fetch_markdown` function, where each result is returned as a dictionary containing the **title**, **URL**, and **Markdown content**.\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        timeout (int): Maximum time to wait for response in seconds (default: 10)\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message (prefixed with 'Error')\n",
                "    \"\"\"\n",
                "    try:\n",
                "        response = httpx.get(url, timeout=timeout, follow_redirects=True)\n",
                "        response.raise_for_status()\n",
                "        return response.text\n",
                "    except Exception as e:\n",
                "        return f\"Error fetching URL {url}: {e}\"\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(query: str, max_results: int = 5, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their structured content.\n",
                "    \n",
                "    Args:\n",
                "        query (str): The search query.\n",
                "        max_results (int): Maximum number of results to fetch (default: 5).\n",
                "        timeout (int): Maximum time to wait for response in seconds (default: 10).\n",
                "        \n",
                "    Returns:\n",
                "        list: A list of dictionaries. Each dictionary contains 'title', 'url', and \n",
                "              either 'markdown' content or an 'error' message.\n",
                "    \"\"\"\n",
                "    # TODO: Update the docstring to indicate the function now returns a list of dictionaries (Done above)\n",
                "    \n",
                "    try:\n",
                "        ddgs = DDGS(timeout=timeout)\n",
                "        # Convert generator to list to work with a fixed set of results\n",
                "        results = list(ddgs.text(query, max_results=max_results))\n",
                "        \n",
                "        # This list will store the dictionaries\n",
                "        markdown_pages = []\n",
                "        \n",
                "        for result in results:\n",
                "            url = result.get(\"href\")\n",
                "            title = result.get(\"title\", \"No Title Found\")\n",
                "            \n",
                "            # 1. Handle missing URL in search result\n",
                "            if not url or not url.startswith('http'):\n",
                "                markdown_pages.append({\n",
                "                    \"title\": title,\n",
                "                    \"url\": url if url else \"N/A\",\n",
                "                    \"markdown\": None,\n",
                "                    \"error\": \"Skipped: Invalid or missing URL in search result.\"\n",
                "                })\n",
                "                continue\n",
                "\n",
                "            try:\n",
                "                # Use the updated fetch_web_page, which returns HTML or an error string\n",
                "                html_content = fetch_web_page(url, timeout=timeout)\n",
                "                \n",
                "                # Check if fetch_web_page returned an error\n",
                "                if html_content.startswith(\"Error fetching URL\"):\n",
                "                    # 2. Handle HTTP/Fetch Errors\n",
                "                    markdown_pages.append({\n",
                "                        \"title\": title,\n",
                "                        \"url\": url,\n",
                "                        \"markdown\": None,\n",
                "                        \"error\": html_content\n",
                "                    })\n",
                "                    continue\n",
                "                    \n",
                "                # Process successful fetch\n",
                "                markdown = html_to_markdown_converter(html_content)\n",
                "                \n",
                "                # TODO: Instead of returning the markdown directly, append a dictionary with title, url, and markdown to the list\n",
                "                markdown_pages.append({\n",
                "                    \"title\": title,\n",
                "                    \"url\": url,\n",
                "                    \"markdown\": markdown,\n",
                "                    \"error\": None # Indicate success\n",
                "                })\n",
                "                \n",
                "            except Exception as e:\n",
                "                # 3. Handle Unexpected Errors during conversion or processing\n",
                "                markdown_pages.append({\n",
                "                    \"title\": title,\n",
                "                    \"url\": url,\n",
                "                    \"markdown\": None,\n",
                "                    \"error\": f\"Unexpected processing error: {e}\"\n",
                "                })\n",
                "                \n",
                "        return markdown_pages\n",
                "        \n",
                "    except Exception as e:\n",
                "        # 4. Handle Critical Errors during DDGS search initialization or execution\n",
                "        return [{\n",
                "            \"title\": \"N/A\",\n",
                "            \"url\": \"N/A\",\n",
                "            \"markdown\": None,\n",
                "            \"error\": f\"CRITICAL SEARCH ERROR: {e}\"\n",
                "        }]\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Adding Robust Error Handling"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
