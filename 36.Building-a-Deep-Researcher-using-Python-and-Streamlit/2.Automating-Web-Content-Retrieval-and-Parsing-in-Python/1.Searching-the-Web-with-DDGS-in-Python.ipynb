{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Searching the Web with DDGS in Python\n",
                "\n",
                "Tentu, berikut adalah konversi teks Anda ke format Markdown dalam bahasa Inggris:\n",
                "\n",
                "# Introduction - The World of Programming Languages\n",
                "\n",
                "Welcome to the first lesson of our course, **\"Automating Web Content Retrieval and Parsing in Python\"**. In this course, you will learn how to build a research tool that can search the web, gather content, and process information automatically.\n",
                "\n",
                "Automating web search is a key part of building modern research tools. Instead of searching for information manually, you can write a Python module to do it for you. This saves time and allows you to collect and process large amounts of information quickly.\n",
                "\n",
                "In this lesson, we will focus on using **DDGS**, a metasearch library that aggregates results from diverse web search services.\n",
                "\n",
                "-----\n",
                "\n",
                "### Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will know how to:\n",
                "\n",
                "  * Search the web using `DDGS` in Python.\n",
                "  * Fetch the first result from your search.\n",
                "  * Convert the web page content into a readable format.\n",
                "\n",
                "Let’s get started\\!\n",
                "\n",
                "-----\n",
                "\n",
                "## Using DDGS to Search the Web\n",
                "\n",
                "The **DDGS** library allows you to perform web searches directly from Python. This is helpful because you can automate the process of finding information online.\n",
                "\n",
                "`DDGS` works as a metasearch tool: it automatically selects and queries different search backends (such as DuckDuckGo, Brave, or others) to provide you with a diverse set of results. You do not need to choose the backend yourself; the library handles this for you.\n",
                "\n",
                "First, let’s see how to import the `DDGS` library and perform a simple search.\n",
                "\n",
                "```python\n",
                "from ddgs import DDGS\n",
                "\n",
                "query = \"Python programming\"\n",
                "results = DDGS().text(query, max_results=1)\n",
                "print(results)\n",
                "```\n",
                "\n",
                "**Code Explanation:**\n",
                "\n",
                "  * We import the `DDGS` class from the library.\n",
                "  * We define a search query, in this case, `\"Python programming\"`.\n",
                "  * We call the **`.text()`** method to perform the search and ask for just one result (`max_results=1`).\n",
                "  * The `results` variable will contain a list of search results.\n",
                "\n",
                "**Sample Output:**\n",
                "\n",
                "```\n",
                "[{'title': 'Welcome to Python.org', 'href': 'https://www.python.org/', 'body': 'The official home of the Python Programming Language.'}]\n",
                "```\n",
                "\n",
                "Each result is a **dictionary** with keys like `title`, `href` (the URL), and `body` (a short description).\n",
                "\n",
                "> **Note:** On CodeSignal, the `ddgs` library is already installed, so you do not need to install it yourself. On your own computer, you would install it using `pip install ddgs`.\n",
                "\n",
                "-----\n",
                "\n",
                "## Extracting and Fetching the First Search Result\n",
                "\n",
                "Now that we have search results, let’s extract the URL of the first result and fetch the web page content.\n",
                "\n",
                "First, let’s check if we got any results and get the URL:\n",
                "\n",
                "```python\n",
                "if results and \"href\" in results[0]:\n",
                "    url = results[0][\"href\"]\n",
                "    print(\"First result URL:\", url)\n",
                "else:\n",
                "    print(\"No valid results found.\")\n",
                "```\n",
                "\n",
                "**Code Explanation:**\n",
                "\n",
                "  * We check if `results` is not empty and if the first result has an `\"href\"` key.\n",
                "  * If so, we extract the URL and print it.\n",
                "  * If not, we print a message saying no valid results were found.\n",
                "\n",
                "> Even though we request only one result with `max_results=1`, `DDGS().text()` always returns a **list**. To access the actual result, we need to extract the first item from the list using **`results[0]`**, even if there's only one result.\n",
                "\n",
                "Next, let’s fetch the content of the web page using the **httpx** library:\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "\n",
                "try:\n",
                "    response = httpx.get(url, timeout=10)\n",
                "    response.raise_for_status()\n",
                "\n",
                "    html_content = response.text\n",
                "    print(\"Fetched web page content (first 200 characters):\")\n",
                "    print(html_content[:200])\n",
                "except Exception as e:\n",
                "    print(f\"Error fetching URL {url}: {e}\")\n",
                "```\n",
                "\n",
                "**Code Explanation:**\n",
                "\n",
                "  * We use `httpx.get()` to download the web page at the given URL.\n",
                "  * The `timeout=10` argument means the request will wait up to 10 seconds.\n",
                "  * `response.raise_for_status()` will raise an error if the request fails.\n",
                "  * We print the first 200 characters of the fetched web page content.\n",
                "  * If there is an error, we print an error message.\n",
                "\n",
                "**Sample Output:**\n",
                "\n",
                "```\n",
                "Fetched web page content (first 200 characters):<!doctype html><html lang=\"en\"><head>    <title>Welcome to Python.org</title>    ...\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Converting Web Content to Markdown\n",
                "\n",
                "Web pages are usually written in HTML, which can be hard to read. To make the content easier to work with, we can convert it to **Markdown**, a simpler text format.\n",
                "\n",
                "We will use the **`html_to_markdown`** library for this. Here’s how you can do it:\n",
                "\n",
                "```python\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "markdown_content = convert_to_markdown(html_content)\n",
                "print(\"Web page content in Markdown (first 200 characters):\")\n",
                "print(markdown_content[:200])\n",
                "```\n",
                "\n",
                "**Code Explanation:**\n",
                "\n",
                "  * We import the `convert_to_markdown` function.\n",
                "  * We pass the HTML content to this function, and it returns a Markdown version.\n",
                "  * We print the first 200 characters of the Markdown content.\n",
                "\n",
                "**Sample Output:**\n",
                "\n",
                "```\n",
                "Web page content in Markdown (first 200 characters):# Welcome to Python.org\n",
                "\n",
                "The official home of the Python Programming Language....\n",
                "```\n",
                "\n",
                "Markdown is much easier to read and process than raw HTML, which is why we use this step.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and What’s Next\n",
                "\n",
                "In this lesson, you learned how to:\n",
                "\n",
                "  * Use the `DDGS` library to search the web from Python.\n",
                "  * Extract the first search result and fetch its web page content using `httpx`.\n",
                "  * Convert the web page’s HTML to Markdown for easier reading.\n",
                "\n",
                "These steps are the **foundation** for building an automated research tool. In the practice exercises that follow, you will get hands-on experience with these skills. You will write your own code to search, fetch, and convert web content, preparing you for more advanced features in future lessons.\n",
                "\n",
                "Let’s move on to the practice exercises and start building your **DeepResearcher**\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Your First Web Search with DDGS\n",
                "\n",
                "Now that you've learned about the DDGS library, let's put that knowledge into practice! In this exercise, you'll write a simple script that performs a web search using DuckDuckGo's search engine.\n",
                "\n",
                "Your task is to complete the code that:\n",
                "\n",
                "Uses DDGS to search for \"Python programming\"\n",
                "Gets the first search result\n",
                "Prints the raw results list to see its structure\n",
                "Prints the first result dictionary to examine its contents\n",
                "This hands-on experience will help you understand how search results are structured when using DDGS. Understanding this data structure is essential as you build more advanced search functionality in upcoming exercises.\n",
                "\n",
                "Note: The DDGS library may output log messages to STDERR when you run your code. Don’t be alarmed—these logs show the steps the library is taking to perform your search (such as which backend it is using). You can read them to better understand what’s happening under the hood.\n",
                "\n",
                "By completing this exercise, you'll take your first step toward creating an automated research tool that can search the web efficiently!\n",
                "\n",
                "\n",
                "```python\n",
                "from ddgs import DDGS\n",
                "\n",
                "# Define our search query\n",
                "query = \"Python programming\"\n",
                "\n",
                "# TODO: Perform the search using DDGS().text() with the query and max_results=1\n",
                "\n",
                "# TODO: Print the raw results list with a label \"Search results:\"\n",
                "\n",
                "# TODO: Check if results exist and have at least one item\n",
                "# If results exist, print the first result dictionary with a label \"First result details:\"\n",
                "# Otherwise, print \"No results found.\"\n",
                "```\n",
                "\n",
                "I see you want the previous response converted to English. Here is the completed Python script and the explanation in English.\n",
                "\n",
                "## Your First Web Search with DDGS (Completed Script)\n",
                "\n",
                "Here is the completed Python script that performs the search, prints the raw result list, and then prints the details of the first result.\n",
                "\n",
                "```python\n",
                "from ddgs import DDGS\n",
                "\n",
                "# Define our search query\n",
                "query = \"Python programming\"\n",
                "\n",
                "# TODO: Perform the search using DDGS().text() with the query and max_results=1\n",
                "results = DDGS().text(query, max_results=1)\n",
                "\n",
                "# TODO: Print the raw results list with a label \"Search results:\"\n",
                "print(\"Search results:\")\n",
                "print(results)\n",
                "\n",
                "# TODO: Check if results exist and have at least one item\n",
                "# If results exist, print the first result dictionary with a label \"First result details:\"\n",
                "# Otherwise, print \"No results found.\"\n",
                "print(\"-\" * 30) # Separator for clarity\n",
                "\n",
                "if results and len(results) > 0:\n",
                "    first_result = results[0]\n",
                "    print(\"First result details:\")\n",
                "    print(first_result)\n",
                "else:\n",
                "    print(\"No results found.\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Explanation and Learning Points 💡\n",
                "\n",
                "The code you completed executes essential steps for understanding `DDGS` results:\n",
                "\n",
                "1.  **The Search:** The line `results = DDGS().text(query, max_results=1)` performs the search. Although `max_results=1` is specified, the output (stored in the `results` variable) is always a **list**.\n",
                "2.  **Data Structure:** When you print `results`, you see that it's a list containing a single element. This element is a **dictionary** that holds the actual result details (title, URL, etc.).\n",
                "3.  **Accessing the First Result:** The `if results and len(results) > 0:` check ensures the list isn't empty. The line `first_result = results[0]` then uses the **index `[0]`** to extract the actual result dictionary from within the list, allowing you to access fields like `title`, `href`, and `body` individually.\n",
                "\n",
                "You now know that the search results from `DDGS().text()` are a **list of dictionaries**, a crucial data structure for navigation in the next exercises."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting URLs from Search Results\n",
                "\n",
                "Excellent job on your first search! Now that you understand the structure of search results, let's focus on extracting specific information from them.\n",
                "\n",
                "In this exercise, you'll learn how to safely extract the URL from a search result.\n",
                "\n",
                "Your task is to complete the code that:\n",
                "\n",
                "Checks whether the search returned any results\n",
                "Verifies whether the first result contains a valid URL (the href key)\n",
                "Prints the URL with a helpful message if both conditions are met\n",
                "Prints \"No valid results found.\" if any condition fails\n",
                "This skill is crucial for building reliable web tools that can handle unexpected data formats or empty results. By mastering URL extraction, you'll be ready to move on to fetching and processing web content in the next exercises!\n",
                "\n",
                "\n",
                "```python\n",
                "from ddgs import DDGS\n",
                "\n",
                "# Define our search query\n",
                "query = \"Python programming\"\n",
                "\n",
                "# Perform the search and get results\n",
                "results = DDGS().text(query, max_results=1)\n",
                "\n",
                "# TODO: Check if results exist and if the first result contains a \"href\" key\n",
                "# TODO: If both conditions are met, extract the URL and print it with a message\n",
                "# TODO: If any condition fails, print \"No valid results found.\"\n",
                "\n",
                "```\n",
                "\n",
                "You've successfully set up the search; now let's focus on the safe extraction of the URL.\n",
                "\n",
                "Here is the completed Python script:\n",
                "\n",
                "```python\n",
                "from ddgs import DDGS\n",
                "\n",
                "# Define our search query\n",
                "query = \"Python programming\"\n",
                "\n",
                "# Perform the search and get results\n",
                "results = DDGS().text(query, max_results=1)\n",
                "\n",
                "# TODO: Check if results exist and if the first result contains a \"href\" key\n",
                "# TODO: If both conditions are met, extract the URL and print it with a message\n",
                "# TODO: If any condition fails, print \"No valid results found.\"\n",
                "\n",
                "# Check if the results list is not empty (results) AND \n",
                "# check if the first item (results[0]) is a dictionary AND \n",
                "# if the \"href\" key exists within that first item.\n",
                "if results and \"href\" in results[0]:\n",
                "    # Both conditions met: Safely extract the URL\n",
                "    url = results[0][\"href\"]\n",
                "    print(f\"Successfully extracted URL: {url}\")\n",
                "else:\n",
                "    # Condition failed: Either no results or the result structure was unexpected\n",
                "    print(\"No valid results found.\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Explanation of Safe Extraction\n",
                "\n",
                "The core of this exercise lies in the robust conditional check: `if results and \"href\" in results[0]:`\n",
                "\n",
                "  * **`results`**: In Python, when a list is used in a boolean context (like an `if` statement), it evaluates to `True` if it is **not empty**, and `False` if it is empty. This is the simplest way to check if any results were returned.\n",
                "  * **`\"href\" in results[0]`**: The `results[0]` accesses the first dictionary item in the list. The `\"href\" in ...` part then checks if the necessary key exists within that dictionary. This prevents a `KeyError` if the DDGS result structure unexpectedly changes or if a result object is malformed.\n",
                "\n",
                "By combining these two checks with the **`and`** operator, your script is **reliable** and ready to handle various search outcomes, ensuring you only proceed with URL extraction (`url = results[0][\"href\"]`) when the data is confirmed to be present and structured correctly."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fetching Web Content with httpx\n",
                "\n",
                "Now that you've successfully extracted URLs from search results, let's learn how to actually fetch the content from those URLs! In this exercise, you'll create a function that downloads web page content using the httpx library.\n",
                "\n",
                "Your task is to implement a fetch_web_page function that:\n",
                "\n",
                "Takes a URL and timeout as input parameters\n",
                "Uses httpx to make a GET request to that URL\n",
                "Handles any errors that might occur during the request\n",
                "Returns the HTML content if successful, or an error message if something goes wrong\n",
                "This is an essential skill for any web automation tool — after finding a URL through search, you need to be able to reliably download its content for further processing.\n",
                "\n",
                "By completing this exercise, you'll add another important component to your automated research toolkit. Soon, you'll be able to combine searching and fetching to create a powerful web research assistant!\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "\n",
                "def fetch_web_page(url, timeout=10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        timeout (int): Maximum time to wait for response in seconds\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message\n",
                "    \"\"\"\n",
                "    # TODO: Use a try/except block to handle potential errors\n",
                "    \n",
                "    # TODO: Inside the try block, use httpx.get() to fetch the URL with the timeout parameter\n",
                "    \n",
                "    # TODO: Call raise_for_status() on the response to handle HTTP error codes\n",
                "    \n",
                "    # TODO: Return the text content of the response\n",
                "    \n",
                "    # TODO: In the except block, catch any exceptions and return an error message\n",
                "\n",
                "# Test the function with a sample URL\n",
                "if __name__ == \"__main__\":\n",
                "    sample_url = \"https://www.youtube.com/\"\n",
                "    content = fetch_web_page(sample_url)\n",
                "    \n",
                "    # Print the first 200 characters to see a preview\n",
                "    print(f\"Content from {sample_url} (first 200 characters):\")\n",
                "    print(content[:200])\n",
                "\n",
                "```\n",
                "\n",
                "That's the logical next step\\! Reliable content fetching is vital. Here is the completed Python script implementing the `fetch_web_page` function with robust error handling using the `httpx` library.\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "\n",
                "def fetch_web_page(url, timeout=10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \n",
                "    Args:\n",
                "        url (str): The URL to fetch\n",
                "        timeout (int): Maximum time to wait for response in seconds\n",
                "        \n",
                "    Returns:\n",
                "        str: The HTML content of the page or an error message\n",
                "    \"\"\"\n",
                "    # TODO: Use a try/except block to handle potential errors\n",
                "    try:\n",
                "        # TODO: Inside the try block, use httpx.get() to fetch the URL with the timeout parameter\n",
                "        response = httpx.get(url, timeout=timeout)\n",
                "        \n",
                "        # TODO: Call raise_for_status() on the response to handle HTTP error codes\n",
                "        # This will raise an exception for 4xx or 5xx responses\n",
                "        response.raise_for_status()\n",
                "        \n",
                "        # TODO: Return the text content of the response\n",
                "        return response.text\n",
                "        \n",
                "    # TODO: In the except block, catch any exceptions and return an error message\n",
                "    except httpx.HTTPStatusError as e:\n",
                "        # Handles 4xx or 5xx errors (e.g., 404 Not Found)\n",
                "        return f\"ERROR: HTTP error occurred: {e}\"\n",
                "    except httpx.TimeoutException:\n",
                "        # Handles timeout errors\n",
                "        return f\"ERROR: Request timed out after {timeout} seconds.\"\n",
                "    except httpx.RequestError as e:\n",
                "        # Handles other request-related issues (e.g., DNS error, connection failed)\n",
                "        return f\"ERROR: An error occurred during the request: {e}\"\n",
                "    except Exception as e:\n",
                "        # Catches any other unexpected exceptions\n",
                "        return f\"ERROR: An unexpected error occurred: {e}\"\n",
                "\n",
                "\n",
                "# Test the function with a sample URL\n",
                "if __name__ == \"__main__\":\n",
                "    sample_url = \"https://www.youtube.com/\"\n",
                "    content = fetch_web_page(sample_url)\n",
                "    \n",
                "    # Print the first 200 characters to see a preview\n",
                "    print(f\"Content from {sample_url} (first 200 characters):\")\n",
                "    \n",
                "    # Use slicing [:200] to safely print only the beginning of the string\n",
                "    # This avoids trying to slice if 'content' is a short error message.\n",
                "    preview = content\n",
                "    if content.startswith(\"ERROR:\"):\n",
                "        print(preview)\n",
                "    else:\n",
                "        print(preview[:200])\n",
                "\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Key Takeaways from Error Handling\n",
                "\n",
                "The real power in this function is the **`try/except` block** and the strategic use of `response.raise_for_status()`:\n",
                "\n",
                "1.  **`response.raise_for_status()`**: This method automatically checks the HTTP status code. If the code is a **$400$-level (Client Error)** or **$500$-level (Server Error)**, it immediately raises an `httpx.HTTPStatusError` exception, which we catch. This ensures your code doesn't try to process an error page.\n",
                "2.  **Specific Exception Catching**: By catching `httpx.HTTPStatusError`, `httpx.TimeoutException`, and `httpx.RequestError` separately, your function can return highly specific error messages. This is much better than just catching a general `Exception` because it helps debug exactly *why* the request failed—was it a network issue, a timeout, or a server problem?\n",
                "\n",
                "Your research tool now has the ability to **safely and reliably** download web content, a critical function for the next steps\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Converting HTML to Readable Markdown\n",
                "\n",
                "Fantastic work on fetching web content! Now that you can download HTML from websites, let's make that content more readable by converting it to Markdown format.\n",
                "\n",
                "In this exercise, you'll implement the html_to_markdown_converter function, which transforms raw HTML into clean, readable Markdown text.\n",
                "\n",
                "Your task is to complete the function that:\n",
                "\n",
                "Takes an HTML string as input\n",
                "Uses the convert_to_markdown function to transform the HTML\n",
                "Removes the leading and trailing whitespaces.\n",
                "Trims the result to a specified length (default: 500 characters)\n",
                "Returns the processed Markdown content\n",
                "This conversion step is crucial for your research tool because it makes web content much easier to read and process programmatically. HTML can be complex and cluttered, while Markdown provides a clean, simple text format that preserves the important structure.\n",
                "\n",
                "By completing this exercise, you'll have all the core components needed for a basic web research pipeline: searching, fetching, and converting content into a useful format!\n",
                "\n",
                "\n",
                "```python\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    # TODO: Convert HTML to Markdown using the convert_to_markdown function\n",
                "    \n",
                "    # TODO: Trim the Markdown content to the specified length\n",
                "    \n",
                "    # TODO: Return the trimmed Markdown content\n",
                "\n",
                "# Test the function with sample HTML\n",
                "if __name__ == \"__main__\":\n",
                "    # Sample HTML content\n",
                "    sample_html = \"\"\"\n",
                "    <!DOCTYPE html>\n",
                "    <html>\n",
                "    <head>\n",
                "        <title>Python Programming</title>\n",
                "    </head>\n",
                "    <body>\n",
                "        <h1>Introduction to Python</h1>\n",
                "        <p>Python is a <strong>powerful</strong> programming language that's great for beginners.</p>\n",
                "        <ul>\n",
                "            <li>Easy to learn</li>\n",
                "            <li>Versatile</li>\n",
                "            <li>Large community</li>\n",
                "        </ul>\n",
                "        <p>Learn more at <a href=\"https://www.python.org\">python.org</a>.</p>\n",
                "    </body>\n",
                "    </html>\n",
                "    \"\"\"\n",
                "    \n",
                "    # Convert HTML to Markdown and print the result\n",
                "    markdown_result = html_to_markdown_converter(sample_html)\n",
                "    print(\"Converted Markdown content:\")\n",
                "    print(markdown_result)\n",
                "\n",
                "```\n",
                "\n",
                "Great job reaching the final step of the content pipeline\\! Converting HTML to clean Markdown makes the data instantly useful.\n",
                "\n",
                "Here is the completed Python script for the `html_to_markdown_converter` function:\n",
                "\n",
                "```python\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \n",
                "    Args:\n",
                "        html_content (str): The HTML content to convert\n",
                "        max_length (int): Maximum length of the returned Markdown (default: 500)\n",
                "        \n",
                "    Returns:\n",
                "        str: The converted and trimmed Markdown content\n",
                "    \"\"\"\n",
                "    # TODO: Convert HTML to Markdown using the convert_to_markdown function\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    \n",
                "    # Remove leading and trailing whitespaces\n",
                "    markdown_content = markdown_content.strip()\n",
                "    \n",
                "    # TODO: Trim the Markdown content to the specified length\n",
                "    trimmed_content = markdown_content[:max_length]\n",
                "    \n",
                "    # Optional: Add an ellipsis if the content was actually trimmed\n",
                "    if len(markdown_content) > max_length:\n",
                "        trimmed_content += \"...\"\n",
                "    \n",
                "    # TODO: Return the trimmed Markdown content\n",
                "    return trimmed_content\n",
                "\n",
                "# Test the function with sample HTML\n",
                "if __name__ == \"__main__\":\n",
                "    # Sample HTML content\n",
                "    sample_html = \"\"\"\n",
                "    <!DOCTYPE html>\n",
                "    <html>\n",
                "    <head>\n",
                "        <title>Python Programming</title>\n",
                "    </head>\n",
                "    <body>\n",
                "        <h1>Introduction to Python</h1>\n",
                "        <p>Python is a <strong>powerful</strong> programming language that's great for beginners.</p>\n",
                "        <ul>\n",
                "            <li>Easy to learn</li>\n",
                "            <li>Versatile</li>\n",
                "            <li>Large community</li>\n",
                "        </ul>\n",
                "        <p>Learn more at <a href=\"https://www.python.org\">python.org</a>. This is some extra filler text to make sure the trimming works correctly and demonstrates how the content gets cut off at the specified max_length. This is very useful for creating short previews or summaries of long web pages before deciding to process the full content.</p>\n",
                "    </body>\n",
                "    </html>\n",
                "    \"\"\"\n",
                "    \n",
                "    # Convert HTML to Markdown and print the result\n",
                "    markdown_result = html_to_markdown_converter(sample_html, max_length=200) # Testing with a shorter length (200)\n",
                "    print(\"Converted Markdown content (max 200 chars):\")\n",
                "    print(markdown_result)\n",
                "    \n",
                "    print(\"\\n---\")\n",
                "    \n",
                "    # Testing with a very long length to ensure full content is returned\n",
                "    markdown_result_full = html_to_markdown_converter(sample_html, max_length=1000) \n",
                "    print(\"Converted Markdown content (full length):\")\n",
                "    print(markdown_result_full)\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## The Value of Markdown Conversion\n",
                "\n",
                "This simple function finalizes your basic research pipeline. By implementing the conversion and trimming, you achieve two goals:\n",
                "\n",
                "1.  **Readability**: The `convert_to_markdown` function transforms noisy HTML tags (like `<h1>`, `<ul>`, `<a href=\"...\">`) into clean, universally readable format (`#`, `*`, `[text](url)`).\n",
                "2.  **Efficiency**: The **`.strip()`** and **string slicing (`[:max_length]`)** methods ensure that the content is clean and limited in size. Trimming is vital in real-world applications to **save processing time and storage space** when dealing with vast amounts of web content.\n",
                "\n",
                "You now have a complete research tool skeleton capable of **searching**, **fetching**, and **processing** data into a usable format\\! What's the next advanced feature you'd like to integrate into your tool?"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
