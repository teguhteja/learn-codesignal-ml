{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Avoiding Common Pitfalls in Our Web Searcher\n",
                "\n",
                "# Making Your Web Searcher More Reliable\n",
                "\n",
                "Welcome back\\! So far, you've learned how to search the web using **Python** and how to build a module that fetches and processes web content. In this lesson, we'll focus on making your **web searcher** more reliable by avoiding common mistakes that can cause problems in automated research.\n",
                "\n",
                "When you build tools that interact with the web, you'll often run into issues like duplicate results, broken links, or slow responses. If you don't handle these problems, your tool might waste time, give you bad data, or even stop working. By learning how to avoid these pitfalls, you'll make your web searcher much more robust and useful.\n",
                "\n",
                "-----\n",
                "\n",
                "## Recall: Our Web Searcher Workflow\n",
                "\n",
                "Let's quickly remind ourselves what our web searcher does. In the previous lessons, you learned how to:\n",
                "\n",
                "1.  Use the **`DDGS`** library to search the web for a query.\n",
                "2.  Fetch the content of the top search results using the **`httpx`** library.\n",
                "3.  Convert the HTML content of each page to Markdown using **`html_to_markdown`**.\n",
                "\n",
                "All of these steps are combined in a function that takes a search query and returns a list of results, each with a title, URL, and Markdown content. Now, let's see how we can improve this process by handling some common issues.\n",
                "\n",
                "-----\n",
                "\n",
                "## Tracking and Skipping Visited URLs\n",
                "\n",
                "One common problem is processing the same web page more than once. This can happen if the same URL appears in multiple searches or if your code is run multiple times. To avoid this, we need a way to **remember which pages we have already visited**.\n",
                "\n",
                "1.  **Create a Set:**\n",
                "    Let's start by creating a set to keep track of visited URLs. Sets are useful because they don't allow duplicate values, and checking if a value is in a set is very fast.\n",
                "\n",
                "    ```python\n",
                "    _visited_pages: Set[str] = set()\n",
                "    ```\n",
                "\n",
                "2.  **Check and Skip:**\n",
                "    Now, before we fetch a page, we check if its URL is already in `_visited_pages`. If it is, we skip it:\n",
                "\n",
                "    ```python\n",
                "    if not url or url in _visited_pages:\n",
                "        continue  # skip already-visited or invalid URLs\n",
                "    ```\n",
                "\n",
                "      * `not url` checks if the URL is missing or empty.\n",
                "      * `url in _visited_pages` checks if we have already seen this URL.\n",
                "\n",
                "3.  **Mark as Visited:**\n",
                "    After we successfully fetch and process a page, we add its URL to the set:\n",
                "\n",
                "    ```python\n",
                "    _visited_pages.add(url)\n",
                "    ```\n",
                "\n",
                "This way, we make sure we don't process the same page twice.\n",
                "\n",
                "-----\n",
                "\n",
                "## Handling Errors When Fetching Pages\n",
                "\n",
                "Another common issue is that some web pages might be **broken, slow, or unreachable**. If your code tries to fetch a page and something goes wrong, it could crash or get stuck.\n",
                "\n",
                "To handle this, we use a `try` and `except` block when fetching each page:\n",
                "\n",
                "```python\n",
                "try:\n",
                "    # 1. Attempt to fetch the page and handle redirects\n",
                "    response = httpx.get(url, timeout=timeout, follow_redirects=True)\n",
                "    # 2. Raise an exception for bad status codes (4xx, 5xx)\n",
                "    response.raise_for_status()\n",
                "\n",
                "    html = response.text\n",
                "    markdown = convert_to_markdown(html)\n",
                "\n",
                "    # 3. Add to visited only on success\n",
                "    _visited_pages.add(url)\n",
                "\n",
                "    markdown_pages.append({\n",
                "        \"title\": title,\n",
                "        \"url\": url,\n",
                "        \"markdown\": markdown\n",
                "    })\n",
                "except Exception as e:\n",
                "    # 4. Handle the error\n",
                "    _visited_pages.add(url)  # mark it to avoid retrying this bad page\n",
                "    markdown_pages.append({\n",
                "        \"title\": title or \"Error\",\n",
                "        \"url\": url,\n",
                "        \"markdown\": f\"**Error fetching content from** `{url}`: {e}\"\n",
                "    })\n",
                "```\n",
                "\n",
                "**Key Takeaways:**\n",
                "\n",
                "  * The **`try`** block attempts to fetch the page and convert it to Markdown.\n",
                "  * If anything goes wrong (e.g., page doesn't load or server returns an error), the code jumps to the **`except`** block.\n",
                "  * In the `except` block, we still add the URL to `_visited_pages` so we don't try it again.\n",
                "  * We add a result to `markdown_pages` with a clear **error message**, so we know what went wrong.\n",
                "\n",
                "**Example Output:**\n",
                "\n",
                "```markdown\n",
                "**Error fetching content from** `http://example.com/badpage`: 404 Client Error: Not Found for url: http://example.com/badpage\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Using Timeouts and Safe Search Settings\n",
                "\n",
                "To keep your tool fast and responsive, and to ensure you get appropriate results, you should use timeouts and safe search settings.\n",
                "\n",
                "1.  **Setting a Timeout:**\n",
                "    When fetching a page, we set a `timeout` parameter:\n",
                "\n",
                "    ```python\n",
                "    response = httpx.get(url, timeout=timeout, follow_redirects=True)\n",
                "    ```\n",
                "\n",
                "    The `timeout` makes sure that if a page takes too long to load, your code will stop waiting and move on.\n",
                "\n",
                "2.  **Safe Search and Region:**\n",
                "    When searching with `DDGS`, we can set the `safesearch` and `region` parameters:\n",
                "\n",
                "    ```python\n",
                "    results = ddgs.text(\n",
                "        query,\n",
                "        region=region,\n",
                "        safesearch=safesearch,\n",
                "        max_results=max_results\n",
                "    )\n",
                "    ```\n",
                "\n",
                "      * `safesearch` helps filter out inappropriate or irrelevant results.\n",
                "      * `region` can help you get results that are more relevant to your location or language.\n",
                "\n",
                "**Example Timeout Error:**\n",
                "\n",
                "If a page takes too long to load, you might see an error like:\n",
                "\n",
                "```markdown\n",
                "**Error fetching content from** `http://slowwebsite.com`: TimeoutException\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and What's Next\n",
                "\n",
                "In this lesson, you learned how to make your web searcher more reliable by:\n",
                "\n",
                "  * **Tracking and skipping** already-visited URLs to avoid duplicates and save time.\n",
                "  * **Handling errors** when fetching pages, so your tool doesn't crash or get stuck.\n",
                "  * Using **timeouts** and **safe search** settings to keep your searches fast and appropriate.\n",
                "\n",
                "These improvements will help you build a more robust and efficient research tool. In the next practice exercises, you'll get a chance to apply these ideas and see how they make your web searcher stronger. Good luck\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Skipping Duplicate URLs for Efficiency\n",
                "\n",
                "Now that you've learned about the importance of tracking visited URLs, let's put this knowledge into practice! In this exercise, you'll implement the URL checking mechanism we discussed in the lesson.\n",
                "\n",
                "Your task is to modify the search_and_fetch_markdown function to avoid processing duplicate URLs. You need to add a global set to keep track of all the visited URLs and a condition that checks whether a URL is empty or has already been visited before attempting to fetch it.\n",
                "\n",
                "The provided code in the app.py file will run two similar searches that might return overlapping results. When your code is working correctly, you'll see that:\n",
                "\n",
                "The first search processes all new URLs\n",
                "The second search processes only URLs that weren't found in the first search\n",
                "The _visited_pages set grows appropriately\n",
                "This improvement will make your web searcher much more efficient by preventing it from doing the same work twice!\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "from typing import List, Dict, Set\n",
                "\n",
                "\n",
                "# TODO: Create a global set to track visited URLs\n",
                "\n",
                "\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \"\"\"\n",
                "    response = httpx.get(url, timeout=timeout)\n",
                "    response.raise_for_status()\n",
                "    return response.text\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    Skips already-visited URLs.\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(query, max_results=max_results)\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        # TODO: Add a condition to check if the URL is empty or already visited\n",
                "        # If either condition is true, use 'continue' to skip to the next result\n",
                "\n",
                "        try:\n",
                "            html = fetch_web_page(url, timeout=timeout)\n",
                "            markdown = html_to_markdown_converter(html)\n",
                "            # TODO: Add url to global visited pages set\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "        except Exception as e:\n",
                "            print(f\"Error: {str(e)}\")\n",
                "\n",
                "    return markdown_pages\n",
                "\n",
                "```\n",
                "\n",
                "Here is the corrected and completed Python code for `app.py`. I've added the global set, the check to skip visited or empty URLs, and the logic to mark URLs as visited.\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "from typing import List, Dict, Set\n",
                "\n",
                "\n",
                "# TODO: Create a global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \"\"\"\n",
                "    response = httpx.get(url, timeout=timeout)\n",
                "    response.raise_for_status()\n",
                "    return response.text\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    Skips already-visited URLs.\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(query, max_results=max_results)\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        # TODO: Add a condition to check if the URL is empty or already visited\n",
                "        # If either condition is true, use 'continue' to skip to the next result\n",
                "        if not url or url in _visited_pages:\n",
                "            print(f\"Skipping duplicate or invalid URL: {url}\")\n",
                "            continue\n",
                "\n",
                "        try:\n",
                "            html = fetch_web_page(url, timeout=timeout)\n",
                "            markdown = html_to_markdown_converter(html)\n",
                "            \n",
                "            # TODO: Add url to global visited pages set\n",
                "            _visited_pages.add(url)\n",
                "            \n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "        except Exception as e:\n",
                "            # We still add the URL to the set even if fetching failed to prevent retrying a 'bad' URL\n",
                "            _visited_pages.add(url)\n",
                "            print(f\"Error fetching {url}: {str(e)}\")\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "### Key Changes Implemented:\n",
                "\n",
                "1.  **Global Set Declaration:**\n",
                "\n",
                "    ```python\n",
                "    _visited_pages: Set[str] = set()\n",
                "    ```\n",
                "\n",
                "    A global `set` named `_visited_pages` was created outside the function to maintain state across multiple function calls.\n",
                "\n",
                "2.  **Skipping Logic:**\n",
                "\n",
                "    ```python\n",
                "    if not url or url in _visited_pages:\n",
                "        print(f\"Skipping duplicate or invalid URL: {url}\")\n",
                "        continue\n",
                "    ```\n",
                "\n",
                "    This condition uses `continue` to efficiently skip the rest of the loop iteration if the URL is empty (`not url`) or has already been processed (`url in _visited_pages`).\n",
                "\n",
                "3.  **Adding to Set:**\n",
                "\n",
                "    ```python\n",
                "    _visited_pages.add(url)\n",
                "    ```\n",
                "\n",
                "    The URL is added to the set only **after** the `fetch_web_page` call succeeds, but **before** the page is added to the final results list. I also added a line within the `except` block to ensure bad/failing URLs are marked as visited and aren't retried unnecessarily."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Graceful Error Handling for Web Requests\n",
                "\n",
                "Now that you've implemented URL tracking to avoid duplicates, let's focus on another critical aspect of web searching: error handling! In the real world, not all web requests succeed — sites go down, connections time out, and servers return errors.\n",
                "\n",
                "Your task is to complete the error handling in the search_and_fetch_markdown function. When an exception occurs during fetching, you need to:\n",
                "\n",
                "Add the URL to _visited_pages so you don't try to fetch it again\n",
                "Create a properly formatted error message that shows which URL failed and what happened\n",
                "Add this error information to the results list instead of letting the program crash\n",
                "The provided test file will try to fetch content from both valid and invalid URLs. When your code works correctly, you'll see that your program continues running even when it encounters bad URLs and provides helpful error messages instead of crashing.\n",
                "\n",
                "This improvement will make your web searcher much more robust when dealing with the unpredictable nature of the internet!\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \"\"\"\n",
                "    response = httpx.get(url, timeout=timeout)\n",
                "    response.raise_for_status()\n",
                "    return response.text\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    Skips already-visited URLs.\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(query, max_results=max_results)\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue  # skip already-visited or invalid URLs\n",
                "\n",
                "        try:\n",
                "            html = fetch_web_page(url, timeout=timeout)\n",
                "            markdown = html_to_markdown_converter(html)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "        except Exception as e:\n",
                "            # TODO: Add the URL to _visited_pages to avoid retrying this bad page\n",
                "\n",
                "            # TODO: Add an entry to markdown_pages with an error message that includes:\n",
                "            # - The title (or \"Error\" if title is empty)\n",
                "            # - The URL\n",
                "            # - A formatted error message showing which URL failed and what the error was\n",
                "            pass\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "This exercise requires you to complete the `try...except` block to ensure your web searcher handles failed requests gracefully.\n",
                "\n",
                "Here is the completed Python code for the `search_and_fetch_markdown` function, which now includes the logic to mark bad URLs as visited and append a descriptive error message to the results list.\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \"\"\"\n",
                "    response = httpx.get(url, timeout=timeout)\n",
                "    response.raise_for_status()\n",
                "    return response.text\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    Skips already-visited URLs.\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(query, max_results=max_results)\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue  # skip already-visited or invalid URLs\n",
                "\n",
                "        try:\n",
                "            html = fetch_web_page(url, timeout=timeout)\n",
                "            markdown = html_to_markdown_converter(html)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "        except Exception as e:\n",
                "            # TODO: Add the URL to _visited_pages to avoid retrying this bad page\n",
                "            _visited_pages.add(url)\n",
                "\n",
                "            # TODO: Add an entry to markdown_pages with an error message that includes:\n",
                "            # - The title (or \"Error\" if title is empty)\n",
                "            # - The URL\n",
                "            # - A formatted error message showing which URL failed and what the error was\n",
                "            error_message = f\"**Error fetching content from** `{url}`: {e}\"\n",
                "            markdown_pages.append({\n",
                "                \"title\": title or \"Error fetching page\",\n",
                "                \"url\": url,\n",
                "                \"markdown\": error_message\n",
                "            })\n",
                "\n",
                "    return markdown_pages\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Resetting URL Tracking for Fresh Searches\n",
                "\n",
                "Cosmo\n",
                "Just now\n",
                "Read message aloud\n",
                "You've done a fantastic job handling duplicate URLs and errors! Now, let's add another important feature — the ability to reset our URL tracking system.\n",
                "\n",
                "Sometimes, you'll want to run the same search again or process the same URLs after making changes to your code. For this, you need a way to clear the list of visited pages.\n",
                "\n",
                "Your task has two parts:\n",
                "\n",
                "Complete the clear_visited_pages() function in the web_searcher.py file to reset the global _visited_pages set.\n",
                "\n",
                "In the test file, add the missing function calls to demonstrate how clearing works:\n",
                "\n",
                "Start with a clean slate\n",
                "Run a search and see which URLs are processed\n",
                "Clear the visited pages\n",
                "Run the same search again to show that previously visited URLs are processed again\n",
                "When your code works correctly, you'll see that after clearing the visited pages, the same search will process the same URLs again as if they were new. This feature makes your web searcher more flexible for repeated research tasks!\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \"\"\"\n",
                "    response = httpx.get(url, timeout=timeout)\n",
                "    response.raise_for_status()\n",
                "    return response.text\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "# TODO: Implement the clear_visited_pages() function\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    Skips already-visited URLs.\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(query, max_results=max_results)\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue  # skip already-visited or invalid URLs\n",
                "\n",
                "        try:\n",
                "            html = fetch_web_page(url, timeout=timeout)\n",
                "            markdown = html_to_markdown_converter(html)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "        except Exception as e:\n",
                "            _visited_pages.add(url)  # mark it to avoid retrying this bad page\n",
                "            markdown_pages.append({\n",
                "                \"title\": title or \"Error\",\n",
                "                \"url\": url,\n",
                "                \"markdown\": f\"**Error fetching content from** `{url}`: {e}\"\n",
                "            })\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "Tentu, saya akan melengkapi fungsi `clear_visited_pages()` dan memberikan contoh bagaimana fungsi tersebut dapat digunakan dalam konteks pengujian.\n",
                "\n",
                "### 1\\. Implementasi `clear_visited_pages()`\n",
                "\n",
                "Anda dapat mengimplementasikan fungsi ini dengan memanggil metode `.clear()` pada *set* global `_visited_pages`.\n",
                "\n",
                "Berikut adalah kode Python yang sudah diperbarui, termasuk fungsi yang diminta:\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \"\"\"\n",
                "    response = httpx.get(url, timeout=timeout)\n",
                "    response.raise_for_status()\n",
                "    return response.text\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "# TODO: Implement the clear_visited_pages() function\n",
                "def clear_visited_pages():\n",
                "    \"\"\"\n",
                "    Resets the global set of visited URLs, allowing previously processed URLs to be fetched again.\n",
                "    \"\"\"\n",
                "    global _visited_pages\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    Skips already-visited URLs.\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    results = ddgs.text(query, max_results=max_results)\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue  # skip already-visited or invalid URLs\n",
                "\n",
                "        try:\n",
                "            html = fetch_web_page(url, timeout=timeout)\n",
                "            markdown = html_to_markdown_converter(html)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "        except Exception as e:\n",
                "            _visited_pages.add(url)  # mark it to avoid retrying this bad page\n",
                "            markdown_pages.append({\n",
                "                \"title\": title or \"Error\",\n",
                "                \"url\": url,\n",
                "                \"markdown\": f\"**Error fetching content from** `{url}`: {e}\"\n",
                "            })\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### 2\\. Contoh Penggunaan untuk Pengujian Ulang\n",
                "\n",
                "Berikut adalah skrip contoh (misalnya di file test) yang menunjukkan bagaimana fungsi `clear_visited_pages()` digunakan untuk memungkinkan URL yang sama diproses kembali:\n",
                "\n",
                "```python\n",
                "# Asumsikan kode di atas ada di file web_searcher.py\n",
                "\n",
                "# Query yang sama digunakan untuk dua kali pencarian\n",
                "SEARCH_QUERY = \"Python programming\"\n",
                "\n",
                "# --- Langkah 1: Bersihkan status awal (Memastikan set kosong) ---\n",
                "clear_visited_pages()\n",
                "print(f\"Status Awal: _visited_pages memiliki {len(_visited_pages)} URL.\")\n",
                "\n",
                "# --- Langkah 2: Jalankan pencarian pertama ---\n",
                "print(\"\\n--- Running Search 1 ---\")\n",
                "results1 = search_and_fetch_markdown(SEARCH_QUERY, max_results=3)\n",
                "print(f\"Search 1 memproses {len(results1)} hasil.\")\n",
                "print(f\"Status Setelah Search 1: _visited_pages memiliki {len(_visited_pages)} URL.\")\n",
                "# Pada titik ini, semua URL unik telah ditambahkan ke _visited_pages.\n",
                "\n",
                "# --- Langkah 3: Jalankan pencarian kedua (tanpa membersihkan) ---\n",
                "print(\"\\n--- Running Search 2 (Without Clearing) ---\")\n",
                "results2 = search_and_fetch_markdown(SEARCH_QUERY, max_results=3)\n",
                "print(f\"Search 2 memproses {len(results2)} hasil (hanya yang baru/berbeda).\")\n",
                "print(f\"Status Setelah Search 2: _visited_pages memiliki {len(_visited_pages)} URL.\")\n",
                "# Jika semua hasil sama dengan Search 1, results2 akan kosong atau sangat sedikit.\n",
                "\n",
                "# --- Langkah 4: Bersihkan status ---\n",
                "print(\"\\n--- Clearing Visited Pages ---\")\n",
                "clear_visited_pages()\n",
                "print(f\"Status Setelah Clear: _visited_pages memiliki {len(_visited_pages)} URL.\")\n",
                "\n",
                "# --- Langkah 5: Jalankan pencarian ketiga (Setelah membersihkan) ---\n",
                "print(\"\\n--- Running Search 3 (After Clearing) ---\")\n",
                "results3 = search_and_fetch_markdown(SEARCH_QUERY, max_results=3)\n",
                "print(f\"Search 3 memproses {len(results3)} hasil.\")\n",
                "print(f\"Status Setelah Search 3: _visited_pages memiliki {len(_visited_pages)} URL.\")\n",
                "# Karena set telah di-reset, Search 3 akan memproses URL yang sama dengan Search 1.\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Customizing Search Results with Parameters\n",
                "\n",
                "You've made excellent progress with URL tracking and error handling! Now, let's enhance our web searcher by customizing the search results themselves. In the lesson, we mentioned the safesearch and region parameters that can help filter and localize your search results.\n",
                "\n",
                "Your task is to add this parameters in the search_and_fetch_markdown function and update the DDGS search call to properly use them.\n",
                "\n",
                "The provided tests in app.py will help you experiment with:\n",
                "\n",
                "Different safesearch levels (\"off\", \"moderate\", \"strict\") to see how content filtering works\n",
                "Different region settings (like \"us-en\", \"uk-en\", \"de-de\") to see how results vary by location\n",
                "By implementing these parameters, you'll make your web searcher more adaptable to different research needs and audiences. This is especially useful when you need to filter out inappropriate content or find region-specific information!\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \"\"\"\n",
                "    response = httpx.get(url, timeout=timeout)\n",
                "    response.raise_for_status()\n",
                "    return response.text\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "def clear_visited_pages() -> None:\n",
                "    \"\"\"Clear the global set of visited pages.\"\"\"\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "# TODO: Add a region parameter with \"wt-wt\" as default and a safesearch parameter with \"moderate\" as default\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    timeout: int = 10\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    Skips already-visited URLs.\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    # TODO: Update the search call to pass the region and safesearch parameters\n",
                "    results = ddgs.text(\n",
                "        query,\n",
                "        max_results=max_results\n",
                "    )\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue  # skip already-visited or invalid URLs\n",
                "\n",
                "        try:\n",
                "            html = fetch_web_page(url, timeout=timeout)\n",
                "            markdown = html_to_markdown_converter(html)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "        except Exception as e:\n",
                "            _visited_pages.add(url)  # mark it to avoid retrying this bad page\n",
                "            markdown_pages.append({\n",
                "                \"title\": title or \"Error\",\n",
                "                \"url\": url,\n",
                "                \"markdown\": f\"**Error fetching content from** `{url}`: {e}\"\n",
                "            })\n",
                "\n",
                "    return markdown_pages\n",
                "\n",
                "# app.py\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "def test_safesearch_settings():\n",
                "    \"\"\"Test how different safesearch settings affect search results.\"\"\"\n",
                "    \n",
                "    # The query that might return different results based on safesearch\n",
                "    query = \"controversial topics 2023\"\n",
                "    max_results = 3\n",
                "    \n",
                "    print(f\"Testing safesearch settings with query: '{query}'\")\n",
                "    \n",
                "    # Test with safesearch off\n",
                "    print(\"\\n--- SafeSearch: OFF ---\")\n",
                "    clear_visited_pages()  # Start fresh for each test\n",
                "    # TODO: Call search_and_fetch_markdown with safesearch=\"off\"\n",
                "    results_off = None\n",
                "    \n",
                "    print(f\"Found {len(results_off) if results_off else 0} results:\")\n",
                "    for i, result in enumerate(results_off or [], 1):\n",
                "        print(f\"{i}. {result['title']} - {result['url']}\")\n",
                "    \n",
                "    # Test with safesearch moderate\n",
                "    print(\"\\n--- SafeSearch: MODERATE ---\")\n",
                "    clear_visited_pages()\n",
                "    # TODO: Call search_and_fetch_markdown with safesearch=\"moderate\"\n",
                "    results_moderate = None\n",
                "    \n",
                "    print(f\"Found {len(results_moderate) if results_moderate else 0} results:\")\n",
                "    for i, result in enumerate(results_moderate or [], 1):\n",
                "        print(f\"{i}. {result['title']} - {result['url']}\")\n",
                "    \n",
                "    # Test with safesearch strict\n",
                "    print(\"\\n--- SafeSearch: STRICT ---\")\n",
                "    clear_visited_pages()\n",
                "    # TODO: Call search_and_fetch_markdown with safesearch=\"strict\"\n",
                "    results_strict = None\n",
                "    \n",
                "    print(f\"Found {len(results_strict) if results_strict else 0} results:\")\n",
                "    for i, result in enumerate(results_strict or [], 1):\n",
                "        print(f\"{i}. {result['title']} - {result['url']}\")\n",
                "    \n",
                "    # Compare results\n",
                "    print(\"\\n--- COMPARISON ---\")\n",
                "    \n",
                "    # TODO: Add code to compare the results from different safesearch settings\n",
                "    # Hint: Create sets of URLs from each result list and compare them\n",
                "\n",
                "\n",
                "def test_region_settings():\n",
                "    \"\"\"Test how different region settings affect search results.\"\"\"\n",
                "    \n",
                "    # A query that might return different results based on region\n",
                "    query = \"local news today\"\n",
                "    max_results = 3\n",
                "    \n",
                "    # Define regions to test\n",
                "    regions = {\n",
                "        \"us-en\": \"United States (English)\",\n",
                "        \"uk-en\": \"United Kingdom (English)\",\n",
                "        \"de-de\": \"Germany (German)\"\n",
                "    }\n",
                "    \n",
                "    print(f\"Testing region settings with query: '{query}'\")\n",
                "    \n",
                "    # Store results for each region\n",
                "    all_results = {}\n",
                "    \n",
                "    # Test each region\n",
                "    for region_code, region_name in regions.items():\n",
                "        print(f\"\\n--- Region: {region_name} ({region_code}) ---\")\n",
                "        clear_visited_pages()  # Start fresh for each test\n",
                "        \n",
                "        # TODO: Call search_and_fetch_markdown with the current region_code\n",
                "        results = None\n",
                "        \n",
                "        all_results[region_code] = results or []\n",
                "        \n",
                "        print(f\"Found {len(results) if results else 0} results:\")\n",
                "        for i, result in enumerate(results or [], 1):\n",
                "            print(f\"{i}. {result['title']} - {result['url']}\")\n",
                "    \n",
                "    # Compare results between regions\n",
                "    print(\"\\n--- COMPARISON ---\")\n",
                "    \n",
                "    # TODO: Add code to compare the results from different regions\n",
                "    # Hint: Create sets of URLs for each region and compare them\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    test_safesearch_settings()\n",
                "    test_region_settings()\n",
                "```\n",
                "\n",
                "Tentu, saya akan melengkapi fungsi `search_and_fetch_markdown` di `web_searcher.py` dengan parameter `region` dan `safesearch`, serta menyelesaikan *file test* (`app.py`) dengan memanggil fungsi tersebut menggunakan parameter yang berbeda.\n",
                "\n",
                "## 1\\. Kode `web_searcher.py` yang Diperbarui\n",
                "\n",
                "Saya telah menambahkan parameter `region` dan `safesearch` ke `search_and_fetch_markdown` dan memperbarui panggilan `ddgs.text()`:\n",
                "\n",
                "```python\n",
                "import httpx\n",
                "from typing import List, Dict, Set\n",
                "from ddgs import DDGS\n",
                "from html_to_markdown import convert_to_markdown\n",
                "\n",
                "# Global set to track visited URLs\n",
                "_visited_pages: Set[str] = set()\n",
                "\n",
                "\n",
                "def fetch_web_page(url, timeout: int = 10):\n",
                "    \"\"\"\n",
                "    Fetch the content of a web page using httpx.\n",
                "    \"\"\"\n",
                "    response = httpx.get(url, timeout=timeout)\n",
                "    response.raise_for_status()\n",
                "    return response.text\n",
                "\n",
                "\n",
                "def html_to_markdown_converter(html_content, max_length=500):\n",
                "    \"\"\"\n",
                "    Convert HTML content to Markdown format and trim to specified length.\n",
                "    \"\"\"\n",
                "    markdown_content = convert_to_markdown(html_content)\n",
                "    trimmed_markdown = markdown_content.strip()[:max_length]\n",
                "    return trimmed_markdown\n",
                "\n",
                "\n",
                "def clear_visited_pages() -> None:\n",
                "    \"\"\"Clear the global set of visited pages.\"\"\"\n",
                "    global _visited_pages\n",
                "    _visited_pages.clear()\n",
                "\n",
                "\n",
                "# TODO: Add a region parameter with \"wt-wt\" as default and a safesearch parameter with \"moderate\" as default\n",
                "def search_and_fetch_markdown(\n",
                "    query: str,\n",
                "    max_results: int = 5,\n",
                "    timeout: int = 10,\n",
                "    region: str = \"wt-wt\",  # Default: Worldwide/All regions\n",
                "    safesearch: str = \"moderate\" # Default: Moderate filtering\n",
                ") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Search the web for the query, fetch multiple results, and return their Markdown content.\n",
                "    Skips already-visited URLs.\n",
                "    \"\"\"\n",
                "    ddgs = DDGS(timeout=timeout)\n",
                "    # TODO: Update the search call to pass the region and safesearch parameters\n",
                "    results = ddgs.text(\n",
                "        query,\n",
                "        max_results=max_results,\n",
                "        region=region,\n",
                "        safesearch=safesearch\n",
                "    )\n",
                "    markdown_pages = []\n",
                "\n",
                "    for result in results:\n",
                "        url = result.get(\"href\")\n",
                "        title = result.get(\"title\", \"\")\n",
                "\n",
                "        if not url or url in _visited_pages:\n",
                "            continue  # skip already-visited or invalid URLs\n",
                "\n",
                "        try:\n",
                "            html = fetch_web_page(url, timeout=timeout)\n",
                "            markdown = html_to_markdown_converter(html)\n",
                "            _visited_pages.add(url)\n",
                "            markdown_pages.append({\n",
                "                \"title\": title,\n",
                "                \"url\": url,\n",
                "                \"markdown\": markdown\n",
                "            })\n",
                "        except Exception as e:\n",
                "            _visited_pages.add(url)  # mark it to avoid retrying this bad page\n",
                "            markdown_pages.append({\n",
                "                \"title\": title or \"Error\",\n",
                "                \"url\": url,\n",
                "                \"markdown\": f\"**Error fetching content from** `{url}`: {e}\"\n",
                "            })\n",
                "\n",
                "    return markdown_pages\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## 2\\. Kode `app.py` yang Diperbarui\n",
                "\n",
                "Berikut adalah kode pengujian yang sudah dilengkapi dengan panggilan fungsi yang benar dan logika perbandingan hasil:\n",
                "\n",
                "```python\n",
                "# app.py\n",
                "from deepresearcher.web.web_searcher import search_and_fetch_markdown, clear_visited_pages\n",
                "\n",
                "def test_safesearch_settings():\n",
                "    \"\"\"Test how different safesearch settings affect search results.\"\"\"\n",
                "    \n",
                "    # The query that might return different results based on safesearch\n",
                "    query = \"controversial topics 2023\"\n",
                "    max_results = 3\n",
                "    \n",
                "    print(f\"Testing safesearch settings with query: '{query}'\")\n",
                "    \n",
                "    # Test with safesearch off\n",
                "    print(\"\\n--- SafeSearch: OFF ---\")\n",
                "    clear_visited_pages()  # Start fresh for each test\n",
                "    # TODO: Call search_and_fetch_markdown with safesearch=\"off\"\n",
                "    results_off = search_and_fetch_markdown(query, max_results=max_results, safesearch=\"off\")\n",
                "    \n",
                "    print(f\"Found {len(results_off) if results_off else 0} results:\")\n",
                "    for i, result in enumerate(results_off or [], 1):\n",
                "        print(f\"{i}. {result['title']} - {result['url']}\")\n",
                "    \n",
                "    # Test with safesearch moderate\n",
                "    print(\"\\n--- SafeSearch: MODERATE ---\")\n",
                "    clear_visited_pages()\n",
                "    # TODO: Call search_and_fetch_markdown with safesearch=\"moderate\"\n",
                "    results_moderate = search_and_fetch_markdown(query, max_results=max_results, safesearch=\"moderate\")\n",
                "    \n",
                "    print(f\"Found {len(results_moderate) if results_moderate else 0} results:\")\n",
                "    for i, result in enumerate(results_moderate or [], 1):\n",
                "        print(f\"{i}. {result['title']} - {result['url']}\")\n",
                "    \n",
                "    # Test with safesearch strict\n",
                "    print(\"\\n--- SafeSearch: STRICT ---\")\n",
                "    clear_visited_pages()\n",
                "    # TODO: Call search_and_fetch_markdown with safesearch=\"strict\"\n",
                "    results_strict = search_and_fetch_markdown(query, max_results=max_results, safesearch=\"strict\")\n",
                "    \n",
                "    print(f\"Found {len(results_strict) if results_strict else 0} results:\")\n",
                "    for i, result in enumerate(results_strict or [], 1):\n",
                "        print(f\"{i}. {result['title']} - {result['url']}\")\n",
                "    \n",
                "    # Compare results\n",
                "    print(\"\\n--- COMPARISON ---\")\n",
                "    \n",
                "    # TODO: Add code to compare the results from different safesearch settings\n",
                "    urls_off = {r['url'] for r in results_off}\n",
                "    urls_moderate = {r['url'] for r in results_moderate}\n",
                "    urls_strict = {r['url'] for r in results_strict}\n",
                "    \n",
                "    print(f\"URLs in OFF but NOT in MODERATE: {urls_off - urls_moderate}\")\n",
                "    print(f\"URLs in MODERATE but NOT in STRICT: {urls_moderate - urls_strict}\")\n",
                "\n",
                "\n",
                "def test_region_settings():\n",
                "    \"\"\"Test how different region settings affect search results.\"\"\"\n",
                "    \n",
                "    # A query that might return different results based on region\n",
                "    query = \"local news today\"\n",
                "    max_results = 3\n",
                "    \n",
                "    # Define regions to test\n",
                "    regions = {\n",
                "        \"us-en\": \"United States (English)\",\n",
                "        \"uk-en\": \"United Kingdom (English)\",\n",
                "        \"de-de\": \"Germany (German)\"\n",
                "    }\n",
                "    \n",
                "    print(f\"Testing region settings with query: '{query}'\")\n",
                "    \n",
                "    # Store results for each region\n",
                "    all_results = {}\n",
                "    \n",
                "    # Test each region\n",
                "    for region_code, region_name in regions.items():\n",
                "        print(f\"\\n--- Region: {region_name} ({region_code}) ---\")\n",
                "        clear_visited_pages()  # Start fresh for each test\n",
                "        \n",
                "        # TODO: Call search_and_fetch_markdown with the current region_code\n",
                "        results = search_and_fetch_markdown(query, max_results=max_results, region=region_code)\n",
                "        \n",
                "        all_results[region_code] = results or []\n",
                "        \n",
                "        print(f\"Found {len(results) if results else 0} results:\")\n",
                "        for i, result in enumerate(results or [], 1):\n",
                "            print(f\"{i}. {result['title']} - {result['url']}\")\n",
                "    \n",
                "    # Compare results between regions\n",
                "    print(\"\\n--- COMPARISON ---\")\n",
                "    \n",
                "    # TODO: Add code to compare the results from different regions\n",
                "    urls_us = {r['url'] for r in all_results['us-en']}\n",
                "    urls_uk = {r['url'] for r in all_results['uk-en']}\n",
                "    urls_de = {r['url'] for r in all_results['de-de']}\n",
                "    \n",
                "    print(f\"URLs found ONLY in US: {urls_us - urls_uk - urls_de}\")\n",
                "    print(f\"URLs found ONLY in UK: {urls_uk - urls_us - urls_de}\")\n",
                "    print(f\"URLs found ONLY in DE: {urls_de - urls_us - urls_uk}\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    test_safesearch_settings()\n",
                "    test_region_settings()\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
