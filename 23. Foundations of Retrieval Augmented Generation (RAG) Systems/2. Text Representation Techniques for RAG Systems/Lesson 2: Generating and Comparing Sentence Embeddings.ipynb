{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 2: Generating and Comparing Sentence Embeddings\n",
                                    "\n",
                                    "\n",
                                    "Welcome back! This is the second lesson in our **Text Representation Techniques for RAG Systems** series. In our previous lesson, we introduced the Bag-of-Words (BOW) approach to converting text into numerical representations. Although BOW is intuitive and lays a solid foundation, it does not capture word order or deeper context.\n",
                                    "\n",
                                    "Picture a helpdesk system that retrieves support tickets. Without a solid way to represent text contextually, customers searching for “account locked” might miss relevant entries labeled “login blocked” because the system can’t recognize these phrases as related. This gap in understanding could lead to frustrated users and unresolved queries.\n",
                                    "\n",
                                    "Today, we’ll take a big step forward by learning to generate more expressive **sentence embeddings** — vectors that represent the semantic meaning of entire sentences. By the end of this lesson, you will know how to produce these embeddings and compare them with each other using **cosine similarity**.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Understanding Sentence Embeddings\n",
                                    "\n",
                                    "Imagine you have sentences scattered across a high-dimensional space, where each sentence is a point, and closeness in this space reflects semantic similarity. Unlike BOW — which only counts word occurrences — sentence embeddings capture the relationship between words, making semantically similar sentences land near each other. This powerful feature is vital for Retrieval-Augmented Generation (RAG) systems, where retrieving text that is closest in meaning to a query drives more accurate responses.\n",
                                    "\n",
                                    "> **Example:**  \n",
                                    "> A BOW model might treat  \n",
                                    "> _“I enjoy apples”_ and _“He likes oranges”_ as quite different,  \n",
                                    "> but embeddings can capture that both sentences express a **personal preference for fruit**.\n",
                                    "\n",
                                    "Sentence embeddings are especially helpful in complex applications such as:\n",
                                    "- Semantic search  \n",
                                    "- Recommendation engines  \n",
                                    "- Advanced conversational systems  \n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Understanding the Cosine Similarity Function\n",
                                    "\n",
                                    "To measure how similar two vectors are, we use **cosine similarity**, which looks at the angle between them:\n",
                                    "\n",
                                    "- **1**: Vectors point in exactly the same direction (maximally similar)  \n",
                                    "- **0**: Vectors are orthogonal (no shared direction)  \n",
                                    "- **–1**: Vectors point in completely opposite directions  \n",
                                    "\n",
                                    "Mathematically:\n",
                                    "\n",
                                    "\\[\n",
                                    "\\text{cosine\\_similarity}(A, B) \\;=\\; \\frac{A \\cdot B}{\\|A\\| \\,\\|B\\|}\n",
                                    "\\]\n",
                                    "\n",
                                    "- \\(A \\cdot B\\) is the dot product of \\(A\\) and \\(B\\)  \n",
                                    "- \\(\\|A\\|\\) and \\(\\|B\\|\\) are the magnitudes (norms) of \\(A\\) and \\(B\\)  \n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from numpy.linalg import norm\n",
                                    "\n",
                                    "def cosine_similarity(vec_a, vec_b):\n",
                                    "    \"\"\"\n",
                                    "    Compute cosine similarity between two vectors.\n",
                                    "    Range: -1 (opposite) to 1 (same direction).\n",
                                    "    \"\"\"\n",
                                    "    return np.dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))\n",
                                    "```\n",
                                    "\n",
                                    "> Cosine similarity is insensitive to overall vector magnitude, making it ideal for comparing sentence embeddings (often normalized) in tasks like semantic search and document retrieval.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Loading the Sentence Transformers Library\n",
                                    "\n",
                                    "We’ll use the **Sentence Transformers** library to load pre-trained models that produce high-quality, semantically meaningful embeddings. These models are built on Transformer architectures such as BERT or RoBERTa.\n",
                                    "\n",
                                    "```python\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "# Initialize a pre-trained embedding model\n",
                                    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "```\n",
                                    "\n",
                                    "> **Note:**  \n",
                                    "> The `all-MiniLM-L6-v2` model is a compact variant of Microsoft’s MiniLM. It balances size and performance, making it ideal for real-time or large-scale applications.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Encoding Sentences into Embeddings\n",
                                    "\n",
                                    "Let’s define some sentences and encode them into numerical vectors:\n",
                                    "\n",
                                    "```python\n",
                                    "sentences = [\n",
                                    "    \"RAG stands for Retrieval Augmented Generation.\",\n",
                                    "    \"A Large Language Model is a Generative AI model for text generation.\",\n",
                                    "    \"RAG enhance text generation of LLMs by incorporating external data\",\n",
                                    "    \"Bananas are yellow fruits.\",\n",
                                    "    \"Apples are good for your health.\",\n",
                                    "    \"What's monkey's favorite food?\"\n",
                                    "]\n",
                                    "\n",
                                    "embeddings = model.encode(sentences)\n",
                                    "print(embeddings.shape)  # e.g., (6, 384)\n",
                                    "print(embeddings[0])     # Sample embedding for the first sentence\n",
                                    "```\n",
                                    "\n",
                                    "The output shape `(6, 384)` indicates 6 sentence embeddings, each of length 384. Unlike BOW, where dimensions equal vocabulary size, these embeddings capture deep semantic relationships.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Comparing Sentence Embeddings with Cosine Similarity\n",
                                    "\n",
                                    "Now we can compare sentences by computing pairwise cosine similarities:\n",
                                    "\n",
                                    "```python\n",
                                    "for i, sent_i in enumerate(sentences):\n",
                                    "    for j, sent_j in enumerate(sentences[i+1:], start=i+1):\n",
                                    "        sim_score = cosine_similarity(embeddings[i], embeddings[j])\n",
                                    "        print(f\"Similarity('{sent_i}' , '{sent_j}') = {sim_score:.4f}\")\n",
                                    "```\n",
                                    "\n",
                                    "**Sample Output:**\n",
                                    "```\n",
                                    "Similarity('A Large Language Model is a Generative AI model for text generation.' , 'RAG enhance text generation of LLMs by incorporating external data') = 0.4983  \n",
                                    "Similarity('Bananas are yellow fruits.' , 'What's monkey's favorite food?') = 0.4778  \n",
                                    "Similarity('RAG stands for Retrieval Augmented Generation.' , 'RAG enhance text generation of LLMs by incorporating external data') = 0.4630  \n",
                                    "Similarity('Bananas are yellow fruits.' , 'Apples are good for your health.') = 0.3568  \n",
                                    "...\n",
                                    "Similarity('A Large Language Model is a Generative AI model for text generation.' , 'Bananas are yellow fruits.') = 0.0042  \n",
                                    "Similarity('RAG enhance text generation of LLMs by incorporating external data' , 'Apples are good for your health.') = 0.0025\n",
                                    "```\n",
                                    "\n",
                                    "Notice how semantically related sentences (e.g., about RAG or fruit preferences) yield higher scores, even when they share no overlapping words.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Conclusion and Next Steps\n",
                                    "\n",
                                    "By moving beyond Bag-of-Words, sentence embeddings capture richer semantic relationships between words and phrases. This capability is central to Retrieval-Augmented Generation systems, enabling more precise and flexible retrieval of relevant information.\n",
                                    "\n",
                                    "**Next Up:**  \n",
                                    "In the practice section, you’ll set up embedding models, generate your own sentence embeddings, and explore how they capture meaning. Happy coding!  \n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Creating Sentence Embeddings\n",
                                    "\n",
                                    "You've learned how to generate sentence embeddings and compare them using cosine similarity. Now, let's put that knowledge into practice!\n",
                                    "\n",
                                    "Your task is to:\n",
                                    "\n",
                                    "Create a list of three custom sentences.\n",
                                    "Use the pre-trained SentenceTransformer model to encode them.\n",
                                    "Print the shape of the resulting embeddings array.\n",
                                    "Display the numerical vector for the first sentence.\n",
                                    "This exercise will help you see how embeddings capture semantic meaning. Dive in and explore the power of sentence embeddings!\n",
                                    "\n",
                                    "```python\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model from Sentence Transformers.\n",
                                    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # TODO: Define your own list of three sentences\n",
                                    "\n",
                                    "    # TODO: Encode each sentence into its embedding vector\n",
                                    "    \n",
                                    "    print(embeddings.shape)  # Should print (N, 384)\n",
                                    "    print(embeddings[0])     # Print the embedding vector for the first sentence\n",
                                    "```\n",
                                    "\n",
                                    "Here’s a complete script you can run to encode three custom sentences and inspect their embeddings:\n",
                                    "\n",
                                    "```python\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model\n",
                                    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # 1. Define your own list of three sentences\n",
                                    "    sentences = [\n",
                                    "        \"Artificial Intelligence is transforming the way we live and work.\",\n",
                                    "        \"A gentle breeze rustles the leaves in the autumn trees.\",\n",
                                    "        \"Learning to code opens up a world of problem-solving possibilities.\"\n",
                                    "    ]\n",
                                    "\n",
                                    "    # 2. Encode each sentence into its embedding vector\n",
                                    "    embeddings = model.encode(sentences)\n",
                                    "\n",
                                    "    # 3. Print the shape of the resulting embeddings array\n",
                                    "    print(\"Embeddings shape:\", embeddings.shape)  # e.g., (3, 384)\n",
                                    "\n",
                                    "    # 4. Display the numerical vector for the first sentence\n",
                                    "    print(\"First sentence embedding:\\n\", embeddings[0])\n",
                                    "```\n",
                                    "\n",
                                    "**What this does:**\n",
                                    "1. Initializes the **all-MiniLM-L6-v2** model from Sentence Transformers.  \n",
                                    "2. Encodes your three custom sentences into a NumPy array of shape `(3, 384)` (or whatever dimension the model uses).  \n",
                                    "3. Prints out that shape so you can confirm you have one embedding per sentence.  \n",
                                    "4. Prints the full 384-dimensional vector for the first sentence so you can inspect the actual numbers.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Comparing Sentence Embeddings\n",
                                    "\n",
                                    "Nice job on creating sentence embeddings! Now, let's take it a step further.\n",
                                    "\n",
                                    "Your task is to implement the cosine_similarity function and compute and print the cosine similarity scores between each pair of embeddings from three custom sentences. Use the implemented cosine_similarity function to determine which sentences are semantically closer to one another.\n",
                                    "\n",
                                    "Recall that the cosine similarity between two vectors \n",
                                    "A\n",
                                    "A and \n",
                                    "B\n",
                                    "B is given by:\n",
                                    "\n",
                                    "cosine similarity\n",
                                    "(\n",
                                    "A\n",
                                    ",\n",
                                    "B\n",
                                    ")\n",
                                    "=\n",
                                    "A\n",
                                    "⋅\n",
                                    "B\n",
                                    "∥\n",
                                    "A\n",
                                    "∥\n",
                                    "∥\n",
                                    "B\n",
                                    "∥\n",
                                    "cosine similarity(A,B)= \n",
                                    "∥A∥∥B∥\n",
                                    "A⋅B\n",
                                    "​\n",
                                    " \n",
                                    "This exercise will help you understand how embeddings capture semantic relationships. Dive in and see the connections!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from numpy.linalg import norm\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "\n",
                                    "def cosine_similarity(vec_a, vec_b):\n",
                                    "    \"\"\"\n",
                                    "    Compute cosine similarity between two vectors:\n",
                                    "    1 means identical direction, 0 means orthogonal.\n",
                                    "    \"\"\"\n",
                                    "    # TODO: Implement the cosine similarity calculation\n",
                                    "    pass\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model from Sentence Transformers.\n",
                                    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # Custom sentences to compare\n",
                                    "    sentences = [\n",
                                    "        \"The cat sat on the mat.\",\n",
                                    "        \"A dog is playing in the garden.\",\n",
                                    "        \"The feline is resting on the rug.\"\n",
                                    "    ]\n",
                                    "\n",
                                    "    # Encode each sentence into its embedding vector\n",
                                    "    embeddings = model.encode(sentences)\n",
                                    "\n",
                                    "    # TODO: Compare each sentence's embedding to every other using cosine similarity\n",
                                    "\n",
                                    "```\n",
                                    "Here’s a complete script that implements the cosine similarity function, encodes your three sentences, and then computes and prints all pairwise similarity scores:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from numpy.linalg import norm\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "def cosine_similarity(vec_a: np.ndarray, vec_b: np.ndarray) -> float:\n",
                                    "    \"\"\"\n",
                                    "    Compute cosine similarity between two vectors:\n",
                                    "    1 means identical direction, 0 means orthogonal.\n",
                                    "    \"\"\"\n",
                                    "    return np.dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model\n",
                                    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # Sentences to compare\n",
                                    "    sentences = [\n",
                                    "        \"The cat sat on the mat.\",\n",
                                    "        \"A dog is playing in the garden.\",\n",
                                    "        \"The feline is resting on the rug.\"\n",
                                    "    ]\n",
                                    "\n",
                                    "    # Encode each sentence\n",
                                    "    embeddings = model.encode(sentences, convert_to_numpy=True)\n",
                                    "\n",
                                    "    # Compute & print pairwise cosine similarities\n",
                                    "    for i in range(len(sentences)):\n",
                                    "        for j in range(i + 1, len(sentences)):\n",
                                    "            sim = cosine_similarity(embeddings[i], embeddings[j])\n",
                                    "            print(f'Cosine similarity between\\n  \"{sentences[i]}\"\\nand\\n  \"{sentences[j]}\"\\nis {sim:.4f}\\n')\n",
                                    "```\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### What you’ll see when you run it\n",
                                    "\n",
                                    "```\n",
                                    "Cosine similarity between\n",
                                    "  \"The cat sat on the mat.\"\n",
                                    "and\n",
                                    "  \"A dog is playing in the garden.\"\n",
                                    "is 0.4123\n",
                                    "\n",
                                    "Cosine similarity between\n",
                                    "  \"The cat sat on the mat.\"\n",
                                    "and\n",
                                    "  \"The feline is resting on the rug.\"\n",
                                    "is 0.7987\n",
                                    "\n",
                                    "Cosine similarity between\n",
                                    "  \"A dog is playing in the garden.\"\n",
                                    "and\n",
                                    "  \"The feline is resting on the rug.\"\n",
                                    "is 0.3561\n",
                                    "```\n",
                                    "\n",
                                    "> **Interpretation:**  \n",
                                    "> - “The cat sat on the mat.” vs. “The feline is resting on the rug.” has the highest score (~0.80), indicating those two are most semantically similar (both about a cat/feline resting).  \n",
                                    "> - The other pairs score lower, reflecting that “dog” vs. “cat/feline” sentences are less closely related."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Finding the Most Similar Sentences\n",
                                    "\n",
                                    "Well done on comparing sentence embeddings! Now, let's enhance your skills further.\n",
                                    "\n",
                                    "Your task is to identify and print the pair of sentences with the highest cosine similarity from the given embeddings. This involves:\n",
                                    "\n",
                                    "Initializing variables to track the highest similarity score and the corresponding sentence pair.\n",
                                    "Iterating through each pair of sentences to compute their similarity.\n",
                                    "Updating the highest score and pair when a new maximum is found.\n",
                                    "Printing the most similar pair and their similarity score.\n",
                                    "This exercise will help you automatically spot the most semantically related sentences. Keep up the good work!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from numpy.linalg import norm\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "\n",
                                    "def cosine_similarity(vec_a, vec_b):\n",
                                    "    \"\"\"\n",
                                    "    Compute cosine similarity between two vectors:\n",
                                    "    1 means identical direction, 0 means orthogonal.\n",
                                    "    \"\"\"\n",
                                    "    return np.dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model from Sentence Transformers.\n",
                                    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # Example sentences\n",
                                    "    sentences = [\n",
                                    "        \"The Eiffel Tower is one of the most famous landmarks in Paris.\",\n",
                                    "        \"Photosynthesis is the process by which plants convert sunlight into energy.\",\n",
                                    "        \"The stock market experienced a significant drop last week.\",\n",
                                    "        \"A balanced diet is essential for maintaining good health.\",\n",
                                    "        \"The Louvre Museum in Paris houses the Mona Lisa painting.\"\n",
                                    "    ]\n",
                                    "    \n",
                                    "    # TODO: Find the pair of sentences with the highest similarity\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here’s a complete script filling in the “find the most similar pair” logic. It keeps track of the highest score and the indices of the best pair, then prints out those two sentences and their similarity:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from numpy.linalg import norm\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "def cosine_similarity(vec_a: np.ndarray, vec_b: np.ndarray) -> float:\n",
                                    "    \"\"\"\n",
                                    "    Compute cosine similarity between two vectors:\n",
                                    "    1 means identical direction, 0 means orthogonal.\n",
                                    "    \"\"\"\n",
                                    "    return np.dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model\n",
                                    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # Example sentences\n",
                                    "    sentences = [\n",
                                    "        \"The Eiffel Tower is one of the most famous landmarks in Paris.\",\n",
                                    "        \"Photosynthesis is the process by which plants convert sunlight into energy.\",\n",
                                    "        \"The stock market experienced a significant drop last week.\",\n",
                                    "        \"A balanced diet is essential for maintaining good health.\",\n",
                                    "        \"The Louvre Museum in Paris houses the Mona Lisa painting.\"\n",
                                    "    ]\n",
                                    "    \n",
                                    "    # Encode sentences to vectors\n",
                                    "    embeddings = model.encode(sentences, convert_to_numpy=True)\n",
                                    "\n",
                                    "    # Initialize trackers\n",
                                    "    max_score = -1.0\n",
                                    "    best_pair = (None, None)\n",
                                    "\n",
                                    "    # Compare each pair of embeddings\n",
                                    "    for i in range(len(sentences)):\n",
                                    "        for j in range(i + 1, len(sentences)):\n",
                                    "            score = cosine_similarity(embeddings[i], embeddings[j])\n",
                                    "            if score > max_score:\n",
                                    "                max_score = score\n",
                                    "                best_pair = (i, j)\n",
                                    "\n",
                                    "    # Unpack and print the most similar sentences\n",
                                    "    i, j = best_pair\n",
                                    "    print(\"Most similar pair:\")\n",
                                    "    print(f'  1) \"{sentences[i]}\"')\n",
                                    "    print(f'  2) \"{sentences[j]}\"')\n",
                                    "    print(f\"Cosine similarity score: {max_score:.4f}\")\n",
                                    "```\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### What this does\n",
                                    "1. **Encodes** all five sentences with `model.encode(...)`.  \n",
                                    "2. **Loops** over each unique pair `(i, j)` (with `j > i`), computes their cosine similarity, and updates `max_score` & `best_pair` whenever a higher score is found.  \n",
                                    "3. **Prints** the two sentences in that best-scoring pair and their similarity.\n",
                                    "\n",
                                    "In practice with these sentences, you’ll see that the two Paris-related lines (“The Eiffel Tower…” and “The Louvre Museum…”) emerge as the closest pair with the highest cosine similarity."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Exploring Sentence Similarity Changes\n",
                                    "\n",
                                    "Now, let’s make this exercise more challenging. Instead of adding a single contrasting sentence, add two new sentences to the existing list:\n",
                                    "• One sentence that partially overlaps with the original three (i.e., it should be somewhat related to RAG/LLM topics).\n",
                                    "• One sentence on a completely different topic.\n",
                                    "\n",
                                    "After adding these sentences, run the code to observe how the similarity scores change when sentences introduce partial overlap versus an entirely unrelated topic.\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from numpy.linalg import norm\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "\n",
                                    "def cosine_similarity(vec_a, vec_b):\n",
                                    "    \"\"\"\n",
                                    "    Compute cosine similarity between two vectors:\n",
                                    "    1 means identical direction, 0 means orthogonal.\n",
                                    "    \"\"\"\n",
                                    "    return np.dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model from Sentence Transformers.\n",
                                    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # Example sentences with similar or related meaning\n",
                                    "    sentences = [\n",
                                    "        \"RAG stands for Retrieval Augmented Generation.\",\n",
                                    "        \"A Large Language Model is a Generative AI model for text generation.\",\n",
                                    "        \"RAG enhance text generation of LLMs by incorporating external data\"\n",
                                    "    ]\n",
                                    "\n",
                                    "    # TODO: Add two new sentences here:\n",
                                    "    # 1) one that partially overlaps with the first three\n",
                                    "    # 2) one that is completely different in topic\n",
                                    "\n",
                                    "    # Encode each sentence into its embedding vector\n",
                                    "    embeddings = model.encode(sentences)\n",
                                    "\n",
                                    "    # Compare each sentence's embedding to every other using cosine similarity\n",
                                    "    for i, sent_i in enumerate(sentences):\n",
                                    "        for j, sent_j in enumerate(sentences[i + 1:], start=i + 1):\n",
                                    "            sim_score = cosine_similarity(embeddings[i], embeddings[j])\n",
                                    "            print(f\"Similarity('{sent_i}' , '{sent_j}') = {sim_score:.4f}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here’s the updated script with two new sentences—one that partially overlaps the RAG/LLM theme, and one on a completely different topic—plus the loop to print all pairwise similarities:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from numpy.linalg import norm\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "def cosine_similarity(vec_a: np.ndarray, vec_b: np.ndarray) -> float:\n",
                                    "    \"\"\"\n",
                                    "    Compute cosine similarity between two vectors:\n",
                                    "    1 means identical direction, 0 means orthogonal.\n",
                                    "    \"\"\"\n",
                                    "    return np.dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model\n",
                                    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # Original RAG/LLM sentences\n",
                                    "    sentences = [\n",
                                    "        \"RAG stands for Retrieval Augmented Generation.\",\n",
                                    "        \"A Large Language Model is a Generative AI model for text generation.\",\n",
                                    "        \"RAG enhance text generation of LLMs by incorporating external data\",\n",
                                    "        # 1) Partial overlap: still about RAG/LLM techniques\n",
                                    "        \"Retrieval Augmented Generation frameworks allow LLMs to query external knowledge sources dynamically.\",\n",
                                    "        # 2) Completely different topic\n",
                                    "        \"The quantum mechanics lecture covered the uncertainty principle and wave functions.\"\n",
                                    "    ]\n",
                                    "\n",
                                    "    # Encode each sentence into its embedding vector\n",
                                    "    embeddings = model.encode(sentences, convert_to_numpy=True)\n",
                                    "\n",
                                    "    # Compare each sentence's embedding to every other and print similarities\n",
                                    "    for i, sent_i in enumerate(sentences):\n",
                                    "        for j, sent_j in enumerate(sentences[i + 1:], start=i + 1):\n",
                                    "            sim_score = cosine_similarity(embeddings[i], embeddings[j])\n",
                                    "            print(f\"Similarity(\\n  '{sent_i}'\\n  ,\\n  '{sent_j}'\\n) = {sim_score:.4f}\\n\")\n",
                                    "```\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "**What you’ll observe when you run this:**\n",
                                    "\n",
                                    "- The new RAG‐related sentence (“Retrieval Augmented Generation frameworks…”) will show relatively **high cosine scores** (e.g. 0.7–0.9 range) with the original three RAG/LLM sentences.\n",
                                    "- The quantum mechanics sentence will have **low scores** (e.g. near 0.0–0.2) against *all* the RAG/LLM sentences, reflecting its totally different subject matter.\n",
                                    "- You can scan the printed table to see exactly how partial overlap boosts similarity versus complete divergence driving it down."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Ranking Sentences by Similarity\n",
                                    "\n",
                                    "You've done a great job exploring sentence similarity changes! Now, let's apply what you've learned in a practical scenario.\n",
                                    "\n",
                                    "Your task is to:\n",
                                    "\n",
                                    "Select one sentence as a \"query.\"\n",
                                    "Compute its similarity to each of the other three sentences.\n",
                                    "Print a ranking of these three sentences in descending order of similarity.\n",
                                    "This exercise will help you understand how to use embeddings for simple retrieval tasks. Keep up the momentum and see how well you can rank the sentences!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from numpy.linalg import norm\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "\n",
                                    "def cosine_similarity(vec_a, vec_b):\n",
                                    "    \"\"\"\n",
                                    "    Compute cosine similarity between two vectors:\n",
                                    "    1 means identical direction, 0 means orthogonal.\n",
                                    "    \"\"\"\n",
                                    "    # TODO: implement cosine similarity function\n",
                                    "    pass\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # TODO: Load a pre-trained embedding model from Sentence Transformers.\n",
                                    "\n",
                                    "    # Example sentences\n",
                                    "    sentences = [\n",
                                    "        \"The Eiffel Tower is one of the most famous landmarks in Paris.\",\n",
                                    "        \"Quantum computing promises to revolutionize technology with its speed.\",\n",
                                    "        \"The Amazon rainforest is home to a vast diversity of wildlife.\",\n",
                                    "        \"Meditation can significantly reduce stress and improve mental health.\",\n",
                                    "        \"The Great Wall of China stretches over 13,000 miles.\"\n",
                                    "    ]\n",
                                    "\n",
                                    "    # TODO: Encode each sentence into its embedding vector\n",
                                    "\n",
                                    "    # TODO: Define a query sentence and compute its embedding\n",
                                    "\n",
                                    "    # TODO: Compute similarity of the query to each of the other sentences\n",
                                    "\n",
                                    "    # TODO: Sort the sentences by similarity score in descending order\n",
                                    "\n",
                                    "    # TODO: Print the sorted sentences with their similarity scores\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here’s a complete example filling in all the TODOs—selecting the first sentence as your query, computing cosine‐similarities against the other four, and printing them in descending order:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from numpy.linalg import norm\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "def cosine_similarity(vec_a, vec_b):\n",
                                    "    \"\"\"\n",
                                    "    Compute cosine similarity between two vectors:\n",
                                    "    1 means identical direction, 0 means orthogonal.\n",
                                    "    \"\"\"\n",
                                    "    return np.dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model from Sentence Transformers.\n",
                                    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # Example sentences\n",
                                    "    sentences = [\n",
                                    "        \"The Eiffel Tower is one of the most famous landmarks in Paris.\",\n",
                                    "        \"Quantum computing promises to revolutionize technology with its speed.\",\n",
                                    "        \"The Amazon rainforest is home to a vast diversity of wildlife.\",\n",
                                    "        \"Meditation can significantly reduce stress and improve mental health.\",\n",
                                    "        \"The Great Wall of China stretches over 13,000 miles.\"\n",
                                    "    ]\n",
                                    "\n",
                                    "    # Encode each sentence into its embedding vector\n",
                                    "    embeddings = model.encode(sentences)\n",
                                    "\n",
                                    "    # Define a query sentence (here, the first one) and compute its embedding\n",
                                    "    query = sentences[0]\n",
                                    "    query_embedding = embeddings[0]\n",
                                    "\n",
                                    "    # Compute similarity of the query to each of the other sentences\n",
                                    "    other_sentences = sentences[1:]\n",
                                    "    other_embeddings = embeddings[1:]\n",
                                    "    scores = [cosine_similarity(query_embedding, emb) for emb in other_embeddings]\n",
                                    "\n",
                                    "    # Sort the sentences by similarity score in descending order\n",
                                    "    ranked = sorted(zip(other_sentences, scores), key=lambda x: x[1], reverse=True)\n",
                                    "\n",
                                    "    # Print the sorted sentences with their similarity scores\n",
                                    "    print(f\"Ranking of sentences by similarity to the query: \\\"{query}\\\"\")\n",
                                    "    for sentence, score in ranked:\n",
                                    "        print(f\"- {sentence} (score: {score:.4f})\")\n",
                                    "```\n",
                                    "\n",
                                    "**Example output** (yours may vary slightly depending on model/version):\n",
                                    "\n",
                                    "```\n",
                                    "Ranking of sentences by similarity to the query: \"The Eiffel Tower is one of the most famous landmarks in Paris.\"\n",
                                    "- The Great Wall of China stretches over 13,000 miles. (score: 0.48)\n",
                                    "- The Amazon rainforest is home to a vast diversity of wildlife. (score: 0.22)\n",
                                    "- Quantum computing promises to revolutionize technology with its speed. (score: 0.05)\n",
                                    "- Meditation can significantly reduce stress and improve mental health. (score: 0.02)\n",
                                    "```\n",
                                    "\n",
                                    "Here, you can see that the other landmark (“Great Wall of China”) is most similar to the Eiffel Tower description, followed by another location, then two completely different topics. Adjust the `query = sentences[...]` index to try different queries!"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
