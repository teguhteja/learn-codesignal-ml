{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 2: Generating and Comparing Sentence Embeddings\n",
                                    "\n",
                                    "\n",
                                    "Welcome back! This is the second lesson in our **Text Representation Techniques for RAG Systems** series. In our previous lesson, we introduced the Bag-of-Words (BOW) approach to converting text into numerical representations. Although BOW is intuitive and lays a solid foundation, it does not capture word order or deeper context.\n",
                                    "\n",
                                    "Picture a helpdesk system that retrieves support tickets. Without a solid way to represent text contextually, customers searching for “account locked” might miss relevant entries labeled “login blocked” because the system can’t recognize these phrases as related. This gap in understanding could lead to frustrated users and unresolved queries.\n",
                                    "\n",
                                    "Today, we’ll take a big step forward by learning to generate more expressive **sentence embeddings** — vectors that represent the semantic meaning of entire sentences. By the end of this lesson, you will know how to produce these embeddings and compare them with each other using **cosine similarity**.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Understanding Sentence Embeddings\n",
                                    "\n",
                                    "Imagine you have sentences scattered across a high-dimensional space, where each sentence is a point, and closeness in this space reflects semantic similarity. Unlike BOW — which only counts word occurrences — sentence embeddings capture the relationship between words, making semantically similar sentences land near each other. This powerful feature is vital for Retrieval-Augmented Generation (RAG) systems, where retrieving text that is closest in meaning to a query drives more accurate responses.\n",
                                    "\n",
                                    "> **Example:**  \n",
                                    "> A BOW model might treat  \n",
                                    "> _“I enjoy apples”_ and _“He likes oranges”_ as quite different,  \n",
                                    "> but embeddings can capture that both sentences express a **personal preference for fruit**.\n",
                                    "\n",
                                    "Sentence embeddings are especially helpful in complex applications such as:\n",
                                    "- Semantic search  \n",
                                    "- Recommendation engines  \n",
                                    "- Advanced conversational systems  \n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Understanding the Cosine Similarity Function\n",
                                    "\n",
                                    "To measure how similar two vectors are, we use **cosine similarity**, which looks at the angle between them:\n",
                                    "\n",
                                    "- **1**: Vectors point in exactly the same direction (maximally similar)  \n",
                                    "- **0**: Vectors are orthogonal (no shared direction)  \n",
                                    "- **–1**: Vectors point in completely opposite directions  \n",
                                    "\n",
                                    "Mathematically:\n",
                                    "\n",
                                    "\\[\n",
                                    "\\text{cosine\\_similarity}(A, B) \\;=\\; \\frac{A \\cdot B}{\\|A\\| \\,\\|B\\|}\n",
                                    "\\]\n",
                                    "\n",
                                    "- \\(A \\cdot B\\) is the dot product of \\(A\\) and \\(B\\)  \n",
                                    "- \\(\\|A\\|\\) and \\(\\|B\\|\\) are the magnitudes (norms) of \\(A\\) and \\(B\\)  \n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from numpy.linalg import norm\n",
                                    "\n",
                                    "def cosine_similarity(vec_a, vec_b):\n",
                                    "    \"\"\"\n",
                                    "    Compute cosine similarity between two vectors.\n",
                                    "    Range: -1 (opposite) to 1 (same direction).\n",
                                    "    \"\"\"\n",
                                    "    return np.dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))\n",
                                    "```\n",
                                    "\n",
                                    "> Cosine similarity is insensitive to overall vector magnitude, making it ideal for comparing sentence embeddings (often normalized) in tasks like semantic search and document retrieval.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Loading the Sentence Transformers Library\n",
                                    "\n",
                                    "We’ll use the **Sentence Transformers** library to load pre-trained models that produce high-quality, semantically meaningful embeddings. These models are built on Transformer architectures such as BERT or RoBERTa.\n",
                                    "\n",
                                    "```python\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "# Initialize a pre-trained embedding model\n",
                                    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "```\n",
                                    "\n",
                                    "> **Note:**  \n",
                                    "> The `all-MiniLM-L6-v2` model is a compact variant of Microsoft’s MiniLM. It balances size and performance, making it ideal for real-time or large-scale applications.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Encoding Sentences into Embeddings\n",
                                    "\n",
                                    "Let’s define some sentences and encode them into numerical vectors:\n",
                                    "\n",
                                    "```python\n",
                                    "sentences = [\n",
                                    "    \"RAG stands for Retrieval Augmented Generation.\",\n",
                                    "    \"A Large Language Model is a Generative AI model for text generation.\",\n",
                                    "    \"RAG enhance text generation of LLMs by incorporating external data\",\n",
                                    "    \"Bananas are yellow fruits.\",\n",
                                    "    \"Apples are good for your health.\",\n",
                                    "    \"What's monkey's favorite food?\"\n",
                                    "]\n",
                                    "\n",
                                    "embeddings = model.encode(sentences)\n",
                                    "print(embeddings.shape)  # e.g., (6, 384)\n",
                                    "print(embeddings[0])     # Sample embedding for the first sentence\n",
                                    "```\n",
                                    "\n",
                                    "The output shape `(6, 384)` indicates 6 sentence embeddings, each of length 384. Unlike BOW, where dimensions equal vocabulary size, these embeddings capture deep semantic relationships.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Comparing Sentence Embeddings with Cosine Similarity\n",
                                    "\n",
                                    "Now we can compare sentences by computing pairwise cosine similarities:\n",
                                    "\n",
                                    "```python\n",
                                    "for i, sent_i in enumerate(sentences):\n",
                                    "    for j, sent_j in enumerate(sentences[i+1:], start=i+1):\n",
                                    "        sim_score = cosine_similarity(embeddings[i], embeddings[j])\n",
                                    "        print(f\"Similarity('{sent_i}' , '{sent_j}') = {sim_score:.4f}\")\n",
                                    "```\n",
                                    "\n",
                                    "**Sample Output:**\n",
                                    "```\n",
                                    "Similarity('A Large Language Model is a Generative AI model for text generation.' , 'RAG enhance text generation of LLMs by incorporating external data') = 0.4983  \n",
                                    "Similarity('Bananas are yellow fruits.' , 'What's monkey's favorite food?') = 0.4778  \n",
                                    "Similarity('RAG stands for Retrieval Augmented Generation.' , 'RAG enhance text generation of LLMs by incorporating external data') = 0.4630  \n",
                                    "Similarity('Bananas are yellow fruits.' , 'Apples are good for your health.') = 0.3568  \n",
                                    "...\n",
                                    "Similarity('A Large Language Model is a Generative AI model for text generation.' , 'Bananas are yellow fruits.') = 0.0042  \n",
                                    "Similarity('RAG enhance text generation of LLMs by incorporating external data' , 'Apples are good for your health.') = 0.0025\n",
                                    "```\n",
                                    "\n",
                                    "Notice how semantically related sentences (e.g., about RAG or fruit preferences) yield higher scores, even when they share no overlapping words.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Conclusion and Next Steps\n",
                                    "\n",
                                    "By moving beyond Bag-of-Words, sentence embeddings capture richer semantic relationships between words and phrases. This capability is central to Retrieval-Augmented Generation systems, enabling more precise and flexible retrieval of relevant information.\n",
                                    "\n",
                                    "**Next Up:**  \n",
                                    "In the practice section, you’ll set up embedding models, generate your own sentence embeddings, and explore how they capture meaning. Happy coding!  \n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Creating Sentence Embeddings\n",
                                    "\n",
                                    "You've learned how to generate sentence embeddings and compare them using cosine similarity. Now, let's put that knowledge into practice!\n",
                                    "\n",
                                    "Your task is to:\n",
                                    "\n",
                                    "Create a list of three custom sentences.\n",
                                    "Use the pre-trained SentenceTransformer model to encode them.\n",
                                    "Print the shape of the resulting embeddings array.\n",
                                    "Display the numerical vector for the first sentence.\n",
                                    "This exercise will help you see how embeddings capture semantic meaning. Dive in and explore the power of sentence embeddings!\n",
                                    "\n",
                                    "```python\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model from Sentence Transformers.\n",
                                    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # TODO: Define your own list of three sentences\n",
                                    "\n",
                                    "    # TODO: Encode each sentence into its embedding vector\n",
                                    "    \n",
                                    "    print(embeddings.shape)  # Should print (N, 384)\n",
                                    "    print(embeddings[0])     # Print the embedding vector for the first sentence\n",
                                    "```\n",
                                    "\n",
                                    "Here’s a complete script you can run to encode three custom sentences and inspect their embeddings:\n",
                                    "\n",
                                    "```python\n",
                                    "from sentence_transformers import SentenceTransformer\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a pre-trained embedding model\n",
                                    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                                    "\n",
                                    "    # 1. Define your own list of three sentences\n",
                                    "    sentences = [\n",
                                    "        \"Artificial Intelligence is transforming the way we live and work.\",\n",
                                    "        \"A gentle breeze rustles the leaves in the autumn trees.\",\n",
                                    "        \"Learning to code opens up a world of problem-solving possibilities.\"\n",
                                    "    ]\n",
                                    "\n",
                                    "    # 2. Encode each sentence into its embedding vector\n",
                                    "    embeddings = model.encode(sentences)\n",
                                    "\n",
                                    "    # 3. Print the shape of the resulting embeddings array\n",
                                    "    print(\"Embeddings shape:\", embeddings.shape)  # e.g., (3, 384)\n",
                                    "\n",
                                    "    # 4. Display the numerical vector for the first sentence\n",
                                    "    print(\"First sentence embedding:\\n\", embeddings[0])\n",
                                    "```\n",
                                    "\n",
                                    "**What this does:**\n",
                                    "1. Initializes the **all-MiniLM-L6-v2** model from Sentence Transformers.  \n",
                                    "2. Encodes your three custom sentences into a NumPy array of shape `(3, 384)` (or whatever dimension the model uses).  \n",
                                    "3. Prints out that shape so you can confirm you have one embedding per sentence.  \n",
                                    "4. Prints the full 384-dimensional vector for the first sentence so you can inspect the actual numbers.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Comparing Sentence Embeddings"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Finding the Most Similar Sentences"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Exploring Sentence Similarity Changes"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Ranking Sentences by Similarity"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
