{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 2: RAG in Action: A Simple Workflow\n",
                                    "\n",
                                    "\n",
                                    "Welcome to our second lesson in the **“Introduction to RAG”** course! In the previous lesson, you learned how RAG evolved from traditional Information Retrieval (IR). Today, we'll connect those ideas to concrete code and illustrate a simple end-to-end RAG workflow. By the end of this lesson, you'll see how **indexing**, **retrieval**, **prompt augmentation**, and **final text generation** fit together to produce a targeted answer.\n",
                                    "\n",
                                    "In this lesson, we showcase a scenario involving **“Project Chimera.”** Think of this as an internal project of a company—here, it’s just an example to demonstrate how a naive, context-free system can invent inaccurate details, whereas a RAG-based system will provide reliable answers using information from an authoritative knowledge base. We’re deliberately using extremely simplified methods (like simple keyword matching) to illustrate each part of a RAG pipeline. Later, we will explore more realistic and robust approaches—such as embeddings and vector databases—for each component.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## The RAG Workflow: Four Key Steps\n",
                                    "\n",
                                    "1. **Indexing:** Documents are structured in a way that makes them easy to search.  \n",
                                    "2. **Retrieval:** The most relevant piece of text is fetched based on a user query.  \n",
                                    "3. **Prompt (Query) Augmentation:** The retrieved text is combined with the user’s question to form a context-rich prompt.  \n",
                                    "4. **Generation:** A language model processes the prompt and produces a final answer anchored to the provided text.\n",
                                    "\n",
                                    "This process ensures answers are backed by your data, reducing the risk of fabricated or off-topic responses.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## 1. Indexing: Organizing Documents\n",
                                    "\n",
                                    "We start by defining our knowledge base. Below, **Project Chimera** serves as the example domain:\n",
                                    "\n",
                                    "```python\n",
                                    "KNOWLEDGE_BASE = {\n",
                                    "    \"doc1\": {\n",
                                    "        \"title\": \"Project Chimera Overview\",\n",
                                    "        \"content\": (\n",
                                    "            \"Project Chimera is a research initiative focused on developing \"\n",
                                    "            \"novel bio-integrated interfaces. It aims to merge biological \"\n",
                                    "            \"systems with advanced computing technologies.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc2\": {\n",
                                    "        \"title\": \"Chimera's Neural Interface\",\n",
                                    "        \"content\": (\n",
                                    "            \"The core component of Project Chimera is a neural interface \"\n",
                                    "            \"that allows for bidirectional communication between the brain \"\n",
                                    "            \"and external devices. This interface uses biocompatible \"\n",
                                    "            \"nanomaterials.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc3\": {\n",
                                    "        \"title\": \"Applications of Chimera\",\n",
                                    "        \"content\": (\n",
                                    "            \"Potential applications of Project Chimera include advanced \"\n",
                                    "            \"prosthetics, treatment of neurological disorders, and enhanced \"\n",
                                    "            \"human-computer interaction. Ethical considerations are paramount.\"\n",
                                    "        )\n",
                                    "    }\n",
                                    "}\n",
                                    "```\n",
                                    "\n",
                                    "- We define a Python dictionary named `KNOWLEDGE_BASE` that contains multiple documents.  \n",
                                    "- Each entry has an ID (e.g., `\"doc1\"`) and both a `title` and `content` field.  \n",
                                    "- **Project Chimera** information is now the authoritative data source for the RAG system.\n",
                                    "\n",
                                    "> *Note:* This is a very simplified approach for educational purposes; in production, your knowledge base would likely be a vector database or other scalable store. We’ll cover that in Course 3!\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## 2. Retrieval: Locating Relevant Information\n",
                                    "\n",
                                    "Next, we create a function to return the best document from our knowledge base, based on simple keyword overlap:\n",
                                    "\n",
                                    "```python\n",
                                    "def rag_retrieval(query, documents):\n",
                                    "    query_words = set(query.lower().split())\n",
                                    "    best_doc_id = None\n",
                                    "    best_overlap = 0\n",
                                    "    \n",
                                    "    for doc_id, doc in documents.items():\n",
                                    "        # Compare the query words with the document's content words\n",
                                    "        doc_words = set(doc[\"content\"].lower().split())\n",
                                    "        overlap = len(query_words.intersection(doc_words))\n",
                                    "        \n",
                                    "        if overlap > best_overlap:\n",
                                    "            best_overlap = overlap\n",
                                    "            best_doc_id = doc_id\n",
                                    "    \n",
                                    "    # Return the best document, or None if nothing matched\n",
                                    "    return documents.get(best_doc_id)\n",
                                    "```\n",
                                    "\n",
                                    "- The query is split into lowercase words and stored in a set.  \n",
                                    "- Each document’s text is similarly tokenized.  \n",
                                    "- The function picks the document with the greatest word overlap.  \n",
                                    "- If no match is found, it returns `None`.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## 3. Query Augmentation: Creating Context-Rich Prompts\n",
                                    "\n",
                                    "Once we retrieve the relevant document, we augment the user’s original question with that document’s content. This additional context significantly reduces hallucinations:\n",
                                    "\n",
                                    "```python\n",
                                    "def rag_generation(query, document):\n",
                                    "    if document:\n",
                                    "        snippet = f\"{document['title']}: {document['content']}\"\n",
                                    "        prompt = f\"Using the following information: '{snippet}', answer: {query}\"\n",
                                    "    else:\n",
                                    "        prompt = f\"No relevant information found. Answer directly: {query}\"\n",
                                    "    return prompt\n",
                                    "```\n",
                                    "\n",
                                    "- If a document was found, we include its title and content as a “snippet” in the prompt.  \n",
                                    "- We pass this combined query+snippet to the language model.  \n",
                                    "- If no document matches, we still form a direct prompt.\n",
                                    "\n",
                                    "**Example prompt** for the query `\"What is the main goal of Project Chimera?\"`:\n",
                                    "\n",
                                    "> ```\n",
                                    "> Using the following information: 'Chimera's Neural Interface: The core component of Project Chimera is a neural interface that allows for bidirectional communication between the brain and external devices. This interface uses biocompatible nanomaterials.', answer: What is the main goal of Project Chimera?\n",
                                    "> ```\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## 4. Generation: Producing Tailored Answers\n",
                                    "\n",
                                    "Finally, let’s compare a naive approach (which might invent details) with our RAG approach (which leverages the knowledge base):\n",
                                    "\n",
                                    "```python\n",
                                    "def get_llm_response(prompt):\n",
                                    "    \"\"\"\n",
                                    "    This function interfaces with a language model to generate a response\n",
                                    "    based on the provided prompt.\n",
                                    "    \"\"\"\n",
                                    "    pass  # Replace with actual LLM API call\n",
                                    "\n",
                                    "def naive_generation(query):\n",
                                    "    # Ignores the knowledge base entirely\n",
                                    "    prompt = f\"Answer directly the following query: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "def rag_generation(query, document):\n",
                                    "    # Augments the prompt via the knowledge base\n",
                                    "    if document:\n",
                                    "        snippet = f\"{document['title']}: {document['content']}\"\n",
                                    "        prompt = f\"Using the following information: '{snippet}', answer: {query}\"\n",
                                    "    else:\n",
                                    "        prompt = f\"No relevant information found. Answer directly: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "# Demonstration\n",
                                    "query = \"What is the main goal of Project Chimera?\"\n",
                                    "\n",
                                    "naive_answer = naive_generation(query)\n",
                                    "print(\"Naive approach:\", naive_answer)\n",
                                    "\n",
                                    "doc = rag_retrieval(query, KNOWLEDGE_BASE)\n",
                                    "rag_answer = rag_generation(query, doc)\n",
                                    "print(\"RAG approach:\", rag_answer)\n",
                                    "```\n",
                                    "\n",
                                    "- **`naive_generation`** may produce a plausible-sounding but incorrect answer.  \n",
                                    "- **`rag_generation`** uses the retrieved snippet to ground its response, reducing hallucinations.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Conclusion and Next Steps\n",
                                    "\n",
                                    "You’ve now implemented:\n",
                                    "\n",
                                    "1. A simple knowledge-base indexing scheme.  \n",
                                    "2. Basic retrieval to find the most relevant document.  \n",
                                    "3. Prompt augmentation to combine user queries with reference data.  \n",
                                    "4. Generation that relies on actual context, lowering the chance of hallucinations.\n",
                                    "\n",
                                    "Next up, you’ll get hands-on practice with these steps in coding exercises. As you progress, you’ll explore how RAG can be extended with **embeddings**, **vector databases**, and more robust retrieval techniques for complex tasks and real-world domains. Let’s continue unlocking the full power of RAG—onward!  \n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implementing Simple Document Retrieval\n",
                                    "\n",
                                    "Hello! You've been doing well so far. In this exercise, you'll practice implementing a simple retrieval function to find the most relevant document from a knowledge base using keyword overlap.\n",
                                    "\n",
                                    "Your task is to complete the rag_retrieval function. Here's what you need to do:\n",
                                    "\n",
                                    "Split the query into lowercase words and store them in a set.\n",
                                    "For each document, split its content into lowercase words and store them in a set.\n",
                                    "Calculate the overlap between the query words and the document words.\n",
                                    "Return the document with the highest overlap.\n",
                                    "Keep up the good work, and let's see how you can apply what you've learned!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "KNOWLEDGE_BASE = {\n",
                                    "    \"doc1\": {\n",
                                    "        \"title\": \"Project Chimera Overview\",\n",
                                    "        \"content\": (\n",
                                    "            \"Project Chimera is a research initiative focused on developing \"\n",
                                    "            \"novel bio-integrated interfaces. It aims to merge biological \"\n",
                                    "            \"systems with advanced computing technologies.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc2\": {\n",
                                    "        \"title\": \"Chimera's Neural Interface\",\n",
                                    "        \"content\": (\n",
                                    "            \"The core component of Project Chimera is a neural interface \"\n",
                                    "            \"that allows for bidirectional communication between the brain \"\n",
                                    "            \"and external devices. This interface uses biocompatible \"\n",
                                    "            \"nanomaterials.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc3\": {\n",
                                    "        \"title\": \"Applications of Chimera\",\n",
                                    "        \"content\": (\n",
                                    "            \"Potential applications of Project Chimera include advanced \"\n",
                                    "            \"prosthetics, treatment of neurological disorders, and enhanced \"\n",
                                    "            \"human-computer interaction. Ethical considerations are paramount.\"\n",
                                    "        )\n",
                                    "    }\n",
                                    "}\n",
                                    "\n",
                                    "\n",
                                    "def naive_generation(query):\n",
                                    "    prompt = f\"Answer directly the following query: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "\n",
                                    "def rag_retrieval(query, documents):\n",
                                    "    # TODO: Split the query into lowercase words and store them in a set\n",
                                    "    # TODO: For each document, split its content into lowercase words and store them in a set\n",
                                    "    # TODO: Calculate the overlap between the query words and document words\n",
                                    "    # TODO: Return the document with the highest overlap\n",
                                    "    pass\n",
                                    "\n",
                                    "\n",
                                    "def rag_generation(query, document):\n",
                                    "    if document:\n",
                                    "        snippet = f\"{document['title']}: {document['content']}\"\n",
                                    "        prompt = f\"Using the following information: '{snippet}', answer: {query}\"\n",
                                    "    else:\n",
                                    "        prompt = f\"No relevant information found. Answer directly: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    query = \"What is the main goal of Project Chimera?\"\n",
                                    "    print(\"Naive approach:\", naive_generation(query))\n",
                                    "    retrieved_doc = rag_retrieval(query, KNOWLEDGE_BASE)\n",
                                    "    print(\"RAG approach:\", rag_generation(query, retrieved_doc))\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here’s the updated `rag_retrieval` function with the TODOs filled in. It splits the query and each document’s content into lowercase word sets, computes the overlap, and returns the document with the highest overlap (or `None` if there’s no match):\n",
                                    "\n",
                                    "```python\n",
                                    "def rag_retrieval(query, documents):\n",
                                    "    # Split the query into lowercase words\n",
                                    "    query_words = set(query.lower().split())\n",
                                    "    \n",
                                    "    best_doc_id = None\n",
                                    "    best_overlap = 0\n",
                                    "    \n",
                                    "    for doc_id, doc in documents.items():\n",
                                    "        # Split the document content into lowercase words\n",
                                    "        doc_words = set(doc[\"content\"].lower().split())\n",
                                    "        # Compute the number of shared words\n",
                                    "        overlap = len(query_words.intersection(doc_words))\n",
                                    "        \n",
                                    "        if overlap > best_overlap:\n",
                                    "            best_overlap = overlap\n",
                                    "            best_doc_id = doc_id\n",
                                    "    \n",
                                    "    # Return the best-matching document (or None)\n",
                                    "    return documents.get(best_doc_id)\n",
                                    "```\n",
                                    "\n",
                                    "If you plug this back into your script, the RAG approach should now correctly pick the document with the most keyword overlap."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Fix the Retrieval Bug\n",
                                    "\n",
                                    "Welcome back! You've been doing well so far. In this exercise, you'll work on fixing a bug in the rag_retrieval function. The current implementation has a critical flaw: it only looks at the document's content but completely ignores the title field, which often contains important context.\n",
                                    "\n",
                                    "Your goal is to modify the retrieval logic to consider both the title and content when calculating relevance, giving appropriate weight to matches found in either field.\n",
                                    "\n",
                                    "Dive in and see how you can improve the code!\n",
                                    "\n",
                                    "```python\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "KNOWLEDGE_BASE = {\n",
                                    "    \"doc1\": {\n",
                                    "        \"title\": \"Project Chimera Overview\",\n",
                                    "        \"content\": (\n",
                                    "            \"research initiative focused on developing novel bio-integrated \"\n",
                                    "            \"interfaces, aiming at merging biological systems with advanced \"\n",
                                    "            \"computing technologies.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc2\": {\n",
                                    "        \"title\": \"Chimera's Core Component\",\n",
                                    "        \"content\": (\n",
                                    "            \"a neural interface that allows for bidirectional communication \"\n",
                                    "            \"between the brain and external devices, using biocompatible \"\n",
                                    "            \"nanomaterials.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc3\": {\n",
                                    "        \"title\": \"Applications of Chimera\",\n",
                                    "        \"content\": (\n",
                                    "            \"advanced prosthetics, treatment of neurological disorders, \"\n",
                                    "            \"enhanced human-computer interaction.\"\n",
                                    "        )\n",
                                    "    }\n",
                                    "}\n",
                                    "\n",
                                    "\n",
                                    "def naive_generation(query):\n",
                                    "    prompt = f\"Answer directly the following query: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "\n",
                                    "def rag_retrieval(query, documents):\n",
                                    "    query_words = set(query.lower().split())\n",
                                    "    best_doc_id = None\n",
                                    "    best_score = 0\n",
                                    "    \n",
                                    "    for doc_id, doc in documents.items():\n",
                                    "        # TODO: Fix the bug by considering both title and content\n",
                                    "        # Currently only looking at content, ignoring potentially relevant titles\n",
                                    "        doc_words = set(doc[\"content\"].lower().split())\n",
                                    "        score = len(query_words.intersection(doc_words))\n",
                                    "        \n",
                                    "        if score > best_score:\n",
                                    "            best_score = score\n",
                                    "            best_doc_id = doc_id\n",
                                    "            \n",
                                    "    return documents.get(best_doc_id)\n",
                                    "\n",
                                    "\n",
                                    "def rag_generation(query, document):\n",
                                    "    if document:\n",
                                    "        snippet = f\"{document['title']}: {document['content']}\"\n",
                                    "        prompt = f\"Using the following information: '{snippet}', answer: {query}\"\n",
                                    "    else:\n",
                                    "        prompt = f\"No relevant information found. Answer directly: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    query = \"What are the applications of Project Chimera?\"\n",
                                    "    print(\"Naive approach:\", naive_generation(query))\n",
                                    "    retrieved_doc = rag_retrieval(query, KNOWLEDGE_BASE)\n",
                                    "    print(\"RAG approach:\", rag_generation(query, retrieved_doc))\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here’s one way to incorporate both title and content into your relevance score, with a heavier weight on title‐matches (e.g. title matches count twice as much as content matches), and a simple regex‐based tokenizer to strip punctuation:\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "KNOWLEDGE_BASE = {\n",
                                    "    \"doc1\": {\n",
                                    "        \"title\": \"Project Chimera Overview\",\n",
                                    "        \"content\": (\n",
                                    "            \"research initiative focused on developing novel bio-integrated \"\n",
                                    "            \"interfaces, aiming at merging biological systems with advanced \"\n",
                                    "            \"computing technologies.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc2\": {\n",
                                    "        \"title\": \"Chimera's Core Component\",\n",
                                    "        \"content\": (\n",
                                    "            \"a neural interface that allows for bidirectional communication \"\n",
                                    "            \"between the brain and external devices, using biocompatible \"\n",
                                    "            \"nanomaterials.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc3\": {\n",
                                    "        \"title\": \"Applications of Chimera\",\n",
                                    "        \"content\": (\n",
                                    "            \"advanced prosthetics, treatment of neurological disorders, \"\n",
                                    "            \"enhanced human-computer interaction.\"\n",
                                    "        )\n",
                                    "    }\n",
                                    "}\n",
                                    "\n",
                                    "\n",
                                    "def naive_generation(query):\n",
                                    "    prompt = f\"Answer directly the following query: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "\n",
                                    "def tokenize(text):\n",
                                    "    \"\"\"\n",
                                    "    Simple word tokenizer: lowercase + extract alphanumeric chunks.\n",
                                    "    \"\"\"\n",
                                    "    return set(re.findall(r\"\\w+\", text.lower()))\n",
                                    "\n",
                                    "\n",
                                    "def rag_retrieval(query, documents, title_weight=2):\n",
                                    "    query_words = tokenize(query)\n",
                                    "\n",
                                    "    best_doc_id = None\n",
                                    "    best_score = 0\n",
                                    "\n",
                                    "    for doc_id, doc in documents.items():\n",
                                    "        title_words   = tokenize(doc[\"title\"])\n",
                                    "        content_words = tokenize(doc[\"content\"])\n",
                                    "\n",
                                    "        title_matches   = len(query_words & title_words)\n",
                                    "        content_matches = len(query_words & content_words)\n",
                                    "\n",
                                    "        # give title matches more impact\n",
                                    "        score = title_weight * title_matches + content_matches\n",
                                    "\n",
                                    "        if score > best_score:\n",
                                    "            best_score = score\n",
                                    "            best_doc_id = doc_id\n",
                                    "\n",
                                    "    return documents.get(best_doc_id)\n",
                                    "\n",
                                    "\n",
                                    "def rag_generation(query, document):\n",
                                    "    if document:\n",
                                    "        snippet = f\"{document['title']}: {document['content']}\"\n",
                                    "        prompt = f\"Using the following information: '{snippet}', answer: {query}\"\n",
                                    "    else:\n",
                                    "        prompt = f\"No relevant information found. Answer directly: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    query = \"What are the applications of Project Chimera?\"\n",
                                    "    print(\"Naive approach:\", naive_generation(query))\n",
                                    "\n",
                                    "    retrieved_doc = rag_retrieval(query, KNOWLEDGE_BASE)\n",
                                    "    print(\"RAG approach:\", rag_generation(query, retrieved_doc))\n",
                                    "```\n",
                                    "\n",
                                    "### What changed?\n",
                                    "\n",
                                    "1. **Tokenization**  \n",
                                    "   We added a `tokenize()` helper that lowercases and strips punctuation, so `\"Chimera?\"` and `\"Chimera\"` are treated the same.  \n",
                                    "2. **Title vs. content**  \n",
                                    "   We now separately count how many query‐words appear in the title (`title_matches`) and in the content (`content_matches`).  \n",
                                    "3. **Weighted scoring**  \n",
                                    "   By default we set `title_weight=2`, so each title‐match contributes twice as much as a content‐match. You can tweak `title_weight` to whatever balance you like.  \n",
                                    "\n",
                                    "With this in place, a query like “What are the applications of Project Chimera?” will correctly pick up on the title “Applications of Chimera” rather than just matching on the body text."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Fix the RAG Generation Bug\n",
                                    "\n",
                                    "Welcome back! You've been doing well so far. In this exercise, you'll focus on fixing a bug in the rag_generation function. This function is supposed to create a context-rich prompt using the retrieved document, but there's a small issue that needs your attention.\n",
                                    "\n",
                                    "Remember, every bug you fix is a step toward mastering the RAG workflow. Keep up the good work!\n",
                                    "\n",
                                    "```python\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "KNOWLEDGE_BASE = {\n",
                                    "    \"doc1\": {\n",
                                    "        \"title\": \"Project Chimera Overview\",\n",
                                    "        \"content\": (\n",
                                    "            \"Project Chimera is a research initiative focused on developing \"\n",
                                    "            \"novel bio-integrated interfaces. It aims to merge biological \"\n",
                                    "            \"systems with advanced computing technologies.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc2\": {\n",
                                    "        \"title\": \"Chimera's Neural Interface\",\n",
                                    "        \"content\": (\n",
                                    "            \"The core component of Project Chimera is a neural interface \"\n",
                                    "            \"that allows for bidirectional communication between the brain \"\n",
                                    "            \"and external devices. This interface uses biocompatible \"\n",
                                    "            \"nanomaterials.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc3\": {\n",
                                    "        \"title\": \"Applications of Chimera\",\n",
                                    "        \"content\": (\n",
                                    "            \"Potential applications of Project Chimera include advanced \"\n",
                                    "            \"prosthetics, treatment of neurological disorders, and enhanced \"\n",
                                    "            \"human-computer interaction. Ethical considerations are paramount.\"\n",
                                    "        )\n",
                                    "    }\n",
                                    "}\n",
                                    "\n",
                                    "\n",
                                    "def naive_generation(query):\n",
                                    "    prompt = f\"Answer directly the following query: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "\n",
                                    "def rag_retrieval(query, documents):\n",
                                    "    query_words = set(query.lower().split())\n",
                                    "    best_doc_id = None\n",
                                    "    best_overlap = 0\n",
                                    "    for doc_id, doc in documents.items():\n",
                                    "        doc_words = set(doc[\"content\"].lower().split())\n",
                                    "        overlap = len(query_words.intersection(doc_words))\n",
                                    "        if overlap > best_overlap:\n",
                                    "            best_overlap = overlap\n",
                                    "            best_doc_id = doc_id\n",
                                    "    return documents.get(best_doc_id)\n",
                                    "\n",
                                    "\n",
                                    "def rag_generation(query, document):\n",
                                    "    if document:\n",
                                    "        prompt = f\"Using the retrieved information, answer: {query}\"\n",
                                    "    else:\n",
                                    "        prompt = f\"No relevant information found. Answer directly: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    query = \"What is the main goal of Project Chimera?\"\n",
                                    "    print(\"Naive approach:\", naive_generation(query))\n",
                                    "    retrieved_doc = rag_retrieval(query, KNOWLEDGE_BASE)\n",
                                    "    print(\"RAG approach:\", rag_generation(query, retrieved_doc))\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here’s a fixed version of **rag_generation** that actually injects the retrieved document’s title and content into the LLM prompt:\n",
                                    "\n",
                                    "```python\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "def rag_generation(query, document):\n",
                                    "    if document:\n",
                                    "        # build a context snippet from title + content\n",
                                    "        snippet = f\"{document['title']}: {document['content']}\"\n",
                                    "        prompt = (\n",
                                    "            f\"Using the following information:\\n\\n\"\n",
                                    "            f\"\\\"{snippet}\\\"\\n\\n\"\n",
                                    "            f\"Answer the question: {query}\"\n",
                                    "        )\n",
                                    "    else:\n",
                                    "        prompt = f\"No relevant information found. Answer directly: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "```\n",
                                    "\n",
                                    "**What changed?**\n",
                                    "1. **Include title + content**  \n",
                                    "   We compose a `snippet` from both `document['title']` and `document['content']`, so the LLM sees the full context.  \n",
                                    "2. **Clear, contextual prompt**  \n",
                                    "   We wrap the snippet in quotes and label it as “the following information,” then append the user’s question.  \n",
                                    "3. **Fallback unchanged**  \n",
                                    "   If no document was retrieved, we still fall back to a direct answer prompt.\n",
                                    "\n",
                                    "With this in place, your RAG-based call will actually condition its answer on the most relevant doc, rather than ignoring it entirely."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## RAG: Retrieving Multiple Relevant Documents\n",
                                    "\n",
                                    "Nice work making it this far! Now, let's enhance our RAG implementation.\n",
                                    "\n",
                                    "The current rag_retrieval function only fetches one document. Update it to retrieve all documents with word overlap.\n",
                                    "\n",
                                    "Modify rag_retrieval to return all documents that have a word overlap with the query.\n",
                                    "Update the rag_generation call to handle a list of documents.\n",
                                    "This will provide more context to the language model, potentially leading to more accurate and complete answers. Let's see how much better we can make our RAG system!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "KNOWLEDGE_BASE = {\n",
                                    "    \"doc1\": {\n",
                                    "        \"title\": \"Project Chimera Overview\",\n",
                                    "        \"content\": (\n",
                                    "            \"Project Chimera is a research initiative focused on developing \"\n",
                                    "            \"novel bio-integrated interfaces. It aims to merge biological \"\n",
                                    "            \"systems with advanced computing technologies.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc2\": {\n",
                                    "        \"title\": \"Chimera's Neural Interface\",\n",
                                    "        \"content\": (\n",
                                    "            \"The core component of Project Chimera is a neural interface \"\n",
                                    "            \"that allows for bidirectional communication between the brain \"\n",
                                    "            \"and external devices. This interface uses biocompatible \"\n",
                                    "            \"nanomaterials.\"\n",
                                    "        )\n",
                                    "    },\n",
                                    "    \"doc3\": {\n",
                                    "        \"title\": \"Applications of Chimera\",\n",
                                    "        \"content\": (\n",
                                    "            \"Potential applications of Project Chimera include advanced \"\n",
                                    "            \"prosthetics, treatment of neurological disorders, and enhanced \"\n",
                                    "            \"human-computer interaction. Ethical considerations are paramount.\"\n",
                                    "        )\n",
                                    "    }\n",
                                    "}\n",
                                    "\n",
                                    "def naive_generation(query):\n",
                                    "    prompt = f\"Answer directly the following query: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "\n",
                                    "def rag_retrieval(query, documents):\n",
                                    "    query_words = set(query.lower().split())\n",
                                    "    best_doc_id = None\n",
                                    "    best_overlap = 0\n",
                                    "    for doc_id, doc in documents.items():\n",
                                    "        doc_words = set(doc[\"content\"].lower().split())\n",
                                    "        overlap = len(query_words.intersection(doc_words))\n",
                                    "        if overlap > best_overlap:\n",
                                    "            best_overlap = overlap\n",
                                    "            best_doc_id = doc_id\n",
                                    "    # TODO: Modify this function to return ALL relevant documents\n",
                                    "    # and not just the one with the highest overlap.\n",
                                    "    return documents.get(best_doc_id)\n",
                                    "\n",
                                    "\n",
                                    "def rag_generation(query, document):\n",
                                    "    # TODO: Modify rag_generation to handle a list of documents.\n",
                                    "    if document:\n",
                                    "        snippet = f\"{document['title']}: {document['content']}\"\n",
                                    "        prompt = f\"Using the following information: '{snippet}', answer: {query}\"\n",
                                    "    else:\n",
                                    "        prompt = f\"No relevant information found. Answer directly: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    query = \"What are the applications of Project Chimera?\"\n",
                                    "    print(\"Naive approach:\", naive_generation(query))\n",
                                    "    retrieved_doc = rag_retrieval(query, KNOWLEDGE_BASE)\n",
                                    "    print(\"RAG approach:\", rag_generation(query, retrieved_doc))\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here’s a fixed version of **rag_generation** that actually injects the retrieved document’s title and content into the LLM prompt:\n",
                                    "\n",
                                    "```python\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "def rag_generation(query, document):\n",
                                    "    if document:\n",
                                    "        # build a context snippet from title + content\n",
                                    "        snippet = f\"{document['title']}: {document['content']}\"\n",
                                    "        prompt = (\n",
                                    "            f\"Using the following information:\\n\\n\"\n",
                                    "            f\"\\\"{snippet}\\\"\\n\\n\"\n",
                                    "            f\"Answer the question: {query}\"\n",
                                    "        )\n",
                                    "    else:\n",
                                    "        prompt = f\"No relevant information found. Answer directly: {query}\"\n",
                                    "    return get_llm_response(prompt)\n",
                                    "```\n",
                                    "\n",
                                    "**What changed?**\n",
                                    "1. **Include title + content**  \n",
                                    "   We compose a `snippet` from both `document['title']` and `document['content']`, so the LLM sees the full context.  \n",
                                    "2. **Clear, contextual prompt**  \n",
                                    "   We wrap the snippet in quotes and label it as “the following information,” then append the user’s question.  \n",
                                    "3. **Fallback unchanged**  \n",
                                    "   If no document was retrieved, we still fall back to a direct answer prompt.\n",
                                    "\n",
                                    "With this in place, your RAG-based call will actually condition its answer on the most relevant doc, rather than ignoring it entirely."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Building a Simple RAG Pipeline"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
