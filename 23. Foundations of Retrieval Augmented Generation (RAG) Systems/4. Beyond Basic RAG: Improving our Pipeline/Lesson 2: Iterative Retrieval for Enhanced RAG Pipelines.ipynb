{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 2: Iterative Retrieval for Enhanced RAG Pipelines\n",
                                    "\n",
                                    "\n",
                                    "Welcome back to this second lesson in the **Beyond Basic RAG: Improving our Pipeline** course! In the previous lesson, we explored ways to ensure that your language model stays grounded by responding only with information taken from retrieved context. That approach helps us avoid hallucinations and keeps the output reliable.\n",
                                    "\n",
                                    "In this lesson, we’ll improve the pipeline even further by making the retrieval process more **iterative**. Instead of collecting chunks of text just once before moving on to generation, we’ll refine our queries step by step. This multi-stage retrieval can pinpoint the most relevant information and produce a more helpful final context.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## The Concept of Iterative Retrieval\n",
                                    "\n",
                                    "Imagine a scenario where a user asks:\n",
                                    "\n",
                                    "> “Tell me about the regulations for staff members.”\n",
                                    "\n",
                                    "The question might be too broad. A typical one-shot retrieval might find some relevant chunks, but you might also want to narrow in on “internal policies” or “mandatory forms” for more precision.\n",
                                    "\n",
                                    "**Iterative retrieval** does exactly that:\n",
                                    "\n",
                                    "1. **Retrieve** an initial chunk based on the user’s query.\n",
                                    "2. **Refine** that query with a new keyword from the retrieved chunk (e.g., “internal” or “policies”).\n",
                                    "3. **Repeat** until you’ve gathered a set of chunks that thoroughly answers the question—or until improvements level off.\n",
                                    "\n",
                                    "This multi-pass approach can drastically improve the depth and breadth of the retrieved information, making your final context more complete.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Practical Example: Iterative Retrieval in Action\n",
                                    "\n",
                                    "**User query:**\n",
                                    "\n",
                                    "> “Tell me about the regulations for staff members.”\n",
                                    "\n",
                                    "**Sample chunks in DB:**\n",
                                    "\n",
                                    "* **Chunk 1:**\n",
                                    "\n",
                                    "  > “Our company requires that all staff members adhere to internal policies such as punctuality, dress code, and ethical behavior...”\n",
                                    "\n",
                                    "* **Chunk 2:**\n",
                                    "\n",
                                    "  > “Regulations for staff emphasize adherence to both internal policies and government standards, covering conduct, reporting, ...”\n",
                                    "\n",
                                    "**Iteration 1**\n",
                                    "\n",
                                    "* **Query:** “Tell me about the regulations for staff members”\n",
                                    "* **Best match:** Chunk 1 (score: 0.87)\n",
                                    "* **Extracted keyword:** “internal”\n",
                                    "\n",
                                    "**Iteration 2**\n",
                                    "\n",
                                    "* **Updated Query:** “Tell me about the regulations for staff members internal”\n",
                                    "* **Best match:** Chunk 2 (score: 0.93)\n",
                                    "* Since further refinement doesn’t significantly improve the score, the process stops.\n",
                                    "\n",
                                    "The system then uses these accumulated chunks to generate a grounded and comprehensive answer.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### Retrieving the Best Chunk\n",
                                    "\n",
                                    "Define a function to fetch the best matching chunk given a query:\n",
                                    "\n",
                                    "```python\n",
                                    "def retrieve_best_chunk(query_text, collection, n_results=1):\n",
                                    "    \"\"\"\n",
                                    "    Retrieve the best matching chunk from the collection based on the given query.\n",
                                    "    Returns:\n",
                                    "      best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "      (or None, None, None if retrieval fails)\n",
                                    "    \"\"\"\n",
                                    "    # Perform a similarity search for the provided query\n",
                                    "    retrieval = collection.query(query_texts=[query_text], n_results=n_results)\n",
                                    "    \n",
                                    "    # If nothing is found, return None\n",
                                    "    if not retrieval['documents'][0]:\n",
                                    "        return None, None, None\n",
                                    "\n",
                                    "    # Extract the best match from the results\n",
                                    "    best_chunk_text = retrieval['documents'][0][0]\n",
                                    "    best_distance   = retrieval['distances'][0][0]\n",
                                    "    \n",
                                    "    # Convert 'distance' to a simple similarity score\n",
                                    "    best_chunk_score    = 1 / (1 + best_distance)\n",
                                    "    best_chunk_metadata = retrieval['metadatas'][0][0]\n",
                                    "    \n",
                                    "    return best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "```\n",
                                    "\n",
                                    "**Key points:**\n",
                                    "\n",
                                    "* We query our collection (chunks in a vector DB) for `query_text`.\n",
                                    "* If no chunks are returned, we exit gracefully with `None`.\n",
                                    "* Otherwise, we pick the top chunk and compute a similarity score via `1 / (1 + distance)`.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### Extracting and Refining Queries\n",
                                    "\n",
                                    "Once you’ve retrieved a chunk, extract a refinement keyword to make the next query more specific:\n",
                                    "\n",
                                    "```python\n",
                                    "def extract_refinement_keyword(chunk_text, current_query):\n",
                                    "    \"\"\"\n",
                                    "    Extract a single keyword from the chunk that is not already in the current query.\n",
                                    "    - Ignores stopwords and short words.\n",
                                    "    - Picks the longest remaining candidate.\n",
                                    "    \"\"\"\n",
                                    "    # Convert text to lowercase words\n",
                                    "    chunk_words = re.findall(r'\\b\\w+\\b', chunk_text.lower())\n",
                                    "    query_words = set(re.findall(r'\\b\\w+\\b', current_query.lower()))\n",
                                    "\n",
                                    "    # Filter out stopwords, words already in query, or very short words\n",
                                    "    candidate_words = [\n",
                                    "        w for w in chunk_words\n",
                                    "        if w not in STOPWORDS and w not in query_words and len(w) > 4\n",
                                    "    ]\n",
                                    "\n",
                                    "    if not candidate_words:\n",
                                    "        return \"\"\n",
                                    "\n",
                                    "    # Pick the longest candidate word\n",
                                    "    refine_word = max(candidate_words, key=len)\n",
                                    "    return refine_word\n",
                                    "\n",
                                    "def refine_query(current_query, refine_word):\n",
                                    "    \"\"\"\n",
                                    "    Append the chosen refine_word to the current query if it exists.\n",
                                    "    \"\"\"\n",
                                    "    if not refine_word:\n",
                                    "        return current_query\n",
                                    "    return f\"{current_query} {refine_word}\"\n",
                                    "```\n",
                                    "\n",
                                    "**Why this helps:**\n",
                                    "\n",
                                    "* **Filtering stopwords** reduces noise.\n",
                                    "* **Skipping existing words** avoids repetition.\n",
                                    "* **Choosing longer words** often yields more meaningful refinements.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### Putting It All Together: Iterative Retrieval\n",
                                    "\n",
                                    "Here’s a condensed multi-step loop:\n",
                                    "\n",
                                    "```python\n",
                                    "def iterative_retrieval(query, collection, steps=3):\n",
                                    "    \"\"\"\n",
                                    "    Multi-step retrieval with a simple query refinement approach:\n",
                                    "      1) Retrieve the best chunk for the current query.\n",
                                    "      2) Extract and append one new keyword.\n",
                                    "      3) Repeat until no improvements or max steps reached.\n",
                                    "    \"\"\"\n",
                                    "    accumulated_chunks  = []\n",
                                    "    current_query       = query\n",
                                    "    best_score_so_far   = 0.0\n",
                                    "\n",
                                    "    for step in range(steps):\n",
                                    "        best_chunk_text, best_chunk_score, metadata = retrieve_best_chunk(current_query, collection)\n",
                                    "        \n",
                                    "        # Stop if no improvement\n",
                                    "        if not best_chunk_text or best_chunk_score <= best_score_so_far + IMPROVEMENT_THRESHOLD:\n",
                                    "            break\n",
                                    "\n",
                                    "        best_score_so_far = best_chunk_score\n",
                                    "        accumulated_chunks.append({\n",
                                    "            'step': step + 1,\n",
                                    "            'query': current_query,\n",
                                    "            'retrieved_chunk': best_chunk_text,\n",
                                    "            'score': best_chunk_score\n",
                                    "        })\n",
                                    "\n",
                                    "        refine_word   = extract_refinement_keyword(best_chunk_text, current_query)\n",
                                    "        current_query = refine_query(current_query, refine_word)\n",
                                    "    \n",
                                    "    return accumulated_chunks\n",
                                    "```\n",
                                    "\n",
                                    "* We limit to a fixed number of iterations (e.g., 3).\n",
                                    "* Each pass retrieves, scores, and—if improved—refines the query.\n",
                                    "* The result is a list of chunks offering richer context than a single retrieval.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Practical Considerations\n",
                                    "\n",
                                    "* **Heuristics:** Longest-word extraction and score thresholds are simple; adjust them to your needs.\n",
                                    "* **Performance Trade-offs:** More iterations = better context but higher cost.\n",
                                    "* **Limitations:** If the initial chunk is irrelevant, further steps might not help much.\n",
                                    "* **Advanced Refinements:** Consider NLP techniques (e.g., keyphrase extraction) for smarter query expansion.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Conclusion and Next Steps\n",
                                    "\n",
                                    "You’ve now seen how **iterative retrieval** can make your RAG pipeline more robust by gradually honing in on the most relevant information. This lesson builds on our previous work grounding generation, and now you have an even better way to gather the right context in the first place.\n",
                                    "\n",
                                    "Coming up next, you’ll get hands-on practice implementing and tweaking iterative retrieval strategies. Experiment with different thresholds, iteration counts, or NLP-based refiners—each tweak can significantly boost your RAG system’s performance.\n",
                                    "\n",
                                    "Stay curious and keep refining! You’re making great strides toward a complete, high-performing Retrieval-Augmented Generation pipeline.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Inverted Distance Similarity Score\n",
                                    "\n",
                                    "In this activity, you'll dive into the heart of the retrieve_best_chunk function. Your mission is to complete the calculation of the inverted-distance similarity score. This score is crucial as it transforms the distance metric into a similarity measure, helping us identify the most relevant text chunk for a given query.\n",
                                    "\n",
                                    "Yor task today is to fill in the missing part of the formula to compute the similarity score using the inverted distance method. Remember, the formula should convert a smaller distance into a higher similarity score.\n",
                                    "\n",
                                    "By completing this exercise, you'll gain a deeper understanding of how similarity scores are derived and their role in refining query results. Once you're done, test your code to see how well it retrieves the best matching chunk. Happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "\n",
                                    "# Optional: set this to 0.0 if you don't need to enforce a minimum improvement.\n",
                                    "IMPROVEMENT_THRESHOLD = 0.02\n",
                                    "\n",
                                    "# Common stopwords to ignore when extracting keywords\n",
                                    "STOPWORDS = set([\n",
                                    "    \"the\",\"and\",\"is\",\"in\",\"of\",\"to\",\"a\",\"that\",\"for\",\"on\",\"with\",\"as\",\"it\",\"by\",\n",
                                    "    \"this\",\"are\",\"was\",\"at\",\"from\",\"or\",\"be\",\"which\",\"not\",\"can\",\"also\",\"have\",\n",
                                    "    \"has\",\"had\",\"we\",\"they\",\"you\",\"he\",\"she\",\"his\",\"her\",\"its\",\"our\",\"us\",\"their\",\n",
                                    "    \"them\",\"i\",\"do\",\"does\",\"did\",\"just\",\"so\",\"if\",\"may\",\"will\",\"shall\",\"more\",\"most\",\n",
                                    "    \"some\",\"many\",\"any\",\"all\",\"what\",\"about\",\"would\",\"could\",\"should\",\"where\",\"when\",\n",
                                    "    \"why\",\"how\"\n",
                                    "])\n",
                                    "\n",
                                    "def retrieve_best_chunk(query_text, collection, n_results=1):\n",
                                    "    \"\"\"\n",
                                    "    Retrieve the best matching chunk from the collection based on the given query.\n",
                                    "    Returns:\n",
                                    "      best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "      (or None, None, None if retrieval fails)\n",
                                    "    \"\"\"\n",
                                    "    retrieval = collection.query(query_texts=[query_text], n_results=n_results)\n",
                                    "    if not retrieval['documents'][0]:\n",
                                    "        return None, None, None\n",
                                    "\n",
                                    "    best_chunk_text = retrieval['documents'][0][0]\n",
                                    "    best_distance = retrieval['distances'][0][0]\n",
                                    "    # TODO: Complete the inverted-distance similarity score formula.\n",
                                    "    # Hint: Use 1 / (1 + best_distance) to convert distance to similarity.\n",
                                    "    best_chunk_score = _____\n",
                                    "    best_chunk_metadata = retrieval['metadatas'][0][0]\n",
                                    "    return best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "\n",
                                    "def extract_refinement_keyword(chunk_text, current_query):\n",
                                    "    \"\"\"\n",
                                    "    Extract a single keyword from the chunk that is not in the current query.\n",
                                    "    - Ignores stopwords and short words.\n",
                                    "    - Chooses the longest candidate word found in the chunk.\n",
                                    "    If no suitable keyword is found, returns an empty string.\n",
                                    "    \"\"\"\n",
                                    "    chunk_words = re.findall(r'\\b\\w+\\b', chunk_text.lower())\n",
                                    "    query_words = set(re.findall(r'\\b\\w+\\b', current_query.lower()))\n",
                                    "\n",
                                    "    # Filter out stopwords, words already in the query, or very short words\n",
                                    "    candidate_words = [\n",
                                    "        w for w in chunk_words\n",
                                    "        if w not in STOPWORDS and w not in query_words and len(w) > 4\n",
                                    "    ]\n",
                                    "\n",
                                    "    if not candidate_words:\n",
                                    "        return \"\"\n",
                                    "\n",
                                    "    # Pick the longest candidate\n",
                                    "    refine_word = max(candidate_words, key=len)\n",
                                    "    return refine_word\n",
                                    "\n",
                                    "def refine_query(current_query, refine_word):\n",
                                    "    \"\"\"\n",
                                    "    Append the chosen refine_word to the current query if it exists.\n",
                                    "    \"\"\"\n",
                                    "    if not refine_word:\n",
                                    "        return current_query\n",
                                    "    return f\"{current_query} {refine_word}\"\n",
                                    "\n",
                                    "def iterative_retrieval(query, collection, steps=3):\n",
                                    "    \"\"\"\n",
                                    "    Multi-step retrieval with a simple query refinement approach:\n",
                                    "      1) Retrieve the best chunk for the current query.\n",
                                    "      2) Extract one new keyword from that chunk and add it to the query (optional).\n",
                                    "      3) Stop if no improvement in similarity or no new keyword is found.\n",
                                    "      4) Collect the chunks from each step for final context.\n",
                                    "\n",
                                    "    This version uses a global IMPROVEMENT_THRESHOLD to decide if we should keep refining.\n",
                                    "    \"\"\"\n",
                                    "    accumulated_chunks = []\n",
                                    "    current_query = query\n",
                                    "    best_score_so_far = 0.0\n",
                                    "\n",
                                    "    for step in range(steps):\n",
                                    "        print(f\"Iteration {step+1}, current query: '{current_query}'\")\n",
                                    "\n",
                                    "        best_chunk_text, best_chunk_score, best_chunk_metadata = retrieve_best_chunk(current_query, collection)\n",
                                    "        if not best_chunk_text:\n",
                                    "            print(\"No chunks found at this step. Ending refinement.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        print(f\"Best chunk text (first 50 chars): '{best_chunk_text[:50]}...' | Score: {best_chunk_score:.4f}\")\n",
                                    "\n",
                                    "        # Check if this step yields improvement, if that's a point of interest\n",
                                    "        if best_chunk_score - best_score_so_far < IMPROVEMENT_THRESHOLD:\n",
                                    "            print(\"Improvement threshold not met. Stopping refinements.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        best_score_so_far = best_chunk_score\n",
                                    "\n",
                                    "        # Store the chunk information\n",
                                    "        accumulated_chunks.append({\n",
                                    "            'step': step + 1,\n",
                                    "            'query': current_query,\n",
                                    "            'retrieved_chunk': {\n",
                                    "                'text': best_chunk_text,\n",
                                    "                'metadata': best_chunk_metadata\n",
                                    "            },\n",
                                    "            'score': best_chunk_score\n",
                                    "        })\n",
                                    "\n",
                                    "        # Attempt to refine the query by extracting a new keyword\n",
                                    "        refine_word = extract_refinement_keyword(best_chunk_text, current_query)\n",
                                    "        if not refine_word:\n",
                                    "            print(\"No suitable keyword found for further refinement.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        # Update the current query with the chosen keyword\n",
                                    "        print(f\"Refining query with keyword: {refine_word}\")\n",
                                    "        current_query = refine_query(current_query, refine_word)\n",
                                    "\n",
                                    "    return accumulated_chunks\n",
                                    "\n",
                                    "def build_final_context(iteration_results):\n",
                                    "    \"\"\"\n",
                                    "    Combine all retrieved chunks from each iteration into one context block.\n",
                                    "    Return a fallback message if no chunks were retrieved.\n",
                                    "    \"\"\"\n",
                                    "    if not iteration_results:\n",
                                    "        return \"No relevant information was found after iterative retrieval.\"\n",
                                    "\n",
                                    "    lines = []\n",
                                    "    for result in iteration_results:\n",
                                    "        lines.append(\n",
                                    "            f\"- Step {result['step']} (Score={result['score']:.4f}): {result['retrieved_chunk']['text']}\"\n",
                                    "        )\n",
                                    "    return \"\\n\".join(lines)\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Small demonstration with a sample corpus\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\", chunk_size=40)\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"iterative_collection\")\n",
                                    "\n",
                                    "    initial_query = \"What internal policies apply specifically to employees?\"\n",
                                    "    iteration_results = iterative_retrieval(initial_query, collection, steps=3)\n",
                                    "\n",
                                    "    final_context = build_final_context(iteration_results)\n",
                                    "    print(f\"\\nFinal combined context:\\n{final_context}\")\n",
                                    "    # Next step could be feeding 'final_context' to your LLM with the refined or original query.\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Thought for a couple of seconds\n",
                                    "\n",
                                    "\n",
                                    "Here’s the updated `retrieve_best_chunk` with the inverted‐distance similarity score filled in:\n",
                                    "\n",
                                    "```python\n",
                                    "def retrieve_best_chunk(query_text, collection, n_results=1):\n",
                                    "    \"\"\"\n",
                                    "    Retrieve the best matching chunk from the collection based on the given query.\n",
                                    "    Returns:\n",
                                    "      best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "      (or None, None, None if retrieval fails)\n",
                                    "    \"\"\"\n",
                                    "    retrieval = collection.query(query_texts=[query_text], n_results=n_results)\n",
                                    "    if not retrieval['documents'][0]:\n",
                                    "        return None, None, None\n",
                                    "\n",
                                    "    best_chunk_text = retrieval['documents'][0][0]\n",
                                    "    best_distance   = retrieval['distances'][0][0]\n",
                                    "    # Convert distance into similarity: smaller distance → higher score\n",
                                    "    best_chunk_score = 1 / (1 + best_distance)\n",
                                    "    best_chunk_metadata = retrieval['metadatas'][0][0]\n",
                                    "    return best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "```\n",
                                    "\n",
                                    "With that in place, you can run your iterative workflow and you’ll see that chunks with smaller distances now yield higher similarity scores. Happy coding!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Limit Chunks in Iterative Retrieval\n",
                                    "\n",
                                    "n the previous exercise, you successfully refined queries by prioritizing keywords. Now, let's elevate your skills by adding a new feature to our iterative retrieval process! Your mission is to introduce a maximum chunk limit to the iterative_retrieval function, ensuring the retrieval process remains efficient and manageable.\n",
                                    "\n",
                                    "Here's what you need to do:\n",
                                    "\n",
                                    "Expand the iterative_retrieval function to accept a new parameter, max_chunks.\n",
                                    "Modify the loop so that it stops if the number of retrieved chunks reaches this max_chunks limit.\n",
                                    "Once you've made the changes, test your code to see how it gracefully handles the chunk limit. Enjoy the challenge, and happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "\n",
                                    "# Optional: set this to 0.0 if you don't need to enforce a minimum improvement.\n",
                                    "IMPROVEMENT_THRESHOLD = 0.02\n",
                                    "\n",
                                    "# Common stopwords to ignore when extracting keywords\n",
                                    "STOPWORDS = set([\n",
                                    "    \"the\",\"and\",\"is\",\"in\",\"of\",\"to\",\"a\",\"that\",\"for\",\"on\",\"with\",\"as\",\"it\",\"by\",\n",
                                    "    \"this\",\"are\",\"was\",\"at\",\"from\",\"or\",\"be\",\"which\",\"not\",\"can\",\"also\",\"have\",\n",
                                    "    \"has\",\"had\",\"we\",\"they\",\"you\",\"he\",\"she\",\"his\",\"her\",\"its\",\"our\",\"us\",\"their\",\n",
                                    "    \"them\",\"i\",\"do\",\"does\",\"did\",\"just\",\"so\",\"if\",\"may\",\"will\",\"shall\",\"more\",\"most\",\n",
                                    "    \"some\",\"many\",\"any\",\"all\",\"what\",\"about\",\"would\",\"could\",\"should\",\"where\",\"when\",\n",
                                    "    \"why\",\"how\"\n",
                                    "])\n",
                                    "\n",
                                    "def retrieve_best_chunk(query_text, collection, n_results=1):\n",
                                    "    \"\"\"\n",
                                    "    Retrieve the best matching chunk from the collection based on the given query.\n",
                                    "    Returns:\n",
                                    "      best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "      (or None, None, None if retrieval fails)\n",
                                    "    \"\"\"\n",
                                    "    retrieval = collection.query(query_texts=[query_text], n_results=n_results)\n",
                                    "    if not retrieval['documents'][0]:\n",
                                    "        return None, None, None\n",
                                    "\n",
                                    "    best_chunk_text = retrieval['documents'][0][0]\n",
                                    "    best_distance = retrieval['distances'][0][0]\n",
                                    "    best_chunk_score = 1 / (1 + best_distance)  # Simple inverted distance\n",
                                    "    best_chunk_metadata = retrieval['metadatas'][0][0]\n",
                                    "    return best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "\n",
                                    "def extract_refinement_keyword(chunk_text, current_query):\n",
                                    "    \"\"\"\n",
                                    "    Extract a single keyword from the chunk that is not in the current query.\n",
                                    "    - Ignores stopwords and short words.\n",
                                    "    - Chooses the longest candidate word found in the chunk.\n",
                                    "    If no suitable keyword is found, returns an empty string.\n",
                                    "    \"\"\"\n",
                                    "    chunk_words = re.findall(r'\\b\\w+\\b', chunk_text.lower())\n",
                                    "    query_words = set(re.findall(r'\\b\\w+\\b', current_query.lower()))\n",
                                    "\n",
                                    "    candidate_words = [\n",
                                    "        w for w in chunk_words\n",
                                    "        if w not in STOPWORDS and w not in query_words and len(w) > 4\n",
                                    "    ]\n",
                                    "\n",
                                    "    if not candidate_words:\n",
                                    "        return \"\"\n",
                                    "    refine_word = max(candidate_words, key=len)\n",
                                    "    return refine_word\n",
                                    "\n",
                                    "def refine_query(current_query, refine_word):\n",
                                    "    \"\"\"\n",
                                    "    Append the chosen refine_word to the current query if it exists.\n",
                                    "    \"\"\"\n",
                                    "    if not refine_word:\n",
                                    "        return current_query\n",
                                    "    return f\"{current_query} {refine_word}\"\n",
                                    "\n",
                                    "# TODO: Add the max_chunks parameter to control the maximum number of chunks to retrieve\n",
                                    "def iterative_retrieval(query, collection, steps=3):\n",
                                    "    \"\"\"\n",
                                    "    Multi-step retrieval with a simple query refinement approach:\n",
                                    "      1) Retrieve the best chunk for the current query.\n",
                                    "      2) Extract one new keyword from that chunk and add it to the query (optional).\n",
                                    "      3) Stop if no improvement in similarity or no new keyword is found.\n",
                                    "      4) Collect the chunks from each step for final context.\n",
                                    "\n",
                                    "    This version uses a global IMPROVEMENT_THRESHOLD to decide if we should keep refining.\n",
                                    "    \"\"\"\n",
                                    "    accumulated_chunks = []\n",
                                    "    current_query = query\n",
                                    "    best_score_so_far = 0.0\n",
                                    "\n",
                                    "    for step in range(steps):\n",
                                    "        print(f\"Iteration {step+1}, current query: '{current_query}'\")\n",
                                    "\n",
                                    "        best_chunk_text, best_chunk_score, best_chunk_metadata = retrieve_best_chunk(current_query, collection)\n",
                                    "        if not best_chunk_text:\n",
                                    "            print(\"No chunks found at this step. Ending refinement.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        print(f\"Best chunk text (first 50 chars): '{best_chunk_text[:50]}...' | Score: {best_chunk_score:.4f}\")\n",
                                    "\n",
                                    "        if best_chunk_score - best_score_so_far < IMPROVEMENT_THRESHOLD:\n",
                                    "            print(\"Improvement threshold not met. Stopping refinements.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        best_score_so_far = best_chunk_score\n",
                                    "\n",
                                    "        # Store the chunk information\n",
                                    "        accumulated_chunks.append({\n",
                                    "            'step': step + 1,\n",
                                    "            'query': current_query,\n",
                                    "            'retrieved_chunk': {\n",
                                    "                'text': best_chunk_text,\n",
                                    "                'metadata': best_chunk_metadata\n",
                                    "            },\n",
                                    "            'score': best_chunk_score\n",
                                    "        })\n",
                                    "\n",
                                    "        # TODO: Add a check here to stop the loop if we've reached max_chunks\n",
                                    "\n",
                                    "        refine_word = extract_refinement_keyword(best_chunk_text, current_query)\n",
                                    "        if not refine_word:\n",
                                    "            print(\"No suitable keyword found for further refinement.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        print(f\"Refining query with keyword: {refine_word}\")\n",
                                    "        current_query = refine_query(current_query, refine_word)\n",
                                    "\n",
                                    "    return accumulated_chunks\n",
                                    "\n",
                                    "def build_final_context(iteration_results):\n",
                                    "    \"\"\"\n",
                                    "    Combine all retrieved chunks from each iteration into one context block.\n",
                                    "    Return a fallback message if no chunks were retrieved.\n",
                                    "    \"\"\"\n",
                                    "    if not iteration_results:\n",
                                    "        return \"No relevant information was found after iterative retrieval.\"\n",
                                    "\n",
                                    "    lines = []\n",
                                    "    for result in iteration_results:\n",
                                    "        lines.append(\n",
                                    "            f\"- Step {result['step']} (Score={result['score']:.4f}): {result['retrieved_chunk']['text']}\"\n",
                                    "        )\n",
                                    "    return \"\\n\".join(lines)\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Small demonstration with a sample corpus\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\", chunk_size=40)\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"iterative_collection\")\n",
                                    "\n",
                                    "    initial_query = \"What internal policies apply specifically to employees?\"\n",
                                    "    # TODO: Update the function call to include the max_chunks parameter\n",
                                    "    iteration_results = iterative_retrieval(initial_query, collection, steps=5)\n",
                                    "\n",
                                    "    final_context = build_final_context(iteration_results)\n",
                                    "    print(f\"\\nFinal combined context:\\n{final_context}\")\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "**Deciding how to respond with code**\n",
                                    "\n",
                                    "Okay, I need to provide the updated code for iterative retrieval. It seems the user gave code with TODO comments and wants me to modify it. We have a document called \"rag\\_context\\_management.py\" with related code, but it doesn't cover iterative retrieval.\n",
                                    "\n",
                                    "I’m wondering whether to create a new document for this specific code or just provide the update right in a markdown code snippet. I'll choose the direct code snippet approach for now, since that seems to be what the user needs.\n",
                                    "\n",
                                    "\n",
                                    "Thought for 4 seconds\n",
                                    "\n",
                                    "\n",
                                    "Here’s the `iterative_retrieval` with a new `max_chunks` parameter and a stop‐condition once that limit is reached:\n",
                                    "\n",
                                    "```python\n",
                                    "def iterative_retrieval(query, collection, steps=3, max_chunks=None):\n",
                                    "    \"\"\"\n",
                                    "    Multi-step retrieval with optional chunk‐limit:\n",
                                    "      1) Retrieve the best chunk for the current query.\n",
                                    "      2) Extract one new keyword and refine the query.\n",
                                    "      3) Stop if no improvement, no new keyword, max steps, or max_chunks reached.\n",
                                    "    \"\"\"\n",
                                    "    accumulated_chunks = []\n",
                                    "    current_query      = query\n",
                                    "    best_score_so_far  = 0.0\n",
                                    "\n",
                                    "    for step in range(steps):\n",
                                    "        print(f\"Iteration {step+1}, current query: '{current_query}'\")\n",
                                    "\n",
                                    "        best_chunk_text, best_chunk_score, best_chunk_metadata = retrieve_best_chunk(current_query, collection)\n",
                                    "        if not best_chunk_text:\n",
                                    "            print(\"No chunks found at this step. Ending refinement.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        print(f\"Best chunk (score={best_chunk_score:.4f}): '{best_chunk_text[:50]}...'\")\n",
                                    "\n",
                                    "        # Stop if improvement threshold not met\n",
                                    "        if best_chunk_score - best_score_so_far < IMPROVEMENT_THRESHOLD:\n",
                                    "            print(\"Improvement threshold not met. Stopping refinements.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        best_score_so_far = best_chunk_score\n",
                                    "\n",
                                    "        # Record this chunk\n",
                                    "        accumulated_chunks.append({\n",
                                    "            'step': step + 1,\n",
                                    "            'query': current_query,\n",
                                    "            'retrieved_chunk': {\n",
                                    "                'text':     best_chunk_text,\n",
                                    "                'metadata': best_chunk_metadata\n",
                                    "            },\n",
                                    "            'score': best_chunk_score\n",
                                    "        })\n",
                                    "\n",
                                    "        # **New:** stop if we've collected enough chunks\n",
                                    "        if max_chunks is not None and len(accumulated_chunks) >= max_chunks:\n",
                                    "            print(f\"Reached max_chunks={max_chunks}. Ending retrieval.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        # Try to extract a refinement keyword\n",
                                    "        refine_word = extract_refinement_keyword(best_chunk_text, current_query)\n",
                                    "        if not refine_word:\n",
                                    "            print(\"No suitable keyword found for further refinement.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        print(f\"Refining query with keyword: {refine_word}\")\n",
                                    "        current_query = refine_query(current_query, refine_word)\n",
                                    "\n",
                                    "    return accumulated_chunks\n",
                                    "```\n",
                                    "\n",
                                    "And in your `__main__`, you’d call it like this:\n",
                                    "\n",
                                    "```python\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\", chunk_size=40)\n",
                                    "    collection   = build_chroma_collection(chunked_docs, collection_name=\"iterative_collection\")\n",
                                    "\n",
                                    "    initial_query    = \"What internal policies apply specifically to employees?\"\n",
                                    "    # Set steps=5 but stop as soon as max_chunks=3 is reached\n",
                                    "    iteration_results = iterative_retrieval(\n",
                                    "        initial_query,\n",
                                    "        collection,\n",
                                    "        steps=5,\n",
                                    "        max_chunks=3\n",
                                    "    )\n",
                                    "\n",
                                    "    final_context = build_final_context(iteration_results)\n",
                                    "    print(f\"\\nFinal combined context:\\n{final_context}\")\n",
                                    "```\n",
                                    "\n",
                                    "This way, no matter how many `steps` you allow, the loop will exit once you’ve gathered `max_chunks` chunks.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Enhance Iterative Retrieval Process\n",
                                    "\n",
                                    "Welcome back! You've done a fantastic job refining queries by prioritizing keywords in the previous exercise. Now, let's elevate your RAG pipeline by enhancing the iterative retrieval process to handle multiple keywords.\n",
                                    "\n",
                                    "Your objective is to modify the iterative_retrieval function to extract and append up to two new keywords from the best chunk in each iteration. This will make your retrieval process more robust and precise.\n",
                                    "\n",
                                    "Here's what you need to consider:\n",
                                    "\n",
                                    "The current setup only extracts a single keyword per iteration. Adjust the logic to gather multiple keywords.\n",
                                    "Ensure that these keywords are meaningful and not already part of the current query.\n",
                                    "Append these keywords to the query to refine it further for the next retrieval step.\n",
                                    "By implementing this enhancement, you'll gain valuable experience in building a more sophisticated retrieval system. Once you've made the changes, observe how the retrieval process becomes more effective with each iteration. Enjoy the challenge and happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "\n",
                                    "# Optional: set this to 0.0 if you don't need to enforce a minimum improvement.\n",
                                    "IMPROVEMENT_THRESHOLD = 0.02\n",
                                    "\n",
                                    "# Common stopwords to ignore when extracting keywords\n",
                                    "STOPWORDS = set([\n",
                                    "    \"the\",\"and\",\"is\",\"in\",\"of\",\"to\",\"a\",\"that\",\"for\",\"on\",\"with\",\"as\",\"it\",\"by\",\n",
                                    "    \"this\",\"are\",\"was\",\"at\",\"from\",\"or\",\"be\",\"which\",\"not\",\"can\",\"also\",\"have\",\n",
                                    "    \"has\",\"had\",\"we\",\"they\",\"you\",\"he\",\"she\",\"his\",\"her\",\"its\",\"our\",\"us\",\"their\",\n",
                                    "    \"them\",\"i\",\"do\",\"does\",\"did\",\"just\",\"so\",\"if\",\"may\",\"will\",\"shall\",\"more\",\"most\",\n",
                                    "    \"some\",\"many\",\"any\",\"all\",\"what\",\"about\",\"would\",\"could\",\"should\",\"where\",\"when\",\n",
                                    "    \"why\",\"how\"\n",
                                    "])\n",
                                    "\n",
                                    "def retrieve_best_chunk(query_text, collection, n_results=1):\n",
                                    "    \"\"\"\n",
                                    "    Retrieve the best matching chunk from the collection based on the given query.\n",
                                    "    Returns:\n",
                                    "      best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "      (or None, None, None if retrieval fails)\n",
                                    "    \"\"\"\n",
                                    "    retrieval = collection.query(query_texts=[query_text], n_results=n_results)\n",
                                    "    if not retrieval['documents'][0]:\n",
                                    "        return None, None, None\n",
                                    "\n",
                                    "    best_chunk_text = retrieval['documents'][0][0]\n",
                                    "    best_distance = retrieval['distances'][0][0]\n",
                                    "    best_chunk_score = 1 / (1 + best_distance)  # Simple inverted distance\n",
                                    "    best_chunk_metadata = retrieval['metadatas'][0][0]\n",
                                    "    return best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "\n",
                                    "# TODO: Rename this function to extract_refinement_keywords and modify it to:\n",
                                    "# 1. Accept a max_keywords parameter\n",
                                    "# 2. Return a list of keywords instead of a single word\n",
                                    "# 3. Sort candidates by length and return the top max_keywords\n",
                                    "def extract_refinement_keyword(chunk_text, current_query):\n",
                                    "    \"\"\"\n",
                                    "    Extract a single keyword from the chunk that is not in the current query.\n",
                                    "    - Ignores stopwords and short words.\n",
                                    "    - Chooses the longest candidate word found in the chunk.\n",
                                    "    If no suitable keyword is found, returns an empty string.\n",
                                    "    \"\"\"\n",
                                    "    chunk_words = re.findall(r'\\b\\w+\\b', chunk_text.lower())\n",
                                    "    query_words = set(re.findall(r'\\b\\w+\\b', current_query.lower()))\n",
                                    "\n",
                                    "    candidate_words = [\n",
                                    "        w for w in chunk_words\n",
                                    "        if w not in STOPWORDS and w not in query_words and len(w) > 4\n",
                                    "    ]\n",
                                    "\n",
                                    "    if not candidate_words:\n",
                                    "        return \"\"\n",
                                    "\n",
                                    "    # Pick the longest candidate\n",
                                    "    refine_word = max(candidate_words, key=len)\n",
                                    "    return refine_word\n",
                                    "\n",
                                    "def refine_query(current_query, refine_word):\n",
                                    "    \"\"\"\n",
                                    "    Append the chosen refine_word to the current query if it exists.\n",
                                    "    \"\"\"\n",
                                    "    if not refine_word:\n",
                                    "        return current_query\n",
                                    "    return f\"{current_query} {refine_word}\"\n",
                                    "\n",
                                    "def iterative_retrieval(query, collection, steps=3):\n",
                                    "    \"\"\"\n",
                                    "    Multi-step retrieval with a simple query refinement approach:\n",
                                    "      1) Retrieve the best chunk for the current query.\n",
                                    "      2) Extract one new keyword from that chunk and add it to the query (optional).\n",
                                    "      3) Stop if no improvement in similarity or no new keyword is found.\n",
                                    "      4) Collect the chunks from each step for final context.\n",
                                    "\n",
                                    "    This version uses a global IMPROVEMENT_THRESHOLD to decide if we should keep refining.\n",
                                    "    \"\"\"\n",
                                    "    accumulated_chunks = []\n",
                                    "    current_query = query\n",
                                    "    best_score_so_far = 0.0\n",
                                    "\n",
                                    "    for step in range(steps):\n",
                                    "        print(f\"Iteration {step+1}, current query: '{current_query}'\")\n",
                                    "\n",
                                    "        best_chunk_text, best_chunk_score, best_chunk_metadata = retrieve_best_chunk(current_query, collection)\n",
                                    "        if not best_chunk_text:\n",
                                    "            print(\"No chunks found at this step. Ending refinement.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        print(f\"Best chunk text (first 50 chars): '{best_chunk_text[:50]}...' | Score: {best_chunk_score:.4f}\")\n",
                                    "\n",
                                    "        if best_chunk_score - best_score_so_far < IMPROVEMENT_THRESHOLD:\n",
                                    "            print(\"Improvement threshold not met. Stopping refinements.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        best_score_so_far = best_chunk_score\n",
                                    "\n",
                                    "        accumulated_chunks.append({\n",
                                    "            'step': step + 1,\n",
                                    "            'query': current_query,\n",
                                    "            'retrieved_chunk': {\n",
                                    "                'text': best_chunk_text,\n",
                                    "                'metadata': best_chunk_metadata\n",
                                    "            },\n",
                                    "            'score': best_chunk_score\n",
                                    "        })\n",
                                    "\n",
                                    "        # TODO: Update this section to:\n",
                                    "        # 1. Call the renamed extract_refinement_keywords function\n",
                                    "        # 2. Loop through the returned keywords and add each to the query\n",
                                    "        refine_word = extract_refinement_keyword(best_chunk_text, current_query)\n",
                                    "        if not refine_word:\n",
                                    "            print(\"No suitable keyword found for further refinement.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        print(f\"Refining query with keyword: {refine_word}\")\n",
                                    "        current_query = refine_query(current_query, refine_word)\n",
                                    "\n",
                                    "    return accumulated_chunks\n",
                                    "\n",
                                    "def build_final_context(iteration_results):\n",
                                    "    \"\"\"\n",
                                    "    Combine all retrieved chunks from each iteration into one context block.\n",
                                    "    Return a fallback message if no chunks were retrieved.\n",
                                    "    \"\"\"\n",
                                    "    if not iteration_results:\n",
                                    "        return \"No relevant information was found after iterative retrieval.\"\n",
                                    "\n",
                                    "    lines = []\n",
                                    "    for result in iteration_results:\n",
                                    "        lines.append(\n",
                                    "            f\"- Step {result['step']} (Score={result['score']:.4f}): {result['retrieved_chunk']['text']}\"\n",
                                    "        )\n",
                                    "    return \"\\n\".join(lines)\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Small demonstration with a sample corpus\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\", chunk_size=40)\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"iterative_collection\")\n",
                                    "\n",
                                    "    initial_query = \"What internal policies apply specifically to employees?\"\n",
                                    "    iteration_results = iterative_retrieval(initial_query, collection, steps=3)\n",
                                    "\n",
                                    "    final_context = build_final_context(iteration_results)\n",
                                    "    print(f\"\\nFinal combined context:\\n{final_context}\")\n",
                                    "```\n",
                                    "\n",
                                    "**Refining iterative\\_retrieval logic**\n",
                                    "\n",
                                    "I’ll need to modify the `iterative_retrieval` to fetch and add up to two new keywords from the best chunk in each loop iteration. I'll adjust the `extract_refinement_keywords` function to accept a `max_keywords` parameter, rename it accordingly, and return the top N keywords sorted by length. We’ll refine the query by appending these keywords one by one, ensuring they are not duplicates. I'll check if the new approach improves the result based on the score.\n",
                                    "\n",
                                    "\n",
                                    "Thought for 5 seconds\n",
                                    "\n",
                                    "\n",
                                    "Here’s how you can extend your pipeline to pull up to two new keywords each iteration:\n",
                                    "\n",
                                    "1. **Rename & expand** `extract_refinement_keyword` → `extract_refinement_keywords` to accept `max_keywords` and return a list.\n",
                                    "2. **Sort** candidates by length (longer → shorter) and pick the top N.\n",
                                    "3. In **`iterative_retrieval`**, call the new extractor, bail out if it returns nothing, then loop over each returned keyword to refine the query.\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "\n",
                                    "IMPROVEMENT_THRESHOLD = 0.02\n",
                                    "\n",
                                    "STOPWORDS = set([\n",
                                    "    \"the\",\"and\",\"is\",\"in\",\"of\",\"to\",\"a\",\"that\",\"for\",\"on\",\"with\",\"as\",\"it\",\"by\",\n",
                                    "    \"this\",\"are\",\"was\",\"at\",\"from\",\"or\",\"be\",\"which\",\"not\",\"can\",\"also\",\"have\",\n",
                                    "    \"has\",\"had\",\"we\",\"they\",\"you\",\"he\",\"she\",\"his\",\"her\",\"its\",\"our\",\"us\",\"their\",\n",
                                    "    \"them\",\"i\",\"do\",\"does\",\"did\",\"just\",\"so\",\"if\",\"may\",\"will\",\"shall\",\"more\",\"most\",\n",
                                    "    \"some\",\"many\",\"any\",\"all\",\"what\",\"about\",\"would\",\"could\",\"should\",\"where\",\"when\",\n",
                                    "    \"why\",\"how\"\n",
                                    "])\n",
                                    "\n",
                                    "def retrieve_best_chunk(query_text, collection, n_results=1):\n",
                                    "    retrieval = collection.query(query_texts=[query_text], n_results=n_results)\n",
                                    "    if not retrieval['documents'][0]:\n",
                                    "        return None, None, None\n",
                                    "\n",
                                    "    best_chunk_text = retrieval['documents'][0][0]\n",
                                    "    best_distance = retrieval['distances'][0][0]\n",
                                    "    best_chunk_score = 1 / (1 + best_distance)\n",
                                    "    best_chunk_metadata = retrieval['metadatas'][0][0]\n",
                                    "    return best_chunk_text, best_chunk_score, best_chunk_metadata\n",
                                    "\n",
                                    "def extract_refinement_keywords(chunk_text, current_query, max_keywords=2):\n",
                                    "    \"\"\"\n",
                                    "    Extract up to `max_keywords` new keywords from `chunk_text`:\n",
                                    "    - Excludes stopwords, words ≤4 chars, and anything already in `current_query`.\n",
                                    "    - Returns the longest candidates first.\n",
                                    "    \"\"\"\n",
                                    "    chunk_words = re.findall(r'\\b\\w+\\b', chunk_text.lower())\n",
                                    "    query_words = set(re.findall(r'\\b\\w+\\b', current_query.lower()))\n",
                                    "\n",
                                    "    candidates = {\n",
                                    "        w for w in chunk_words\n",
                                    "        if w not in STOPWORDS and w not in query_words and len(w) > 4\n",
                                    "    }\n",
                                    "    # sort by length descending, pick top max_keywords\n",
                                    "    sorted_candidates = sorted(candidates, key=len, reverse=True)\n",
                                    "    return sorted_candidates[:max_keywords]\n",
                                    "\n",
                                    "def refine_query(current_query, refine_word):\n",
                                    "    if not refine_word:\n",
                                    "        return current_query\n",
                                    "    return f\"{current_query} {refine_word}\"\n",
                                    "\n",
                                    "def iterative_retrieval(query, collection, steps=3, max_new_keywords=2):\n",
                                    "    accumulated_chunks = []\n",
                                    "    current_query = query\n",
                                    "    best_score_so_far = 0.0\n",
                                    "\n",
                                    "    for step in range(steps):\n",
                                    "        print(f\"Iteration {step+1}, current query: '{current_query}'\")\n",
                                    "        best_chunk_text, best_chunk_score, best_chunk_metadata = retrieve_best_chunk(current_query, collection)\n",
                                    "        if not best_chunk_text:\n",
                                    "            print(\"No chunks found. Ending.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        print(f\" → Chunk: '{best_chunk_text[:50]}...' | Score: {best_chunk_score:.4f}\")\n",
                                    "        if best_chunk_score - best_score_so_far < IMPROVEMENT_THRESHOLD:\n",
                                    "            print(\"No significant improvement. Stopping.\")\n",
                                    "            break\n",
                                    "        best_score_so_far = best_chunk_score\n",
                                    "\n",
                                    "        accumulated_chunks.append({\n",
                                    "            'step': step + 1,\n",
                                    "            'query': current_query,\n",
                                    "            'retrieved_chunk': {\n",
                                    "                'text': best_chunk_text,\n",
                                    "                'metadata': best_chunk_metadata\n",
                                    "            },\n",
                                    "            'score': best_chunk_score\n",
                                    "        })\n",
                                    "\n",
                                    "        # **NEW**: grab up to 2 new keywords\n",
                                    "        new_keywords = extract_refinement_keywords(best_chunk_text, current_query, max_new_keywords)\n",
                                    "        if not new_keywords:\n",
                                    "            print(\"No new keywords found. Ending.\")\n",
                                    "            break\n",
                                    "\n",
                                    "        for kw in new_keywords:\n",
                                    "            print(f\"Refining query with: {kw}\")\n",
                                    "            current_query = refine_query(current_query, kw)\n",
                                    "\n",
                                    "    return accumulated_chunks\n",
                                    "\n",
                                    "def build_final_context(iteration_results):\n",
                                    "    if not iteration_results:\n",
                                    "        return \"No relevant information found.\"\n",
                                    "\n",
                                    "    return \"\\n\".join(\n",
                                    "        f\"- Step {r['step']} (Score={r['score']:.4f}): {r['retrieved_chunk']['text']}\"\n",
                                    "        for r in iteration_results\n",
                                    "    )\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\", chunk_size=40)\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"iterative_collection\")\n",
                                    "\n",
                                    "    initial_query = \"What internal policies apply specifically to employees?\"\n",
                                    "    results = iterative_retrieval(initial_query, collection, steps=3, max_new_keywords=2)\n",
                                    "    print(\"\\nFinal context:\\n\", build_final_context(results))\n",
                                    "```\n",
                                    "\n",
                                    "**What changed?**\n",
                                    "\n",
                                    "* **`extract_refinement_keywords`** now returns a list of up to `max_keywords` new terms, sorted by length.\n",
                                    "* **`iterative_retrieval`** loops over each new keyword in one iteration—appending both before the next retrieval.\n",
                                    "* Everything else (threshold check, chunk collection) remains the same. This should make your query sharpening faster and more nuanced.\n"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
