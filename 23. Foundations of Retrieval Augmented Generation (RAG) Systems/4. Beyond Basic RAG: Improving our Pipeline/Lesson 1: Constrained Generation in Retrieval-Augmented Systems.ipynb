{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 1: Constrained Generation in Retrieval-Augmented Systems\n",
                                    "\n",
                                    "\n",
                                    "Welcome to the first lesson of the **“Beyond Basic RAG: Improving our Pipeline”** course, part of the **“Foundations of RAG Systems”** path! In previous courses, you delved into the basics of Retrieval-Augmented Generation (RAG), exploring text representation with a focus on embeddings and vector databases. In this course, we’ll embark on an exciting journey to enhance our RAG systems with advanced techniques.\n",
                                    "\n",
                                    "Our focus in this initial lesson is **constrained generation**, a powerful method to ensure that language model responses remain anchored in the retrieved context—avoiding speculation or unrelated content. Get ready to elevate your RAG skills and build more reliable systems!\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Theoretical Foundations of Constrained Generation\n",
                                    "\n",
                                    "When employing large language models (LLMs) in real-world applications, **accuracy** and **fidelity** to a trusted dataset are paramount. Even advanced LLMs can produce incorrect or fabricated information—often termed “hallucinations.” This is where constrained generation becomes indispensable. In essence, it is a form of advanced prompt engineering: we carefully craft instructions so the LLM:\n",
                                    "\n",
                                    "* **Uses only the data you supply** (the “retrieved context”).\n",
                                    "* **Provides disclaimers or refusal messages** when context is insufficient.\n",
                                    "* **Optionally cites** which part of the content it used.\n",
                                    "\n",
                                    "By shaping the prompt and enforcing rule-based fallback mechanisms, we instruct the LLM to remain grounded in the retrieved context. The result is a system less prone to made-up facts and more consistent with the original knowledge source.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Why Constrained Generation Is Important\n",
                                    "\n",
                                    "LLM hallucinations can be misleading. Imagine an application confidently presenting policies or regulations **not** present in your knowledge base—this can create confusion or compliance issues. With constrained generation:\n",
                                    "\n",
                                    "* The model remains **grounded** in the retrieved context only.\n",
                                    "* Uncertain or unavailable information triggers a fallback message like **“No sufficient data.”**\n",
                                    "* You can require the model to **cite lines** to verify the answer’s source, building user trust.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Defining the Constrained Generation Function\n",
                                    "\n",
                                    "We’ll start by defining a function that enforces these constraints:\n",
                                    "\n",
                                    "```python\n",
                                    "def generate_with_constraints(query, retrieved_context, strategy=\"base\"):\n",
                                    "    \"\"\"\n",
                                    "    Thoroughly enforce model reliance on 'retrieved_context' when answering 'query.'\n",
                                    "\n",
                                    "    The 'strategy' parameter allows for different prompt template variations:\n",
                                    "      1) Base approach: Provide context, instruct LLM not to use outside info.\n",
                                    "      2) Strict approach: Provide context with explicit disclaimers if the answer is not found.\n",
                                    "      3) Citation approach: Provide context, then request the LLM to cite the relevant lines.\n",
                                    "\n",
                                    "    Robust fallback:\n",
                                    "      - If 'retrieved_context' is empty, respond with an apology or neutral statement.\n",
                                    "      - Optionally log each stage for debugging or performance analysis.\n",
                                    "    \"\"\"\n",
                                    "    # Provide a safe fallback if no context is retrieved\n",
                                    "    if not retrieved_context.strip():\n",
                                    "        return (\"I'm sorry, but I couldn't find any relevant information.\", \"No context used.\")\n",
                                    "\n",
                                    "    # Choose a prompt template based on strategy\n",
                                    "    if strategy == \"base\":\n",
                                    "        # Base approach: instruct to use the context and not rely on external info\n",
                                    "        prompt = (\n",
                                    "            \"Use the following context to answer the question in a concise manner.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"strict\":\n",
                                    "        # Strict approach: explicitly disallow info beyond the provided context\n",
                                    "        prompt = (\n",
                                    "            \"You must ONLY use the context provided below. If you cannot find the answer in the context, say: 'No sufficient data'.\\n\"\n",
                                    "            \"Do not provide any information not found in the context.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"cite\":\n",
                                    "        # Citation approach: require references to lines used\n",
                                    "        prompt = (\n",
                                    "            \"Answer strictly from the provided context, and list the lines you used as evidence with 'Cited lines:'.\\n\"\n",
                                    "            \"If the context does not contain the information, respond with: 'Not available in the retrieved texts.'\\n\\n\"\n",
                                    "            f\"Provided context (label lines as needed):\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    # …\n",
                                    "    response = get_llm_response(prompt)\n",
                                    "\n",
                                    "    # Attempt to parse out 'Cited lines:' if present\n",
                                    "    segments = response.split(\"Cited lines:\")\n",
                                    "    if len(segments) == 2:\n",
                                    "        answer_part, used_context_part = segments\n",
                                    "        return answer_part.strip(), used_context_part.strip()\n",
                                    "    else:\n",
                                    "        return response.strip(), \"No explicit lines cited.\"\n",
                                    "```\n",
                                    "\n",
                                    "**How it works**:\n",
                                    "\n",
                                    "1. **Fallback**: If no context was retrieved, the function immediately returns an apology.\n",
                                    "2. **Strategies**:\n",
                                    "\n",
                                    "   * **Base**: Use context; don’t use external info.\n",
                                    "   * **Strict**: Only use context; reply “No sufficient data” if absent.\n",
                                    "   * **Citation**: Answer from context and cite lines; or state “Not available in the retrieved texts.”\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Demonstration of Retrieval and Constrained Generation\n",
                                    "\n",
                                    "```python\n",
                                    "# 1. Load and chunk a corpus\n",
                                    "chunked_docs = load_and_chunk_corpus(\"data/corpus.json\")\n",
                                    "\n",
                                    "# 2. Build a collection in a vector database\n",
                                    "collection = build_chroma_collection(chunked_docs, collection_name=\"corpus_collection\")\n",
                                    "\n",
                                    "# 3. Run a sample query\n",
                                    "query = \"Highlight the main policies that apply to employees.\"\n",
                                    "retrieval_results = collection.query(query_texts=[query], n_results=2)\n",
                                    "\n",
                                    "# 4. Construct the retrieved context from top matches\n",
                                    "if not retrieval_results['documents'][0]:\n",
                                    "    retrieved_context = \"\"\n",
                                    "else:\n",
                                    "    retrieved_context = \"\\n\".join([\"- \" + doc_text for doc_text in retrieval_results['documents'][0]])\n",
                                    "\n",
                                    "# 5. Execute constrained generation function\n",
                                    "strategy = \"strict\"\n",
                                    "answer, used_context = generate_with_constraints(query, retrieved_context, strategy=strategy)\n",
                                    "\n",
                                    "print(\"Answer:\", answer)\n",
                                    "print(\"Cited Context:\", used_context)\n",
                                    "```\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Practical Example: A Policy FAQ Bot\n",
                                    "\n",
                                    "Consider an HR FAQ bot with access to internal policy documents. When employees ask about vacation rules:\n",
                                    "\n",
                                    "1. The bot **retrieves** relevant sections from the knowledge base.\n",
                                    "2. It calls `generate_with_constraints(..., strategy=\"strict\")`.\n",
                                    "3. If the policy is documented, it returns an accurate answer; otherwise, “No sufficient data.”\n",
                                    "4. For transparency, use `strategy=\"cite\"` to include specific policy line references.\n",
                                    "\n",
                                    "This workflow ensures your FAQ bot avoids hallucinations and remains grounded in official documents.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Conclusion and Next Steps\n",
                                    "\n",
                                    "Constrained generation is essential for keeping RAG systems tightly bound to authentic sources. By tailoring prompt instructions and incorporating fallback logic, you reduce the risk of misinformation and ensure answers stay grounded in retrieved documents.\n",
                                    "\n",
                                    "**Next Steps**:\n",
                                    "\n",
                                    "* Experiment with different prompt styles and strategies to fine-tune strictness or citation detail.\n",
                                    "* Evaluate system behavior by omitting key context and observing fallback responses.\n",
                                    "* Integrate these strategies into broader real-world scenarios and measure accuracy under varied user requests.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Constrained Prompt Completion Challenge\n",
                                    "\n",
                                    "Welcome to your first hands-on exercise in enhancing Retrieval-Augmented Generation (RAG) systems! In this activity, you'll dive into the world of constrained generation by completing a crucial part of the generate_with_constraints function. Your mission is to fill in the missing lines for the base prompt strategy. This strategy ensures that the language model provides answers strictly from the retrieved context.\n",
                                    "\n",
                                    "Here's what you need to do:\n",
                                    "\n",
                                    "Focus on the if strategy == \"base\": section of the function.\n",
                                    "Complete the prompt so that it instructs the model to use the provided context for answering the query.\n",
                                    "By completing this exercise, you'll reinforce your understanding of how to keep language models grounded in the retrieved context, a key skill in building reliable RAG systems. Enjoy the process of crafting those prompts!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "def generate_with_constraints(query, retrieved_context, strategy=\"base\"):\n",
                                    "    \"\"\"\n",
                                    "    Thoroughly enforce model reliance on 'retrieved_context' when answering 'query.'\n",
                                    "\n",
                                    "    The 'strategy' parameter allows for different prompt template variations:\n",
                                    "      1) Base approach: Provide context, instruct LLM not to use outside info, \n",
                                    "         and respond with 'No sufficient data' if the context is insufficient.\n",
                                    "      2) Strict approach: Provide context with explicit disclaimers if the answer is not found.\n",
                                    "      3) Citation approach: Provide context, then request the LLM to cite the relevant lines.\n",
                                    "\n",
                                    "    Robust fallback:\n",
                                    "      - If 'retrieved_context' is empty, respond with an apology or neutral statement.\n",
                                    "      - Optionally log each stage for debugging or performance analysis.\n",
                                    "    \"\"\"\n",
                                    "    # Provide a safe fallback if no context is retrieved\n",
                                    "    if not retrieved_context.strip():\n",
                                    "        return (\"I'm sorry, but I couldn't find any relevant information.\", \"No context used.\")\n",
                                    "\n",
                                    "    # Choose a prompt template based on strategy\n",
                                    "    if strategy == \"base\":\n",
                                    "        # TODO: Complete the base prompt template to instruct the model to use the provided context\n",
                                    "        prompt = (\n",
                                    "            \"________________________________________\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"strict\":\n",
                                    "        # Strict approach: explicitly disallow info beyond the provided context\n",
                                    "        prompt = (\n",
                                    "            \"You must ONLY use the context provided below. If you cannot find the answer in the context, say: 'No sufficient data'.\\n\"\n",
                                    "            \"Do not provide any information not found in the context.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"cite\":\n",
                                    "        # Citation approach: require references to lines used\n",
                                    "        prompt = (\n",
                                    "            \"Answer strictly from the provided context, and list the lines you used as evidence with 'Cited lines:'.\\n\"\n",
                                    "            \"If the context does not contain the information, respond with: 'Not available in the retrieved texts.'\\n\\n\"\n",
                                    "            f\"Provided context (label lines as needed):\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "\n",
                                    "    # Print the prompt for debugging or inspection\n",
                                    "    print(f\"Prompt: \\n {prompt}\\n\")\n",
                                    "\n",
                                    "    # Make call to the LLM\n",
                                    "    response = get_llm_response(prompt)\n",
                                    "\n",
                                    "    # Attempt to parse out 'Cited lines:' if present\n",
                                    "    segments = response.split(\"Cited lines:\")\n",
                                    "    if len(segments) == 2:\n",
                                    "        answer_part, used_context_part = segments\n",
                                    "        return answer_part.strip(), used_context_part.strip()\n",
                                    "    else:\n",
                                    "        # If the LLM didn't provide citations, treat the entire response as the answer\n",
                                    "        return response.strip(), \"No explicit lines cited.\"\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Example usage demonstrating retrieval followed by constrained generation\n",
                                    "\n",
                                    "    # 1. Load and chunk a corpus\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\")\n",
                                    "\n",
                                    "    # 2. Build a collection in a vector database\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"corpus_collection\")\n",
                                    "\n",
                                    "    # 3. Run a sample query\n",
                                    "    query = \"Highlight the main policies that apply to employees.\"\n",
                                    "    retrieval_results = collection.query(query_texts=[query], n_results=2)\n",
                                    "\n",
                                    "    # 4. Construct the retrieved context from top matches\n",
                                    "    if not retrieval_results['documents'][0]:\n",
                                    "        retrieved_context = \"\"\n",
                                    "    else:\n",
                                    "        retrieved_context = \"\\n\".join([\"- \" + doc_text for doc_text in retrieval_results['documents'][0]])\n",
                                    "\n",
                                    "    # 5. Execute constrained generation function for demonstration\n",
                                    "    for strategy in (\"base\", \"strict\", \"cite\"):\n",
                                    "        answer, used_context = generate_with_constraints(query, retrieved_context, strategy=strategy)\n",
                                    "        print(f\"Strategy: {strategy}\")\n",
                                    "        print(f\"Constrained generation answer:\\n{answer}\")\n",
                                    "        print(f\"Context or lines used: {used_context}\\n\")\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "def generate_with_constraints(query, retrieved_context, strategy=\"base\"):\n",
                                    "    \"\"\"\n",
                                    "    Thoroughly enforce model reliance on 'retrieved_context' when answering 'query.'\n",
                                    "\n",
                                    "    The 'strategy' parameter allows for different prompt template variations:\n",
                                    "      1) Base approach: Provide context, instruct LLM not to use outside info,\n",
                                    "         and respond with 'No sufficient data' if the context is insufficient.\n",
                                    "      2) Strict approach: Provide context with explicit disclaimers if the answer is not found.\n",
                                    "      3) Citation approach: Provide context, then request the LLM to cite the relevant lines.\n",
                                    "\n",
                                    "    Robust fallback:\n",
                                    "      - If 'retrieved_context' is empty, respond with an apology or neutral statement.\n",
                                    "      - Optionally log each stage for debugging or performance analysis.\n",
                                    "    \"\"\"\n",
                                    "    # Provide a safe fallback if no context is retrieved\n",
                                    "    if not retrieved_context.strip():\n",
                                    "        return (\"I'm sorry, but I couldn't find any relevant information.\", \"No context used.\")\n",
                                    "\n",
                                    "    # Choose a prompt template based on strategy\n",
                                    "    if strategy == \"base\":\n",
                                    "        # Base approach: use only the provided context, no outside knowledge, fallback if missing\n",
                                    "        prompt = (\n",
                                    "            \"You are an AI assistant. Use **only** the context provided below to answer the question.\\n\"\n",
                                    "            \"Do not draw on any outside information or make unsupported assumptions.\\n\"\n",
                                    "            \"If the answer is not contained within the context, respond with: 'No sufficient data'.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\\n\"\n",
                                    "            f\"Question: {query}\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"strict\":\n",
                                    "        # Strict approach: explicitly disallow info beyond the provided context\n",
                                    "        prompt = (\n",
                                    "            \"You must ONLY use the context provided below. If you cannot find the answer in the context, say: 'No sufficient data'.\\n\"\n",
                                    "            \"Do not provide any information not found in the context.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"cite\":\n",
                                    "        # Citation approach: require references to lines used\n",
                                    "        prompt = (\n",
                                    "            \"Answer strictly from the provided context, and list the lines you used as evidence with 'Cited lines:'.\\n\"\n",
                                    "            \"If the context does not contain the information, respond with: 'Not available in the retrieved texts.'\\n\\n\"\n",
                                    "            f\"Provided context (label lines as needed):\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "\n",
                                    "    # Print the prompt for debugging or inspection\n",
                                    "    print(f\"Prompt: \\n{prompt}\\n\")\n",
                                    "\n",
                                    "    # Make call to the LLM\n",
                                    "    response = get_llm_response(prompt)\n",
                                    "\n",
                                    "    # Attempt to parse out 'Cited lines:' if present\n",
                                    "    segments = response.split(\"Cited lines:\")\n",
                                    "    if len(segments) == 2:\n",
                                    "        answer_part, used_context_part = segments\n",
                                    "        return answer_part.strip(), used_context_part.strip()\n",
                                    "    else:\n",
                                    "        # If the LLM didn't provide citations, treat the entire response as the answer\n",
                                    "        return response.strip(), \"No explicit lines cited.\"\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Mastering the Strict Prompt Strategy\n",
                                    "\n",
                                    "Well done on mastering the base prompt strategy in the previous exercise! Now, let's elevate your skills by exploring the strict prompt strategy. Your goal is to complete the missing lines in the generate_with_constraints function for the strict strategy. This method ensures that the language model provides answers strictly from the retrieved context or gives a \"No sufficient data\" disclaimer when the context is insufficient.\n",
                                    "\n",
                                    "Focus on the following aspects:\n",
                                    "\n",
                                    "In the if strategy == \"strict\": section, fill in the blanks to craft a prompt that:\n",
                                    "Instructs the model to use only the provided context.\n",
                                    "Clearly states that if the answer isn't found in the context, the model should respond with \"No sufficient data.\"\n",
                                    "Explicitly forbids the use of any external information.\n",
                                    "By completing this exercise, you'll enhance your skills in crafting precise prompts that keep language models grounded in the retrieved context, a crucial aspect of building reliable RAG systems. Enjoy the challenge and happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "def generate_with_constraints(query, retrieved_context, strategy=\"base\"):\n",
                                    "    \"\"\"\n",
                                    "    Thoroughly enforce model reliance on 'retrieved_context' when answering 'query.'\n",
                                    "  \n",
                                    "    The 'strategy' parameter allows for different prompt template variations:\n",
                                    "      1) Base approach: Provide context, instruct LLM not to use outside info.\n",
                                    "      2) Strict approach: Provide context with explicit disclaimers if the answer is not found.\n",
                                    "      3) Citation approach: Provide context, then request the LLM to cite the relevant lines.\n",
                                    "\n",
                                    "    Robust fallback:\n",
                                    "      - If 'retrieved_context' is empty, respond with an apology or neutral statement.\n",
                                    "      - Optionally log each stage for debugging or performance analysis.\n",
                                    "    \"\"\"\n",
                                    "    # Check if we have no retrieved context\n",
                                    "    if not retrieved_context.strip():\n",
                                    "        return (\"I'm sorry, but I couldn't find any relevant information.\", \"No context used.\")\n",
                                    "\n",
                                    "    # Multiple template examples\n",
                                    "    if strategy == \"base\":\n",
                                    "        # Base approach\n",
                                    "        prompt = (\n",
                                    "            \"Use the following context to answer the question in a concise manner.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"strict\":\n",
                                    "        # TODO: Complete the strict prompt template to:\n",
                                    "        # 1. Instruct the model to use ONLY the provided context\n",
                                    "        # 2. Specify that 'No sufficient data' should be returned when answer isn't in context\n",
                                    "        # 3. Explicitly forbid using external information\n",
                                    "        prompt = (\n",
                                    "            \"________________________________________\\n\"\n",
                                    "            \"________________________________________\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"cite\":\n",
                                    "        # Citation approach\n",
                                    "        prompt = (\n",
                                    "            \"Answer strictly from the provided context, and list the lines you used as evidence with 'Cited lines:'.\\n\"\n",
                                    "            \"If the context does not contain the information, respond with: 'Not available in the retrieved texts.'\\n\\n\"\n",
                                    "            f\"Provided context (label lines as needed):\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "  \n",
                                    "    print(f\"Prompt: \\n {prompt}\\n\")\n",
                                    "\n",
                                    "    # Make call to LLM\n",
                                    "    response = get_llm_response(prompt)\n",
                                    "\n",
                                    "    # Attempt to parse out 'Cited lines:' if present\n",
                                    "    segments = response.split(\"Cited lines:\")\n",
                                    "    if len(segments) == 2:\n",
                                    "        answer_part, used_context_part = segments\n",
                                    "        return answer_part.strip(), used_context_part.strip()\n",
                                    "    else:\n",
                                    "        # If the LLM didn't adhere fully, we store the entire response as the answer\n",
                                    "        return response.strip(), \"No explicit lines cited.\"\n",
                                    "      \n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Example demonstration \n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\")\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"corpus_collection\")\n",
                                    "\n",
                                    "    # Example query that might yield relevant or no results\n",
                                    "    query = \"Highlight the main policies that apply to employees.\"\n",
                                    "    retrieval_results = collection.query(query_texts=[query], n_results=2)\n",
                                    "\n",
                                    "    if not retrieval_results['documents'][0]:\n",
                                    "        # Fallback: no relevant chunk\n",
                                    "        retrieved_context = \"\"\n",
                                    "    else:\n",
                                    "        # Build retrieved context from top matches\n",
                                    "        retrieved_context = \"\\n\".join([\"- \" + doc_text for doc_text in retrieval_results['documents'][0]])\n",
                                    "  \n",
                                    "    for strategy in (\"base\", \"strict\", \"cite\"):\n",
                                    "        answer, used_context = generate_with_constraints(query, retrieved_context, strategy=strategy)\n",
                                    "        print(f\"Constrained generation answer:\\n{answer}\")\n",
                                    "        print(f\"Context or lines used:\\n{used_context}\\n\")\n",
                                    "```\n",
                                    "\n",
                                    "To complete the strict prompt template in the `generate_with_constraints` function, you need to ensure that the prompt clearly instructs the language model to rely solely on the provided context, specify that it should return \"No sufficient data\" if the answer isn't found within the context, and explicitly forbid the use of any external information. Here's how you can craft the prompt for the strict strategy:\n",
                                    "\n",
                                    "```python\n",
                                    "elif strategy == \"strict\":\n",
                                    "    # Strict approach\n",
                                    "    prompt = (\n",
                                    "        \"Using only the provided context below, answer the question. If the answer is not found in the context, \"\n",
                                    "        \"respond with 'No sufficient data'. Do not use any external information.\\n\\n\"\n",
                                    "        f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "        f\"Question: '{query}'\\n\"\n",
                                    "        \"Answer:\"\n",
                                    "    )\n",
                                    "```\n",
                                    "\n",
                                    "This prompt template:\n",
                                    "- **Directs the model** to use only the provided context for generating the answer.\n",
                                    "- **Sets a clear instruction** for the model to respond with \"No sufficient data\" if the answer cannot be derived from the provided context.\n",
                                    "- **Prohibits the use of external information**, ensuring that the model's response is strictly based on the context given.\n",
                                    "\n",
                                    "This approach is crucial for applications where accuracy and source reliability are paramount, and it helps in maintaining the integrity of the answers provided by the language model."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Citation Prompt Strategy Implementation\n",
                                    "\n",
                                    "In the previous exercise, you successfully navigated the intricacies of the strict prompt strategy. Now, it's time to explore the citation prompt strategy, where you'll enhance the generate_with_constraints function by completing the missing lines. This approach emphasizes transparency by ensuring the language model cites the lines it used from the retrieved context.\n",
                                    "\n",
                                    "Here's your mission:\n",
                                    "\n",
                                    "Focus on the if strategy == \"cite\": section of the function.\n",
                                    "Fill in the blanks to craft a prompt that:\n",
                                    "Instructs the model to answer strictly from the provided context.\n",
                                    "Requires the model to list the lines it used as evidence with \"Cited lines:\".\n",
                                    "Ensures that if the context does not contain the information, the model responds with \"Not available in the retrieved texts.\"\n",
                                    "By completing this exercise, you'll refine your ability to create prompts that not only keep language models grounded in the retrieved context but also provide traceability of the information source. This is a crucial skill for building reliable and transparent RAG systems. Dive in and enjoy the coding challenge!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "def generate_with_constraints(query, retrieved_context, strategy=\"base\"):\n",
                                    "    \"\"\"\n",
                                    "    Thoroughly enforce model reliance on 'retrieved_context' when answering 'query.'\n",
                                    "    \n",
                                    "    The 'strategy' parameter allows for different prompt template variations:\n",
                                    "      1) Base approach: Provide context, instruct LLM not to use outside info.\n",
                                    "      2) Strict approach: Provide context with explicit disclaimers if the answer is not found.\n",
                                    "      3) Citation approach: Provide context, then request the LLM to cite the relevant lines.\n",
                                    "\n",
                                    "    Robust fallback:\n",
                                    "      - If 'retrieved_context' is empty, respond with an apology or neutral statement.\n",
                                    "      - Optionally log each stage for debugging or performance analysis.\n",
                                    "    \"\"\"\n",
                                    "    # If there is no retrieved context, return fallback\n",
                                    "    if not retrieved_context.strip():\n",
                                    "        return (\"I'm sorry, but I couldn't find any relevant information.\", \"No context used.\")\n",
                                    "\n",
                                    "    # Choose a prompt template based on the strategy\n",
                                    "    if strategy == \"base\":\n",
                                    "        prompt = (\n",
                                    "            \"Use the following context to answer the question in a concise manner.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"strict\":\n",
                                    "        prompt = (\n",
                                    "            \"You must ONLY use the context provided below. If you cannot find the answer in the context, say: 'No sufficient data'.\\n\"\n",
                                    "            \"Do not provide any information not found in the context.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"cite\":\n",
                                    "        # TODO: Complete the citation prompt template to:\n",
                                    "        # 1. Request answers strictly from the provided context\n",
                                    "        # 2. Require listing of used lines as evidence with 'Cited lines:'\n",
                                    "        # 3. Specify the fallback response when information is not found\n",
                                    "        prompt = (\n",
                                    "            \"________________________________________\\n\"\n",
                                    "            \"________________________________________\\n\\n\"\n",
                                    "            f\"Provided context (______):\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "\n",
                                    "    # Print prompt for debugging\n",
                                    "    print(f\"Prompt: \\n{prompt}\\n\")\n",
                                    "\n",
                                    "    # Send prompt to the LLM\n",
                                    "    response = get_llm_response(prompt)\n",
                                    "\n",
                                    "    # Attempt to parse out 'Cited lines:' if present\n",
                                    "    segments = response.split(\"Cited lines:\")\n",
                                    "    if len(segments) == 2:\n",
                                    "        answer_part, used_context_part = segments\n",
                                    "        return answer_part.strip(), used_context_part.strip()\n",
                                    "    else:\n",
                                    "        return response.strip(), \"No explicit lines cited.\"\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Demonstration of retrieval plus constrained generation\n",
                                    "\n",
                                    "    # 1. Load and chunk corpus\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\")\n",
                                    "\n",
                                    "    # 2. Create a collection in our vector database\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"corpus_collection\")\n",
                                    "\n",
                                    "    # 3. Sample query\n",
                                    "    query = \"Highlight the main policies that apply to employees.\"\n",
                                    "    retrieval_results = collection.query(query_texts=[query], n_results=2)\n",
                                    "\n",
                                    "    # 4. Construct retrieved context from top matches\n",
                                    "    if not retrieval_results['documents'][0]:\n",
                                    "        retrieved_context = \"\"\n",
                                    "    else:\n",
                                    "        retrieved_context = \"\\n\".join([\"- \" + doc_text for doc_text in retrieval_results['documents'][0]])\n",
                                    "\n",
                                    "    # 5. Generate answers using different strategies\n",
                                    "    for strategy in (\"base\", \"strict\", \"cite\"):\n",
                                    "        answer, used_context = generate_with_constraints(query, retrieved_context, strategy=strategy)\n",
                                    "        print(f\"--- Strategy: {strategy} ---\")\n",
                                    "        print(\"Answer:\")\n",
                                    "        print(answer)\n",
                                    "        print(\"Context or lines used:\")\n",
                                    "        print(used_context)\n",
                                    "        print()\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To complete the citation prompt strategy in the `generate_with_constraints` function, you need to ensure that the prompt instructs the language model to answer strictly from the provided context and to cite the specific lines used as evidence. Additionally, the prompt should specify a fallback response if the information is not found in the context. Here's how you can craft the prompt for the citation strategy:\n",
                                    "\n",
                                    "```python\n",
                                    "elif strategy == \"cite\":\n",
                                    "    # Citation approach\n",
                                    "    prompt = (\n",
                                    "        \"Answer the question using only the provided context. If the context does not contain the information, \"\n",
                                    "        \"respond with 'Not available in the retrieved texts.' Cite the specific lines used as evidence with 'Cited lines:'.\\n\\n\"\n",
                                    "        f\"Provided context:\\n{retrieved_context}\\n\"\n",
                                    "        f\"Question: '{query}'\\n\"\n",
                                    "        \"Answer:\"\n",
                                    "    )\n",
                                    "```\n",
                                    "\n",
                                    "This prompt template:\n",
                                    "- **Directs the model** to use only the provided context for generating the answer.\n",
                                    "- **Requires the model to cite** the specific lines from the context that support the answer, enhancing transparency and traceability.\n",
                                    "- **Specifies a clear fallback response** (\"Not available in the retrieved texts\") if the answer cannot be derived from the provided context.\n",
                                    "\n",
                                    "This approach is particularly useful in scenarios where it's crucial to trace the source of the information provided by the model, ensuring accountability and reliability in the responses."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Reimplement Constrained Generation Function\n",
                                    "\n",
                                    "Congratulations on reaching the final challenge of this unit! You've done an excellent job so far, especially in the previous exercise where you tackled context handling. Now, it's time to bring everything together and reimplement the entire generate_with_constraints function from scratch. This will deepen your understanding of constrained generation in Retrieval-Augmented Generation (RAG) systems.\n",
                                    "\n",
                                    "Your objective is to ensure that the language model strictly adheres to the retrieved context, providing accurate and context-based answers. Here's what you need to focus on:\n",
                                    "\n",
                                    "Handle situations where no context is available by returning a neutral fallback message.\n",
                                    "Construct prompts based on the strategy parameter, implementing logic for three strategies:\n",
                                    "Base Approach: Provide the context and instruct the model not to use external information.\n",
                                    "Strict Approach: Ensure the model uses only the provided context and responds with \"No sufficient data\" if the answer isn't found.\n",
                                    "Citation Approach: Require the model to cite the lines used as evidence, responding with \"Not available in the retrieved texts\" if the necessary information is missing.\n",
                                    "After querying the language model, parse the response to separate the main answer from any cited lines.\n",
                                    "By completing this exercise, you'll enhance your ability to create reliable and trustworthy RAG systems. Dive in, and let your expertise shine!\n",
                                    "\n",
                                    "```python\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "def generate_with_constraints(query, retrieved_context, strategy=\"base\"):\n",
                                    "    \"\"\"\n",
                                    "    Thoroughly enforce model reliance on 'retrieved_context' when answering 'query.'\n",
                                    "\n",
                                    "    This function reimplements the entire constrained generation logic from scratch.\n",
                                    "    It supports three strategies:\n",
                                    "      1) Base approach: Provide the context, instruct the model not to use external info.\n",
                                    "      2) Strict approach: Provide the context, strictly disallow info not found in the context,\n",
                                    "         and say \"No sufficient data\" if the answer isn't found.\n",
                                    "      3) Citation approach: Provide context and ask the model to cite lines (if used).\n",
                                    "         If the needed info isn't present, respond with \"Not available in the retrieved texts.\"\n",
                                    "\n",
                                    "    Fallback behavior:\n",
                                    "      - If no context is provided, return an apology message and \"No context used.\"\n",
                                    "      - After receiving the LLM response, split on 'Cited lines:' if present.\n",
                                    "    \"\"\"\n",
                                    "    # TODO: Implement the fallback for empty context\n",
                                    "    # Hint: Check if retrieved_context is empty and return appropriate message\n",
                                    "\n",
                                    "    # TODO: Implement prompt construction based on strategy\n",
                                    "    # Hint: Use if/elif to handle different strategies (base, strict, cite)\n",
                                    "    # Each strategy should have its own prompt template\n",
                                    "\n",
                                    "    # TODO: Get response from language model using the constructed prompt\n",
                                    "    # Hint: Use get_llm_response() function\n",
                                    "\n",
                                    "    # TODO: Parse the response to extract answer and cited lines (if present)\n",
                                    "    # Hint: Look for \"Cited lines:\" marker in the response\n",
                                    "\n",
                                    "    # TODO: Return the parsed response\n",
                                    "    # Hint: Return format should be (answer, context_used)\n",
                                    "    pass\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Demonstration of retrieval and constrained generation\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\")\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"corpus_collection\")\n",
                                    "\n",
                                    "    # Example user query\n",
                                    "    query = \"Highlight the main policies that apply to employees.\"\n",
                                    "    retrieval_results = collection.query(query_texts=[query], n_results=2)\n",
                                    "\n",
                                    "    # Build or clear the context\n",
                                    "    if not retrieval_results['documents'][0]:\n",
                                    "        retrieved_context = \"\"\n",
                                    "    else:\n",
                                    "        retrieved_context = \"\\n\".join([\"- \" + text for text in retrieval_results['documents'][0]])\n",
                                    "\n",
                                    "    # Test each strategy\n",
                                    "    for chosen_strategy in (\"base\", \"strict\", \"cite\"):\n",
                                    "        answer, used_context = generate_with_constraints(\n",
                                    "            query,\n",
                                    "            retrieved_context,\n",
                                    "            strategy=chosen_strategy\n",
                                    "        )\n",
                                    "        print(f\"Strategy: {chosen_strategy}\")\n",
                                    "        print(\"Constrained generation answer:\")\n",
                                    "        print(answer)\n",
                                    "        print(\"Context or lines used:\")\n",
                                    "        print(used_context)\n",
                                    "        print()\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To reimplement the `generate_with_constraints` function effectively, we need to ensure that it handles different strategies for generating responses based on the provided context. Here's a complete implementation that addresses all the requirements:\n",
                                    "\n",
                                    "```python\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "def generate_with_constraints(query, retrieved_context, strategy=\"base\"):\n",
                                    "    \"\"\"\n",
                                    "    Enforce model reliance on 'retrieved_context' when answering 'query.'\n",
                                    "    \"\"\"\n",
                                    "    # Handle the case where no context is available\n",
                                    "    if not retrieved_context.strip():\n",
                                    "        return (\"I'm sorry, but I couldn't find any relevant information.\", \"No context used.\")\n",
                                    "\n",
                                    "    # Construct the prompt based on the specified strategy\n",
                                    "    if strategy == \"base\":\n",
                                    "        prompt = (\n",
                                    "            \"Using the following context, answer the question without using external information.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"strict\":\n",
                                    "        prompt = (\n",
                                    "            \"Using only the provided context below, answer the question. If the answer is not found in the context, \"\n",
                                    "            \"respond with 'No sufficient data'. Do not use any external information.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"cite\":\n",
                                    "        prompt = (\n",
                                    "            \"Answer the question using only the provided context. Cite the specific lines used as evidence with 'Cited lines:'. \"\n",
                                    "            \"If the context does not contain the information, respond with 'Not available in the retrieved texts.'\\n\\n\"\n",
                                    "            f\"Provided context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "\n",
                                    "    # Get response from the language model\n",
                                    "    response = get_llm_response(prompt)\n",
                                    "\n",
                                    "    # Parse the response to extract the answer and any cited lines\n",
                                    "    if 'Cited lines:' in response:\n",
                                    "        answer_part, cited_lines_part = response.split('Cited lines:')\n",
                                    "        return answer_part.strip(), cited_lines_part.strip()\n",
                                    "    else:\n",
                                    "        return response.strip(), \"No explicit lines cited.\"\n",
                                    "\n",
                                    "# Example usage\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load and chunk corpus\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\")\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"corpus_collection\")\n",
                                    "\n",
                                    "    # Example user query\n",
                                    "    query = \"Highlight the main policies that apply to employees.\"\n",
                                    "    retrieval_results = collection.query(query_texts=[query], n_results=2)\n",
                                    "\n",
                                    "    # Build or clear the context\n",
                                    "    retrieved_context = \"\"\n",
                                    "    if retrieval_results['documents'][0]:\n",
                                    "        retrieved_context = \"\\n\".join([\"- \" + text for text in retrieval_results['documents'][0]])\n",
                                    "\n",
                                    "    # Test each strategy\n",
                                    "    for chosen_strategy in (\"base\", \"strict\", \"cite\"):\n",
                                    "        answer, used_context = generate_with_constraints(\n",
                                    "            query,\n",
                                    "            retrieved_context,\n",
                                    "            strategy=chosen_strategy\n",
                                    "        )\n",
                                    "        print(f\"Strategy: {chosen_strategy}\")\n",
                                    "        print(\"Constrained generation answer:\")\n",
                                    "        print(answer)\n",
                                    "        print(\"Context or lines used:\")\n",
                                    "        print(used_context)\n",
                                    "        print()\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "This implementation:\n",
                                    "- Handles cases where no context is available by returning a neutral fallback message.\n",
                                    "- Constructs different prompts based on the strategy parameter (`base`, `strict`, `cite`), each tailored to enforce specific constraints on the model's response.\n",
                                    "- Parses the model's response to separate the main answer from any cited lines, ensuring clarity and traceability of the information source.\n",
                                    "\n",
                                    "This function is now ready to be used in a Retrieval-Augmented Generation system, providing reliable and contextually accurate answers based on the strategy chosen."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Smart Context Management\n",
                                    "\n",
                                    "Welcome back! You've been doing an excellent job mastering the fundamentals of Retrieval-Augmented Generation (RAG). Previously, you focused on ensuring that language models adhere to the retrieved context and provide clear, context-based answers. Now, let's dive into a real-world challenge: managing context length effectively.\n",
                                    "\n",
                                    "In this exercise, your goal is to enhance the generate_with_constraints function to handle situations where the retrieved context exceeds the language model's token limit, for example 4,096 tokens. This is crucial for maintaining the quality and reliability of the responses.\n",
                                    "\n",
                                    "Here's what you need to focus on:\n",
                                    "\n",
                                    "Check the length of the retrieved_context. If it exceeds the token limit, you'll need to take action.\n",
                                    "Truncate the context smartly: Ensure that you preserve whole sentences while trimming the context to fit within the token limit.\n",
                                    "Append a warning: If truncation occurs, add \"[Context truncated]\" to the answer to inform users that the context was shortened.\n",
                                    "By implementing these changes, you'll ensure that your RAG system remains efficient and effective, even when\n",
                                    "\n",
                                    "```python\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "def generate_with_constraints(query, retrieved_context, strategy=\"base\"):\n",
                                    "    \"\"\"\n",
                                    "    Thoroughly enforce model reliance on 'retrieved_context' when answering 'query.'\n",
                                    "    \n",
                                    "    Now includes context-length validation and smart truncation if the context \n",
                                    "    exceeds a rough limit of 4096 tokens (approx. word-based).\n",
                                    "\n",
                                    "    The 'strategy' parameter allows for different prompt template variations:\n",
                                    "      1) Base approach: Provide context, instruct LLM not to use outside info.\n",
                                    "      2) Strict approach: Provide context with explicit disclaimers if the answer is not found.\n",
                                    "      3) Citation approach: Provide context, then request the LLM to cite the relevant lines.\n",
                                    "\n",
                                    "    Robust fallback:\n",
                                    "      - If 'retrieved_context' is empty, respond with an apology or neutral statement.\n",
                                    "      - If 'retrieved_context' is too long, it is truncated while preserving whole sentences,\n",
                                    "        and \"[Context truncated]\" is appended to the answer to warn the user.\n",
                                    "    \"\"\"\n",
                                    "    # Provide a safe fallback if no context is retrieved\n",
                                    "    if not retrieved_context.strip():\n",
                                    "        return (\"I'm sorry, but I couldn't find any relevant information.\", \"No context used.\")\n",
                                    "\n",
                                    "    # TODO: Define the maximum token limit for the context\n",
                                    "    MAX_TOKENS = None\n",
                                    "\n",
                                    "    # TODO: Implement context length validation and smart truncation\n",
                                    "    # Check if context exceeds token limit and truncate while preserving whole sentences\n",
                                    "    # Hint: Split into words first to check length, then into sentences for truncation\n",
                                    "    words = retrieved_context.split()\n",
                                    "    truncated = False\n",
                                    "\n",
                                    "    # Build the prompt based on strategy\n",
                                    "    if strategy == \"base\":\n",
                                    "        prompt = (\n",
                                    "            \"Use the following context to answer the question in a concise manner.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"strict\":\n",
                                    "        prompt = (\n",
                                    "            \"You must ONLY use the context provided below. If you cannot find the answer in the context, say: 'No sufficient data'.\\n\"\n",
                                    "            \"Do not provide any information not found in the context.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"cite\":\n",
                                    "        prompt = (\n",
                                    "            \"Answer strictly from the provided context, and list the lines you used as evidence with 'Cited lines:'.\\n\"\n",
                                    "            \"If the context does not contain the information, respond with: 'Not available in the retrieved texts.'\\n\\n\"\n",
                                    "            f\"Provided context (label lines as needed):\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "\n",
                                    "    print(f\"Prompt: \\n {prompt}\\n\")\n",
                                    "\n",
                                    "    # Query the language model\n",
                                    "    response = get_llm_response(prompt)\n",
                                    "\n",
                                    "    # Attempt to parse out 'Cited lines:' if present\n",
                                    "    segments = response.split(\"Cited lines:\")\n",
                                    "    if len(segments) == 2:\n",
                                    "        answer_part, used_context_part = segments\n",
                                    "        final_answer = answer_part.strip()\n",
                                    "        cited_part = used_context_part.strip()\n",
                                    "    else:\n",
                                    "        final_answer = response.strip()\n",
                                    "        cited_part = \"No explicit lines cited.\"\n",
                                    "\n",
                                    "    # TODO: Add truncation warning to the answer if context was truncated\n",
                                    "\n",
                                    "    return final_answer, cited_part\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Demonstration of retrieval and constrained generation\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\")\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"corpus_collection\")\n",
                                    "\n",
                                    "    # Example query that might yield relevant or no results\n",
                                    "    query = \"Highlight the main policies that apply to employees.\"\n",
                                    "    retrieval_results = collection.query(query_texts=[query], n_results=2)\n",
                                    "\n",
                                    "    if not retrieval_results['documents'][0]:\n",
                                    "        retrieved_context = \"\"\n",
                                    "    else:\n",
                                    "        retrieved_context = \"\\n\".join([\"- \" + doc_text for doc_text in retrieval_results['documents'][0]])\n",
                                    "\n",
                                    "    for strategy_option in (\"base\", \"strict\", \"cite\"):\n",
                                    "        answer, used_context = generate_with_constraints(query, retrieved_context, strategy=strategy_option)\n",
                                    "        print(f\"Strategy: {strategy_option}\")\n",
                                    "        print(f\"Constrained generation answer:\\n{answer}\")\n",
                                    "        print(f\"Context or lines used:\\n{used_context}\\n\")\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "def generate_with_constraints(query, retrieved_context, strategy=\"base\"):    \"\"\"\n",
                                    "    Thoroughly enforce model reliance on 'retrieved_context' when answering 'query.'\n",
                                    "\n",
                                    "    Now includes context-length validation and smart truncation if the context \n",
                                    "    exceeds a rough limit of 4096 tokens (approx. word-based).\n",
                                    "\n",
                                    "    The 'strategy' parameter allows for different prompt template variations:\n",
                                    "      1) Base approach: Provide context, instruct LLM not to use outside info.\n",
                                    "      2) Strict approach: Provide context with explicit disclaimers if the answer is not found.\n",
                                    "      3) Citation approach: Provide context, then request the LLM to cite the relevant lines.\n",
                                    "\n",
                                    "    Robust fallback:\n",
                                    "      - If 'retrieved_context' is empty, respond with an apology or neutral statement.\n",
                                    "      - If 'retrieved_context' is too long, it is truncated while preserving whole sentences,\n",
                                    "        and \"[Context truncated]\" is appended to the answer to warn the user.\n",
                                    "    \"\"\"\n",
                                    "    # Safe fallback if no context\n",
                                    "    if not retrieved_context.strip():\n",
                                    "        return (\"I'm sorry, but I couldn't find any relevant information.\", \"No context used.\")\n",
                                    "\n",
                                    "    # Define the maximum token limit (approximate word-based)\n",
                                    "    MAX_TOKENS = 4096\n",
                                    "    words = retrieved_context.split()\n",
                                    "    truncated = False\n",
                                    "\n",
                                    "    # Smart truncation: preserve whole sentences\n",
                                    "    if len(words) > MAX_TOKENS:\n",
                                    "        sentences = re.split(r'(?<=[.!?])\\s+', retrieved_context)\n",
                                    "        selected_sentences = []\n",
                                    "        word_count = 0\n",
                                    "        for sent in sentences:\n",
                                    "            sent_words = sent.split()\n",
                                    "            if word_count + len(sent_words) <= MAX_TOKENS:\n",
                                    "                selected_sentences.append(sent)\n",
                                    "                word_count += len(sent_words)\n",
                                    "            else:\n",
                                    "                break\n",
                                    "        retrieved_context = ' '.join(selected_sentences)\n",
                                    "        truncated = True\n",
                                    "\n",
                                    "    # Build the prompt based on strategy\n",
                                    "    if strategy == \"base\":\n",
                                    "        prompt = (\n",
                                    "            \"Use the following context to answer the question in a concise manner.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"strict\":\n",
                                    "        prompt = (\n",
                                    "            \"You must ONLY use the context provided below. If you cannot find the answer in the context, say: 'No sufficient data'.\\n\"\n",
                                    "            \"Do not provide any information not found in the context.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    elif strategy == \"cite\":\n",
                                    "        prompt = (\n",
                                    "            \"Answer strictly from the provided context, and list the lines you used as evidence with 'Cited lines:'.\\n\"\n",
                                    "            \"If the context does not contain the information, respond with: 'Not available in the retrieved texts.'\\n\\n\"\n",
                                    "            f\"Provided context (label lines as needed):\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "    else:\n",
                                    "        # Fallback to base if unknown strategy\n",
                                    "        prompt = (\n",
                                    "            \"Use the following context to answer the question in a concise manner.\\n\\n\"\n",
                                    "            f\"Context:\\n{retrieved_context}\\n\"\n",
                                    "            f\"Question: '{query}'\\n\"\n",
                                    "            \"Answer:\"\n",
                                    "        )\n",
                                    "\n",
                                    "    print(f\"Prompt: \\n {prompt}\\n\")\n",
                                    "\n",
                                    "    # Query the language model\n",
                                    "    response = get_llm_response(prompt)\n",
                                    "\n",
                                    "    # Parse out 'Cited lines:' if present\n",
                                    "    segments = response.split(\"Cited lines:\")\n",
                                    "    if len(segments) == 2:\n",
                                    "        answer_part, used_context_part = segments\n",
                                    "        final_answer = answer_part.strip()\n",
                                    "        cited_part = used_context_part.strip()\n",
                                    "    else:\n",
                                    "        final_answer = response.strip()\n",
                                    "        cited_part = \"No explicit lines cited.\"\n",
                                    "\n",
                                    "    # Append truncation warning if needed\n",
                                    "    if truncated:\n",
                                    "        final_answer += \" [Context truncated]\"\n",
                                    "\n",
                                    "    return final_answer, cited_part\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Demonstration of retrieval and constrained generation\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\")\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"corpus_collection\")\n",
                                    "\n",
                                    "    query = \"Highlight the main policies that apply to employees.\"\n",
                                    "    retrieval_results = collection.query(query_texts=[query], n_results=2)\n",
                                    "\n",
                                    "    if not retrieval_results['documents'][0]:\n",
                                    "        retrieved_context = \"\"\n",
                                    "    else:\n",
                                    "        retrieved_context = \"\\n\".join([\"- \" + doc_text for doc_text in retrieval_results['documents'][0]])\n",
                                    "\n",
                                    "    for strategy_option in (\"base\", \"strict\", \"cite\"):\n",
                                    "        answer, used_context = generate_with_constraints(query, retrieved_context, strategy=strategy_option)\n",
                                    "        print(f\"Strategy: {strategy_option}\")\n",
                                    "        print(f\"Constrained generation answer:\\n{answer}\")\n",
                                    "        print(f\"Context or lines used:\\n{used_context}\\n\")\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "I’ve implemented smart truncation to preserve whole sentences when the context exceeds 4,096 tokens and appended a “[Context truncated]” warning to the final answer. Let me know if you’d like any tweaks or further enhancements!"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
