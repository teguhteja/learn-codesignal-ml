{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 4: Combining Lexical and Embedding-Based Retrieval in RAG Systems\n",
                                    "\n",
                                    "\n",
                                    "We are now in the fourth and final lesson of this course on **Beyond Basic RAG: Improving Our Pipeline!** Up to this point, we have explored ways to enhance Retrieval-Augmented Generation (RAG) systems by refining chunking strategies and leveraging advanced retrieval methods. In this lesson, you will learn how to merge a lexical-based retrieval approach (using Okapi BM25) with your existing embedding-based retrieval mechanism, creating a powerful **hybrid retrieval** pipeline.\n",
                                    "\n",
                                    "By the end of this lesson, you should be able to:\n",
                                    "\n",
                                    "- Grasp the intuition behind Okapi BM25 for lexical retrieval.  \n",
                                    "- Construct a BM25 index on your corpus.  \n",
                                    "- Combine BM25 scores with embedding-based retrieval scores using a configurable weight parameter, **alpha**.  \n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Understanding the Okapi BM25 Algorithm  \n",
                                    "Within the category of lexical-based search methods, **Okapi BM25** is a popular choice. It focuses on the presence of specific keywords, rewarding relevant chunks that contain more occurrences of the query terms. At the same time, it avoids overemphasizing repeated words by incorporating a saturation effect.\n",
                                    "\n",
                                    "**A few core ideas behind BM25:**\n",
                                    "- **Term Frequency (TF):** More keyword matches in a chunk can signal higher relevance.  \n",
                                    "- **Document Length Normalization:** BM25 accounts for chunk length, ensuring that very long chunks with many repeated words are not unfairly scored.  \n",
                                    "- Although the underlying formula has several parameters and normalizations, the general purpose is straightforward: **favor chunks containing the search terms, but don’t let them dominate purely by repeating keywords.**  \n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Building a BM25 Index  \n",
                                    "Here is a simple function that builds a BM25 index from your chunked corpus. We assume you already have a collection of text chunks ready.\n",
                                    "\n",
                                    "```python\n",
                                    "from rank_bm25 import BM25Okapi\n",
                                    "\n",
                                    "def build_bm25_index(chunks):\n",
                                    "    \"\"\"\n",
                                    "    Build a BM25Okapi index from the chunk texts for lexical-based retrieval.\n",
                                    "    BM25 scores typically range in magnitude depending on the corpus.\n",
                                    "    \"\"\"\n",
                                    "    # Convert each chunk's text into a list of lowercased tokens\n",
                                    "    corpus = [c[\"text\"].lower().split() for c in chunks]\n",
                                    "    return BM25Okapi(corpus)\n",
                                    "````\n",
                                    "\n",
                                    "In this snippet:\n",
                                    "\n",
                                    "1. We lowercase and split each chunk’s text into tokens.\n",
                                    "2. We pass the tokenized corpus into `BM25Okapi` to create our lexical index.\n",
                                    "\n",
                                    "Later, we’ll score new queries on this index to get relevance.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Merging BM25 and Embedding-Based Retrieval\n",
                                    "\n",
                                    "### BM25 Scoring & Similarity Calculation\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "\n",
                                    "def hybrid_retrieval(query, chunks, bm25, collection, top_k=3, alpha=0.5):\n",
                                    "    \"\"\"\n",
                                    "    Merge BM25 and embedding-based results:\n",
                                    "      1) Compute BM25 scores for each chunk.\n",
                                    "      2) Get embedding distances and convert to similarities.\n",
                                    "      3) Normalize both BM25 and embeddings to [0,1].\n",
                                    "      4) Combine scores using final_score = alpha * bm25_norm + (1-alpha) * embed_sim.\n",
                                    "      5) Sort by final score descending, and return the top_k results.\n",
                                    "    \"\"\"\n",
                                    "    # Tokenize the query for BM25\n",
                                    "    tokenized_query = query.lower().split()\n",
                                    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
                                    "\n",
                                    "    # Query the embedding-based store for candidate chunks\n",
                                    "    embed_results = collection.query(\n",
                                    "        query_texts=[query],\n",
                                    "        n_results=min(top_k * 5, len(chunks))\n",
                                    "    )\n",
                                    "\n",
                                    "    # Convert distances to similarities\n",
                                    "    embed_scores_dict = {}\n",
                                    "    for i in range(len(embed_results['documents'][0])):\n",
                                    "        idx = embed_results['ids'][0][i]\n",
                                    "        distance = embed_results['distances'][0][i]\n",
                                    "        similarity = 1 / (1 + distance)\n",
                                    "        embed_scores_dict[idx] = similarity\n",
                                    "```\n",
                                    "\n",
                                    "* **Tokenization & BM25:** We lowercase and split the query, then call `bm25.get_scores()`.\n",
                                    "* **Embedding Retrieval:** We fetch the top candidates via our embedding store.\n",
                                    "* **Distance → Similarity:** We use `1 / (1 + distance)` to turn distances into similarity scores.\n",
                                    "\n",
                                    "### Normalizing Scores & Final Ranking\n",
                                    "\n",
                                    "```python\n",
                                    "    # Merge scores\n",
                                    "    merged = []\n",
                                    "    bm25_min, bm25_max = (\n",
                                    "        (min(bm25_scores), max(bm25_scores))\n",
                                    "        if bm25_scores.size > 0 else (0, 1)\n",
                                    "    )\n",
                                    "    for i, chunk in enumerate(chunks):\n",
                                    "        bm25_raw = bm25_scores[i]\n",
                                    "        bm25_norm = (\n",
                                    "            (bm25_raw - bm25_min) / (bm25_max - bm25_min)\n",
                                    "            if bm25_max != bm25_min else 0.0\n",
                                    "        )\n",
                                    "        embed_sim = embed_scores_dict.get(i, 0.0)\n",
                                    "        final_score = alpha * bm25_norm + (1 - alpha) * embed_sim\n",
                                    "        merged.append((i, final_score))\n",
                                    "\n",
                                    "    # Sort by combined score, highest first\n",
                                    "    merged.sort(key=lambda x: x[1], reverse=True)\n",
                                    "    top_results = merged[:top_k]\n",
                                    "    return [(idx, chunks[idx], score) for (idx, score) in top_results]\n",
                                    "```\n",
                                    "\n",
                                    "Here’s what happens:\n",
                                    "\n",
                                    "1. **Find min/max BM25 scores** for normalization.\n",
                                    "2. **Normalize** each BM25 score to the `[0,1]` range.\n",
                                    "3. **Combine** normalized BM25 (`bm25_norm`) and embedding similarity (`embed_sim`) via the weight `alpha`.\n",
                                    "4. **Sort** by the final combined score and return the top k chunks.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Putting It All Together\n",
                                    "\n",
                                    "```python\n",
                                    "# Build corpus chunks, BM25 index, and embedding-based store\n",
                                    "chunked_docs = load_and_chunk_corpus(..., 40)\n",
                                    "bm25_index   = build_bm25_index(chunked_docs)\n",
                                    "collection   = build_chroma_collection(chunked_docs)\n",
                                    "\n",
                                    "# Perform hybrid retrieval\n",
                                    "query   = \"What do our internal company policies state?\"\n",
                                    "results = hybrid_retrieval(\n",
                                    "    query, chunked_docs, bm25_index, collection,\n",
                                    "    top_k=3, alpha=0.6\n",
                                    ")\n",
                                    "\n",
                                    "# Inspect the results\n",
                                    "if not results:\n",
                                    "    print(\"No chunks found. You may want to provide a generic response.\")\n",
                                    "else:\n",
                                    "    for chunk_idx, chunk_data, final_score in results:\n",
                                    "        print(f\"Chunk {chunk_idx} | Score: {final_score:.4f}\")\n",
                                    "        print(\"Text:\", chunk_data['text'])\n",
                                    "        print(\"-----\")\n",
                                    "```\n",
                                    "\n",
                                    "In this example we:\n",
                                    "\n",
                                    "* **Load** and **chunk** our document set.\n",
                                    "* **Build** both the BM25 index and the embedding-based collection.\n",
                                    "* **Run** `hybrid_retrieval` with a chosen `alpha`.\n",
                                    "* **Display** the top results by their combined scores.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Choosing the Alpha Parameter\n",
                                    "\n",
                                    "The **alpha** parameter determines the balance between lexical and semantic retrieval:\n",
                                    "\n",
                                    "* **Higher Alpha** (e.g., 0.7 or 0.8):\n",
                                    "  Prioritize exact keyword matches. Ideal for legal or technical documents where precise terminology matters.\n",
                                    "\n",
                                    "* **Lower Alpha** (e.g., 0.3 or 0.2):\n",
                                    "  Emphasize semantic understanding. Best for creative writing or conversational queries where context is key.\n",
                                    "\n",
                                    "* **Balanced Alpha** (e.g., 0.5):\n",
                                    "  Give equal weight to lexical precision and semantic context—a good starting point.\n",
                                    "\n",
                                    "> *Tip:* Experiment with different alpha values to see how they affect retrieval quality in your specific use case.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Conclusion and Next Steps\n",
                                    "\n",
                                    "In this lesson, you explored how to enhance retrieval accuracy by combining Okapi BM25 with embedding-based methods. This hybrid approach ensures you capture both exact keyword matches and semantic relevance, reducing the chance of missing important chunks due to vocabulary differences.\n",
                                    "\n",
                                    "**Next steps:**\n",
                                    "\n",
                                    "* Test your hybrid pipeline with various queries.\n",
                                    "* Adjust chunk sizes, scoring thresholds, and the alpha parameter.\n",
                                    "* Observe and measure retrieval improvements in your downstream RAG tasks.\n",
                                    "\n",
                                    "Happy experimenting!\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Enhance Hybrid Retrieval Function\n",
                                    "\n",
                                    "\n",
                                    "In the previous exercise, you successfully combined lexical and embedding-based retrieval methods. Now, let’s enhance the `hybrid_retrieval` function to make it even more powerful.\n",
                                    "\n",
                                    "## Your Objective\n",
                                    "\n",
                                    "1. **Locate the placeholder** in the `hybrid_retrieval` function where the conversion should take place.  \n",
                                    "2. **Apply the formula**  \n",
                                    "   ```python\n",
                                    "   similarity = 1 / (1 + distance)\n",
                                    "````\n",
                                    "\n",
                                    "to calculate the similarity score from each embedding distance.\n",
                                    "3\\. **Store the result** correctly for each chunk in the `embed_scores_dict`.\n",
                                    "\n",
                                    "By completing this, you’ll deepen your understanding of hybrid retrieval systems. Remember, the goal is to balance the precision of lexical matches with the contextual understanding of embeddings.\n",
                                    "\n",
                                    "Enjoy the process, and happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from rank_bm25 import BM25Okapi\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "\n",
                                    "\n",
                                    "def build_bm25_index(chunks):\n",
                                    "    \"\"\"\n",
                                    "    Build a BM25Okapi index from the chunk texts for lexical-based retrieval.\n",
                                    "    Note BM25 scores often range roughly between 0 and 10 (depending on corpus).\n",
                                    "    \"\"\"\n",
                                    "    corpus = [c[\"text\"].lower().split() for c in chunks]\n",
                                    "    return BM25Okapi(corpus)\n",
                                    "\n",
                                    "\n",
                                    "def hybrid_retrieval(query, chunks, bm25, collection, top_k=3, alpha=0.5):\n",
                                    "    \"\"\"\n",
                                    "    Merge BM25 and embedding-based results.\n",
                                    "    Steps:\n",
                                    "      1) Compute BM25 scores for each chunk. (Higher = better)\n",
                                    "      2) Get embedding distances from ChromaDB for a candidate set.\n",
                                    "      3) Convert distances to similarity (e.g., similarity ~ 1/(1+distance)).\n",
                                    "      4) Normalize both BM25 and similarity to [0,1] and combine with weighting:\n",
                                    "         final_score = alpha * BM25_normalized + (1-alpha) * embedding_similarity\n",
                                    "      5) Sort by final score in descending order.\n",
                                    "\n",
                                    "    'alpha' controls how much weight lexical vs. embedding-based similarity gets.\n",
                                    "    In practice, you might do cross-validation or user acceptance testing to find a good alpha.\n",
                                    "    \"\"\"\n",
                                    "    tokenized_query = query.lower().split()\n",
                                    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
                                    "    bm25_min, bm25_max = (min(bm25_scores), max(bm25_scores)) if bm25_scores.size > 0 else (0, 1)\n",
                                    "\n",
                                    "    embed_results = collection.query(query_texts=[query], n_results=min(top_k*5, len(chunks)))\n",
                                    "    embed_scores_dict = {}\n",
                                    "    for i in range(len(embed_results['documents'][0])):\n",
                                    "        idx = embed_results['ids'][0][i]\n",
                                    "        distance = embed_results['distances'][0][i]\n",
                                    "        # TODO: Convert distance to similarity\n",
                                    "        similarity = _____\n",
                                    "        embed_scores_dict[idx] = similarity\n",
                                    "\n",
                                    "    merged = []\n",
                                    "    for i, chunk in enumerate(chunks):\n",
                                    "        bm25_raw = bm25_scores[i]\n",
                                    "        if bm25_max != bm25_min:\n",
                                    "            bm25_norm = (bm25_raw - bm25_min) / (bm25_max - bm25_min)\n",
                                    "        else:\n",
                                    "            bm25_norm = 0.0\n",
                                    "\n",
                                    "        embed_sim = embed_scores_dict.get(i, 0.0)\n",
                                    "        final_score = alpha * bm25_norm + (1 - alpha) * embed_sim\n",
                                    "        merged.append((i, final_score))\n",
                                    "\n",
                                    "    merged.sort(key=lambda x: x[1], reverse=True)\n",
                                    "    top_results = merged[:top_k]\n",
                                    "\n",
                                    "    print(f\"Top results by combined BM25 + embeddings for query: '{query}'\")\n",
                                    "    for idx, score in top_results:\n",
                                    "        print(f\"Chunk: '{chunks[idx]['text'][:50]}...' | Score: {score:.4f}\")\n",
                                    "    return [(idx, chunks[idx], score) for (idx, score) in top_results]\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\", 40)\n",
                                    "    bm25_index = build_bm25_index(chunked_docs)\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"hybrid_collection\")\n",
                                    "\n",
                                    "    query = \"What do our internal company policies state?\"\n",
                                    "    results = hybrid_retrieval(query, chunked_docs, bm25_index, collection, top_k=3, alpha=0.6)\n",
                                    "    if not results:\n",
                                    "        print(\"No chunks found. Fallback to a naive or apology answer.\")\n",
                                    "    else:\n",
                                    "        for r in results:\n",
                                    "            chunk_idx, chunk_data, final_score = r\n",
                                    "            print(f\"Chunk ID: {chunk_idx}, Score: {final_score:.4f}, Text: {chunk_data['text']}\")\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Fix BM25 Normalization Bug\n",
                                    "\n",
                                    "\n",
                                    "In the previous exercise, you successfully combined lexical and embedding-based retrieval methods. Now, let’s enhance the `hybrid_retrieval` function to make it even more powerful.\n",
                                    "\n",
                                    "1. **Locate the placeholder** in the `hybrid_retrieval` function where the conversion should take place.\n",
                                    "2. **Apply the formula**\n",
                                    "\n",
                                    "   ```python\n",
                                    "   similarity = 1 / (1 + distance)\n",
                                    "   ```\n",
                                    "\n",
                                    "to calculate the similarity score from each embedding distance.\n",
                                    "3. **Store the result** correctly for each chunk in the `embed_scores_dict`.\n",
                                    "\n",
                                    "By completing this, you’ll deepen your understanding of hybrid retrieval systems. Remember, the goal is to balance the precision of lexical matches with the contextual understanding of embeddings.\n",
                                    "\n",
                                    "Enjoy the process, and happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from rank_bm25 import BM25Okapi\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "\n",
                                    "\n",
                                    "def build_bm25_index(chunks):\n",
                                    "    \"\"\"\n",
                                    "    Build a BM25Okapi index from the chunk texts for lexical-based retrieval.\n",
                                    "    Note BM25 scores often range roughly between 0 and 10 (depending on corpus).\n",
                                    "    \"\"\"\n",
                                    "    corpus = [c[\"text\"].lower().split() for c in chunks]\n",
                                    "    return BM25Okapi(corpus)\n",
                                    "\n",
                                    "\n",
                                    "def hybrid_retrieval(query, chunks, bm25, collection, top_k=3, alpha=0.5):\n",
                                    "    \"\"\"\n",
                                    "    Merge BM25 and embedding-based results.\n",
                                    "    Steps:\n",
                                    "      1) Compute BM25 scores for each chunk. (Higher = better)\n",
                                    "      2) Get embedding distances from ChromaDB for a candidate set.\n",
                                    "      3) Convert distances to similarity (e.g., similarity ~ 1/(1+distance)).\n",
                                    "      4) Normalize both BM25 and similarity to [0,1] and combine with weighting:\n",
                                    "         final_score = alpha * BM25_normalized + (1-alpha) * embedding_similarity\n",
                                    "      5) Sort by final score in descending order.\n",
                                    "\n",
                                    "    'alpha' controls how much weight lexical vs. embedding-based similarity gets.\n",
                                    "    In practice, you might do cross-validation or user acceptance testing to find a good alpha.\n",
                                    "    \"\"\"\n",
                                    "    tokenized_query = query.lower().split()\n",
                                    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
                                    "    bm25_min, bm25_max = (min(bm25_scores), max(bm25_scores)) if bm25_scores.size > 0 else (0, 1)\n",
                                    "\n",
                                    "    embed_results = collection.query(query_texts=[query], n_results=min(top_k*5, len(chunks)))\n",
                                    "    embed_scores_dict = {}\n",
                                    "    for i in range(len(embed_results['documents'][0])):\n",
                                    "        idx = embed_results['ids'][0][i]\n",
                                    "        distance = embed_results['distances'][0][i]\n",
                                    "        # TODO: Convert distance to similarity\n",
                                    "        similarity = _____\n",
                                    "        embed_scores_dict[idx] = similarity\n",
                                    "\n",
                                    "    merged = []\n",
                                    "    for i, chunk in enumerate(chunks):\n",
                                    "        bm25_raw = bm25_scores[i]\n",
                                    "        if bm25_max != bm25_min:\n",
                                    "            bm25_norm = (bm25_raw - bm25_min) / (bm25_max - bm25_min)\n",
                                    "        else:\n",
                                    "            bm25_norm = 0.0\n",
                                    "\n",
                                    "        embed_sim = embed_scores_dict.get(i, 0.0)\n",
                                    "        final_score = alpha * bm25_norm + (1 - alpha) * embed_sim\n",
                                    "        merged.append((i, final_score))\n",
                                    "\n",
                                    "    merged.sort(key=lambda x: x[1], reverse=True)\n",
                                    "    top_results = merged[:top_k]\n",
                                    "\n",
                                    "    print(f\"Top results by combined BM25 + embeddings for query: '{query}'\")\n",
                                    "    for idx, score in top_results:\n",
                                    "        print(f\"Chunk: '{chunks[idx]['text'][:50]}...' | Score: {score:.4f}\")\n",
                                    "    return [(idx, chunks[idx], score) for (idx, score) in top_results]\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\", 40)\n",
                                    "    bm25_index = build_bm25_index(chunked_docs)\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"hybrid_collection\")\n",
                                    "\n",
                                    "    query = \"What do our internal company policies state?\"\n",
                                    "    results = hybrid_retrieval(query, chunked_docs, bm25_index, collection, top_k=3, alpha=0.6)\n",
                                    "    if not results:\n",
                                    "        print(\"No chunks found. Fallback to a naive or apology answer.\")\n",
                                    "    else:\n",
                                    "        for r in results:\n",
                                    "            chunk_idx, chunk_data, final_score = r\n",
                                    "            print(f\"Chunk ID: {chunk_idx}, Score: {final_score:.4f}, Text: {chunk_data['text']}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Refine Your Retrieval System\n",
                                    "\n",
                                    "Congratulations on successfully integrating lexical and embedding-based retrieval methods in the previous exercise! Now, let's enhance the accuracy of our hybrid retrieval system by focusing on the BM25 normalization process.\n",
                                    "\n",
                                    "In the hybrid_retrieval function, there's a bug that needs to be fixed. Dive in and make your retrieval system even more robust!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from rank_bm25 import BM25Okapi\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "\n",
                                    "\n",
                                    "def build_bm25_index(chunks):\n",
                                    "    \"\"\"\n",
                                    "    Build a BM25Okapi index from the chunk texts for lexical-based retrieval.\n",
                                    "    Note BM25 scores often range roughly between 0 and 10 (depending on corpus).\n",
                                    "    \"\"\"\n",
                                    "    corpus = [c[\"text\"].lower().split() for c in chunks]\n",
                                    "    return BM25Okapi(corpus)\n",
                                    "\n",
                                    "\n",
                                    "def hybrid_retrieval(query, chunks, bm25, collection, top_k=3, alpha=0.5):\n",
                                    "    \"\"\"\n",
                                    "    Merge BM25 and embedding-based results.\n",
                                    "    Steps:\n",
                                    "      1) Compute BM25 scores for each chunk. (Higher = better)\n",
                                    "      2) Get embedding distances from ChromaDB for a candidate set.\n",
                                    "      3) Convert distances to similarity (e.g., similarity ~ 1/(1+distance)).\n",
                                    "      4) Normalize both BM25 and similarity to [0,1] and combine with weighting:\n",
                                    "         final_score = alpha * BM25_normalized + (1-alpha) * embedding_similarity\n",
                                    "      5) Sort by final score in descending order.\n",
                                    "\n",
                                    "    'alpha' controls how much weight lexical vs. embedding-based similarity gets.\n",
                                    "    In practice, you might do cross-validation or user acceptance testing to find a good alpha.\n",
                                    "    \"\"\"\n",
                                    "    tokenized_query = query.lower().split()\n",
                                    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
                                    "    bm25_min, bm25_max = (min(bm25_scores), max(bm25_scores)) if bm25_scores.size > 0 else (0, 1)\n",
                                    "\n",
                                    "    embed_results = collection.query(query_texts=[query], n_results=min(top_k * 5, len(chunks)))\n",
                                    "    embed_scores_dict = {}\n",
                                    "    for i in range(len(embed_results['documents'][0])):\n",
                                    "        idx = embed_results['ids'][0][i]\n",
                                    "        distance = embed_results['distances'][0][i]\n",
                                    "        similarity = 1 / (1 + distance)\n",
                                    "        embed_scores_dict[idx] = similarity\n",
                                    "\n",
                                    "    merged = []\n",
                                    "    for i, chunk in enumerate(chunks):\n",
                                    "        bm25_raw = bm25_scores[i]\n",
                                    "        if bm25_max != bm25_min:\n",
                                    "            bm25_norm = (bm25_raw - bm25_min) / (bm25_max - bm25_min)\n",
                                    "        else:\n",
                                    "            bm25_norm = 1.0 \n",
                                    "\n",
                                    "        embed_sim = embed_scores_dict.get(i, 0.0)\n",
                                    "        final_score = alpha * bm25_norm + (1 - alpha) * embed_sim\n",
                                    "        merged.append((i, final_score))\n",
                                    "\n",
                                    "    merged.sort(key=lambda x: x[1], reverse=True)\n",
                                    "    top_results = merged[:top_k]\n",
                                    "\n",
                                    "    print(f\"Top results by combined BM25 + embeddings for query: '{query}'\")\n",
                                    "    for idx, score in top_results:\n",
                                    "        print(f\"Chunk: '{chunks[idx]['text'][:50]}...' | Score: {score:.4f}\")\n",
                                    "    return [(idx, chunks[idx], score) for (idx, score) in top_results]\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\", 40)\n",
                                    "    bm25_index = build_bm25_index(chunked_docs)\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"hybrid_collection\")\n",
                                    "\n",
                                    "    query = \"What do our internal company policies state?\"\n",
                                    "    results = hybrid_retrieval(query, chunked_docs, bm25_index, collection, top_k=3, alpha=0.6)\n",
                                    "    if not results:\n",
                                    "        print(\"No chunks found. Fallback to a naive or apology answer.\")\n",
                                    "    else:\n",
                                    "        for r in results:\n",
                                    "            chunk_idx, chunk_data, final_score = r\n",
                                    "            print(f\"Chunk ID: {chunk_idx}, Score: {final_score:.4f}, Text: {chunk_data['text']}\")\n",
                                    "            \n",
                                    "```\n",
                                    "\n",
                                    "**Fixing BM25 bug**\n",
                                    "\n",
                                    "The user's looking for help to focus on the BM25 normalization process in the hybrid retrieval system. There's a bug in the \"else\" clause where if `bm25_max == bm25_min`, we incorrectly set `bm25_norm` to 1.0, but earlier we had used 0.0 for this case. Looks like I need to revise this part of the code to fix that inconsistency so it correctly handles this special case. Let me get this sorted out!\n",
                                    "\n",
                                    "**Fixing BM25 normalization**\n",
                                    "\n",
                                    "The issue here is the behavior when all BM25 scores are identical, causing `bm25_norm` to be set to 1.0 when ideally, it should be 0.0 (or maybe 0.5). In cases where all scores are equal — meaning no relative difference in relevance — we typically set the normalized score to 0.0. The fix is clear: change the assignment of `bm25_norm` in the `else` clause to 0.0. This will prevent artificially boosting all chunks when there's no variation in score.\n",
                                    "\n",
                                    "\n",
                                    "Thought for 8 seconds\n",
                                    "\n",
                                    "\n",
                                    "````markdown\n",
                                    "## Fixing the BM25 Normalization Bug\n",
                                    "\n",
                                    "In the degenerate case where all BM25 scores are identical (so `bm25_max == bm25_min`), setting the normalized score to `1.0` artificially boosts every chunk equally. Instead, we should fall back to `0.0` so that the embedding-based similarity can drive the ranking when BM25 provides no differentiation.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### Change this:\n",
                                    "\n",
                                    "```python\n",
                                    "    if bm25_max != bm25_min:\n",
                                    "        bm25_norm = (bm25_raw - bm25_min) / (bm25_max - bm25_min)\n",
                                    "    else:\n",
                                    "        bm25_norm = 1.0 \n",
                                    "````\n",
                                    "\n",
                                    "### To this:\n",
                                    "\n",
                                    "```python\n",
                                    "    if bm25_max != bm25_min:\n",
                                    "        bm25_norm = (bm25_raw - bm25_min) / (bm25_max - bm25_min)\n",
                                    "    else:\n",
                                    "        bm25_norm = 0.0\n",
                                    "```\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### Full Updated `hybrid_retrieval` Snippet\n",
                                    "\n",
                                    "```python\n",
                                    "def hybrid_retrieval(query, chunks, bm25, collection, top_k=3, alpha=0.5):\n",
                                    "    tokenized_query = query.lower().split()\n",
                                    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
                                    "    bm25_min, bm25_max = (min(bm25_scores), max(bm25_scores)) if bm25_scores.size > 0 else (0, 1)\n",
                                    "\n",
                                    "    embed_results = collection.query(\n",
                                    "        query_texts=[query],\n",
                                    "        n_results=min(top_k * 5, len(chunks))\n",
                                    "    )\n",
                                    "    embed_scores_dict = {}\n",
                                    "    for i in range(len(embed_results['documents'][0])):\n",
                                    "        idx = embed_results['ids'][0][i]\n",
                                    "        distance = embed_results['distances'][0][i]\n",
                                    "        similarity = 1 / (1 + distance)\n",
                                    "        embed_scores_dict[idx] = similarity\n",
                                    "\n",
                                    "    merged = []\n",
                                    "    for i, chunk in enumerate(chunks):\n",
                                    "        bm25_raw = bm25_scores[i]\n",
                                    "        if bm25_max != bm25_min:\n",
                                    "            bm25_norm = (bm25_raw - bm25_min) / (bm25_max - bm25_min)\n",
                                    "        else:\n",
                                    "            bm25_norm = 0.0  # <-- fixed here\n",
                                    "\n",
                                    "        embed_sim = embed_scores_dict.get(i, 0.0)\n",
                                    "        final_score = alpha * bm25_norm + (1 - alpha) * embed_sim\n",
                                    "        merged.append((i, final_score))\n",
                                    "\n",
                                    "    merged.sort(key=lambda x: x[1], reverse=True)\n",
                                    "    return [(idx, chunks[idx], score) for idx, score in merged[:top_k]]\n",
                                    "```\n",
                                    "\n",
                                    "With this change, when BM25 provides no variation, the system relies fully on embedding similarities rather than inflating every chunk’s lexical score.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "3be011e8",
                           "metadata": {},
                           "source": [
                                    "Congratulations on fixing the BM25 normalization bug in the previous exercise! Now, let's elevate your hybrid retrieval system by refining the scoring process to ensure that only the most relevant chunks are selected.\n",
                                    "\n",
                                    "Your objective is to enhance the hybrid_retrieval function by implementing a filtering mechanism. This involves discarding chunks whose combined score falls below 0.2 before proceeding to the top k selection. By doing so, you ensure that only the most pertinent segments are considered, improving the precision of your retrieval system.\n",
                                    "\n",
                                    "Here's a concise breakdown of what you need to achieve:\n",
                                    "\n",
                                    "Integrate logic within the hybrid_retrieval function to filter out chunks with a final score of less than 0.2.\n",
                                    "Ensure this filtering step occurs before sorting and selecting the top k results.\n",
                                    "By implementing this improvement, you'll enhance the effectiveness of your retrieval system, providing users with the most relevant information. Dive in and make your system even more precise!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "from rank_bm25 import BM25Okapi\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "\n",
                                    "\n",
                                    "def build_bm25_index(chunks):\n",
                                    "    \"\"\"\n",
                                    "    Build a BM25Okapi index from the chunk texts for lexical-based retrieval.\n",
                                    "    Note BM25 scores often range roughly between 0 and 10 (depending on corpus).\n",
                                    "    \"\"\"\n",
                                    "    corpus = [c[\"text\"].lower().split() for c in chunks]\n",
                                    "    return BM25Okapi(corpus)\n",
                                    "\n",
                                    "\n",
                                    "def hybrid_retrieval(query, chunks, bm25, collection, top_k=3, alpha=0.5):\n",
                                    "    \"\"\"\n",
                                    "    Merge BM25 and embedding-based results.\n",
                                    "    Steps:\n",
                                    "      1) Compute BM25 scores for each chunk. (Higher = better)\n",
                                    "      2) Get embedding distances from ChromaDB for a candidate set.\n",
                                    "      3) Convert distances to similarity (e.g., similarity ~ 1/(1+distance)).\n",
                                    "      4) Normalize both BM25 and similarity to [0,1] and combine with weighting:\n",
                                    "         final_score = alpha * BM25_normalized + (1 - alpha) * embedding_similarity\n",
                                    "      5) Discard any chunk with final_score < 0.2\n",
                                    "      6) Sort remaining chunks by final score in descending order.\n",
                                    "      7) Return the top_k results.\n",
                                    "\n",
                                    "    'alpha' controls how much weight lexical vs. embedding-based similarity gets.\n",
                                    "    \"\"\"\n",
                                    "    tokenized_query = query.lower().split()\n",
                                    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
                                    "    bm25_min, bm25_max = (min(bm25_scores), max(bm25_scores)) if bm25_scores.size > 0 else (0, 1)\n",
                                    "\n",
                                    "    embed_results = collection.query(query_texts=[query], n_results=min(top_k * 5, len(chunks)))\n",
                                    "    embed_scores_dict = {}\n",
                                    "    for i in range(len(embed_results['documents'][0])):\n",
                                    "        idx = embed_results['ids'][0][i]\n",
                                    "        distance = embed_results['distances'][0][i]\n",
                                    "        similarity = 1 / (1 + distance)\n",
                                    "        embed_scores_dict[idx] = similarity\n",
                                    "\n",
                                    "    merged = []\n",
                                    "    for i, chunk in enumerate(chunks):\n",
                                    "        bm25_raw = bm25_scores[i]\n",
                                    "        if bm25_max != bm25_min:\n",
                                    "            bm25_norm = (bm25_raw - bm25_min) / (bm25_max - bm25_min)\n",
                                    "        else:\n",
                                    "            bm25_norm = 0.0\n",
                                    "\n",
                                    "        embed_sim = embed_scores_dict.get(i, 0.0)\n",
                                    "        final_score = alpha * bm25_norm + (1 - alpha) * embed_sim\n",
                                    "        merged.append((i, final_score))\n",
                                    "\n",
                                    "    # TODO: Filter out chunks with scores below 0.2 before sorting and selecting top_k results\n",
                                    "    merged.sort(key=lambda x: x[1], reverse=True)\n",
                                    "    top_results = merged[:top_k]\n",
                                    "\n",
                                    "    print(f\"Top results by combined BM25 + embeddings for query: '{query}'\")\n",
                                    "    for idx, score in top_results:\n",
                                    "        print(f\"Chunk: '{chunks[idx]['text'][:50]}...' | Score: {score:.4f}\")\n",
                                    "    return [(idx, chunks[idx], score) for (idx, score) in top_results]\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\", 40)\n",
                                    "    bm25_index = build_bm25_index(chunked_docs)\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"hybrid_collection\")\n",
                                    "\n",
                                    "    query = \"What do our internal company policies state?\"\n",
                                    "    results = hybrid_retrieval(query, chunked_docs, bm25_index, collection, top_k=3, alpha=0.6)\n",
                                    "    if not results:\n",
                                    "        print(\"No chunks found. Fallback to a naive or apology answer.\")\n",
                                    "    else:\n",
                                    "        for r in results:\n",
                                    "            chunk_idx, chunk_data, final_score = r\n",
                                    "            print(f\"Chunk ID: {chunk_idx}, Score: {final_score:.4f}, Text: {chunk_data['text']}\")\n",
                                    "```\n",
                                    "\n",
                                    "You're almost there! To implement the filtering step correctly in your `hybrid_retrieval` function, you'll need to discard chunks with a `final_score < 0.2` **before** sorting and slicing the top results.\n",
                                    "\n",
                                    "Here’s the updated section of the function with the filter applied properly:\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### ✅ Apply the Filter (Update Your Function)\n",
                                    "\n",
                                    "Replace this block:\n",
                                    "\n",
                                    "```python\n",
                                    "    # TODO: Filter out chunks with scores below 0.2 before sorting and selecting top_k results\n",
                                    "    merged.sort(key=lambda x: x[1], reverse=True)\n",
                                    "    top_results = merged[:top_k]\n",
                                    "```\n",
                                    "\n",
                                    "With this updated version:\n",
                                    "\n",
                                    "```python\n",
                                    "    # Filter out chunks with score < 0.2\n",
                                    "    filtered = [(i, score) for (i, score) in merged if score >= 0.2]\n",
                                    "\n",
                                    "    # Sort and select top_k from filtered results\n",
                                    "    filtered.sort(key=lambda x: x[1], reverse=True)\n",
                                    "    top_results = filtered[:top_k]\n",
                                    "```\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### 🔍 Why This Works:\n",
                                    "\n",
                                    "* `filtered`: keeps only relevant chunks.\n",
                                    "* Sorting only happens after low-score chunks are discarded.\n",
                                    "* This ensures `top_k` are *high-quality* results only.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "Let me know if you'd like to enhance this further by adding a fallback (e.g., return summary or a “no good matches” message) when all scores fall below 0.2.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "e42e40ef",
                           "metadata": {},
                           "source": [
                                    "Welcome to the next step in mastering hybrid retrieval systems! You've done an excellent job exploring the combination of lexical and embedding-based retrieval methods. Now, it's time to deepen your understanding by reimplementing the build_bm25_index function from scratch, focusing on lexical-based retrieval using the Okapi BM25 algorithm.\n",
                                    "\n",
                                    "Here is your objective:\n",
                                    "\n",
                                    "Begin by tokenizing the text chunks. Convert them to lowercase and split them into words to ensure your index is case-insensitive and optimized for lexical search.\n",
                                    "Utilize the BM25Okapi class to construct the index from these tokenized chunks.\n",
                                    "Return the resulting BM25 object, which will be essential for scoring queries against your corpus.\n",
                                    "By completing this exercise, you'll gain a deeper appreciation for how lexical retrieval works and how it can be integrated into a hybrid system. Remember, the goal is to create an index that efficiently retrieves relevant chunks based on keyword matches. Enjoy the challenge, and happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from rank_bm25 import BM25Okapi\n",
                                    "from data import load_and_chunk_corpus\n",
                                    "from vector_db import build_chroma_collection\n",
                                    "\n",
                                    "def build_bm25_index(chunks):\n",
                                    "    \"\"\"\n",
                                    "    Build a BM25Okapi index from the chunk texts for lexical-based retrieval.\n",
                                    "    Reimplemented from scratch by lowercasing and splitting each chunk into tokens.\n",
                                    "    \"\"\"\n",
                                    "    # TODO: Initialize an empty list to store tokenized chunks\n",
                                    "    corpus = []\n",
                                    "\n",
                                    "    # TODO: Tokenize each chunk's text by converting to lowercase and splitting into words\n",
                                    "\n",
                                    "    # TODO: Create and return the BM25Okapi index using the tokenized corpus\n",
                                    "\n",
                                    "def hybrid_retrieval(query, chunks, bm25, collection, top_k=3, alpha=0.5):\n",
                                    "    \"\"\"\n",
                                    "    Merge BM25 and embedding-based results.\n",
                                    "    Steps:\n",
                                    "      1) Compute BM25 scores for each chunk. (Higher = better)\n",
                                    "      2) Get embedding distances from ChromaDB for a candidate set.\n",
                                    "      3) Convert distances to similarity (e.g., similarity ~ 1/(1+distance)).\n",
                                    "      4) Normalize both BM25 and similarity to [0,1] and combine with weighting:\n",
                                    "         final_score = alpha * BM25_normalized + (1-alpha) * embedding_similarity\n",
                                    "      5) Sort by final score in descending order.\n",
                                    "\n",
                                    "    'alpha' controls how much weight lexical vs. embedding-based similarity gets.\n",
                                    "    In practice, you might do cross-validation or user acceptance testing to find a good alpha.\n",
                                    "    \"\"\"\n",
                                    "    tokenized_query = query.lower().split()\n",
                                    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
                                    "    bm25_min, bm25_max = (min(bm25_scores), max(bm25_scores)) if bm25_scores.size > 0 else (0, 1)\n",
                                    "\n",
                                    "    embed_results = collection.query(query_texts=[query], n_results=min(top_k*5, len(chunks)))\n",
                                    "    embed_scores_dict = {}\n",
                                    "    for i in range(len(embed_results['documents'][0])):\n",
                                    "        idx = embed_results['ids'][0][i]\n",
                                    "        distance = embed_results['distances'][0][i]\n",
                                    "        similarity = 1 / (1 + distance)\n",
                                    "        embed_scores_dict[idx] = similarity\n",
                                    "\n",
                                    "    merged = []\n",
                                    "    for i, chunk in enumerate(chunks):\n",
                                    "        bm25_raw = bm25_scores[i]\n",
                                    "        if bm25_max != bm25_min:\n",
                                    "            bm25_norm = (bm25_raw - bm25_min) / (bm25_max - bm25_min)\n",
                                    "        else:\n",
                                    "            bm25_norm = 0.0\n",
                                    "\n",
                                    "        embed_sim = embed_scores_dict.get(i, 0.0)\n",
                                    "        final_score = alpha * bm25_norm + (1 - alpha) * embed_sim\n",
                                    "        merged.append((i, final_score))\n",
                                    "\n",
                                    "    merged.sort(key=lambda x: x[1], reverse=True)\n",
                                    "    top_results = merged[:top_k]\n",
                                    "\n",
                                    "    print(f\"Top results by combined BM25 + embeddings for query: '{query}'\")\n",
                                    "    for idx, score in top_results:\n",
                                    "        print(f\"Chunk: '{chunks[idx]['text'][:50]}...' | Score: {score:.4f}\")\n",
                                    "    return [(idx, chunks[idx], score) for (idx, score) in top_results]\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    chunked_docs = load_and_chunk_corpus(\"data/corpus.json\", 40)\n",
                                    "    bm25_index = build_bm25_index(chunked_docs)\n",
                                    "    collection = build_chroma_collection(chunked_docs, collection_name=\"hybrid_collection\")\n",
                                    "\n",
                                    "    query = \"What do our internal company policies state?\"\n",
                                    "    results = hybrid_retrieval(query, chunked_docs, bm25_index, collection, top_k=3, alpha=0.6)\n",
                                    "    if not results:\n",
                                    "        print(\"No chunks found. Fallback to a naive or apology answer.\")\n",
                                    "    else:\n",
                                    "        for chunk_idx, chunk_data, final_score in results:\n",
                                    "            print(f\"Chunk ID: {chunk_idx}, Score: {final_score:.4f}, Text: {chunk_data['text']}\")\n",
                                    "\n",
                                    "\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
