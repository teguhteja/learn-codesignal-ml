{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 3: Retrieving and Prompt Building in RAG Systems\n",
                                    "\n",
                                    "Welcome to the third lesson in our journey through **Scaling Up RAG with Vector Databases**! Well done—you’re halfway through this course. In the previous lesson, you learned how to split (or chunk) your text data and store those chunks in a vector database collection. Now, we’ll delve into retrieving the most relevant chunks for any given query and building an LLM prompt to produce more accurate, context-driven answers.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Retrieving the Most Relevant Chunks\n",
                                    "\n",
                                    "Before your LLM can generate a coherent, context-rich answer, you need to fetch the right information. Your vector database (for instance, using Chroma) will rank which document chunks are most relevant for a given query.\n",
                                    "\n",
                                    "```python\n",
                                    "def retrieve_top_chunks(query, collection, top_k=2):\n",
                                    "    \"\"\"\n",
                                    "    Retrieves the top_k chunks relevant to the given query from 'collection'.\n",
                                    "    Returns a list of retrieved chunks, each containing 'chunk' text,\n",
                                    "    'doc_id', and 'distance'.\n",
                                    "    \"\"\"\n",
                                    "    # Search for top_k results matching the user's query\n",
                                    "    results = collection.query(\n",
                                    "        query_texts=[query],\n",
                                    "        n_results=top_k\n",
                                    "    )\n",
                                    "    \n",
                                    "    retrieved_chunks = []\n",
                                    "    \n",
                                    "    # Safeguard in case no results are found\n",
                                    "    if not results['documents'] or not results['documents'][0]:\n",
                                    "        return retrieved_chunks\n",
                                    "\n",
                                    "    # Gather each retrieved chunk, along with its distance score\n",
                                    "    for i in range(len(results['documents'][0])):\n",
                                    "        retrieved_chunks.append({\n",
                                    "            \"chunk\": results['documents'][0][i],\n",
                                    "            \"doc_id\": results['ids'][0][i],\n",
                                    "            \"distance\": results['distances'][0][i]\n",
                                    "        })\n",
                                    "    return retrieved_chunks\n",
                                    "````\n",
                                    "\n",
                                    "### How It Works\n",
                                    "\n",
                                    "1. **Function Definition**\n",
                                    "\n",
                                    "   * `query`: the user’s question or search term\n",
                                    "   * `collection`: the Chroma collection object containing embedded documents\n",
                                    "   * `top_k`: the number of most relevant chunks to retrieve (default = 2)\n",
                                    "\n",
                                    "2. **Vector Search**\n",
                                    "\n",
                                    "   * `collection.query()` performs a vector-based similarity search.\n",
                                    "   * `query_texts=[query]` passes the user’s query as a list (Chroma API expects a list).\n",
                                    "   * `n_results=top_k` specifies how many matching chunks to return.\n",
                                    "\n",
                                    "3. **Results Structure**\n",
                                    "   The query returns a dictionary with keys:\n",
                                    "\n",
                                    "   * **`documents`**: the actual text chunks\n",
                                    "   * **`ids`**: document identifiers\n",
                                    "   * **`distances`**: similarity scores (lower = better match)\n",
                                    "     Each key maps to a nested list: `[[item1, item2, ...]]`.\n",
                                    "\n",
                                    "4. **Processing Results**\n",
                                    "   For each hit, we build a dict with:\n",
                                    "\n",
                                    "   * `\"chunk\"`: text content\n",
                                    "   * `\"doc_id\"`: its identifier\n",
                                    "   * `\"distance\"`: similarity score\n",
                                    "     These are appended to `retrieved_chunks`, which is then returned.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Building a Prompt for the LLM\n",
                                    "\n",
                                    "Once you have your relevant chunks, construct a prompt so the LLM focuses only on those contexts. This reduces hallucinations and enforces factual accuracy.\n",
                                    "\n",
                                    "```python\n",
                                    "def build_prompt(query, retrieved_chunks):\n",
                                    "    \"\"\"\n",
                                    "    Constructs a prompt by restating the 'query' and adding the retrieved chunks\n",
                                    "    as inline context for the LLM.\n",
                                    "    \"\"\"\n",
                                    "    prompt = f\"Question: {query}\\nAnswer using only the following context:\\n\"\n",
                                    "    for rc in retrieved_chunks:\n",
                                    "        prompt += f\"- {rc['chunk']}\\n\"\n",
                                    "    prompt += \"Answer:\"\n",
                                    "    return prompt\n",
                                    "```\n",
                                    "\n",
                                    "**Why this matters:**\n",
                                    "\n",
                                    "* **Controlled Context**: Instructs the LLM to use only the provided text.\n",
                                    "* **Flexibility**: You can tweak the format (bullet points, instructions, etc.) to guide tone and depth.\n",
                                    "* **Clarity**: Restating the question up front reminds the model of the exact task.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Integrating the Corpus and Creating the Collection\n",
                                    "\n",
                                    "To see this in action, first load your corpus data and create a collection in your vector database.\n",
                                    "\n",
                                    "```python\n",
                                    "# Load corpus data from JSON file\n",
                                    "with open('data/corpus.json', 'r') as f:\n",
                                    "    corpus_data = json.load(f)\n",
                                    "\n",
                                    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "client = Client(Settings())\n",
                                    "collection = client.get_or_create_collection(\n",
                                    "    \"rag_collection\",\n",
                                    "    embedding_function=embed_func\n",
                                    ")\n",
                                    "\n",
                                    "# Batch add documents from the corpus data\n",
                                    "documents = [doc['content'] for doc in corpus_data]\n",
                                    "ids = [f\"chunk_{doc['id']}_0\" for doc in corpus_data]\n",
                                    "collection.add(documents=documents, ids=ids)\n",
                                    "```\n",
                                    "\n",
                                    "**Key Details:**\n",
                                    "\n",
                                    "* **Embedding Function**: Uses `SentenceTransformerEmbeddingFunction`. Replaceable with any model you prefer.\n",
                                    "* **Collection**: `get_or_create_collection` retrieves an existing one or creates a new collection automatically.\n",
                                    "* **Bulk Ingestion**: Adding documents in batches maximizes throughput.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Querying the Database and Generating Answers\n",
                                    "\n",
                                    "With your collection ready, you can retrieve chunks and feed them into your LLM:\n",
                                    "\n",
                                    "```python\n",
                                    "query = \"What are some recent technological breakthroughs?\"\n",
                                    "retrieved_docs = retrieve_top_chunks(query, collection, top_k=5)\n",
                                    "final_prompt = build_prompt(query, retrieved_docs)\n",
                                    "answer = get_llm_response(final_prompt)\n",
                                    "\n",
                                    "print(\"Prompt:\\n\")\n",
                                    "print(final_prompt)\n",
                                    "print(\"\\nLLM Answer:\", answer)\n",
                                    "```\n",
                                    "\n",
                                    "**Step by Step:**\n",
                                    "\n",
                                    "1. **Formulate the Query**: Define the user’s question string.\n",
                                    "2. **Retrieve Chunks**: Call `retrieve_top_chunks()` to get the top 5 semantically similar chunks.\n",
                                    "3. **Construct the Prompt**: Use `build_prompt()` to assemble the question + contexts.\n",
                                    "4. **Get LLM Response**: Pass the prompt into your LLM via `get_llm_response()`.\n",
                                    "\n",
                                    "Printing both the prompt and the answer helps you debug and refine retrieval or prompt formatting.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Examining the Output\n",
                                    "\n",
                                    "Here’s an example of the system’s final output:\n",
                                    "\n",
                                    "```\n",
                                    "Prompt:\n",
                                    "\n",
                                    "Question: What are some recent technological breakthroughs?\n",
                                    "Answer using only the following context:\n",
                                    "- The Industrial Revolution brought significant technological and social changes. It reshaped economies and altered the fabric of society. Scholars examine its impact on labor, innovation, and modern industrial practices.\n",
                                    "- Breakthroughs in renewable energy technologies are reducing global dependence on fossil fuels. Solar and wind systems are becoming more efficient and affordable. These innovations are crucial to combating climate change and ensuring a sustainable future.\n",
                                    "- The digital revolution is transforming how we approach health and wellness. Technological innovations, from fitness trackers to health apps, are empowering individuals to manage their well-being. This integration of technology and lifestyle is reshaping daily habits for a healthier future.\n",
                                    "- Advances in medical technology are revolutionizing patient care through new diagnostic and treatment methods. Breakthroughs in imaging and robotics are enhancing the precision of medical procedures. Healthcare professionals are optimistic about the potential for improved outcomes.\n",
                                    "- Scientists are developing renewable materials that could replace traditional plastics. Innovations in biopolymers are leading to sustainable manufacturing practices. These breakthroughs promise to reduce environmental waste and support a circular economy.\n",
                                    "Answer:\n",
                                    "\n",
                                    "LLM Answer: Recent technological breakthroughs include advancements in renewable energy technologies, which are making solar and wind systems more efficient and affordable, thereby reducing global dependence on fossil fuels and aiding in the fight against climate change. Additionally, the digital revolution is enhancing health and wellness through innovations like fitness trackers and health apps, empowering individuals to better manage their well-being. In the medical field, new diagnostic and treatment methods, along with improvements in imaging and robotics, are revolutionizing patient care and enhancing the precision of medical procedures. Furthermore, scientists are developing renewable materials, such as biopolymers, to replace traditional plastics, promoting sustainable manufacturing practices and supporting a circular economy.\n",
                                    "```\n",
                                    "\n",
                                    "In this snippet:\n",
                                    "\n",
                                    "* The prompt instructs the LLM to use only the listed contexts.\n",
                                    "* The final answer focuses on the most relevant chunks—renewable energy, healthcare tech, and sustainable materials—demonstrating how RAG systems reduce hallucinations and maintain factual alignment.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Conclusion and Next Steps\n",
                                    "\n",
                                    "In this lesson, you learned how to:\n",
                                    "\n",
                                    "1. **Retrieve** the most relevant text chunks via semantic similarity.\n",
                                    "2. **Construct** a focused, context-driven prompt to keep the LLM on track.\n",
                                    "\n",
                                    "These steps are central to building a robust Retrieval-Augmented Generation pipeline. By feeding your LLM with precise context, you improve accuracy and trustworthiness.\n",
                                    "\n",
                                    "**Next**, you’ll get hands-on with exercises to test different queries, tweak prompt formats, and observe how the LLM’s responses change. Keep experimenting—your mastery of RAG systems is well underway!\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Enhance Your Retrieval Function\n",
                                    "\n",
                                    "In the previous lesson, you explored the fundamentals of retrieving relevant chunks from a vector database. Now, it's time to apply that knowledge! Your goal is to enhance the retrieve_top_chunks function to accept a user query string and test it against a ChromaDB collection. This exercise will help you verify that the function accurately returns the top matching chunks based on semantic similarity.\n",
                                    "\n",
                                    "To get started, follow these steps:\n",
                                    "\n",
                                    "Implement the retrieve_top_chunks function by using collection.query() to search for documents matching the query. Ensure the query returns the top k results.\n",
                                    "Process the results and append each chunk's information to retrieved_chunks, including chunk text, doc_id, and distance.\n",
                                    "Define a query string to test the retrieval function.\n",
                                    "Print the retrieved results, including chunk text, doc_id, and distance, to ensure the function's precision.\n",
                                    "For instance, if you query \"technological breakthroughs\", the function should return the most relevant chunks from your collection, each accompanied by its doc_id and distance score. Let's get to work!\n",
                                    "\n",
                                    "```python\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "\n",
                                    "\n",
                                    "def retrieve_top_chunks(query, collection, top_k=3):\n",
                                    "    \"\"\"\n",
                                    "    Retrieves the top_k chunks most relevant to the given query from 'collection'.\n",
                                    "    Returns a list of retrieved chunks, each containing 'chunk' text, 'doc_id', and 'distance'.\n",
                                    "    \"\"\"\n",
                                    "    # TODO: Use collection.query() to search for documents matching the query\n",
                                    "    # The query should return top_k results\n",
                                    "\n",
                                    "    # TODO: Add safeguard for empty results\n",
                                    "\n",
                                    "    # TODO: Process the results and append each chunk's information to retrieved_chunks\n",
                                    "    # Each chunk should have: chunk text, doc_id, and distance score\n",
                                    "    pass\n",
                                    "\n",
                                    "\n",
                                    "def build_prompt(query, retrieved_chunks):\n",
                                    "    \"\"\"\n",
                                    "    Constructs an LLM prompt by combining multiple retrieved chunks into a\n",
                                    "    single context block, ensuring the model can handle longer or more detailed answers.\n",
                                    "    \"\"\"\n",
                                    "    prompt = f\"Question: {query}\\nAnswer using only the following context:\\n\"\n",
                                    "    for rc in retrieved_chunks:\n",
                                    "        prompt += f\"- {rc['chunk']}\\n\"\n",
                                    "    prompt += \"Answer:\"\n",
                                    "    return prompt\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a small set of documents from corpus.json\n",
                                    "    with open('data/corpus.json', 'r') as f:\n",
                                    "        corpus_data = json.load(f)\n",
                                    "\n",
                                    "    # Set up the embedding function and create/get a ChromaDB collection\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\"rag_collection\", embedding_function=embed_func)\n",
                                    "\n",
                                    "    # Add documents from corpus_data to the collection\n",
                                    "    documents = [doc['content'] for doc in corpus_data]\n",
                                    "    ids = [f\"chunk_{doc['id']}_0\" for doc in corpus_data]\n",
                                    "    collection.add(documents=documents, ids=ids)\n",
                                    "\n",
                                    "    # TODO: Define a query string to test the retrieval function\n",
                                    "    user_query = \"\"\n",
                                    "    \n",
                                    "    # Retrieve top matches\n",
                                    "    retrieved_chunks = retrieve_top_chunks(user_query, collection, top_k=3)\n",
                                    "\n",
                                    "    # Print the retrieved chunks to verify the function's accuracy\n",
                                    "    for rc in retrieved_chunks:\n",
                                    "        print(\"Chunk:\", rc[\"chunk\"])\n",
                                    "        print(\"Doc ID:\", rc[\"doc_id\"])\n",
                                    "        print(\"Distance:\", rc[\"distance\"])\n",
                                    "        print(\"-\" * 40)\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Thought for a couple of seconds\n",
                                    "\n",
                                    "```python\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "\n",
                                    "\n",
                                    "def retrieve_top_chunks(query, collection, top_k=3):\n",
                                    "    \"\"\"\n",
                                    "    Retrieves the top_k chunks most relevant to the given query from 'collection'.\n",
                                    "    Returns a list of retrieved chunks, each containing 'chunk' text, 'doc_id', and 'distance'.\n",
                                    "    \"\"\"\n",
                                    "    # Perform vector similarity search\n",
                                    "    results = collection.query(\n",
                                    "        query_texts=[query],\n",
                                    "        n_results=top_k\n",
                                    "    )\n",
                                    "\n",
                                    "    retrieved_chunks = []\n",
                                    "\n",
                                    "    # Check if any documents were returned\n",
                                    "    docs = results.get('documents', [])\n",
                                    "    if not docs or not docs[0]:\n",
                                    "        return retrieved_chunks\n",
                                    "\n",
                                    "    # Iterate through top results and collect metadata\n",
                                    "    for i, chunk_text in enumerate(docs[0]):\n",
                                    "        retrieved_chunks.append({\n",
                                    "            \"chunk\": chunk_text,\n",
                                    "            \"doc_id\": results['ids'][0][i],\n",
                                    "            \"distance\": results['distances'][0][i]\n",
                                    "        })\n",
                                    "    return retrieved_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_prompt(query, retrieved_chunks):\n",
                                    "    \"\"\"\n",
                                    "    Constructs an LLM prompt by combining multiple retrieved chunks into a\n",
                                    "    single context block, ensuring the model can handle longer or more detailed answers.\n",
                                    "    \"\"\"\n",
                                    "    prompt = f\"Question: {query}\\nAnswer using only the following context:\\n\"\n",
                                    "    for rc in retrieved_chunks:\n",
                                    "        prompt += f\"- {rc['chunk']}\\n\"\n",
                                    "    prompt += \"Answer:\"\n",
                                    "    return prompt\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load a small set of documents from corpus.json\n",
                                    "    with open('data/corpus.json', 'r') as f:\n",
                                    "        corpus_data = json.load(f)\n",
                                    "\n",
                                    "    # Setup embedding function and ChromaDB collection\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\n",
                                    "        \"rag_collection\",\n",
                                    "        embedding_function=embed_func\n",
                                    "    )\n",
                                    "\n",
                                    "    # Add documents from corpus_data to the collection\n",
                                    "    documents = [doc['content'] for doc in corpus_data]\n",
                                    "    ids = [f\"chunk_{doc['id']}_0\" for doc in corpus_data]\n",
                                    "    collection.add(documents=documents, ids=ids)\n",
                                    "\n",
                                    "    # Define a query string to test the retrieval function\n",
                                    "    user_query = \"technological breakthroughs\"\n",
                                    "\n",
                                    "    # Retrieve top matches\n",
                                    "    retrieved_chunks = retrieve_top_chunks(user_query, collection, top_k=3)\n",
                                    "\n",
                                    "    # Print the retrieved chunks to verify the function's accuracy\n",
                                    "    for rc in retrieved_chunks:\n",
                                    "        print(\"Chunk:\", rc[\"chunk\"])\n",
                                    "        print(\"Doc ID:\", rc[\"doc_id\"])\n",
                                    "        print(\"Distance:\", rc[\"distance\"])\n",
                                    "        print(\"-\" * 40)\n",
                                    "\n",
                                    "    # Optionally, build and display the LLM prompt\n",
                                    "    prompt = build_prompt(user_query, retrieved_chunks)\n",
                                    "    print(\"\\nConstructed Prompt:\\n\")\n",
                                    "    print(prompt)\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Saya telah melengkapi fungsi `retrieve_top_chunks` sesuai instruksi:\n",
                                    "\n",
                                    "* Menggunakan `collection.query()` untuk mencari dan mengembalikan top k hasil.\n",
                                    "* Safeguard saat tidak ada hasil.\n",
                                    "* Menyusun daftar `retrieved_chunks` dengan `chunk`, `doc_id`, dan `distance`.\n",
                                    "* Menetapkan `user_query` sebagai `\"technological breakthroughs\"` untuk pengujian.\n",
                                    "* Menampilkan hasil dan membangun prompt LLM sebagai validasi.\n",
                                    "\n",
                                    "Coba jalankan skrip `enhance_retrieval.py` untuk memverifikasi keluaran retrieval Anda!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Crafting Context-Rich LLM Prompts\n",
                                    "\n",
                                    "Well done on mastering the art of retrieving relevant chunks from a vector database in the previous exercise! Now, let's elevate your skills by integrating the build_prompt function into your existing code. This will enable you to craft a single, context-rich prompt that guides the LLM to generate precise answers.\n",
                                    "\n",
                                    "Here's your mission:\n",
                                    "\n",
                                    "Retrieve the top chunks for a given query and pass these chunks into the build_prompt function.\n",
                                    "Use the generated prompt to obtain an LLM response, employing a placeholder function or script for this purpose.\n",
                                    "Print both the final prompt and the LLM's response to verify that the LLM is effectively using the retrieved chunks to formulate its answer.\n",
                                    "For instance, if your query revolves around \"technological breakthroughs,\" the LLM should provide an answer that reflects the context of the retrieved chunks. This exercise will ensure that your LLM is not just generating answers but doing so with context and accuracy. Embrace the challenge and discover how well your LLM can perform with the right guidance!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "\n",
                                    "def retrieve_top_chunks(query, collection, top_k=3):\n",
                                    "    \"\"\"\n",
                                    "    Retrieves the top_k chunks most relevant to the given query from 'collection'.\n",
                                    "    Returns a list of retrieved chunks, each containing 'chunk' text, 'doc_id', and 'distance'.\n",
                                    "    \"\"\"\n",
                                    "    results = collection.query(\n",
                                    "        query_texts=[query],\n",
                                    "        n_results=top_k\n",
                                    "    )\n",
                                    "    retrieved_chunks = []\n",
                                    "    \n",
                                    "    # Safeguard if no documents are returned\n",
                                    "    if not results['documents'][0]:\n",
                                    "        return retrieved_chunks\n",
                                    "\n",
                                    "    for i in range(len(results['documents'][0])):\n",
                                    "        retrieved_chunks.append({\n",
                                    "            \"chunk\": results['documents'][0][i],\n",
                                    "            \"doc_id\": results['ids'][0][i],\n",
                                    "            \"distance\": results['distances'][0][i]\n",
                                    "        })\n",
                                    "    return retrieved_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_prompt(query, retrieved_chunks):\n",
                                    "    \"\"\"\n",
                                    "    Constructs an LLM prompt by combining multiple retrieved chunks into a\n",
                                    "    single context block, ensuring the model can handle longer or more detailed answers.\n",
                                    "    \"\"\"\n",
                                    "    # TODO: Implement the build_prompt function that creates a prompt string\n",
                                    "    # combining the query and retrieved chunks\n",
                                    "    \n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load corpus data from JSON file\n",
                                    "    with open('data/corpus.json', 'r') as f:\n",
                                    "        corpus_data = json.load(f)\n",
                                    "\n",
                                    "    # Set up the embedding model and initialize a ChromaDB collection\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\"rag_collection\", embedding_function=embed_func)\n",
                                    "\n",
                                    "    # Add documents from corpus_data into the vector database\n",
                                    "    documents = [doc['content'] for doc in corpus_data]\n",
                                    "    ids = [f\"chunk_{doc['id']}_0\" for doc in corpus_data]\n",
                                    "    collection.add(documents=documents, ids=ids)\n",
                                    "\n",
                                    "    # Define a sample query\n",
                                    "    query = \"What are some recent technological breakthroughs?\"\n",
                                    "\n",
                                    "    # TODO: Retrieve chunks, build the prompt, and get the LLM response\n",
                                    "    \n",
                                    "    # TODO: Print the final prompt and LLM answer\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "def retrieve_top_chunks(query, collection, top_k=3):\n",
                                    "    \"\"\"\n",
                                    "    Retrieves the top_k chunks most relevant to the given query from 'collection'.\n",
                                    "    Returns a list of retrieved chunks, each containing 'chunk' text, 'doc_id', and 'distance'.\n",
                                    "    \"\"\"\n",
                                    "    results = collection.query(\n",
                                    "        query_texts=[query],\n",
                                    "        n_results=top_k\n",
                                    "    )\n",
                                    "    retrieved_chunks = []\n",
                                    "    # Safeguard if no documents are returned\n",
                                    "    if not results.get('documents') or not results['documents'][0]:\n",
                                    "        return retrieved_chunks\n",
                                    "    for i, chunk_text in enumerate(results['documents'][0]):\n",
                                    "        retrieved_chunks.append({\n",
                                    "            \"chunk\": chunk_text,\n",
                                    "            \"doc_id\": results['ids'][0][i],\n",
                                    "            \"distance\": results['distances'][0][i]\n",
                                    "        })\n",
                                    "    return retrieved_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_prompt(query, retrieved_chunks):\n",
                                    "    \"\"\"\n",
                                    "    Constructs an LLM prompt by combining the query and retrieved chunks\n",
                                    "    into a single context block for the LLM.\n",
                                    "    \"\"\"\n",
                                    "    prompt = f\"Question: {query}\\nAnswer using only the following context:\\n\"\n",
                                    "    for rc in retrieved_chunks:\n",
                                    "        prompt += f\"- {rc['chunk']}\\n\"\n",
                                    "    prompt += \"Answer:\"\n",
                                    "    return prompt\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load corpus data from JSON file\n",
                                    "    with open('data/corpus.json', 'r') as f:\n",
                                    "        corpus_data = json.load(f)\n",
                                    "\n",
                                    "    # Set up the embedding model and initialize a ChromaDB collection\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\n",
                                    "        \"rag_collection\",\n",
                                    "        embedding_function=embed_func\n",
                                    "    )\n",
                                    "\n",
                                    "    # Batch add documents from corpus_data into the vector database\n",
                                    "    documents = [doc['content'] for doc in corpus_data]\n",
                                    "    ids = [f\"chunk_{doc['id']}_0\" for doc in corpus_data]\n",
                                    "    collection.add(documents=documents, ids=ids)\n",
                                    "\n",
                                    "    # Define a sample query\n",
                                    "    query = \"What are some recent technological breakthroughs?\"\n",
                                    "\n",
                                    "    # Retrieve top chunks\n",
                                    "    retrieved_chunks = retrieve_top_chunks(query, collection, top_k=3)\n",
                                    "\n",
                                    "    # Build the context-rich prompt\n",
                                    "    prompt = build_prompt(query, retrieved_chunks)\n",
                                    "\n",
                                    "    # Get an answer from the LLM\n",
                                    "    answer = get_llm_response(prompt)\n",
                                    "\n",
                                    "    # Print the final prompt and LLM answer\n",
                                    "    print(\"--- Constructed Prompt ---\")\n",
                                    "    print(prompt)\n",
                                    "    print(\"--- LLM Response ---\")\n",
                                    "    print(answer)\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Thought for 4 seconds\n",
                                    "\n",
                                    "\n",
                                    "Saya sudah menambahkan fungsi `build_prompt` serta mengintegrasikannya dengan `retrieve_top_chunks` dan `get_llm_response`. Skrip `craft_context_prompt.py` kini:\n",
                                    "\n",
                                    "1. Memuat data korpus dan menyiapkan koleksi ChromaDB.\n",
                                    "2. Mengambil top-3 chunk relevan untuk query.\n",
                                    "3. Membentuk prompt kontekstual lengkap.\n",
                                    "4. Memanggil LLM lewat `get_llm_response` dan mencetak prompt serta jawaban.\n",
                                    "\n",
                                    "Coba jalankan file ini untuk melihat seberapa akurat LLM memanfaatkan konteks yang diambil!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Metadata-Based Retrieval Enhancement\n",
                                    "\n",
                                    "In the previous exercise, you mastered retrieving relevant chunks from a vector database and using them to build context-rich prompts. Now, let's enhance your skills by incorporating metadata-based filtering into your retrieval process.\n",
                                    "\n",
                                    "Your task is to enhance the retrieve_top_chunks function to include this filtering capability. This involves allowing users to specify a category, such as \"finance\" or \"AI,\" and ensuring that only chunks matching this category are returned. Here's your roadmap:\n",
                                    "\n",
                                    "Modify the retrieve_top_chunks function to create a where dictionary that filters by category if one is provided.\n",
                                    "Use the collection.query() method with the where parameter to perform the query with metadata filtering.\n",
                                    "Process the query results and append each chunk's information to the retrieved_chunks list.\n",
                                    "Define a query and category to test the retrieval function.\n",
                                    "Implement logic to handle cases where no chunks match the specified category by printing a user-friendly message.\n",
                                    "Use the build_prompt function with these filtered chunks to create a single, context-rich prompt.\n",
                                    "For example, if you query for \"technological breakthroughs\" in the \"AI\" category, the function should return the most relevant AI-related chunks. If no chunks match, inform the user with a clear message. This exercise will refine your retrieval skills and ensure your LLM responses are not only accurate but also contextually relevant. Embrace the challenge and see how effectively you can guide the LLM with precise context!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "\n",
                                    "def retrieve_top_chunks(query, collection, category=None, top_k=3):\n",
                                    "    \"\"\"\n",
                                    "    Retrieves the top_k chunks most relevant to the given query from 'collection',\n",
                                    "    optionally filtered by category. Returns a list of retrieved chunks, each\n",
                                    "    containing 'chunk' text, 'doc_id', and 'distance'.\n",
                                    "    \"\"\"\n",
                                    "    # TODO: Create a where dictionary to filter by category if one is provided\n",
                                    "\n",
                                    "    # TODO: Perform the query with metadata filtering using collection.query()\n",
                                    "    # Include the where parameter in the query\n",
                                    "\n",
                                    "    retrieved_chunks = []\n",
                                    "\n",
                                    "    # Safeguard against empty results\n",
                                    "    if not results['documents'] or not results['documents'][0]:\n",
                                    "        return retrieved_chunks\n",
                                    "\n",
                                    "    # TODO: Process query results and append each chunk's information to retrieved_chunks\n",
                                    "\n",
                                    "    return retrieved_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_prompt(query, retrieved_chunks):\n",
                                    "    \"\"\"\n",
                                    "    Constructs an LLM prompt by combining multiple retrieved chunks into a\n",
                                    "    single context block, ensuring the model can provide context-based answers.\n",
                                    "    \"\"\"\n",
                                    "    prompt = f\"Question: {query}\\nAnswer using only the following context:\\n\"\n",
                                    "    for rc in retrieved_chunks:\n",
                                    "        prompt += f\"- {rc['chunk']}\\n\"\n",
                                    "    prompt += \"Answer:\"\n",
                                    "    return prompt\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load corpus data from JSON file\n",
                                    "    with open('data/corpus.json', 'r') as f:\n",
                                    "        corpus_data = json.load(f)\n",
                                    "\n",
                                    "    # Prepare documents, ids, and metadatas\n",
                                    "    documents = [doc['content'] for doc in corpus_data]\n",
                                    "    ids = [f\"chunk_{doc['id']}_0\" for doc in corpus_data]\n",
                                    "    metadatas = [{\"category\": doc[\"category\"]} for doc in corpus_data]\n",
                                    "\n",
                                    "    # Create or retrieve the vector database collection\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\n",
                                    "        \"rag_collection\",\n",
                                    "        embedding_function=embed_func\n",
                                    "    )\n",
                                    "\n",
                                    "    # Add documents with metadata to the collection\n",
                                    "    collection.add(documents=documents, ids=ids, metadatas=metadatas)\n",
                                    "\n",
                                    "    # TODO: Define a query and category to test the retrieval function\n",
                                    "    user_query = \"\"\n",
                                    "    user_category = \"\"\n",
                                    "\n",
                                    "    # TODO: Retrieve chunks matching the query and category\n",
                                    "    retrieved = []\n",
                                    "\n",
                                    "    # TODO: Implement logic to handle empty results, build prompt, and get LLM response\n",
                                    "    # Print appropriate messages or the final prompt and answer\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "Thought for a couple of seconds\n",
                                    "\n",
                                    "```python\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "\n",
                                    "def retrieve_top_chunks(query, collection, category=None, top_k=3):\n",
                                    "    \"\"\"\n",
                                    "    Retrieves the top_k chunks most relevant to the given query from 'collection',\n",
                                    "    optionally filtered by category. Returns a list of retrieved chunks, each\n",
                                    "    containing 'chunk' text, 'doc_id', and 'distance'.\n",
                                    "    \"\"\"\n",
                                    "    # Build metadata filter\n",
                                    "    where = {\"category\": category} if category else {}\n",
                                    "\n",
                                    "    # Perform query with optional metadata filtering\n",
                                    "    results = collection.query(\n",
                                    "        query_texts=[query],\n",
                                    "        n_results=top_k,\n",
                                    "        where=where\n",
                                    "    )\n",
                                    "\n",
                                    "    retrieved_chunks = []\n",
                                    "\n",
                                    "    # Safeguard against empty results\n",
                                    "    docs = results.get(\"documents\", [])\n",
                                    "    if not docs or not docs[0]:\n",
                                    "        return retrieved_chunks\n",
                                    "\n",
                                    "    # Process and collect results\n",
                                    "    for i, chunk_text in enumerate(docs[0]):\n",
                                    "        retrieved_chunks.append({\n",
                                    "            \"chunk\": chunk_text,\n",
                                    "            \"doc_id\": results[\"ids\"][0][i],\n",
                                    "            \"distance\": results[\"distances\"][0][i]\n",
                                    "        })\n",
                                    "    return retrieved_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_prompt(query, retrieved_chunks):\n",
                                    "    \"\"\"\n",
                                    "    Constructs an LLM prompt by combining multiple retrieved chunks into a\n",
                                    "    single context block, ensuring the model can provide context-based answers.\n",
                                    "    \"\"\"\n",
                                    "    prompt = f\"Question: {query}\\nAnswer using only the following context:\\n\"\n",
                                    "    for rc in retrieved_chunks:\n",
                                    "        prompt += f\"- {rc['chunk']}\\n\"\n",
                                    "    prompt += \"Answer:\"\n",
                                    "    return prompt\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load corpus data from JSON file\n",
                                    "    with open('data/corpus.json', 'r') as f:\n",
                                    "        corpus_data = json.load(f)\n",
                                    "\n",
                                    "    # Prepare docs, ids, and metadatas\n",
                                    "    documents = [doc['content'] for doc in corpus_data]\n",
                                    "    ids = [f\"chunk_{doc['id']}_0\" for doc in corpus_data]\n",
                                    "    metadatas = [{\"category\": doc.get(\"category\", \"general\")} for doc in corpus_data]\n",
                                    "\n",
                                    "    # Initialize ChromaDB collection\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\n",
                                    "        \"rag_collection\",\n",
                                    "        embedding_function=embed_func\n",
                                    "    )\n",
                                    "\n",
                                    "    # Add documents with metadata\n",
                                    "    collection.add(\n",
                                    "        documents=documents,\n",
                                    "        ids=ids,\n",
                                    "        metadatas=metadatas\n",
                                    "    )\n",
                                    "\n",
                                    "    # Define query and category to test\n",
                                    "    user_query = \"What are some recent technological breakthroughs?\"\n",
                                    "    user_category = \"AI\"\n",
                                    "\n",
                                    "    # Retrieve using metadata filter\n",
                                    "    retrieved = retrieve_top_chunks(user_query, collection, category=user_category, top_k=3)\n",
                                    "\n",
                                    "    if not retrieved:\n",
                                    "        print(f\"No chunks found matching category '{user_category}' for query '{user_query}'.\")\n",
                                    "    else:\n",
                                    "        # Build context-rich prompt and get LLM response\n",
                                    "        prompt = build_prompt(user_query, retrieved)\n",
                                    "        answer = get_llm_response(prompt)\n",
                                    "\n",
                                    "        # Display results\n",
                                    "        print(\"--- Retrieved Chunks ---\")\n",
                                    "        for rc in retrieved:\n",
                                    "            print(f\"Doc ID: {rc['doc_id']}, Distance: {rc['distance']}\")\n",
                                    "            print(f\"Chunk: {rc['chunk']}\\n\")\n",
                                    "        print(\"--- Constructed Prompt ---\")\n",
                                    "        print(prompt)\n",
                                    "        print(\"--- LLM Response ---\")\n",
                                    "        print(answer)\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Saya telah menambahkan fungsi `retrieve_top_chunks` dengan kemampuan filtering berdasarkan `category` menggunakan parameter `where`. Skrip `metadata_retrieval.py` kini:\n",
                                    "\n",
                                    "1. Membentuk filter metadata (`where`) apabila `category` disediakan.\n",
                                    "2. Memanggil `collection.query()` dengan `where`, `query_texts`, dan `n_results`.\n",
                                    "3. Menangani kasus tidak ada hasil dengan mengembalikan list kosong.\n",
                                    "4. Mengumpulkan hasil relevan beserta `chunk`, `doc_id`, dan `distance`.\n",
                                    "5. Di bagian main, menguji fungsi dengan query dan kategori (“AI”), mencetak pesan jika kosong, atau menampilkan chunks, prompt, dan jawaban dari LLM jika berhasil.\n",
                                    "\n",
                                    "Silakan jalankan untuk memverifikasi filter kategori dan keakuratan konteks!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Refine Retrieval with Distance Threshold\n",
                                    "\n",
                                    "Well done on mastering metadata-based filtering in your previous exercise! Now, let's elevate your retrieval process by incorporating a distance threshold. This enhancement will ensure that only the most semantically relevant chunks are considered, refining your LLM responses for greater precision and contextual accuracy.\n",
                                    "\n",
                                    "To achieve this, follow these steps:\n",
                                    "\n",
                                    "Modify the retrieve_top_chunks function to include a distance threshold. Ensure that only chunks with a similarity distance below a specified cutoff are included. You should choose an appropriate distance threshold yourself!\n",
                                    "Seamlessly integrate this distance-based filtering with the existing metadata filtering. Both category constraints and distance constraints should be applied simultaneously.\n",
                                    "Handle scenarios where no chunks meet the criteria gracefully. Instead of generating a prompt, provide a friendly message indicating that no relevant chunks were found.\n",
                                    "This exercise will deepen your understanding of retrieval logic and reinforce your skills in context construction. Dive in and see how effectively you can guide the LLM with precise and relevant context!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "\n",
                                    "def retrieve_top_chunks(query, collection, category=None, top_k=3, distance_threshold=1.0):\n",
                                    "    \"\"\"\n",
                                    "    Retrieves the top_k chunks most relevant to the given query from 'collection',\n",
                                    "    optionally filtered by category, and only includes those whose distance is\n",
                                    "    below the specified distance_threshold. Returns a list of retrieved chunks,\n",
                                    "    each containing 'chunk', 'doc_id', and 'distance'.\n",
                                    "    \"\"\"\n",
                                    "    where = {\"category\": category} if category is not None else None\n",
                                    "\n",
                                    "    results = collection.query(\n",
                                    "        query_texts=[query],\n",
                                    "        where=where,\n",
                                    "        n_results=top_k\n",
                                    "    )\n",
                                    "\n",
                                    "\n",
                                    "    retrieved_chunks = []\n",
                                    "    if not results[\"documents\"] or not results[\"documents\"][0]:\n",
                                    "        return retrieved_chunks\n",
                                    "\n",
                                    "    # TODO: Process the results and append chunks that meet the distance threshold\n",
                                    "    # For each chunk in results, check if its distance is below distance_threshold\n",
                                    "    # If it qualifies, add it to retrieved_chunks with chunk text, doc_id, and distance\n",
                                    "\n",
                                    "    return retrieved_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_prompt(query, retrieved_chunks):\n",
                                    "    \"\"\"\n",
                                    "    Constructs a prompt by combining the query and retrieved chunks into a\n",
                                    "    context block, guiding the LLM to provide a context-based answer.\n",
                                    "    \"\"\"\n",
                                    "    prompt = f\"Question: {query}\\nAnswer using only the following context:\\n\"\n",
                                    "    for rc in retrieved_chunks:\n",
                                    "        prompt += f\"- {rc['chunk']}\\n\"\n",
                                    "    prompt += \"Answer:\"\n",
                                    "    return prompt\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load corpus data from JSON file\n",
                                    "    with open(\"data/corpus.json\", \"r\") as f:\n",
                                    "        corpus_data = json.load(f)\n",
                                    "\n",
                                    "    # Prepare documents, ids, and metadatas\n",
                                    "    documents = [doc[\"content\"] for doc in corpus_data]\n",
                                    "    ids = [f\"chunk_{doc['id']}_0\" for doc in corpus_data]\n",
                                    "    metadatas = [{\"category\": doc.get(\"category\", \"\")} for doc in corpus_data]\n",
                                    "\n",
                                    "    # Create or retrieve the vector database collection\n",
                                    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\"rag_collection\", embedding_function=embed_func)\n",
                                    "\n",
                                    "    # Add documents with metadata to the collection\n",
                                    "    collection.add(documents=documents, ids=ids, metadatas=metadatas)\n",
                                    "\n",
                                    "    # Define query parameters (query string, category, and distance threshold)\n",
                                    "    user_query = \"What are the latest AI breakthroughs?\"  # Example query\n",
                                    "    user_category = \"Technology\"\n",
                                    "    threshold = 1.0\n",
                                    "\n",
                                    "    # Retrieve and filter chunks\n",
                                    "    filtered_chunks = retrieve_top_chunks(\n",
                                    "        query=user_query,\n",
                                    "        collection=collection,\n",
                                    "        category=user_category,\n",
                                    "        top_k=5,\n",
                                    "        distance_threshold=threshold\n",
                                    "    )\n",
                                    "\n",
                                    "    # TODO: Handle the filtered chunks:\n",
                                    "    # - If no chunks found, print a user-friendly message\n",
                                    "    # - Otherwise, build the prompt and get LLM response\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Thought for a couple of seconds\n",
                                    "\n",
                                    "```python\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "from scripts.llm import get_llm_response\n",
                                    "\n",
                                    "\n",
                                    "def retrieve_top_chunks(query, collection, category=None, top_k=3, distance_threshold=0.8):\n",
                                    "    \"\"\"\n",
                                    "    Retrieves the top_k chunks most relevant to the given query from 'collection',\n",
                                    "    optionally filtered by category, and only includes those whose distance is\n",
                                    "    below the specified distance_threshold. Returns a list of retrieved chunks,\n",
                                    "    each containing 'chunk', 'doc_id', and 'distance'.\n",
                                    "    \"\"\"\n",
                                    "    # Build metadata filter\n",
                                    "    where = {\"category\": category} if category else None\n",
                                    "\n",
                                    "    # Perform query with optional metadata filtering\n",
                                    "    results = collection.query(\n",
                                    "        query_texts=[query],\n",
                                    "        where=where,\n",
                                    "        n_results=top_k\n",
                                    "    )\n",
                                    "\n",
                                    "    retrieved_chunks = []\n",
                                    "    docs = results.get(\"documents\", [])\n",
                                    "    ids = results.get(\"ids\", [])\n",
                                    "    distances = results.get(\"distances\", [])\n",
                                    "\n",
                                    "    # Safeguard against empty results\n",
                                    "    if not docs or not docs[0]:\n",
                                    "        return retrieved_chunks\n",
                                    "\n",
                                    "    # Filter by distance threshold\n",
                                    "    for idx, chunk_text in enumerate(docs[0]):\n",
                                    "        dist = distances[0][idx]\n",
                                    "        if dist <= distance_threshold:\n",
                                    "            retrieved_chunks.append({\n",
                                    "                \"chunk\": chunk_text,\n",
                                    "                \"doc_id\": ids[0][idx],\n",
                                    "                \"distance\": dist\n",
                                    "            })\n",
                                    "    return retrieved_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_prompt(query, retrieved_chunks):\n",
                                    "    \"\"\"\n",
                                    "    Constructs a prompt by combining the query and retrieved chunks into a\n",
                                    "    context block, guiding the LLM to provide a context-based answer.\n",
                                    "    \"\"\"\n",
                                    "    prompt = f\"Question: {query}\\nAnswer using only the following context:\\n\"\n",
                                    "    for rc in retrieved_chunks:\n",
                                    "        prompt += f\"- {rc['chunk']}\\n\"\n",
                                    "    prompt += \"Answer:\"\n",
                                    "    return prompt\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    # Load corpus data from JSON file\n",
                                    "    with open(\"data/corpus.json\", \"r\") as f:\n",
                                    "        corpus_data = json.load(f)\n",
                                    "\n",
                                    "    # Prepare documents, ids, and metadatas\n",
                                    "    documents = [doc[\"content\"] for doc in corpus_data]\n",
                                    "    ids = [f\"chunk_{doc['id']}_0\" for doc in corpus_data]\n",
                                    "    metadatas = [{\"category\": doc.get(\"category\", \"general\")} for doc in corpus_data]\n",
                                    "\n",
                                    "    # Initialize ChromaDB collection\n",
                                    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\n",
                                    "        \"rag_collection\",\n",
                                    "        embedding_function=embed_func\n",
                                    "    )\n",
                                    "\n",
                                    "    # Add documents with metadata\n",
                                    "    collection.add(\n",
                                    "        documents=documents,\n",
                                    "        ids=ids,\n",
                                    "        metadatas=metadatas\n",
                                    "    )\n",
                                    "\n",
                                    "    # Define query parameters\n",
                                    "    user_query = \"What are the latest AI breakthroughs?\"\n",
                                    "    user_category = \"Technology\"\n",
                                    "    threshold = 0.75\n",
                                    "\n",
                                    "    # Retrieve and filter chunks\n",
                                    "    filtered_chunks = retrieve_top_chunks(\n",
                                    "        query=user_query,\n",
                                    "        collection=collection,\n",
                                    "        category=user_category,\n",
                                    "        top_k=5,\n",
                                    "        distance_threshold=threshold\n",
                                    "    )\n",
                                    "\n",
                                    "    # Handle no results case\n",
                                    "    if not filtered_chunks:\n",
                                    "        print(f\"No relevant chunks found for query '{user_query}' \"\n",
                                    "              f\"in category '{user_category}' with threshold <= {threshold}.\")\n",
                                    "    else:\n",
                                    "        # Build the context-rich prompt\n",
                                    "        prompt = build_prompt(user_query, filtered_chunks)\n",
                                    "        # Get answer from the LLM\n",
                                    "        answer = get_llm_response(prompt)\n",
                                    "\n",
                                    "        # Display filtered chunks\n",
                                    "        print(\"--- Filtered Chunks ---\")\n",
                                    "        for rc in filtered_chunks:\n",
                                    "            print(f\"Doc ID: {rc['doc_id']} | Distance: {rc['distance']}\")\n",
                                    "            print(f\"Chunk: {rc['chunk']}\\n\")\n",
                                    "\n",
                                    "        # Display prompt and answer\n",
                                    "        print(\"--- Constructed Prompt ---\")\n",
                                    "        print(prompt)\n",
                                    "        print(\"--- LLM Response ---\")\n",
                                    "        print(answer)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Saya telah menambahkan filter jarak (`distance_threshold`) ke dalam `retrieve_top_chunks`, memastikan hanya potongan dengan nilai jarak di bawah ambang yang dipilih yang dikembalikan. Kode ini juga menggabungkan filter kategori sekaligus menangani kasus tanpa hasil, menampilkan pesan ramah pengguna. Coba jalankan file `refine_retrieval_threshold.py` untuk melihat perbaikan retrieval Anda!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "316639e6",
                           "metadata": {},
                           "source": []
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
