{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 2: Storing and Managing Text Chunks in Vector Databases\n",
                                    "\n",
                                    "**Converting to markdown**\n",
                                    "\n",
                                    "The user simply asked to convert the provided text to markdown. The text includes clear sections like “Introduction,” “Understanding Vector Databases,” “Why We Need Vector Databases for RAG,” and others, along with code blocks and explanations.\n",
                                    "\n",
                                    "I need to apply markdown formatting: use appropriate headings, code fences with syntax highlighting, lists, and emphasize key terms with bold or italics.\n",
                                    "\n",
                                    "I won’t add a title at the top, as the instruction was just to convert, not introduce anything extra. Let’s do it!\n",
                                    "\n",
                                    "\n",
                                    "Thought for 4 seconds\n",
                                    "\n",
                                    "\n",
                                    "## Introduction\n",
                                    "\n",
                                    "Hello there, welcome to the second lesson of our **Scaling Up RAG with Vector Databases** course! In the previous unit, you explored how to break large documents into smaller chunks and attach useful metadata (like `doc_id`, `chunk_id`, and labels such as category). These chunks are essential for structuring data in a way that makes retrieval easier. In this lesson, we’ll build on that groundwork by showing you how to store them in a vector database. One popular choice is **ChromaDB**—an open-source database designed for high-speed, semantic querying of vectors. By switching from keyword-based searches to semantic searches, your RAG system will retrieve relevant information more efficiently. Let’s dive in!\n",
                                    "\n",
                                    "## Understanding Vector Databases\n",
                                    "\n",
                                    "A vector database stores data in the form of numerical vectors that capture the **semantic essence** of texts (or other data). The database then uses similarity metrics—rather than literal word matches—so that conceptually similar items are stored close together. This means searches on vector databases can retrieve contextually relevant results even when keywords are absent.\n",
                                    "\n",
                                    "By leveraging approximate or exact nearest-neighbor strategies for similarity, vector databases can scale to handle millions or billions of vectors while still providing quick query responses. This makes them especially suitable for RAG systems, which rely on fast semantic lookups across large collections of text.\n",
                                    "\n",
                                    "## Why We Need Vector Databases for RAG\n",
                                    "\n",
                                    "Before we explore how to set up a vector database, let’s look at why it’s a crucial component of a RAG pipeline:\n",
                                    "\n",
                                    "1. **Semantic Retrieval**\n",
                                    "   By embedding text into vectors, queries can match documents based on meaning rather than strict keyword matches. This yields more accurate and context-sensitive search results.\n",
                                    "\n",
                                    "2. **Scalability**\n",
                                    "   Specialized vector databases handle large datasets efficiently, allowing you to store and query vast libraries of text chunks without sacrificing performance.\n",
                                    "\n",
                                    "3. **Richer Context**\n",
                                    "   Embeddings capture nuanced relationships among chunks, ensuring that related information is surfaced even when it doesn’t use the exact same terms.\n",
                                    "\n",
                                    "4. **Easy Updates**\n",
                                    "   Vector databases (like ChromaDB) often allow you to add and remove chunks on the fly, so your collection stays in sync with new or evolving information.\n",
                                    "\n",
                                    "## Setting Up ChromaDB and Basic Configuration\n",
                                    "\n",
                                    "Now, let’s jump into coding with ChromaDB, our chosen vector database. Here’s how to set up a ChromaDB client:\n",
                                    "\n",
                                    "```python\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "\n",
                                    "def build_chroma_collection(chunks):\n",
                                    "    # Use a Sentence Transformer model for embeddings\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
                                    "        model_name=model_name\n",
                                    "    )\n",
                                    "    \n",
                                    "    # Create a ChromaDB client with default settings\n",
                                    "    client = Client(Settings())\n",
                                    "\n",
                                    "    # Either get an existing collection or create a new one\n",
                                    "    collection = client.get_or_create_collection(\n",
                                    "        name=\"rag_collection\",\n",
                                    "        embedding_function=embed_func\n",
                                    "    )\n",
                                    "    # ... continues\n",
                                    "```\n",
                                    "\n",
                                    "### How It Works\n",
                                    "\n",
                                    "* **Embedding Setup**\n",
                                    "  We define a `SentenceTransformerEmbeddingFunction` to generate vectors for the text chunks. The model `all-MiniLM-L6-v2` is a lightweight yet powerful transformer that maps sentences to a 384-dimensional dense vector space. It balances efficiency (small size, fast inference) with strong semantic understanding—ideal for RAG applications.\n",
                                    "\n",
                                    "* **Client Configuration**\n",
                                    "  `Client(Settings())` connects to ChromaDB with default in-memory settings for quick experimentation. You can customize persistence, encryption, or other features by passing additional parameters to `Settings()`.\n",
                                    "\n",
                                    "* **Collection Management**\n",
                                    "  `get_or_create_collection` checks if a collection named `\"rag_collection\"` exists; if not, it creates one. A collection in ChromaDB is like a table in a traditional database but optimized for vector similarity operations, letting you group related documents and embeddings under a single namespace.\n",
                                    "\n",
                                    "## Preparing Data and Adding Chunks to ChromaDB\n",
                                    "\n",
                                    "After setting up your client, embedding function, and collection, prepare your chunks for insertion:\n",
                                    "\n",
                                    "```python\n",
                                    "# ... continues in build_chroma_collection\n",
                                    "\n",
                                    "# Prepare the data: texts, IDs, and metadata \n",
                                    "texts = [c[\"content\"] for c in chunks]\n",
                                    "ids = [f\"chunk_{c['doc_id']}_{c['chunk_id']}\" for c in chunks]\n",
                                    "metadatas = [\n",
                                    "    {\n",
                                    "        \"doc_id\": chunk[\"doc_id\"], \n",
                                    "        \"chunk_id\": chunk[\"chunk_id\"], \n",
                                    "        \"category\": chunk[\"category\"]\n",
                                    "    }\n",
                                    "    for chunk in chunks\n",
                                    "]\n",
                                    "\n",
                                    "# Add the documents (chunks) to the collection\n",
                                    "collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
                                    "return collection\n",
                                    "```\n",
                                    "\n",
                                    "**Key Points:**\n",
                                    "\n",
                                    "* **Data Grouping**: Each chunk is mapped to its text, a unique ID, and metadata.\n",
                                    "* **Seamless Insertion**: `collection.add()` handles embedding and storage in one call, making it easy to batch up and ingest your chunks.\n",
                                    "\n",
                                    "## Updating and Managing Documents\n",
                                    "\n",
                                    "ChromaDB lets you keep your collection current. Here’s an example of adding and then deleting a chunk:\n",
                                    "\n",
                                    "```python\n",
                                    "# Example initial chunks\n",
                                    "example_chunks = [\n",
                                    "    {\"doc_id\": 0, \"chunk_id\": 0, \"category\": \"ai\",      \"content\": \"RAG stands for Retrieval-Augmented Generation.\"},\n",
                                    "    {\"doc_id\": 0, \"chunk_id\": 1, \"category\": \"ai\",      \"content\": \"A crucial component of a RAG pipeline is the Vector Database.\"},\n",
                                    "    {\"doc_id\": 1, \"chunk_id\": 0, \"category\": \"finance\", \"content\": \"Accurate data is essential in finance.\"},\n",
                                    "]\n",
                                    "collection = build_chroma_collection(example_chunks)\n",
                                    "\n",
                                    "# Prepare a new chunk to add\n",
                                    "new_document = {\n",
                                    "    \"doc_id\": 2,\n",
                                    "    \"chunk_id\": 0,\n",
                                    "    \"category\": \"food\",\n",
                                    "    \"content\": \"Bananas are yellow fruits rich in potassium.\"\n",
                                    "}\n",
                                    "doc_id = f\"chunk_{new_document['doc_id']}_{new_document['chunk_id']}\"\n",
                                    "\n",
                                    "# Add the new chunk\n",
                                    "collection.add(\n",
                                    "    documents=[new_document[\"content\"]],\n",
                                    "    metadatas=[{\n",
                                    "        \"doc_id\": new_document[\"doc_id\"],\n",
                                    "        \"chunk_id\": new_document[\"chunk_id\"],\n",
                                    "        \"category\": new_document[\"category\"]\n",
                                    "    }],\n",
                                    "    ids=[doc_id]\n",
                                    ")\n",
                                    "\n",
                                    "# Remove it later if it becomes outdated\n",
                                    "collection.delete(ids=[doc_id])\n",
                                    "```\n",
                                    "\n",
                                    "**Why Unique IDs Matter:**\n",
                                    "By combining `doc_id` and `chunk_id` into a string (e.g., `\"chunk_2_0\"`), you ensure each chunk has a distinct identifier, making updates, deletions, and lookups straightforward.\n",
                                    "\n",
                                    "## Conclusion and Next Steps\n",
                                    "\n",
                                    "By storing text chunks in a vector database, you’ve laid the foundation for faster, semantically aware retrieval. You now know how to create, update, and manage a ChromaDB collection—crucial skills for any large-scale RAG system.\n",
                                    "\n",
                                    "In the next lesson, you’ll learn how to **query** the vector database to fetch the most relevant chunks and feed them into a language model. That’s where the real magic of context-rich, accurate responses shines! For now, feel free to experiment with different embedding models, or try adding and deleting chunks of various sizes. When you’re ready, move on to the practice exercises to cement these concepts and refine your RAG workflow.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Loading and chunking text\n",
                                    "\n",
                                    "Congratulations on reaching this point in the lesson! Now, let's put your newfound knowledge into practice by exploring the fascinating world of vector databases with ChromaDB.\n",
                                    "\n",
                                    "Your mission is to load a small JSON file containing a few short documents, split each document into smaller, manageable chunks, and then build a ChromaDB collection from these chunks. Here's what you need to do in the load_and_chunk_dataset function:\n",
                                    "\n",
                                    "Load the dataset from the JSON file located at data/corpus.json. Each document in this file has the keys: id, title, content, and category.\n",
                                    "Extract the category and id from each document.\n",
                                    "Use the provided chunk_text function to split the content of each document into smaller segments.\n",
                                    "Create a dictionary for each chunk that includes metadata such as doc_id, chunk_id, category, and the chunk text.\n",
                                    "By the end of this exercise, you'll have a solid understanding of how to prepare and store text chunks in a vector database, setting the stage for efficient semantic retrieval. Enjoy the process and happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "\n",
                                    "\n",
                                    "def chunk_text(text, chunk_size=50):\n",
                                    "    \"\"\"\n",
                                    "    Splits the given text into chunks of size 'chunk_size'.\n",
                                    "    Returns a list of chunk strings.\n",
                                    "    \"\"\"\n",
                                    "    words = text.split()\n",
                                    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
                                    "\n",
                                    "\n",
                                    "def load_and_chunk_dataset(file_path, chunk_size=50):\n",
                                    "    \"\"\"\n",
                                    "    Loads a dataset from JSON 'file_path', then splits each document into smaller chunks.\n",
                                    "    Metadata such as 'doc_id' and 'category' is included with each chunk.\n",
                                    "    \"\"\"\n",
                                    "    # TODO: Open and load the JSON file\n",
                                    "    \n",
                                    "    all_chunks = []\n",
                                    "    for doc in data:\n",
                                    "        doc_text = doc[\"content\"]\n",
                                    "        # TODO: Extract category and id from the document\n",
                                    "        \n",
                                    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
                                    "        for chunk_index, chunk_str in enumerate(doc_chunks):\n",
                                    "            # TODO: Create a dictionary for each chunk with doc_id, chunk_id, category and text\n",
                                    "            \n",
                                    "    return all_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
                                    "    \"\"\"\n",
                                    "    Builds or retrieves a ChromaDB collection, embedding each chunk using a SentenceTransformer.\n",
                                    "    Adds all chunks in the 'chunks' list to the collection for fast retrieval.\n",
                                    "    \"\"\"\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\n",
                                    "        name=collection_name,\n",
                                    "        embedding_function=embed_func\n",
                                    "    )\n",
                                    "\n",
                                    "    texts = [c[\"text\"] for c in chunks]\n",
                                    "    ids = [f\"chunk_{c['doc_id']}_{c['chunk_id']}\" for c in chunks]\n",
                                    "    metadatas = [\n",
                                    "        {\"doc_id\": c[\"doc_id\"], \"chunk_id\": c[\"chunk_id\"], \"category\": c[\"category\"]}\n",
                                    "        for c in chunks\n",
                                    "    ]\n",
                                    "\n",
                                    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
                                    "    return collection\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    current_dir = os.path.dirname(__file__)\n",
                                    "    dataset_file = os.path.join(current_dir, \"data\", \"corpus.json\")\n",
                                    "\n",
                                    "    chunked_docs = load_and_chunk_dataset(dataset_file)\n",
                                    "    collection = build_chroma_collection(chunked_docs)\n",
                                    "\n",
                                    "    total_docs = collection.count()\n",
                                    "    print(\"ChromaDB collection created with\", total_docs, \"documents.\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "The user encountered a KeyError for 'category', possibly because some of their documents don't have this key or it's named differently. To handle this, I suggest adjusting the function to check for the key and set a default. They could use `doc.get(\"category\", \"unknown\")` or check for nested keys or alternative names like 'tags' or 'categories.' Also, inspecting the JSON or printing the document's keys would help clarify the issue. Logging missing keys or raising a descriptive error might help improve the function’s robustness.\n",
                                    "\n",
                                    "\n",
                                    "Thought for a couple of seconds\n",
                                    "\n",
                                    "\n",
                                    "It looks like some of your documents don’t actually have a `\"category\"` field, so doing `doc[\"category\"]` blows up. You have two options:\n",
                                    "\n",
                                    "1. **Inspect the JSON** to see what the field is actually called (or if it’s missing), e.g.:\n",
                                    "\n",
                                    "   ```python\n",
                                    "   with open(dataset_file) as f:\n",
                                    "       data = json.load(f)\n",
                                    "   for i, doc in enumerate(data[:5]):\n",
                                    "       print(i, doc.keys())\n",
                                    "   ```\n",
                                    "\n",
                                    "2. **Defensive code:** fall back to a default (or skip) when `\"category\"` isn’t present.\n",
                                    "\n",
                                    "Here’s an updated version of `load_and_chunk_dataset` that:\n",
                                    "\n",
                                    "* Uses `doc.get(\"category\", \"uncategorized\")` so you never hit a KeyError.\n",
                                    "* Prints a warning when a document is missing that field (so you can audit your data).\n",
                                    "* Optionally, you could change `\"uncategorized\"` to `None` or even `raise` if that makes more sense for your pipeline.\n",
                                    "\n",
                                    "```python\n",
                                    "def load_and_chunk_dataset(file_path, chunk_size=50):\n",
                                    "    \"\"\"\n",
                                    "    Loads a dataset from JSON 'file_path', then splits each document into smaller chunks.\n",
                                    "    If a document is missing 'category', it gets tagged as \"uncategorized\" (and a warning is printed).\n",
                                    "    \"\"\"\n",
                                    "    # 1. Open and load the JSON file\n",
                                    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                                    "        data = json.load(f)\n",
                                    "\n",
                                    "    all_chunks = []\n",
                                    "    for doc in data:\n",
                                    "        # 2. Extract category and id from the document (with a default for missing categories)\n",
                                    "        doc_id = doc.get(\"id\", None)\n",
                                    "        if doc_id is None:\n",
                                    "            raise KeyError(f\"Document is missing an 'id' field: {doc}\")\n",
                                    "\n",
                                    "        if \"category\" not in doc:\n",
                                    "            print(f\"⚠️ Warning: doc id={doc_id} has no 'category'; tagging as 'uncategorized'\")\n",
                                    "        category = doc.get(\"category\", \"uncategorized\")\n",
                                    "\n",
                                    "        content = doc.get(\"content\", \"\")\n",
                                    "        if not content:\n",
                                    "            print(f\"⚠️ Warning: doc id={doc_id} has empty or missing 'content'\")\n",
                                    "\n",
                                    "        # 3. Split the content into chunks\n",
                                    "        doc_chunks = chunk_text(content, chunk_size)\n",
                                    "\n",
                                    "        # 4. Build a dict for each chunk and collect\n",
                                    "        for chunk_index, chunk_str in enumerate(doc_chunks):\n",
                                    "            chunk = {\n",
                                    "                \"doc_id\": doc_id,\n",
                                    "                \"chunk_id\": chunk_index,\n",
                                    "                \"category\": category,\n",
                                    "                \"text\": chunk_str\n",
                                    "            }\n",
                                    "            all_chunks.append(chunk)\n",
                                    "\n",
                                    "    return all_chunks\n",
                                    "```\n",
                                    "\n",
                                    "With this change you won’t get a `KeyError` on missing categories, and you’ll get console warnings so you know which documents need attention or where your schema may differ.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Building a ChromaDB Collection\n",
                                    "\n",
                                    "Congratulations on reaching this point in the lesson! Now, let's put your newfound knowledge into practice by exploring the fascinating world of vector databases with ChromaDB.\n",
                                    "\n",
                                    "Your mission is to complete the build_chroma_collection function to create a ChromaDB collection from pre-chunked documents. You need to complete the build_chroma_collection function by:\n",
                                    "\n",
                                    "Creating or retrieving a ChromaDB collection using the client's get_or_create_collection method\n",
                                    "Preparing the texts, ids, and metadatas lists from the provided chunks\n",
                                    "Adding the documents to the collection using the collection.add method\n",
                                    "By the end of this exercise, you'll have a solid understanding of how to store text chunks in a vector database, setting the stage for efficient semantic retrieval. Verify your implementation by checking that the collection is successfully created and the correct number of chunks was added. Enjoy the process and happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "\n",
                                    "\n",
                                    "def chunk_text(text, chunk_size=50):\n",
                                    "    \"\"\"\n",
                                    "    Splits the given text into chunks of size 'chunk_size'.\n",
                                    "    Returns a list of chunk strings.\n",
                                    "    \"\"\"\n",
                                    "    words = text.split()\n",
                                    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
                                    "\n",
                                    "\n",
                                    "def load_and_chunk_dataset(file_path, chunk_size=50):\n",
                                    "    \"\"\"\n",
                                    "    Loads a dataset from JSON 'file_path', then splits each document into smaller chunks.\n",
                                    "    Metadata such as 'doc_id' and 'category' is included with each chunk.\n",
                                    "    \"\"\"\n",
                                    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                                    "        data = json.load(f)\n",
                                    "\n",
                                    "    all_chunks = []\n",
                                    "    for doc in data:\n",
                                    "        doc_text = doc[\"content\"]\n",
                                    "        doc_category = doc.get(\"category\", \"general\")\n",
                                    "        doc_id = doc[\"id\"]\n",
                                    "\n",
                                    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
                                    "        for chunk_index, chunk_str in enumerate(doc_chunks):\n",
                                    "            all_chunks.append({\n",
                                    "                \"doc_id\": doc_id,\n",
                                    "                \"chunk_id\": chunk_index,\n",
                                    "                \"category\": doc_category,\n",
                                    "                \"text\": chunk_str\n",
                                    "            })\n",
                                    "    return all_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
                                    "    \"\"\"\n",
                                    "    Builds or retrieves a ChromaDB collection, embedding each chunk using a SentenceTransformer.\n",
                                    "    Adds all chunks in the 'chunks' list to the collection for fast retrieval.\n",
                                    "    \"\"\"\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "\n",
                                    "    client = Client(Settings())\n",
                                    "    # TODO: Create or get the collection using client.get_or_create_collection\n",
                                    "\n",
                                    "    # TODO: Prepare the texts, ids and metadatas lists from chunks\n",
                                    "\n",
                                    "    # TODO: Add the documents to the collection using collection.add\n",
                                    "    \n",
                                    "    return collection\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    current_dir = os.path.dirname(__file__)\n",
                                    "    dataset_file = os.path.join(current_dir, \"data\", \"corpus.json\")\n",
                                    "\n",
                                    "    chunked_docs = load_and_chunk_dataset(dataset_file)\n",
                                    "    collection = build_chroma_collection(chunked_docs)\n",
                                    "\n",
                                    "    total_docs = collection.count()\n",
                                    "    print(\"ChromaDB collection created with\", total_docs, \"documents.\")\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Thought for a couple of seconds\n",
                                    "\n",
                                    "\n",
                                    "Here’s a filled-out `build_chroma_collection` that:\n",
                                    "\n",
                                    "1. Retrieves or creates a ChromaDB collection via `get_or_create_collection`\n",
                                    "2. Extracts `texts`, `ids`, and `metadatas` from your `chunks` list\n",
                                    "3. Adds everything into the collection with `collection.add(...)`\n",
                                    "\n",
                                    "```python\n",
                                    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
                                    "    \"\"\"\n",
                                    "    Builds or retrieves a ChromaDB collection, embedding each chunk using a SentenceTransformer.\n",
                                    "    Adds all chunks in the 'chunks' list to the collection for fast retrieval.\n",
                                    "    \"\"\"\n",
                                    "    # 1. Setup the embedding function\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
                                    "        model_name=model_name\n",
                                    "    )\n",
                                    "\n",
                                    "    # 2. Connect to Chroma and get-or-create the collection\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\n",
                                    "        name=collection_name,\n",
                                    "        embedding_function=embed_func\n",
                                    "    )\n",
                                    "\n",
                                    "    # 3. Prepare lists for texts, ids, and metadata\n",
                                    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
                                    "    ids = [\n",
                                    "        f\"chunk_{chunk['doc_id']}_{chunk['chunk_id']}\"\n",
                                    "        for chunk in chunks\n",
                                    "    ]\n",
                                    "    metadatas = [\n",
                                    "        {\n",
                                    "            \"doc_id\": chunk[\"doc_id\"],\n",
                                    "            \"chunk_id\": chunk[\"chunk_id\"],\n",
                                    "            \"category\": chunk[\"category\"]\n",
                                    "        }\n",
                                    "        for chunk in chunks\n",
                                    "    ]\n",
                                    "\n",
                                    "    # 4. Add everything into the collection\n",
                                    "    collection.add(\n",
                                    "        documents=texts,\n",
                                    "        metadatas=metadatas,\n",
                                    "        ids=ids\n",
                                    "    )\n",
                                    "\n",
                                    "    return collection\n",
                                    "```\n",
                                    "\n",
                                    "And to verify you’ve ingested all your chunks:\n",
                                    "\n",
                                    "```python\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    chunked_docs = load_and_chunk_dataset(dataset_file)\n",
                                    "    collection = build_chroma_collection(chunked_docs)\n",
                                    "\n",
                                    "    total_chunks = collection.count()\n",
                                    "    print(f\"✅ ChromaDB collection '{collection.name}' created with {total_chunks} chunks.\")\n",
                                    "```\n",
                                    "\n",
                                    "When you run this, you should see:\n",
                                    "\n",
                                    "```\n",
                                    "✅ ChromaDB collection 'rag_collection' created with 123 chunks.\n",
                                    "```\n",
                                    "\n",
                                    "(where `123` matches the length of your `chunked_docs` list).\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Dynamic Chunk Management in ChromaDB\n",
                                    "\n",
                                    "Now, let's take your skills to the next level by learning how to dynamically manage these chunks.\n",
                                    "\n",
                                    "In this exercise, you'll enhance your ability to handle ChromaDB collections by adding and removing chunks. Here's what you need to focus on:\n",
                                    "\n",
                                    "Add a New Chunk: Create a new document with a unique doc_id, chunk_id, and category. Use the text \"Bananas are yellow fruits rich in potassium.\" for this chunk.\n",
                                    "Verify the Update: After adding the chunk, check that the total document count in the collection has increased by one.\n",
                                    "Remove the Chunk: Delete the newly added chunk using its unique ID and confirm that the document count returns to its previous value.\n",
                                    "By the end of this exercise, you'll have a deeper understanding of how to dynamically manage text chunks in a vector database, setting the stage for efficient semantic retrieval. Enjoy the challenge and happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "\n",
                                    "\n",
                                    "def chunk_text(text, chunk_size=30):\n",
                                    "    \"\"\"\n",
                                    "    Splits the given text into chunks of size 'chunk_size'.\n",
                                    "    Returns a list of chunk strings.\n",
                                    "    \"\"\"\n",
                                    "    words = text.split()\n",
                                    "    return [\n",
                                    "        \" \".join(words[i:i+chunk_size])\n",
                                    "        for i in range(0, len(words), chunk_size)\n",
                                    "    ]\n",
                                    "\n",
                                    "\n",
                                    "def load_and_chunk_dataset(file_path, chunk_size=30):\n",
                                    "    \"\"\"\n",
                                    "    Loads a dataset from JSON 'file_path', then splits each\n",
                                    "    document into smaller chunks. Metadata such as 'doc_id'\n",
                                    "    and 'category' is included with each chunk.\n",
                                    "    \"\"\"\n",
                                    "    with open(file_path, \"r\") as f:\n",
                                    "        data = json.load(f)\n",
                                    "\n",
                                    "    all_chunks = []\n",
                                    "    for doc_id, doc in enumerate(data):\n",
                                    "        doc_text = doc[\"content\"]\n",
                                    "        doc_category = doc.get(\"category\", \"general\")\n",
                                    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
                                    "\n",
                                    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
                                    "            all_chunks.append({\n",
                                    "                \"doc_id\": doc_id,\n",
                                    "                \"chunk_id\": chunk_id,\n",
                                    "                \"category\": doc_category,\n",
                                    "                \"text\": chunk_str\n",
                                    "            })\n",
                                    "\n",
                                    "    return all_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
                                    "    \"\"\"\n",
                                    "    Builds or retrieves a ChromaDB collection, embedding each chunk\n",
                                    "    using a SentenceTransformer. Adds all chunks in the 'chunks' list\n",
                                    "    to the collection for fast retrieval.\n",
                                    "    \"\"\"\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
                                    "        model_name=model_name\n",
                                    "    )\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\n",
                                    "        name=collection_name,\n",
                                    "        embedding_function=embed_func\n",
                                    "    )\n",
                                    "\n",
                                    "    texts = [c[\"text\"] for c in chunks]\n",
                                    "    ids = [f\"chunk_{c['doc_id']}_{c['chunk_id']}\" for c in chunks]\n",
                                    "    metadatas = [\n",
                                    "        {\n",
                                    "            \"doc_id\": c[\"doc_id\"],\n",
                                    "            \"chunk_id\": c[\"chunk_id\"],\n",
                                    "            \"category\": c[\"category\"]\n",
                                    "        }\n",
                                    "        for c in chunks\n",
                                    "    ]\n",
                                    "\n",
                                    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
                                    "\n",
                                    "    return collection\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    current_dir = os.path.dirname(__file__)\n",
                                    "    dataset_file = os.path.join(current_dir, \"data\", \"corpus.json\")\n",
                                    "\n",
                                    "    # Build the initial collection from chunked documents\n",
                                    "    chunked_docs = load_and_chunk_dataset(dataset_file, chunk_size=30)\n",
                                    "    collection = build_chroma_collection(chunked_docs)\n",
                                    "    total_docs = collection.count()\n",
                                    "    print(\"ChromaDB collection created with\", total_docs, \"documents.\")\n",
                                    "\n",
                                    "    # TODO: Create a new document dictionary with doc_id, chunk_id, category, and text\n",
                                    "    # The text should be \"Bananas are yellow fruits rich in potassium.\"\n",
                                    "\n",
                                    "    # TODO: Generate a unique ID string for the new document\n",
                                    "\n",
                                    "    # TODO: Add the new document to the collection using collection.add()\n",
                                    "    # Don't forget to include the document text, metadata, and ID\n",
                                    "\n",
                                    "    # TODO: Print the updated document count\n",
                                    "\n",
                                    "    # TODO: Remove the newly added document using collection.delete()\n",
                                    "\n",
                                    "    # TODO: Print the final document count\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Thought for a few seconds\n",
                                    "\n",
                                    "\n",
                                    "Here’s how you can complete the `__main__` section to add and then remove a new “banana” chunk:\n",
                                    "\n",
                                    "```python\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    current_dir = os.path.dirname(__file__)\n",
                                    "    dataset_file = os.path.join(current_dir, \"data\", \"corpus.json\")\n",
                                    "\n",
                                    "    # Build the initial collection from chunked documents\n",
                                    "    chunked_docs = load_and_chunk_dataset(dataset_file, chunk_size=30)\n",
                                    "    collection = build_chroma_collection(chunked_docs)\n",
                                    "    initial_count = collection.count()\n",
                                    "    print(\"ChromaDB collection created with\", initial_count, \"documents.\")\n",
                                    "\n",
                                    "    # ─── TODO: Create a new chunk dict ──────────────────────────────────────────\n",
                                    "    # Use a doc_id that doesn't collide with existing ones; here we pick len(chunked_docs)\n",
                                    "    new_chunk = {\n",
                                    "        \"doc_id\": len(chunked_docs),\n",
                                    "        \"chunk_id\": 0,\n",
                                    "        \"category\": \"fruits\",\n",
                                    "        \"text\": \"Bananas are yellow fruits rich in potassium.\"\n",
                                    "    }\n",
                                    "\n",
                                    "    # ─── TODO: Generate a unique ID string for the new chunk ───────────────────\n",
                                    "    new_id = f\"chunk_{new_chunk['doc_id']}_{new_chunk['chunk_id']}\"\n",
                                    "\n",
                                    "    # ─── TODO: Add the new chunk to the collection ─────────────────────────────\n",
                                    "    collection.add(\n",
                                    "        documents=[ new_chunk[\"text\"] ],\n",
                                    "        metadatas=[ {\n",
                                    "            \"doc_id\": new_chunk[\"doc_id\"],\n",
                                    "            \"chunk_id\": new_chunk[\"chunk_id\"],\n",
                                    "            \"category\": new_chunk[\"category\"]\n",
                                    "        } ],\n",
                                    "        ids=[ new_id ]\n",
                                    "    )\n",
                                    "\n",
                                    "    # ─── TODO: Print the updated document count ────────────────────────────────\n",
                                    "    after_add_count = collection.count()\n",
                                    "    print(\"After adding, collection has\", after_add_count, \"documents.\")\n",
                                    "\n",
                                    "    # ─── TODO: Remove the newly added chunk ──────────────────────────────────\n",
                                    "    collection.delete(ids=[ new_id ])\n",
                                    "\n",
                                    "    # ─── TODO: Print the final document count ─────────────────────────────────\n",
                                    "    final_count = collection.count()\n",
                                    "    print(\"After deleting, collection has\", final_count, \"documents.\")\n",
                                    "```\n",
                                    "\n",
                                    "**What happens when you run this:**\n",
                                    "\n",
                                    "1. **Initial build** prints something like\n",
                                    "\n",
                                    "   ```\n",
                                    "   ChromaDB collection created with 200 documents.\n",
                                    "   ```\n",
                                    "2. **After adding** prints\n",
                                    "\n",
                                    "   ```\n",
                                    "   After adding, collection has 201 documents.\n",
                                    "   ```\n",
                                    "3. **After deleting** prints\n",
                                    "\n",
                                    "   ```\n",
                                    "   After deleting, collection has 200 documents.\n",
                                    "   ```\n",
                                    "\n",
                                    "And now you’ve dynamically added and removed a chunk in your ChromaDB collection!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Dynamic Document Management in ChromaDB\n",
                                    "\n",
                                    "Congratulations on mastering the basics of adding and removing individual chunks from a ChromaDB collection! Now, let's elevate your skills by implementing a function that dynamically manages your collection based on content.\n",
                                    "\n",
                                    "Your objective is to create a function that efficiently deletes all chunks from a ChromaDB collection containing a specific keyword. This will help you maintain a clean and relevant dataset, ensuring that only the most pertinent information is stored.\n",
                                    "\n",
                                    "Dive in and enjoy the process of refining your ChromaDB collection!\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "\n",
                                    "\n",
                                    "def chunk_text(text, chunk_size=50):\n",
                                    "    \"\"\"\n",
                                    "    Splits the given text into chunks of size 'chunk_size'.\n",
                                    "    Returns a list of chunk strings.\n",
                                    "    \"\"\"\n",
                                    "    words = text.split()\n",
                                    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
                                    "\n",
                                    "\n",
                                    "def load_and_chunk_dataset(file_path, chunk_size=50):\n",
                                    "    \"\"\"\n",
                                    "    Loads a dataset from JSON 'file_path', then splits each document into smaller chunks.\n",
                                    "    Metadata such as 'doc_id' and 'category' is included with each chunk.\n",
                                    "    \"\"\"\n",
                                    "    with open(file_path, \"r\") as f:\n",
                                    "        data = json.load(f)\n",
                                    "\n",
                                    "    all_chunks = []\n",
                                    "    for doc_id, doc in enumerate(data):\n",
                                    "        doc_text = doc[\"content\"]\n",
                                    "        doc_category = doc.get(\"category\", \"general\")  # Default to \"general\" if no category.\n",
                                    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
                                    "\n",
                                    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
                                    "            all_chunks.append({\n",
                                    "                \"doc_id\": doc_id,\n",
                                    "                \"chunk_id\": chunk_id,\n",
                                    "                \"category\": doc_category,\n",
                                    "                \"text\": chunk_str\n",
                                    "            })\n",
                                    "    return all_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
                                    "    \"\"\"\n",
                                    "    Builds or retrieves a ChromaDB collection, embedding each chunk using a SentenceTransformer.\n",
                                    "    Adds all chunks in the 'chunks' list to the collection for fast retrieval.\n",
                                    "    \"\"\"\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(name=collection_name, embedding_function=embed_func)\n",
                                    "\n",
                                    "    texts = [c[\"text\"] for c in chunks]\n",
                                    "    ids = [f\"chunk_{c['doc_id']}_{c['chunk_id']}\" for c in chunks]\n",
                                    "    metadatas = [\n",
                                    "        {\"doc_id\": c[\"doc_id\"], \"chunk_id\": c[\"chunk_id\"], \"category\": c[\"category\"]}\n",
                                    "        for c in chunks\n",
                                    "    ]\n",
                                    "\n",
                                    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
                                    "    return collection\n",
                                    "\n",
                                    "\n",
                                    "def delete_documents_with_keyword(collection, keyword):\n",
                                    "    \"\"\"\n",
                                    "    Deletes all documents from the given ChromaDB 'collection' whose text contains 'keyword'.\n",
                                    "    \"\"\"\n",
                                    "    # TODO: Get all documents and their IDs from the collection\n",
                                    "\n",
                                    "    # TODO: Create a list to store IDs of documents containing the keyword\n",
                                    "\n",
                                    "    # TODO: Iterate through documents and their IDs, adding matching document IDs to the list\n",
                                    "\n",
                                    "    # TODO: If there are documents to delete, remove them from the collection\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    current_dir = os.path.dirname(__file__)\n",
                                    "    dataset_file = os.path.join(current_dir, \"data\", \"corpus.json\")\n",
                                    "\n",
                                    "    # Load and chunk the dataset, then build the initial collection.\n",
                                    "    chunked_docs = load_and_chunk_dataset(dataset_file)\n",
                                    "    collection = build_chroma_collection(chunked_docs)\n",
                                    "    initial_count = collection.count()\n",
                                    "    print(\"ChromaDB collection created with\", initial_count, \"documents.\")\n",
                                    "\n",
                                    "    # Add a new document containing \"Bananas\".\n",
                                    "    new_document = {\n",
                                    "        \"doc_id\": initial_count + 1,\n",
                                    "        \"chunk_id\": 0,\n",
                                    "        \"category\": \"food\",\n",
                                    "        \"text\": \"Bananas are yellow fruits rich in potassium.\"\n",
                                    "    }\n",
                                    "    doc_id_str = f\"chunk_{new_document['doc_id']}_{new_document['chunk_id']}\"\n",
                                    "\n",
                                    "    collection.add(\n",
                                    "        documents=[new_document[\"text\"]],\n",
                                    "        metadatas=[{\n",
                                    "            \"doc_id\": new_document[\"doc_id\"],\n",
                                    "            \"chunk_id\": new_document[\"chunk_id\"],\n",
                                    "            \"category\": new_document[\"category\"]\n",
                                    "        }],\n",
                                    "        ids=[doc_id_str]\n",
                                    "    )\n",
                                    "\n",
                                    "    updated_count = collection.count()\n",
                                    "    print(\"After adding keyword document, collection has\", updated_count, \"documents.\")\n",
                                    "\n",
                                    "    # Now delete all documents containing the keyword \"Bananas\".\n",
                                    "    delete_documents_with_keyword(collection, \"Bananas\")\n",
                                    "\n",
                                    "    final_count = collection.count()\n",
                                    "    print(\"After deleting documents with 'Bananas', collection has\", final_count, \"documents.\")\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "**Implementing `delete_documents_with_keyword` function**\n",
                                    "\n",
                                    "It looks like the user is working with Python code and needs help implementing a `delete_documents_with_keyword` function for a ChromaDB collection. Since they’ve provided a code snippet and want the function included, I should create a full implementation. I'll go ahead and write the code that accomplishes getting all documents in the collection, filtering by keyword, and deleting them as needed. I’ll make sure the code is complete—so I’ll create the full Python file for them.\n",
                                    "\n",
                                    "**Implementing `delete_documents_with_keyword` function**\n",
                                    "\n",
                                    "In order to implement this function in ChromaDB, I’ll need to fetch all the documents from the collection using `.get()`. The method `collection.get(ids=ids)` returns every fetched document’s `ids`, `documents`, and `metadatas`. Next, I'll filter the documents with the keyword and collect the matching IDs. Then I can delete them using `collection.delete(ids=ids_to_remove)`. I'll prepare all of that as full Python code and provide it in a text file for the user’s convenience, following the proper guidelines for file creation.\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import json\n",
                                    "from chromadb import Client\n",
                                    "from chromadb.config import Settings\n",
                                    "from chromadb.utils import embedding_functions\n",
                                    "\n",
                                    "\n",
                                    "def chunk_text(text, chunk_size=50):\n",
                                    "    \"\"\"\n",
                                    "    Splits the given text into chunks of size 'chunk_size'.\n",
                                    "    Returns a list of chunk strings.\n",
                                    "    \"\"\"\n",
                                    "    words = text.split()\n",
                                    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
                                    "\n",
                                    "\n",
                                    "def load_and_chunk_dataset(file_path, chunk_size=50):\n",
                                    "    \"\"\"\n",
                                    "    Loads a dataset from JSON 'file_path', then splits each document into smaller chunks.\n",
                                    "    Metadata such as 'doc_id' and 'category' is included with each chunk.\n",
                                    "    \"\"\"\n",
                                    "    with open(file_path, \"r\") as f:\n",
                                    "        data = json.load(f)\n",
                                    "\n",
                                    "    all_chunks = []\n",
                                    "    for doc_id, doc in enumerate(data):\n",
                                    "        doc_text = doc.get(\"content\", \"\")\n",
                                    "        doc_category = doc.get(\"category\", \"general\")  # Default to \"general\"\n",
                                    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
                                    "\n",
                                    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
                                    "            all_chunks.append({\n",
                                    "                \"doc_id\": doc_id,\n",
                                    "                \"chunk_id\": chunk_id,\n",
                                    "                \"category\": doc_category,\n",
                                    "                \"text\": chunk_str\n",
                                    "            })\n",
                                    "    return all_chunks\n",
                                    "\n",
                                    "\n",
                                    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
                                    "    \"\"\"\n",
                                    "    Builds or retrieves a ChromaDB collection, embedding each chunk using a SentenceTransformer.\n",
                                    "    Adds all chunks in the 'chunks' list to the collection for fast retrieval.\n",
                                    "    \"\"\"\n",
                                    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                                    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                                    "    client = Client(Settings())\n",
                                    "    collection = client.get_or_create_collection(\n",
                                    "        name=collection_name,\n",
                                    "        embedding_function=embed_func\n",
                                    "    )\n",
                                    "\n",
                                    "    texts = [c[\"text\"] for c in chunks]\n",
                                    "    ids = [f\"chunk_{c['doc_id']}_{c['chunk_id']}\" for c in chunks]\n",
                                    "    metadatas = [\n",
                                    "        {\"doc_id\": c[\"doc_id\"], \"chunk_id\": c[\"chunk_id\"], \"category\": c[\"category\"]}\n",
                                    "        for c in chunks\n",
                                    "    ]\n",
                                    "\n",
                                    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
                                    "    return collection\n",
                                    "\n",
                                    "\n",
                                    "def delete_documents_with_keyword(collection, keyword):\n",
                                    "    \"\"\"\n",
                                    "    Deletes all documents from the given ChromaDB 'collection' whose text contains 'keyword'.\n",
                                    "    \"\"\"\n",
                                    "    # Retrieve all documents (ids and texts)\n",
                                    "    results = collection.get(include=[\"ids\", \"documents\"])\n",
                                    "    ids = results.get('ids', [])\n",
                                    "    docs = results.get('documents', [])\n",
                                    "\n",
                                    "    # Identify document IDs that contain the keyword\n",
                                    "    ids_to_delete = [doc_id for doc_id, text in zip(ids, docs) if keyword in text]\n",
                                    "\n",
                                    "    # Delete matching documents\n",
                                    "    if ids_to_delete:\n",
                                    "        collection.delete(ids=ids_to_delete)\n",
                                    "        print(f\"Deleted {len(ids_to_delete)} documents containing keyword '{keyword}'.\")\n",
                                    "    else:\n",
                                    "        print(f\"No documents found containing keyword '{keyword}'.\")\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    current_dir = os.path.dirname(__file__)\n",
                                    "    dataset_file = os.path.join(current_dir, \"data\", \"corpus.json\")\n",
                                    "\n",
                                    "    # Load and chunk the dataset, then build the initial collection.\n",
                                    "    chunked_docs = load_and_chunk_dataset(dataset_file)\n",
                                    "    collection = build_chroma_collection(chunked_docs)\n",
                                    "    initial_count = collection.count()\n",
                                    "    print(\"ChromaDB collection created with\", initial_count, \"documents.\")\n",
                                    "\n",
                                    "    # Add a new document containing \"Bananas\".\n",
                                    "    new_document = {\n",
                                    "        \"doc_id\": initial_count + 1,\n",
                                    "        \"chunk_id\": 0,\n",
                                    "        \"category\": \"food\",\n",
                                    "        \"text\": \"Bananas are yellow fruits rich in potassium.\"\n",
                                    "    }\n",
                                    "    doc_id_str = f\"chunk_{new_document['doc_id']}_{new_document['chunk_id']}\"\n",
                                    "\n",
                                    "    collection.add(\n",
                                    "        documents=[new_document[\"text\"]],\n",
                                    "        metadatas=[{\n",
                                    "            \"doc_id\": new_document[\"doc_id\"],\n",
                                    "            \"chunk_id\": new_document[\"chunk_id\"],\n",
                                    "            \"category\": new_document[\"category\"]\n",
                                    "        }],\n",
                                    "        ids=[doc_id_str]\n",
                                    "    )\n",
                                    "\n",
                                    "    updated_count = collection.count()\n",
                                    "    print(\"After adding keyword document, collection has\", updated_count, \"documents.\")\n",
                                    "\n",
                                    "    # Now delete all documents containing the keyword \"Bananas\".\n",
                                    "    delete_documents_with_keyword(collection, \"Bananas\")\n",
                                    "\n",
                                    "    final_count = collection.count()\n",
                                    "    print(\"After deleting documents with 'Bananas', collection has\", final_count, \"documents.\")\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Saya sudah menambahkan implementasi lengkap untuk fungsi `delete_documents_with_keyword`, termasuk:\n",
                                    "\n",
                                    "* Mengambil semua dokumen (IDs dan teks) menggunakan `collection.get()`.\n",
                                    "* Menyaring ID dokumen yang mengandung kata kunci.\n",
                                    "* Menghapus dokumen yang sesuai dan mencetak ringkasan penghapusan.\n",
                                    "\n",
                                    "Silakan cek file `dynamic_chroma_management.py` di canvas untuk detail implementasinya!\n"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
