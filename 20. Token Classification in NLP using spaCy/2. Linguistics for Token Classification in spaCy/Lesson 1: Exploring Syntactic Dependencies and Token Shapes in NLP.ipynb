{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 1: Exploring Syntactic Dependencies and Token Shapes in NLP\n",
                                    "\n",
                                    "### Introduction to Syntactic Dependencies\n",
                                    "\n",
                                    "Hello, welcome to the next step in your linguistic journey! In today's lesson, we'll expand upon your foundational linguistic knowledge and step into the world of syntactic dependencies and token shapes. This knowledge will equip you to delve even deeper into the fascinating realm of Natural Language Processing (NLP).\n",
                                    "\n",
                                    "The first stop in our journey is syntactic dependencies. So, what are they? Simply put, syntactic dependencies are the grammatical relationships between words in a sentence. This could be a subject-verb relationship, an adjective-noun relationship, or other types of grammatical relations. Why are they important in NLP? They help us understand how words relate to each other and how they come together to convey meaning in a sentence.\n",
                                    "\n",
                                    "In Python, with the help of SpaCy, we can extract these dependencies easily. Let's take a look at how to do this with our sample text from the Reuters corpus.\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Let's load the English NLP model\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Take a sample text from reuters corpus\n",
                                    "sample_text = reuters.raw(reuters.fileids()[0])\n",
                                    "\n",
                                    "# Pass the text to the nlp object\n",
                                    "doc = nlp(sample_text)\n",
                                    "\n",
                                    "# For syntactic tokens\n",
                                    "print('\\nSyntactic Dependencies:\\n')\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text:<10s} {token.dep_:<10s} {token.head.text:<10s}\")\n",
                                    "```\n",
                                    "\n",
                                    "In each line of output, the first word is the token, the second word is the type of syntactic dependency, and the third word is the head of the token. The head of a token is typically the word that governs the relationship between the words. This simple code gives us a depth of insight into the grammatical structure of the text!\n",
                                    "\n",
                                    "### Unpacking Syntactic Dependencies Output\n",
                                    "\n",
                                    "Alright, let's take a concrete look at the potential output our syntactic dependencies code could produce.\n",
                                    "\n",
                                    "```\n",
                                    "ASIAN      compound   EXPORTERS \n",
                                    "EXPORTERS  nsubj      FEAR      \n",
                                    "FEAR       ccomp      said      \n",
                                    "DAMAGE     nsubj      raised    \n",
                                    "FROM       prep       DAMAGE    \n",
                                    "U.S.-JAPAN compound   friction  \n",
                                    "```\n",
                                    "\n",
                                    "Even at first glance, we can already start to see patterns and relationships emerge from this data. However, to truly gain insights, we must understand what these outcome values mean:\n",
                                    "\n",
                                    "- **ASIAN**: Here, \"ASIAN\" has a compound dependency type. A compound relationship is formed when two nouns come together to form a new noun, such as \"ASIAN EXPORTERS\".\n",
                                    "- **EXPORTERS**: The nominal subject (nsubj) of the verb \"FEAR\" is \"EXPORTERS\". The nominal subject is typically the \"doer\" of the action and corresponds to \"who\" or \"what\" in the sentence.\n",
                                    "- **FEAR**: The ccomp in this case stands for clausal complement, referring to \"FEAR\". These complements are subclauses that provide additional information but usually can't make sense as separate sentences.\n",
                                    "- **DAMAGE**: It is considered to be the nominal subject (nsubj) for the verb \"raised\".\n",
                                    "- **FROM**: Labeled with a prep, which stands for preposition, \"FROM\" provides a relationship between \"U.S.-JAPAN\" and another word in the sentence.\n",
                                    "\n",
                                    "Besides these, there are different types of dependencies that you might encounter as well:\n",
                                    "\n",
                                    "- **relcl**: It stands for relative clause modifier. They use words like \"who\" or \"which\" to provide more detail about the noun.\n",
                                    "- **dobj**: Denotes direct object. This may be the noun or noun phrase that is receiving the action in the sentence.\n",
                                    "- **ROOT**: This is the main verb in any given sentence, to which all other words are connected in a manner that is either direct or indirect.\n",
                                    "- **nsubjpass**: This refers to the nominal subject in a passive sentence. In such sentences, the subject is usually receiving the action of the verb.\n",
                                    "- **pobj**: Stands for object of a preposition. This is usually the noun coming after the preposition in the sentence.\n",
                                    "\n",
                                    "Finally, remember that understanding these dependencies is vital if you want to dive deeply into the grammatical structure and meaning of a sentence. Now that we've dissected syntactic dependencies output, let's move on to our next interesting segment - the exploration of token shapes.\n",
                                    "\n",
                                    "### Delving into Token Shapes\n",
                                    "\n",
                                    "The next concept we'll explore is token shapes. A token shape is a type of transformation applied to the string representation of a token to provide a description of its orthographic structure — in other words, its shape focuses on the form of characters rather than their actual content.\n",
                                    "\n",
                                    "Here's how the transformation works:\n",
                                    "\n",
                                    "- Alphabetic characters are replaced by x or X. Lowercase characters become x and uppercase characters become X.\n",
                                    "- Numeric characters are replaced by d.\n",
                                    "- Sequences of the same character are truncated after length 4.\n",
                                    "\n",
                                    "For example, a word like \"Python\" has an initial uppercase letter followed by lowercase letters, and thus gets transformed to \"Xxxxx\".\n",
                                    "\n",
                                    "Let's see how to get these token shapes using our example text:\n",
                                    "\n",
                                    "```python\n",
                                    "print('\\nToken Shapes:\\n')\n",
                                    "for token in doc[25:]:\n",
                                    "    print(f\"{token.text:<10s} {token.shape_:<10s}\")\n",
                                    "```\n",
                                    "\n",
                                    "When put to work, token shapes can provide valuable insights. You may realize, for instance, that uppercase words are typically proper nouns, and digits represent numerical values, among other patterns.\n",
                                    "\n",
                                    "### Understanding the Token Shapes Output\n",
                                    "\n",
                                    "Looking at the output produced by our code:\n",
                                    "\n",
                                    "```\n",
                                    "seven      xxxx      \n",
                                    "and        xxx       \n",
                                    "12         dd        \n",
                                    "pct        xxx       \n",
                                    "of         xx        \n",
                                    "China      Xxxxx     \n",
                                    "'s         'x     \n",
                                    "```\n",
                                    "\n",
                                    "Here's how to interpret these shapes:\n",
                                    "\n",
                                    "- **seven**: The shape xxxx conveys that \"seven\" is composed of lowercase letters, hinting at its alphabetic nature without indicating specific letters, which helps in analyzing text patterns while abstracting away the details. Note that the shape was truncated to 4 characters.\n",
                                    "- **and**: With a shape of xxx, this indicates that \"and\" consists of three lowercase letters. This distinct shape aids in recognizing small, commonly used words in analyses.\n",
                                    "- **12**: Represented as dd, it clearly illustrates that \"12\" is a numeric token, consisting of two digits. This differentiation is vital for tasks that require numeric value processing or identification.\n",
                                    "- **pct**: The token \"pct\" is shown with a shape xxx, indicating three lowercase letters.\n",
                                    "- **of**: Its shape xx succinctly reflects that \"of\" is a short, two-letter word, all in lowercase. Recognizing such functional tokens is crucial for understanding the grammatical structure of sentences.\n",
                                    "- **China**: The shape Xxxxx signals that \"China\" starts with an uppercase letter followed by lowercase letters, a characteristic feature of proper nouns. This insight is fundamental for tasks like Named Entity Recognition, as it distinguishes proper nouns from other text elements.\n",
                                    "- **'s**: With a shape of 'x, this combination suggests the presence of a punctuation mark followed by a lowercase letter, a common feature in possessive constructions or contractions. Identifying these constructions is essential for parsing and understanding sentence structures.\n",
                                    "\n",
                                    "With this understanding of token shapes, you can now integrate this intelligence into your NLP tasks, yielding even more insightful results!\n",
                                    "\n",
                                    "### Experience the Power of Linguistics\n",
                                    "\n",
                                    "Now that we've extracted syntactic dependencies and token shapes from our text, let's take a moment to reflect on the insights that these features offer. First, the syntactic dependencies give us a good understanding of the grammatical structure of the text. This can be extremely helpful when we're trying to parse sentences and understand the relationships between words.\n",
                                    "\n",
                                    "On the other hand, token shapes allow us to observe patterns in the structure of words. This can be especially useful in tasks such as spam detection, where certain patterns of words or characters might be more common.\n",
                                    "\n",
                                    "On the whole, understanding these linguistic features provides us with a deeper understanding of text, equipping us to perform more nuanced analyses.\n",
                                    "\n",
                                    "### Lesson Summary and Practice\n",
                                    "\n",
                                    "Congratulations on completing this detailed journey into syntactic dependencies and token shapes! You've not only learned what these concepts are, but have also extracted them from a text using Python and spaCy. Remember, linguistics is at the heart of Natural Language Processing, and understanding these features will stand you in good stead for more advanced tasks in this field.\n",
                                    "\n",
                                    "In the upcoming practice exercises, you'll have the opportunity to apply these concepts to various texts. This practice will solidify your understanding and prepare you for the next lesson, where we'll explore the intricacies of semantics in NLP. Happy learning!Executed 1st Code Block\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Filtering Syntactic Dependencies and Token Shapes\n",
                                    "\n",
                                    "Excellent work, Stellar Navigator! Let's further manipulate our textual analysis:\n",
                                    "\n",
                                    "Change the syntactic dependency filter to 'relcl' instead of 'dobj'.\n",
                                    "Alter the token shape filter to select those that are entirely digits.\n",
                                    "Go ahead and adjust the code, then observe your new output.\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Let's load the English NLP model\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Take a sample text from reuters corpus\n",
                                    "sample_text = reuters.raw(reuters.fileids()[0])\n",
                                    "\n",
                                    "# Pass the text to the nlp object\n",
                                    "doc = nlp(sample_text)\n",
                                    "\n",
                                    "# For syntactic tokens\n",
                                    "print('\\nSyntactic Dependencies:\\n')\n",
                                    "for token in doc:\n",
                                    "    if token.dep_ == 'dobj':\n",
                                    "        print(f\"{token.text:<10s} {token.dep_:<10s} {token.head.text:<10s}\")\n",
                                    "\n",
                                    "# For token shapes\n",
                                    "print('\\nToken Shapes:\\n')\n",
                                    "for token in doc:\n",
                                    "    if all(char == 'x' for char in token.shape_):\n",
                                    "        print(f\"{token.text:<10s} {token.shape_:<10s}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here’s the adjusted code to filter syntactic dependencies for 'relcl' instead of 'dobj' and to select token shapes that are entirely digits:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Let's load the English NLP model\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Take a sample text from reuters corpus\n",
                                    "sample_text = reuters.raw(reuters.fileids()[0])\n",
                                    "\n",
                                    "# Pass the text to the nlp object\n",
                                    "doc = nlp(sample_text)\n",
                                    "\n",
                                    "# For syntactic tokens\n",
                                    "print('\\nSyntactic Dependencies:\\n')\n",
                                    "for token in doc:\n",
                                    "    if token.dep_ == 'relcl':  # Changed from 'dobj' to 'relcl'\n",
                                    "        print(f\"{token.text:<10s} {token.dep_:<10s} {token.head.text:<10s}\")\n",
                                    "\n",
                                    "# For token shapes\n",
                                    "print('\\nToken Shapes:\\n')\n",
                                    "for token in doc:\n",
                                    "    if token.shape_ == 'dd':  # Changed to select tokens that are entirely digits\n",
                                    "        print(f\"{token.text:<10s} {token.shape_:<10s}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Key Changes:\n",
                                    "\n",
                                    "1. **Syntactic Dependency Filter:**  \n",
                                    "   The filter for syntactic dependencies has been changed from `dobj` to `relcl`, which will now display tokens that are relative clause modifiers.\n",
                                    "\n",
                                    "2. **Token Shape Filter:**  \n",
                                    "   The condition for token shapes has been modified to check if the shape is exactly 'dd', which indicates that the token consists entirely of digits.\n",
                                    "\n",
                                    "### Expected Output:\n",
                                    "\n",
                                    "When you run this code, you will see two sections of output:\n",
                                    "\n",
                                    "1. **Syntactic Dependencies:**  \n",
                                    "   This will list all tokens that have a syntactic dependency of 'relcl', along with their respective heads.\n",
                                    "\n",
                                    "2. **Token Shapes:**  \n",
                                    "   This will display tokens that are entirely numeric, along with their shape representation.\n",
                                    "\n",
                                    "This adjustment will help you analyze the specific syntactic relationships and numeric tokens in the text from the Reuters corpus. If you have any further questions or need additional modifications, feel free to ask!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Filtering Specific Syntactic Dependencies and Token Shapes\n",
                                    "\n",
                                    "Excellent work, Stellar Navigator! Now, let's make some changes to our linguistic feature extraction code. Your task is to modify this code to only print out the syntactic dependencies of type 'nsubj' and token shapes that are fully uppercase (using isupper function on shape_ object). Both of these changes need to be reflected in your final output. Good luck!\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Let's load the English NLP model\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Take a sample text from reuters corpus\n",
                                    "sample_text = reuters.raw(reuters.fileids()[0])\n",
                                    "\n",
                                    "# Pass the text to the nlp object\n",
                                    "doc = nlp(sample_text)\n",
                                    "\n",
                                    "# For syntactic tokens\n",
                                    "print('\\nSyntactic Dependencies:\\n')\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text:<10s} {token.dep_:<10s} {token.head.text:<10s}\")\n",
                                    "\n",
                                    "# For token shapes\n",
                                    "print('\\nToken Shapes:\\n')\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text:<10s} {token.shape_:<10s}\")\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To modify the provided code to filter for syntactic dependencies of type 'nsubj' and token shapes that are fully uppercase, you can adjust the loops accordingly. Here’s the updated code:\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Let's load the English NLP model\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Take a sample text from reuters corpus\n",
                                    "sample_text = reuters.raw(reuters.fileids()[0])\n",
                                    "\n",
                                    "# Pass the text to the nlp object\n",
                                    "doc = nlp(sample_text)\n",
                                    "\n",
                                    "# For syntactic tokens of type 'nsubj'\n",
                                    "print('\\nSyntactic Dependencies (nsubj):\\n')\n",
                                    "for token in doc:\n",
                                    "    if token.dep_ == 'nsubj':\n",
                                    "        print(f\"{token.text:<10s} {token.dep_:<10s} {token.head.text:<10s}\")\n",
                                    "\n",
                                    "# For token shapes that are fully uppercase\n",
                                    "print('\\nToken Shapes (Fully Uppercase):\\n')\n",
                                    "for token in doc:\n",
                                    "    if token.shape_.isupper():\n",
                                    "        print(f\"{token.text:<10s} {token.shape_:<10s}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Changes Made:\n",
                                    "1. In the first loop, a condition checks if `token.dep_` is equal to `'nsubj'` before printing the token details.\n",
                                    "2. In the second loop, a condition checks if `token.shape_` is fully uppercase using the `isupper()` method before printing the token shapes.\n",
                                    "\n",
                                    "This will ensure that only the relevant syntactic dependencies and token shapes are printed in the output."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Creating Sentence with Unique Dependency and Shape\n",
                                    "\n",
                                    "Nice work, Stellar Navigator! Ready for further exploration? Fill in the missing sections, marked with ____, to create a sentence that includes at least one token starting with a number (shape 'dxxxx') and a syntactic dependency of 'ROOT'. Let's bring it into reality!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "\n",
                                    "# Let's load the English NLP model\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Provide a sentence with a token having a shape 'dxxxx' and a syntactic dependency 'ROOT'\n",
                                    "sentence = ____\n",
                                    "\n",
                                    "# Pass the sentence to the nlp object\n",
                                    "doc = nlp(sentence)\n",
                                    "\n",
                                    "# For syntactic tokens\n",
                                    "print('\\nSyntactic Dependencies:\\n')\n",
                                    "for token in doc:\n",
                                    "    if ____:\n",
                                    "        print(f\"{token.text:<10s} {token.dep_:<10s} {token.head.text:<10s}\")\n",
                                    "\n",
                                    "# For token shapes\n",
                                    "print('\\nToken Shapes:\\n')\n",
                                    "for token in doc:\n",
                                    "    if ____:\n",
                                    "        print(f\"{token.text:<10s} {token.shape_:<10s}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To create a sentence that includes at least one token starting with a number (shape 'dxxxx') and a syntactic dependency of 'ROOT', you can fill in the missing sections as follows:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "\n",
                                    "# Let's load the English NLP model\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Provide a sentence with a token having a shape 'dxxxx' and a syntactic dependency 'ROOT'\n",
                                    "sentence = \"2023 is a year of significant changes.\"\n",
                                    "\n",
                                    "# Pass the sentence to the nlp object\n",
                                    "doc = nlp(sentence)\n",
                                    "\n",
                                    "# For syntactic tokens\n",
                                    "print('\\nSyntactic Dependencies:\\n')\n",
                                    "for token in doc:\n",
                                    "    if token.dep_ == 'ROOT':\n",
                                    "        print(f\"{token.text:<10s} {token.dep_:<10s} {token.head.text:<10s}\")\n",
                                    "\n",
                                    "# For token shapes\n",
                                    "print('\\nToken Shapes:\\n')\n",
                                    "for token in doc:\n",
                                    "    if token.shape_ == 'dxxxx':\n",
                                    "        print(f\"{token.text:<10s} {token.shape_:<10s}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation of the Filled Sections:\n",
                                    "1. **Sentence**: \"2023 is a year of significant changes.\" This sentence includes the token \"2023,\" which starts with a number and has the shape 'dxxxx'.\n",
                                    "2. **Syntactic Dependency Check**: The condition checks if `token.dep_` is equal to 'ROOT'.\n",
                                    "3. **Token Shape Check**: The condition checks if `token.shape_` is equal to 'dxxxx'.\n",
                                    "\n",
                                    "This code will output the syntactic dependencies and token shapes as specified."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Syntactic Dependencies and Token Shapes Filtering\n",
                                    "\n",
                                    "Great strides, Stellar Navigator! Next stop: code additions!\n",
                                    "\n",
                                    "Fill in the missing lines to print syntactic dependencies of the type 'nsubjpass' and token shapes that are not purely alphanumeric (.isalnum can be helpful here). Take your time, and trust your learning. You've got this!\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Let's load the English NLP model\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Take a sample text from reuters corpus\n",
                                    "sample_text = reuters.raw(reuters.fileids()[0])\n",
                                    "\n",
                                    "# Pass the text to the nlp object\n",
                                    "doc = nlp(sample_text)\n",
                                    "\n",
                                    "# For syntactic tokens\n",
                                    "print('\\nSyntactic Dependencies:\\n')\n",
                                    "# TODO: Add a loop to print syntactic dependencies of type 'nsubjpass'\n",
                                    "\n",
                                    "# For token shapes\n",
                                    "print('\\nToken Shapes:\\n')\n",
                                    "# TODO: Add a loop to print token shapes not purely alphanumeric\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "To complete the code by adding loops that print syntactic dependencies of type 'nsubjpass' and token shapes that are not purely alphanumeric, you can fill in the missing lines as follows:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Let's load the English NLP model\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Take a sample text from reuters corpus\n",
                                    "sample_text = reuters.raw(reuters.fileids()[0])\n",
                                    "\n",
                                    "# Pass the text to the nlp object\n",
                                    "doc = nlp(sample_text)\n",
                                    "\n",
                                    "# For syntactic tokens\n",
                                    "print('\\nSyntactic Dependencies:\\n')\n",
                                    "for token in doc:\n",
                                    "    if token.dep_ == 'nsubjpass':\n",
                                    "        print(f\"{token.text:<10s} {token.dep_:<10s} {token.head.text:<10s}\")\n",
                                    "\n",
                                    "# For token shapes\n",
                                    "print('\\nToken Shapes:\\n')\n",
                                    "for token in doc:\n",
                                    "    if not token.text.isalnum():\n",
                                    "        print(f\"{token.text:<10s} {token.shape_:<10s}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation of the Added Lines:\n",
                                    "1. **Syntactic Dependencies Loop**: The loop iterates through each token in the document and checks if the syntactic dependency (`token.dep_`) is equal to 'nsubjpass'. If it is, it prints the token text, its dependency type, and the head token's text.\n",
                                    "  \n",
                                    "2. **Token Shapes Loop**: This loop checks if the token text is not purely alphanumeric using `not token.text.isalnum()`. If the condition is met, it prints the token text and its shape.\n",
                                    "\n",
                                    "This code will effectively filter and display the desired syntactic dependencies and token shapes."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Filtering Syntactic Dependencies and Numerically Initiated Token Shapes\n",
                                    "\n",
                                    "Stellar Navigator, you have demonstrated outstanding groundwork in linguistics. It is now time for the grand finale of this lesson. Assemble a comprehensive Python code that utilizes the SpaCy NLP model to:\n",
                                    "\n",
                                    "Process the text from the Reuters corpus.\n",
                                    "Filter and print tokens with the 'pobj' syntactic dependency.\n",
                                    "Display token shapes for tokens that start with a numeric digit.\n",
                                    "Trust your learning - you can do this!\n",
                                    "\n",
                                    "```python\n",
                                    "# TODO: Import necessary libraries, load the English NLP model, and take a sample text from Reuters corpus\n",
                                    "\n",
                                    "# TODO: Create an NLP object and pass the sample text to it\n",
                                    "\n",
                                    "# TODO: Write a loop to print syntactic dependencies of type 'pobj'\n",
                                    "\n",
                                    "# TODO: Write a loop to print token shapes that start with a digit\n",
                                    "\n",
                                    "\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
