{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 5: Expanding the spaCy NLP Pipeline with Custom Components\n",
                                    "\n",
                                    "\n",
                                    "Welcome to this lesson on expanding the Natural Language Processing (NLP) pipeline with custom components using the spaCy library. Today, we're going to focus on adding extensions in two ways: using a pipeline component or using a getter for precomputing results. You'll learn when to use each method and practice creating meaningful custom components.\n",
                                    "\n",
                                    "## Understanding spaCy Extensions\n",
                                    "\n",
                                    "Extensions in spaCy are an efficient and flexible system for adding extra functionality to the built-in Doc, Token, and Span objects, as well as some other classes such as Language and Vocab. They can be used to add more information to a Token, for example, the length of the sentence where the token is found.\n",
                                    "\n",
                                    "Getter-based extensions are recommended when the attribute computation is straightforward, efficient, and highly dependent on the single Token instance. Such extensions are dynamically computed at the time of access, ensuring up-to-date and context-specific information without upfront computational overhead.\n",
                                    "\n",
                                    "Let's look at an example of a getter-based extension using the Reuters text corpus.\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "# Adding extensions with a getter\n",
                                    "Token.set_extension(\"sentence_len\", getter=lambda token: len(token.sent))\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "```\n",
                                    "\n",
                                    "Here, we created a simple getter-based extension that computes the length of the sentence in which each token is present.\n",
                                    "\n",
                                    "## Accessing Extensions Created with a Getter\n",
                                    "\n",
                                    "To access the `sentence_len` extension for each token, use the following approach:\n",
                                    "\n",
                                    "```python\n",
                                    "# Accessing sentence length information\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text}: {token._.sentence_len}\")\n",
                                    "\n",
                                    "# Example output\n",
                                    "# JAPAN: 51\n",
                                    "# TO: 51\n",
                                    "# REVISE: 51\n",
                                    "# LONG: 51\n",
                                    "```\n",
                                    "\n",
                                    "## Creating a Phonetic Key Extension\n",
                                    "\n",
                                    "Now, let's add a more linguistically meaningful extension, which computes a simple linguistic feature.\n",
                                    "\n",
                                    "Consider phonetic similarity between words. For the sake of simplicity, we'll create a phonetic key consisting of the first two consonants of the word, or if they don't exist, the first two characters. Remember, real phonetic comparison would be much more elaborate and language-dependent.\n",
                                    "\n",
                                    "```python\n",
                                    "def get_phonetic_key(token):\n",
                                    "    non_vowels = [ch for ch in token.text.lower() if ch not in 'aeiou']\n",
                                    "    return ''.join(non_vowels[:2]) if len(non_vowels) > 1 else token.text.lower()[:2]\n",
                                    "\n",
                                    "Token.set_extension('phonetic_key', getter=get_phonetic_key)\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "```\n",
                                    "\n",
                                    "## Accessing the Phonetic Key Extension\n",
                                    "\n",
                                    "After creating the phonetic key extension, you can access it for each token as follows:\n",
                                    "\n",
                                    "```python\n",
                                    "# Accessing the phonetic_key extension:\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text}: {token._.phonetic_key}\")\n",
                                    "\n",
                                    "# Example output\n",
                                    "# JAPAN: jp\n",
                                    "# TO: to\n",
                                    "# REVISE: rv\n",
                                    "# LONG: ln\n",
                                    "```\n",
                                    "\n",
                                    "This segment demonstrates how to use getter-based extensions effectively and retrieve the additional data they provide.\n",
                                    "\n",
                                    "## Understanding spaCy Pipeline Components\n",
                                    "\n",
                                    "In spaCy, a pipeline component is a function that is handed a Doc object, performs a specific operation on it, and then returns the modified Doc. Pipeline components prove useful in adding more complex extensions, especially when the computation is intricate or depends on other Tokens. This function is executed automatically as part of the spaCy pipeline when we run the nlp text processing on a document. This means that during the nlp processing of a text, each pipeline component is invoked in sequence, allowing the modifications or enhancements implemented in the component to be computed and applied to the Doc object in real-time. This design ensures that by the time the full processing is complete, all custom operations defined in the pipeline components have been performed, integrating seamlessly with spaCy's built-in processes for a holistic NLP solution.\n",
                                    "\n",
                                    "Consider this example where we use a pipeline component to calculate the sentence length for each Token:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.language import Language\n",
                                    "from spacy.tokens import Token\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "Token.set_extension('sentence_len', default=None)\n",
                                    "\n",
                                    "@Language.component(\"sentence_len_component\")\n",
                                    "def sentence_len_component(doc): \n",
                                    "    for sent in doc.sents:\n",
                                    "        sent_len = len(sent)\n",
                                    "        for token in sent:\n",
                                    "            token._.set('sentence_len', sent_len)\n",
                                    "    return doc\n",
                                    "\n",
                                    "nlp.add_pipe(\"sentence_len_component\", last=True)\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "```\n",
                                    "\n",
                                    "The `sentence_len_component` calculates the length of each sentence in the Doc and associates this length with every Token within that sentence. This is achieved through `token._.set('sentence_len', sent_len)`, where `token._.set(...)` is used to dynamically assign the computed sentence length to each Token.\n",
                                    "\n",
                                    "The `Language.component` decorator is a powerful feature used to craft custom pipeline components capable of computing and assigning sentence lengths to tokens. Defining a pipeline component is only the first step; integrating it into the spaCy NLP pipeline is a seamless process facilitated by the `add_pipe` method. The first argument of `add_pipe` determines the component to incorporate into the pipeline. For our scenario, we're including the \"sentence_len_component\". The `last=True` parameter, passed to `add_pipe`, ensures this component is appended at the end of the pipeline. Positioning it last is crucial for ensuring that all preceding analyses and modifications have been completed on the Doc object, which allows our custom component to operate with the most current version of the document, incorporating any updates made by earlier components. This strategic placement maximizes the effectiveness and accuracy of the sentence length computation, as it relies on the context established by the pipeline's prior stages.\n",
                                    "\n",
                                    "With this arrangement, each Token is now enriched with data regarding its sentence's length, providing a deeper level of linguistic insight for our analyses. Such enhancements to the spaCy NLP pipeline facilitate the creation of more sophisticated and tailored natural language processing applications.\n",
                                    "\n",
                                    "## Accessing Custom Extensions from Pipeline Components\n",
                                    "\n",
                                    "To retrieve the additional information assigned by our `sentence_len_component` to each token, you can use:\n",
                                    "\n",
                                    "```python\n",
                                    "# Accessing sentence length information from the pipeline component\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text}: {token._.sentence_len}\")\n",
                                    "\n",
                                    "# Example output\n",
                                    "# JAPAN: 51\n",
                                    "# TO: 51\n",
                                    "# REVISE: 51\n",
                                    "# LONG: 51\n",
                                    "```\n",
                                    "\n",
                                    "This retrieves the dynamically assigned sentence length for each token, showcasing how to interact with data generated by pipeline components.\n",
                                    "\n",
                                    "## Lesson Summary and Practice\n",
                                    "\n",
                                    "In this lesson, you've learned two different ways to add custom linguistic features to the spaCy NLP pipeline: using a getter or a pipeline component. By segregating the instruction on how to access custom extensions and components right after their introduction, the lesson aims to reinforce the learning by immediate application of the concepts.\n",
                                    "\n",
                                    "In the upcoming practice exercises, you will have the chance to reinforce these concepts by applying them to real-life scenarios. Through practice, you'll be able to gain a more profound understanding of how these components can enhance your NLP tasks. Unlock the potential of custom pipeline components in spaCy and revolutionize your linguistic analysis!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Modify Phonetic Key Function in spaCy\n",
                                    "\n",
                                    "Great effort, Stellar Navigator! Let's tweak the get_phonetic_key function in the code. Replace the vowels used to generate the phonetic key with non-vowel characters, also known as consonants. Run your code, and observe how the phonetic keys change.\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "def get_phonetic_key(token):\n",
                                    "    vowels = [ch for ch in token.text.lower() if ch in 'aeiou']\n",
                                    "    return ''.join(vowels[:3]) if len(vowels) > 2 else token.text.lower()[:3]\n",
                                    "\n",
                                    "Token.set_extension('phonetic_key', getter=get_phonetic_key)\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "\n",
                                    "for token in doc[:10]:  \n",
                                    "    if not token.is_punct and not token.is_stop:\n",
                                    "        print(f\"{token}: {token._.phonetic_key}\")\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "To modify the `get_phonetic_key` function to generate phonetic keys based on consonants instead of vowels, you can replace the logic that extracts vowels with one that extracts consonants. Hereâ€™s the updated code:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "def get_phonetic_key(token):\n",
                                    "    # Extract consonants instead of vowels\n",
                                    "    consonants = [ch for ch in token.text.lower() if ch not in 'aeiou']\n",
                                    "    return ''.join(consonants[:3]) if len(consonants) > 2 else token.text.lower()[:3]\n",
                                    "\n",
                                    "Token.set_extension('phonetic_key', getter=get_phonetic_key)\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "\n",
                                    "for token in doc[:10]:  \n",
                                    "    if not token.is_punct and not token.is_stop:\n",
                                    "        print(f\"{token}: {token._.phonetic_key}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation of Changes:\n",
                                    "\n",
                                    "- The `get_phonetic_key` function now extracts consonants from the token's text instead of vowels. It uses a list comprehension to filter out characters that are not vowels.\n",
                                    "- The function returns the first three consonants if there are more than two; otherwise, it returns the first three characters of the token in lowercase.\n",
                                    "\n",
                                    "### Example Output:\n",
                                    "\n",
                                    "When you run this modified code, you will see the phonetic keys generated based on the consonants of the first ten tokens in the selected text. The output will look something like this (actual output may vary based on the content of the Reuters dataset):\n",
                                    "\n",
                                    "```\n",
                                    "the: the\n",
                                    "crude: crd\n",
                                    "oil: ol\n",
                                    "prices: prc\n",
                                    "rose: rs\n",
                                    "...\n",
                                    "```\n",
                                    "\n",
                                    "This change will allow you to observe how the phonetic keys differ when generated from consonants instead of vowels.Executed 1st Code Block\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implement Verb Count Pipeline Component\n",
                                    "\n",
                                    "Outstanding work, Stellar Navigator! Now, let's jump to the next stage. Fill in the blanks to create a pipeline component that counts the number of verbs in each sentence and adds this count to each token in the sentence. A verb in a Token can be identified by checking if Token.pos_ represents a 'VERB'.\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from spacy.language import Language\n",
                                    "from nltk.corpus import reuters\n",
                                    "import nltk\n",
                                    "\n",
                                    "nltk.download('reuters')\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "Token.set_extension('verb_count', default=None)\n",
                                    "\n",
                                    "@Language.component(____)\n",
                                    "def ____(___): \n",
                                    "    for sent in doc.sents:\n",
                                    "        verb_count = ____([token for token in sent if token.____ == 'VERB'])\n",
                                    "        for token in ____:\n",
                                    "            token._.____(____, verb_count)\n",
                                    "    return doc\n",
                                    "\n",
                                    "nlp.add_pipe(____, last=True) # Ensure 'count_verbs' comes after all the other standard components\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "\n",
                                    "for token in doc[:100]:\n",
                                    "    if not token.is_punct and not token.is_stop:\n",
                                    "        print(f\"{token}: Verb count in its sentence is {token._.verb_count}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "I'll help you fill in the blanks to create a pipeline component that counts verbs in sentences. Here's the completed code:\n",
                                    "\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from spacy.language import Language\n",
                                    "from nltk.corpus import reuters\n",
                                    "import nltk\n",
                                    "\n",
                                    "nltk.download('reuters')\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "Token.set_extension('verb_count', default=None)\n",
                                    "\n",
                                    "@Language.component(\"count_verbs\")  # Name of the component\n",
                                    "def count_verbs(doc):  # Function name and doc parameter\n",
                                    "    for sent in doc.sents:\n",
                                    "        verb_count = len([token for token in sent if token.pos_ == 'VERB'])  # Count verbs using len()\n",
                                    "        for token in sent:  # Iterate through sentence tokens\n",
                                    "            token._.verb_count = verb_count  # Set the custom extension value\n",
                                    "    return doc\n",
                                    "\n",
                                    "nlp.add_pipe(\"count_verbs\", last=True)  # Add component name\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "\n",
                                    "for token in doc[:100]:\n",
                                    "    if not token.is_punct and not token.is_stop:\n",
                                    "        print(f\"{token}: Verb count in its sentence is {token._.verb_count}\")\n",
                                    "\n",
                                    "Let's break down the filled blanks:\n",
                                    "\n",
                                    "1. `\"count_verbs\"` - The name of the component in the decorator\n",
                                    "2. `count_verbs` - The function name\n",
                                    "3. `doc` - The parameter that receives the processed document\n",
                                    "4. `len` - Function to count the number of verbs in the list comprehension\n",
                                    "5. `pos_` - The attribute that identifies the part of speech tag\n",
                                    "6. `sent` - To iterate through tokens in the current sentence\n",
                                    "7. `verb_count` - Setting the custom extension value\n",
                                    "8. `\"count_verbs\"` - The component name in add_pipe()\n",
                                    "\n",
                                    "This pipeline component will:\n",
                                    "1. Process each sentence in the document\n",
                                    "2. Count the number of verbs in each sentence\n",
                                    "3. Add that count to each token's custom extension 'verb_count'\n",
                                    "4. Return the processed document with the new information attached to each token\n",
                                    "\n",
                                    "The final output will show each non-punctuation, non-stop word token along with the number of verbs in its containing sentence.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Creating a Vowel Detection Custom Extension in spaCy\n",
                                    "\n",
                                    "In this task, your objective is to develop a feature that evaluates whether each token includes any vowels and integrate this functionality as a custom extension in spaCy. Proceed to test your implementation with tokens drawn from a portion of the Reuters corpus.\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "# TODO: Define the getter 'has_vowel' that checks if a token contains a vowel.\n",
                                    "\n",
                                    "# TODO: Register 'has_vowel' as a custom attribute using Token.set_extension here.\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "\n",
                                    "for token in doc[:50]:  \n",
                                    "  if not token.is_punct and not token.is_stop:\n",
                                    "    print(f\"{token}: Contains a vowel? {token._.has_vowels}\")\n",
                                    "\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "5cf99b68",
                           "metadata": {},
                           "source": [
                                    "Good job, Stellar Navigator! For the final challenge, your goal is to write a component that counts the total number of tokens in a sentence that share the same POS (Part-Of-Speech) with a specific token. You will assign this value to each token in the sentence using the same_pos_count extension. Best of luck!\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from spacy.language import Language\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "# TODO: Add 'same_pos_count' extension to the Token class.\n",
                                    "\n",
                                    "# TODO: Create a component named \"count_same_pos\" that counts the total number of tokens in the sentence \n",
                                    "# that share the same POS with the specific token. Assign it to each token in the sentence using the 'same_pos_count' extension.\n",
                                    "\n",
                                    "# TODO: Add the \"count_same_pos\" component to the pipeline after all the other standard components\n",
                                    "\n",
                                    "# Load text from the Reuters dataset\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "\n",
                                    "# TODO: Iterate over tokens and print count of tokens of the same POS in its sentence for each token.\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here's how you can implement a spaCy component that counts the total number of tokens in a sentence that share the same part of speech (POS) with a specific token. The count will be assigned to each token using a custom extension called `same_pos_count`. Below is the completed code:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from spacy.language import Language\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "# Add 'same_pos_count' extension to the Token class.\n",
                                    "Token.set_extension('same_pos_count', default=None)\n",
                                    "\n",
                                    "# Create a component named \"count_same_pos\" that counts the total number of tokens in the sentence \n",
                                    "# that share the same POS with the specific token.\n",
                                    "@Language.component(\"count_same_pos\")\n",
                                    "def count_same_pos(doc):\n",
                                    "    for sent in doc.sents:\n",
                                    "        for token in sent:\n",
                                    "            same_pos_count = sum(1 for t in sent if t.pos_ == token.pos_)  # Count tokens with the same POS\n",
                                    "            token._.same_pos_count = same_pos_count  # Assign the count to the token's extension\n",
                                    "    return doc\n",
                                    "\n",
                                    "# Add the \"count_same_pos\" component to the pipeline after all the other standard components\n",
                                    "nlp.add_pipe(\"count_same_pos\", last=True)\n",
                                    "\n",
                                    "# Load text from the Reuters dataset\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "\n",
                                    "# Iterate over tokens and print count of tokens of the same POS in its sentence for each token.\n",
                                    "for token in doc[:50]:\n",
                                    "    if not token.is_punct and not token.is_stop:\n",
                                    "        print(f\"{token}: Count of same POS tokens in its sentence is {token._.same_pos_count}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation of the Code:\n",
                                    "\n",
                                    "1. **Custom Extension Registration**: The `Token.set_extension` method is used to create a custom attribute `same_pos_count`, which will hold the count of tokens with the same POS.\n",
                                    "\n",
                                    "2. **Component Definition**: The `count_same_pos` function iterates through each sentence in the document. For each token in the sentence, it counts how many tokens share the same POS using a generator expression and assigns this count to the token's `same_pos_count` extension.\n",
                                    "\n",
                                    "3. **Pipeline Integration**: The `count_same_pos` component is added to the spaCy pipeline, ensuring it runs after all other standard components.\n",
                                    "\n",
                                    "4. **Output**: The code processes a portion of the Reuters corpus and prints the count of tokens with the same POS for each non-punctuation, non-stop word token in the first 50 tokens.\n",
                                    "\n",
                                    "This implementation will effectively allow you to analyze the distribution of POS tags within sentences and provide insights into the structure of the text.\n"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
