{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 5: Expanding the spaCy NLP Pipeline with Custom Components\n",
                                    "\n",
                                    "\n",
                                    "Welcome to this lesson on expanding the Natural Language Processing (NLP) pipeline with custom components using the spaCy library. Today, we're going to focus on adding extensions in two ways: using a pipeline component or using a getter for precomputing results. You'll learn when to use each method and practice creating meaningful custom components.\n",
                                    "\n",
                                    "## Understanding spaCy Extensions\n",
                                    "\n",
                                    "Extensions in spaCy are an efficient and flexible system for adding extra functionality to the built-in Doc, Token, and Span objects, as well as some other classes such as Language and Vocab. They can be used to add more information to a Token, for example, the length of the sentence where the token is found.\n",
                                    "\n",
                                    "Getter-based extensions are recommended when the attribute computation is straightforward, efficient, and highly dependent on the single Token instance. Such extensions are dynamically computed at the time of access, ensuring up-to-date and context-specific information without upfront computational overhead.\n",
                                    "\n",
                                    "Let's look at an example of a getter-based extension using the Reuters text corpus.\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "# Adding extensions with a getter\n",
                                    "Token.set_extension(\"sentence_len\", getter=lambda token: len(token.sent))\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "```\n",
                                    "\n",
                                    "Here, we created a simple getter-based extension that computes the length of the sentence in which each token is present.\n",
                                    "\n",
                                    "## Accessing Extensions Created with a Getter\n",
                                    "\n",
                                    "To access the `sentence_len` extension for each token, use the following approach:\n",
                                    "\n",
                                    "```python\n",
                                    "# Accessing sentence length information\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text}: {token._.sentence_len}\")\n",
                                    "\n",
                                    "# Example output\n",
                                    "# JAPAN: 51\n",
                                    "# TO: 51\n",
                                    "# REVISE: 51\n",
                                    "# LONG: 51\n",
                                    "```\n",
                                    "\n",
                                    "## Creating a Phonetic Key Extension\n",
                                    "\n",
                                    "Now, let's add a more linguistically meaningful extension, which computes a simple linguistic feature.\n",
                                    "\n",
                                    "Consider phonetic similarity between words. For the sake of simplicity, we'll create a phonetic key consisting of the first two consonants of the word, or if they don't exist, the first two characters. Remember, real phonetic comparison would be much more elaborate and language-dependent.\n",
                                    "\n",
                                    "```python\n",
                                    "def get_phonetic_key(token):\n",
                                    "    non_vowels = [ch for ch in token.text.lower() if ch not in 'aeiou']\n",
                                    "    return ''.join(non_vowels[:2]) if len(non_vowels) > 1 else token.text.lower()[:2]\n",
                                    "\n",
                                    "Token.set_extension('phonetic_key', getter=get_phonetic_key)\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "```\n",
                                    "\n",
                                    "## Accessing the Phonetic Key Extension\n",
                                    "\n",
                                    "After creating the phonetic key extension, you can access it for each token as follows:\n",
                                    "\n",
                                    "```python\n",
                                    "# Accessing the phonetic_key extension:\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text}: {token._.phonetic_key}\")\n",
                                    "\n",
                                    "# Example output\n",
                                    "# JAPAN: jp\n",
                                    "# TO: to\n",
                                    "# REVISE: rv\n",
                                    "# LONG: ln\n",
                                    "```\n",
                                    "\n",
                                    "This segment demonstrates how to use getter-based extensions effectively and retrieve the additional data they provide.\n",
                                    "\n",
                                    "## Understanding spaCy Pipeline Components\n",
                                    "\n",
                                    "In spaCy, a pipeline component is a function that is handed a Doc object, performs a specific operation on it, and then returns the modified Doc. Pipeline components prove useful in adding more complex extensions, especially when the computation is intricate or depends on other Tokens. This function is executed automatically as part of the spaCy pipeline when we run the nlp text processing on a document. This means that during the nlp processing of a text, each pipeline component is invoked in sequence, allowing the modifications or enhancements implemented in the component to be computed and applied to the Doc object in real-time. This design ensures that by the time the full processing is complete, all custom operations defined in the pipeline components have been performed, integrating seamlessly with spaCy's built-in processes for a holistic NLP solution.\n",
                                    "\n",
                                    "Consider this example where we use a pipeline component to calculate the sentence length for each Token:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.language import Language\n",
                                    "from spacy.tokens import Token\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "Token.set_extension('sentence_len', default=None)\n",
                                    "\n",
                                    "@Language.component(\"sentence_len_component\")\n",
                                    "def sentence_len_component(doc): \n",
                                    "    for sent in doc.sents:\n",
                                    "        sent_len = len(sent)\n",
                                    "        for token in sent:\n",
                                    "            token._.set('sentence_len', sent_len)\n",
                                    "    return doc\n",
                                    "\n",
                                    "nlp.add_pipe(\"sentence_len_component\", last=True)\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "```\n",
                                    "\n",
                                    "The `sentence_len_component` calculates the length of each sentence in the Doc and associates this length with every Token within that sentence. This is achieved through `token._.set('sentence_len', sent_len)`, where `token._.set(...)` is used to dynamically assign the computed sentence length to each Token.\n",
                                    "\n",
                                    "The `Language.component` decorator is a powerful feature used to craft custom pipeline components capable of computing and assigning sentence lengths to tokens. Defining a pipeline component is only the first step; integrating it into the spaCy NLP pipeline is a seamless process facilitated by the `add_pipe` method. The first argument of `add_pipe` determines the component to incorporate into the pipeline. For our scenario, we're including the \"sentence_len_component\". The `last=True` parameter, passed to `add_pipe`, ensures this component is appended at the end of the pipeline. Positioning it last is crucial for ensuring that all preceding analyses and modifications have been completed on the Doc object, which allows our custom component to operate with the most current version of the document, incorporating any updates made by earlier components. This strategic placement maximizes the effectiveness and accuracy of the sentence length computation, as it relies on the context established by the pipeline's prior stages.\n",
                                    "\n",
                                    "With this arrangement, each Token is now enriched with data regarding its sentence's length, providing a deeper level of linguistic insight for our analyses. Such enhancements to the spaCy NLP pipeline facilitate the creation of more sophisticated and tailored natural language processing applications.\n",
                                    "\n",
                                    "## Accessing Custom Extensions from Pipeline Components\n",
                                    "\n",
                                    "To retrieve the additional information assigned by our `sentence_len_component` to each token, you can use:\n",
                                    "\n",
                                    "```python\n",
                                    "# Accessing sentence length information from the pipeline component\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text}: {token._.sentence_len}\")\n",
                                    "\n",
                                    "# Example output\n",
                                    "# JAPAN: 51\n",
                                    "# TO: 51\n",
                                    "# REVISE: 51\n",
                                    "# LONG: 51\n",
                                    "```\n",
                                    "\n",
                                    "This retrieves the dynamically assigned sentence length for each token, showcasing how to interact with data generated by pipeline components.\n",
                                    "\n",
                                    "## Lesson Summary and Practice\n",
                                    "\n",
                                    "In this lesson, you've learned two different ways to add custom linguistic features to the spaCy NLP pipeline: using a getter or a pipeline component. By segregating the instruction on how to access custom extensions and components right after their introduction, the lesson aims to reinforce the learning by immediate application of the concepts.\n",
                                    "\n",
                                    "In the upcoming practice exercises, you will have the chance to reinforce these concepts by applying them to real-life scenarios. Through practice, you'll be able to gain a more profound understanding of how these components can enhance your NLP tasks. Unlock the potential of custom pipeline components in spaCy and revolutionize your linguistic analysis!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Modify Phonetic Key Function in spaCy\n",
                                    "\n",
                                    "Great effort, Stellar Navigator! Let's tweak the get_phonetic_key function in the code. Replace the vowels used to generate the phonetic key with non-vowel characters, also known as consonants. Run your code, and observe how the phonetic keys change.\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "def get_phonetic_key(token):\n",
                                    "    vowels = [ch for ch in token.text.lower() if ch in 'aeiou']\n",
                                    "    return ''.join(vowels[:3]) if len(vowels) > 2 else token.text.lower()[:3]\n",
                                    "\n",
                                    "Token.set_extension('phonetic_key', getter=get_phonetic_key)\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "\n",
                                    "for token in doc[:10]:  \n",
                                    "    if not token.is_punct and not token.is_stop:\n",
                                    "        print(f\"{token}: {token._.phonetic_key}\")\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "To modify the `get_phonetic_key` function to generate phonetic keys based on consonants instead of vowels, you can replace the logic that extracts vowels with one that extracts consonants. Hereâ€™s the updated code:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from spacy.tokens import Token\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "def get_phonetic_key(token):\n",
                                    "    # Extract consonants instead of vowels\n",
                                    "    consonants = [ch for ch in token.text.lower() if ch not in 'aeiou']\n",
                                    "    return ''.join(consonants[:3]) if len(consonants) > 2 else token.text.lower()[:3]\n",
                                    "\n",
                                    "Token.set_extension('phonetic_key', getter=get_phonetic_key)\n",
                                    "\n",
                                    "texts = reuters.raw(categories=['crude', 'coffee', 'gold'])[0:5000]\n",
                                    "doc = nlp(texts)\n",
                                    "\n",
                                    "for token in doc[:10]:  \n",
                                    "    if not token.is_punct and not token.is_stop:\n",
                                    "        print(f\"{token}: {token._.phonetic_key}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation of Changes:\n",
                                    "\n",
                                    "- The `get_phonetic_key` function now extracts consonants from the token's text instead of vowels. It uses a list comprehension to filter out characters that are not vowels.\n",
                                    "- The function returns the first three consonants if there are more than two; otherwise, it returns the first three characters of the token in lowercase.\n",
                                    "\n",
                                    "### Example Output:\n",
                                    "\n",
                                    "When you run this modified code, you will see the phonetic keys generated based on the consonants of the first ten tokens in the selected text. The output will look something like this (actual output may vary based on the content of the Reuters dataset):\n",
                                    "\n",
                                    "```\n",
                                    "the: the\n",
                                    "crude: crd\n",
                                    "oil: ol\n",
                                    "prices: prc\n",
                                    "rose: rs\n",
                                    "...\n",
                                    "```\n",
                                    "\n",
                                    "This change will allow you to observe how the phonetic keys differ when generated from consonants instead of vowels.Executed 1st Code Block\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implement Verb Count Pipeline Component"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Creating a Vowel Detection Custom Extension in spaCy"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
