{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d839a2",
   "metadata": {},
   "source": [
    "# Lesson 4: Lemmatization Nuances in Natural Language Processing with spaCy\n",
    "\n",
    "### Lesson Overview\n",
    "\n",
    "Welcome back! As we move ahead in our Natural Language Processing journey, today's lesson is about a fundamental component of NLP preprocessing — **Lemmatization**. We will get hands-on with the spaCy library to implement lemmatization on our text data.\n",
    "\n",
    "By the end of the lesson, you should be skilled in explaining and implementing lemmatization in your data preprocessing pipeline for NLP tasks.\n",
    "\n",
    "### Understanding Lemmatization\n",
    "\n",
    "Lemmatization, in the context of Natural Language Processing, is the process of reducing any given word to its base form or root.\n",
    "\n",
    "Let's take an example: suppose we have a verb in its past tense, like **flying**. The base form of flying is **fly**. If we perform lemmatization on flying, we get fly. On similar lines, **better** would be reduced to **good**, **mice** would become **mouse**, and so on.\n",
    "\n",
    "So, why lemmatization? Well, while dealing with natural language, it happens quite frequently that we encounter different forms of the same word. For a machine, **better**, **good**, and **best** are different words, even though they essentially express the same thing. When we perform tasks like text classification, these different forms are treated as different features, thus increasing the dimensionality of our dataset. By lemmatizing, we can reduce these variations to their root form, thereby reducing the number of features and making our model more efficient.\n",
    "\n",
    "### spaCy's Capability on Lemmatization\n",
    "\n",
    "spaCy offers a convenient and efficient way to perform lemmatization on text. When spaCy processes any text, it performs lemmatization by default and keeps the lemma (or root form) of each word as an attribute of the word. This attribute can be accessed by simply calling `token.lemma_`, where `token` is the word we're dealing with.\n",
    "\n",
    "Now, let's move onto the practical implementation.\n",
    "\n",
    "### Implementing Lemmatization using spaCy\n",
    "\n",
    "Let's use the provided task as an example to perform lemmatization on a sentence.\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"The striped bats are hanging on their feet and ate best fishes\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n",
    "```\n",
    "In the above code, we initially load the English language model using `nlp = spacy.load(\"en_core_web_sm\")`. We then use this model to process our sentence and convert it to a doc, which is essentially a collection of tokens (or words).\n",
    "\n",
    "Finally, we iterate over each token in the doc and print the token and its corresponding lemma. The lemma of a token can be accessed using the `lemma_` attribute of the token.\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "\n",
    "\n",
    "The the striped stripe bats bat are be hanging hang on on their their feet foot and and ate eat best good fishes fish\n",
    "\n",
    "\n",
    "This output demonstrates how each word from our sentence is processed and reduced to its lemma form. Notice how \"bats\" is converted to \"bat\", and \"ate\" to \"eat\", showcasing the effectiveness of lemmatization in normalizing text.\n",
    "\n",
    "### Use Case for Lemmatization\n",
    "\n",
    "So how does this help in real-world Natural Language Processing tasks? Lemmatization reduces the various inflected forms of a word to a single form. This can significantly reduce the number of unique words in our text (which, in case of text data, means reducing the number of features) without losing significant meaning.\n",
    "\n",
    "Text classification, sentiment analysis, and topic modeling are just a few NLP tasks that can significantly benefit from the dimensionality reduction lemmatization offers. By making the dataset more manageable, more computationally efficient, and more accurate machine learning models can be built.\n",
    "\n",
    "### Lesson Summary and Practice\n",
    "\n",
    "Well done on reaching this point! Today you learned about lemmatization, its importance in NLP data preprocessing, and how to utilize spaCy to perform lemmatization. This knowledge is an integral part of any NLP pipeline and will assist you greatly in future tasks.\n",
    "\n",
    "Up next, we'll be practicing using spaCy's lemmatization functionality on actual datasets and assessing its effects on our text data. This will reinforce your understanding and further boost your spaCy mastery! See you in the next lesson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d50a5d",
   "metadata": {},
   "source": [
    "## Change the Sentence for Lemmatization\n",
    "\n",
    "Stellar Navigator, you're doing a fantastic job! For a small twist, change the sentence in our code to \"Explorers venture through the immense void separating our planet from the red Martian surface, seeking new horizons\". Run the lemmatization process using spaCy and observe the output. Keep going!\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"Telescopes uncover the striking hues of far-off nebulae and galaxies, mesmerizing observers across the globe.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n",
    "```\n",
    "\n",
    "Let's modify the sentence in the code to \"Explorers venture through the immense void separating our planet from the red Martian surface, seeking new horizons\" and run the lemmatization process using spaCy. Here’s the updated code:\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"Explorers venture through the immense void separating our planet from the red Martian surface, seeking new horizons.\" doc = nlp(sentence)\n",
    "\n",
    "for token in doc: print(token.text, token.lemma_)\n",
    "```\n",
    "Now, let's execute this code to observe the output.\n",
    "\n",
    "It seems that the spaCy model en_core_web_sm is not available in the current environment, which is causing the error. However, I can provide you with the expected output based on the lemmatization process for the given sentence.\n",
    "\n",
    "If you run the modified code in your local environment where spaCy is installed, you should see an output similar to this:\n",
    "\n",
    "Explorers explorer\n",
    "venture venture\n",
    "through through\n",
    "the the\n",
    "immense immense\n",
    "void void\n",
    "separating separate\n",
    "our our\n",
    "planet planet\n",
    "from from\n",
    "the the\n",
    "red red\n",
    "Martian Martian\n",
    "surface surface\n",
    "seeking seek\n",
    "new new\n",
    "horizons horizon\n",
    "\n",
    "\n",
    "This output shows how each word from the sentence is processed and reduced to its lemma form. If you have any further questions or need assistance with something else, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7267f",
   "metadata": {},
   "source": [
    "## Lemmatizing Reuters Dataset with spaCy\n",
    "\n",
    "Glad to see you here, Stellar Navigator! Your task is to adjust the raw_text value in our code. Currently, it lemmatizes a string about Mars, but let's examine a real-world dataset. Please replace that string with the first 100 characters from the Reuters dataset, obtained using the 'test/14826' file ID. Observe the lemma output!\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import reuters \n",
    "\n",
    "# Download Reuters dataset\n",
    "nltk.download('reuters')\n",
    "\n",
    "# Load English language model from spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Get the raw text from the sentence and limit it to the leading 100 characters\n",
    "raw_text = \"Mars is the fourth planet in our solar system\"\n",
    "\n",
    "# Process the sentence using the English language model\n",
    "doc = nlp(raw_text)\n",
    "\n",
    "# Loop through each token in doc\n",
    "for token in doc:\n",
    "    # Print each token's text and lemma\n",
    "    print(token.text, token.lemma_)\n",
    "\n",
    "```\n",
    "\n",
    "Let's adjust the code to replace the `raw_text` value with the first 100 characters from the Reuters dataset using the 'test/14826' file ID. Here’s the updated code:\n",
    "```python\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Download Reuters dataset\n",
    "nltk.download('reuters')\n",
    "\n",
    "# Load English language model from spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Get the raw text from the Reuters dataset and limit it to the leading 100 characters\n",
    "raw_text = reuters.raw('test/14826')[:100]\n",
    "\n",
    "# Process the sentence using the English language model\n",
    "doc = nlp(raw_text)\n",
    "\n",
    "# Loop through each token in doc\n",
    "for token in doc:\n",
    "    # Print each token's text and lemma\n",
    "    print(token.text, token.lemma_)\n",
    "```\n",
    "\n",
    "This code will fetch the first 100 characters from the specified Reuters article and perform lemmatization on it. If you run this code in your local environment, you should see the lemmatized output for the text extracted from the Reuters dataset. \n",
    "\n",
    "If you need further assistance or have any questions, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53070fc3",
   "metadata": {},
   "source": [
    "## Lemmatization on Reuters Dataset with spaCy\n",
    "\n",
    "Good effort, Space Voyager! Here's another checkpoint. Fill in the blanks (_____). Download the reuters data from nltk, then retrieve the raw text of the file ID test/14826 and limit it to the leading 50 characters. Further, use spaCy to process the sentences and print the root form (lemma) of each token. Carry on!\n",
    "\n",
    "```python\n",
    "import ____ \n",
    "import nltk\n",
    "from nltk.corpus import ____ \n",
    "\n",
    "nltk.download('reuters')\n",
    "# Specify a fileid for the reuters corpus\n",
    "raw_text = ____('test/14826')[:50]\n",
    "\n",
    "nlp = ____.load('en_core_web_sm')\n",
    "sentence = raw_text\n",
    "doc = ____(sentence)\n",
    "\n",
    "for _____ in doc:\n",
    "    print(_____.text, _____.lemma_)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Here’s the completed code with the blanks filled in:\n",
    "```python\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('reuters')\n",
    "# Specify a fileid for the reuters corpus\n",
    "raw_text = reuters.raw('test/14826')[:50]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = raw_text\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n",
    "```\n",
    "This code will download the Reuters dataset, retrieve the raw text from the specified file ID, limit it to the first 50 characters, and then use spaCy to process the text and print the lemma of each token. If you have any further questions or need additional assistance, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a1196",
   "metadata": {},
   "source": [
    "## Integrating Lemmatization into Text Processing Pipeline\n",
    "\n",
    "Good job, Stellar Navigator! Now, add a crucial line of code to finish the lemmatization pipeline. Fill in the TODO comment by appending lemmas of non-stopwords and non-punctuation characters to our list. Be mindful of our dear Reuters dataset when illustrating your prowess!\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import reuters \n",
    "\n",
    "# Set the verbosity level to False to suppress informational messages\n",
    "nltk.download('reuters', quiet=True)\n",
    "raw_text = reuters.raw('test/14828')[:50]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = raw_text\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Initialize an empty list to store the lemmas\n",
    "lemmas = []\n",
    "\n",
    "# Loop through each token in the doc\n",
    "for token in doc:\n",
    "    # TODO: Add the non-stopwords and non-punctuation token's lemma to the list\n",
    "\n",
    "\n",
    "# Print the list of lemmas\n",
    "print(lemmas)\n",
    "```\n",
    "\n",
    "To complete the lemmatization pipeline, you can fill in the TODO comment by appending the lemmas of non-stopwords and non-punctuation characters to the `lemmas` list. Here's the updated code:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Set the verbosity level to False to suppress informational messages\n",
    "nltk.download('reuters', quiet=True)\n",
    "raw_text = reuters.raw('test/14828')[:50]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = raw_text\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Initialize an empty list to store the lemmas\n",
    "lemmas = []\n",
    "\n",
    "# Loop through each token in the doc\n",
    "for token in doc:\n",
    "    # Add the non-stopwords and non-punctuation token's lemma to the list\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        lemmas.append(token.lemma_)\n",
    "\n",
    "# Print the list of lemmas\n",
    "print(lemmas)\n",
    "```\n",
    "\n",
    "In this code, the condition checks if the token is not a stopword and not punctuation before appending its lemma to the `lemmas` list. This ensures that only meaningful words are included in the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109ad9d",
   "metadata": {},
   "source": [
    "## Lemmatization with spaCy on the Reuters Dataset\n",
    "\n",
    "Fantastic progress, Stellar Navigator! Now, put your acquired skills to the test. Write a Python program that retrieves the Reuters dataset file 'test/14829' and uses spaCy's English language model to tokenize and lemmatize the text, filtering out all stop words along the way. Finally, print each token alongside its corresponding lemma.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import reuters \n",
    "\n",
    "# TODO: Download the Reuters dataset\n",
    "\n",
    "# TODO: Load the English language model from spaCy\n",
    "\n",
    "# TODO: Get the raw text from the 'test/14829' file in the Reuters dataset\n",
    "\n",
    "# TODO: Process the sentence using the English language model\n",
    "\n",
    "# TODO: Loop through each token in doc, and print each token's text and lemma, filtering stop words along the way\n",
    "\n",
    "```\n",
    "\n",
    "Here's a complete Python program that retrieves the Reuters dataset file 'test/14829', uses spaCy's English language model to tokenize and lemmatize the text, and filters out all stop words while printing each token alongside its corresponding lemma:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Download the Reuters dataset\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "# Load the English language model from spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Get the raw text from the 'test/14829' file in the Reuters dataset\n",
    "raw_text = reuters.raw('test/14829')\n",
    "\n",
    "# Process the sentence using the English language model\n",
    "doc = nlp(raw_text)\n",
    "\n",
    "# Loop through each token in doc, and print each token's text and lemma, filtering stop words along the way\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        print(f'Token: {token.text}, Lemma: {token.lemma_}')\n",
    "```\n",
    "\n",
    "In this program, the following steps are performed:\n",
    "\n",
    "1. The Reuters dataset is downloaded using NLTK.\n",
    "2. The English language model is loaded from spaCy.\n",
    "3. The raw text from the specified Reuters file is retrieved.\n",
    "4. The text is processed to create a `doc` object.\n",
    "5. Each token is checked to ensure it is not a stop word or punctuation, and then both the token and its lemma are printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39871c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
