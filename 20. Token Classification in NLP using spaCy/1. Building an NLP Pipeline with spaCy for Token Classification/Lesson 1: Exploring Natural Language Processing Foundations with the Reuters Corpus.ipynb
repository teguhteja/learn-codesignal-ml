{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b27cfdcc",
   "metadata": {},
   "source": [
    "# Lesson 1: Exploring Natural Language Processing Foundations with the Reuters Corpus\n",
    "\n",
    "### Lesson Introduction\n",
    "\n",
    "Hello and welcome! In today's lesson, we dive into the world of Natural Language Processing (NLP). NLP is a branch of artificial intelligence that deals with the interaction between computers and humans using natural language. Today, you'll get introduced to basic NLP concepts, using a popular Python library for natural language processing.\n",
    "\n",
    "### Intro to Natural Language Processing\n",
    "\n",
    "Natural Language Processing, or NLP, is a field of study that focuses on the interactions between human language and computers. It sits at the intersection of computer science, artificial intelligence, and computational linguistics. NLP involves making computers understand, interpret, and manipulate human language. It's an essential tool for transforming unstructured data into actionable information. For example, it can help us understand the sentiments of customers about a product by analyzing online reviews and social media posts.\n",
    "\n",
    "Machine learning and data science play a big role in NLP. They provide the methods to \"teach\" machines how to understand our language. As data scientists, understanding NLP techniques can help us create better models for text analysis.\n",
    "\n",
    "### Investigating the Reuters Dataset\n",
    "\n",
    "To understand natural language processing, we first need to have a dataset to work with. For this course, we'll be using the Reuters Corpus from the Natural Language Toolkit (nltk), which is a set of corpora and lexical resources for natural language processing and machine learning in Python.\n",
    "\n",
    "Let's start by importing the required library and downloading the dataset.\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import nltk\n",
    "\n",
    "# Download the Reuters dataset\n",
    "nltk.download('reuters')\n",
    "\n",
    "Now, our Reuters dataset is downloaded and ready to use.\n",
    "\n",
    "### Exploring Documents in the Reuters Dataset\n",
    "\n",
    "Let's explore our dataset. The first thing to do is to load the dataset and see how many documents there are:\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Load the dataset\n",
    "documents = reuters.fileids()\n",
    "\n",
    "# Print the number of documents\n",
    "print(f\"There are {len(documents)} documents in the Reuters dataset\")\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```\n",
    "There are 10788 documents in the Reuters dataset\n",
    "```\n",
    "\n",
    "Each file ID in this dataset represents a document. We can pick any file ID and see the raw text in it:\n",
    "\n",
    "# Load the text of a single document\n",
    "document_text = reuters.raw(documents[0])\n",
    "\n",
    "# Print the first 500 characters of the document text\n",
    "print(\"\\nThe first 500 characters of the first document:\\n\")\n",
    "print(document_text[:500])\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```\n",
    "The first 500 characters of the first document:\n",
    "\n",
    "ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
    "  Mounting trade friction between the\n",
    "  U.S. And Japan has raised fears among many of Asia's exporting\n",
    "  nations that the row could inflict far-reaching economic\n",
    "  damage, businessmen and officials said.\n",
    "      They told Reuter correspondents in Asian capitals a U.S.\n",
    "  Move against Japan might boost protectionist sentiment in the\n",
    "  U.S. And lead to curbs on American imports of their products.\n",
    "      But some exporters said that while the conflict wo\n",
    "```\n",
    "\n",
    "There you have itâ€”the raw text data we will be dealing with. This may look like quite a lot right now. But, as we go through this course, you'll learn how we can break down and handle text data efficiently using NLP techniques like tokenization, POS tagging, and lemmatization.\n",
    "\n",
    "### Analyzing Document Categories\n",
    "\n",
    "In the Reuters dataset, each document belongs to one or more categories. Understanding these categories will give us a holistic view of our documents.\n",
    "\n",
    "We'll just check the categories of a single document for now:\n",
    "\n",
    "# Print the categories of the first document\n",
    "print(\"\\nThe categories of the first document are:\")\n",
    "print(reuters.categories(documents[0]))\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```\n",
    "The categories of the first document are:\n",
    "['trade']\n",
    "```\n",
    "\n",
    "These categories provide us with a top-level view of what each document is about.\n",
    "\n",
    "### Lesson Summary and Practice\n",
    "\n",
    "There we go! We have taken our first steps into the world of Natural Language Processing by exploring the Reuters Corpus from the Natural Language Toolkit (nltk).\n",
    "\n",
    "As we move forward, we will be working on setting up a proper NLP pipeline and learning key NLP techniques such as tokenization, POS tagging, and lemmatization. All these skills will be extremely useful for your data science and machine learning journey. So, let's keep moving forward and continue exploring these in the upcoming lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1e9a8",
   "metadata": {},
   "source": [
    "## Counting Unique Categories in Reuters Dataset\n",
    "\n",
    "Fantastic work, Stellar Navigator!\n",
    "\n",
    "Now, using the Reuters dataset, alter the starter code to print the total number of unique categories in the dataset, instead of printing all categories and their counts. Remember, the len function can count the elements in a set!\n",
    "\n",
    "```python\n",
    "# Importing the necessary NLTK corpus\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Obtain all (duplicated) categories in the dataset\n",
    "all_categories = reuters.categories()\n",
    "\n",
    "# Get the unique categories by turning the list into a set\n",
    "unique_categories = set(all_categories)\n",
    "\n",
    "# Initialize an empty dictionary to store category counts\n",
    "category_counts = {}\n",
    "\n",
    "# Iterate through the unique categories\n",
    "for category in unique_categories:\n",
    "    # Count the number of documents in each category and store it in the dictionary\n",
    "    category_counts[category] = len(reuters.fileids(category))\n",
    "\n",
    "# Print the category counts dictionary\n",
    "print(category_counts)\n",
    "\n",
    "```\n",
    "\n",
    "I'll modify the code to show just the total count of unique categories in the Reuters dataset:\n",
    "```python\n",
    "# Importing the necessary NLTK corpus\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Obtain all unique categories by converting the list to a set\n",
    "unique_categories = set(reuters.categories())\n",
    "\n",
    "# Print the total number of unique categories\n",
    "print(f\"The Reuters dataset contains {len(unique_categories)} unique categories.\")\n",
    "```\n",
    "\n",
    "This simplified version will output something like:\n",
    "```\n",
    "The Reuters dataset contains 90 unique categories.\n",
    "```\n",
    "\n",
    "The key changes made:\n",
    "* Removed the unnecessary dictionary creation and counting loop since we only need the total count\n",
    "* Used `set()` directly on `reuters.categories()` to get unique categories\n",
    "* Added a more descriptive print statement\n",
    "* Removed the detailed category counts which weren't needed for this task\n",
    "\n",
    "This gives us a cleaner, more focused solution that directly answers the question of how many unique categories exist in the dataset. The set data structure automatically handles the deduplication of categories for us, making the code both efficient and straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8059bf",
   "metadata": {},
   "source": [
    "## Explore 'Tea' Category in Reuters Corpus\n",
    "\n",
    "Superb job, Stellar Navigator!\n",
    "\n",
    "You're progressing perfectly fine. It's time for you to practice making a minor change to the code. In the given starter code, documents associated with the 'sugar' category are being fetched. Your task is to change the category from 'sugar' to 'tea'. Fetch the related documents, and then print the raw text and categories of the first document related to 'tea'. Ready? Your code journey awaits!\n",
    "\n",
    "```python\n",
    "# Importing the necessary NLTK corpus\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Get all documents associated with the 'sugar' category\n",
    "sugar_documents = reuters.fileids('sugar')\n",
    "\n",
    "# Access and print the raw text and categories of the first document in the 'sugar' category\n",
    "first_sugar_doc_text = reuters.raw(sugar_documents[0])\n",
    "print(\"\\nThe first 500 characters of the first 'sugar' document:\\n\")\n",
    "print(first_sugar_doc_text[:500])\n",
    "first_sugar_doc_categories = reuters.categories(sugar_documents[0])\n",
    "print(\"\\nThe categories of the first 'sugar' document are:\")\n",
    "print(first_sugar_doc_categories)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "I'll modify the code to explore documents in the 'tea' category instead of 'sugar':\n",
    "```python\n",
    "# Importing the necessary NLTK corpus\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Get all documents associated with the 'tea' category\n",
    "tea_documents = reuters.fileids('tea')\n",
    "\n",
    "# Access and print the raw text and categories of the first document in the 'tea' category\n",
    "first_tea_doc_text = reuters.raw(tea_documents[0])\n",
    "print(\"\\nThe first 500 characters of the first 'tea' document:\\n\")\n",
    "print(first_tea_doc_text[:500])\n",
    "\n",
    "# Get and print the categories of the first tea document\n",
    "first_tea_doc_categories = reuters.categories(tea_documents[0])\n",
    "print(\"\\nThe categories of the first 'tea' document are:\")\n",
    "print(first_tea_doc_categories)\n",
    "```\n",
    "\n",
    "Key changes made:\n",
    "* Changed variable names from 'sugar' to 'tea' for clarity\n",
    "* Modified the category being searched from 'sugar' to 'tea'\n",
    "* Updated the print statements to reflect we're looking at tea documents\n",
    "* Maintained the same structure but with the new focus on tea-related content\n",
    "\n",
    "This code will show us:\n",
    "1. The first 500 characters of the first document that's categorized under 'tea'\n",
    "2. All the categories that this document belongs to (since Reuters documents can belong to multiple categories)\n",
    "\n",
    "The output would show us a glimpse into how tea-related news was reported in the Reuters corpus, along with any other topics this particular article might cover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef92522",
   "metadata": {},
   "source": [
    "## Fetch Text and Categories for 'Coffee' in Reuters Corpus\n",
    "\n",
    "\n",
    "Spot-on, Stellar Navigator!\n",
    "\n",
    "In this exercise, your objective is to fetch text and categories associated with the 'coffee' category from the Reuters Corpus. Replace each blank space (____) in the code with the correct code statement or variable. Can't wait to read about 'coffee'!\n",
    "\n",
    "```python\n",
    "# Importing the necessary NLTK corpus\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Get all documents associated with the 'coffee' category\n",
    "coffee_documents = reuters.fileids(____)\n",
    "\n",
    "# Access and print the raw text and categories of the first document in the 'coffee' category\n",
    "first_coffee_doc_text = reuters.____(____)\n",
    "print(\"\\nThe first 500 characters of the first 'coffee' document:\\n\")\n",
    "print(____)\n",
    "first_coffee_doc_categories = reuters.____(____)\n",
    "print(\"\\nThe categories of the first 'coffee' document are:\")\n",
    "print(____)\n",
    "\n",
    "```\n",
    "\n",
    "I'll help fill in the blanks to explore the 'coffee' category documents:\n",
    "\n",
    "```python\n",
    "# Importing the necessary NLTK corpus\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Get all documents associated with the 'coffee' category\n",
    "coffee_documents = reuters.fileids('coffee')\n",
    "\n",
    "# Access and print the raw text and categories of the first document in the 'coffee' category\n",
    "first_coffee_doc_text = reuters.raw(coffee_documents[0])\n",
    "print(\"\\nThe first 500 characters of the first 'coffee' document:\\n\")\n",
    "print(first_coffee_doc_text[:500])\n",
    "first_coffee_doc_categories = reuters.categories(coffee_documents[0])\n",
    "print(\"\\nThe categories of the first 'coffee' document are:\")\n",
    "print(first_coffee_doc_categories)\n",
    "```\n",
    "\n",
    "Here's what I filled in:\n",
    "1. `'coffee'` - The category we want to search for\n",
    "2. `raw(coffee_documents[0])` - Get the raw text of the first coffee document\n",
    "3. `first_coffee_doc_text[:500]` - Print first 500 characters of the text\n",
    "4. `categories(coffee_documents[0])` - Get categories for the first coffee document\n",
    "5. `first_coffee_doc_categories` - Print all categories for this document\n",
    "\n",
    "The code will:\n",
    "* Find all documents tagged with the 'coffee' category\n",
    "* Get the text content of the first coffee-related document\n",
    "* Display the first 500 characters of that document\n",
    "* Show all categories associated with that document\n",
    "\n",
    "This gives us a good look at how coffee-related news was covered in the Reuters corpus and how it might intersect with other topics through its category assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376017a1",
   "metadata": {},
   "source": [
    "## Exploring the 'Gas' Category in Reuters Corpus\n",
    "\n",
    "Well done, Stellar Navigator!\n",
    "\n",
    "As a next step, add the missing lines to the provided code to retrieve all documents associated with the 'gas' category from the Reuters Corpus. Then, print the first 500 characters of the raw text from the first document in this category. Continue your remarkable journey through the vast universe of natural language processing!\n",
    "\n",
    "```python\n",
    "# Importing the necessary NLTK corpus\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# TODO: Get all documents associated with the 'gas' category\n",
    "\n",
    "# TODO: Access and print the raw text of the first document in the 'gas' category\n",
    "\n",
    "# Print the number of documents and categories of the first document in the 'gas' category\n",
    "print(f\"\\nThere are {len(gas_documents)} documents in the 'gas' category.\")\n",
    "first_gas_doc_categories = reuters.categories(gas_documents[0])\n",
    "print(f\"The categories of the first 'gas' document are:\\n{first_gas_doc_categories}\")\n",
    "\n",
    "```\n",
    "\n",
    "I'll help complete the code by adding the missing lines to explore the 'gas' category documents:\n",
    "```python\n",
    "# Importing the necessary NLTK corpus\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Get all documents associated with the 'gas' category\n",
    "gas_documents = reuters.fileids('gas')\n",
    "\n",
    "# Access and print the raw text of the first document in the 'gas' category\n",
    "first_gas_doc_text = reuters.raw(gas_documents[0])\n",
    "print(\"\\nThe first 500 characters of the first 'gas' document:\\n\")\n",
    "print(first_gas_doc_text[:500])\n",
    "\n",
    "# Print the number of documents and categories of the first document in the 'gas' category\n",
    "print(f\"\\nThere are {len(gas_documents)} documents in the 'gas' category.\")\n",
    "first_gas_doc_categories = reuters.categories(gas_documents[0])\n",
    "print(f\"The categories of the first 'gas' document are:\\n{first_gas_doc_categories}\")\n",
    "```\n",
    "\n",
    "Here's what I added:\n",
    "1. `gas_documents = reuters.fileids('gas')` - Gets all documents tagged with the 'gas' category\n",
    "2. `first_gas_doc_text = reuters.raw(gas_documents[0])` - Retrieves the raw text of the first gas-related document\n",
    "3. Added print statements to display the first 500 characters of the document\n",
    "\n",
    "The complete code will:\n",
    "* Find all documents in the Reuters corpus tagged with 'gas'\n",
    "* Get the full text of the first gas-related document\n",
    "* Show the first 500 characters of that document\n",
    "* Display the total count of gas-related documents\n",
    "* List all categories associated with the first document\n",
    "\n",
    "This gives us a comprehensive view of how gas-related news was covered in the Reuters corpus, including its frequency and its relationships with other topics through category assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567096b",
   "metadata": {},
   "source": [
    "## Exploring Reuters Corpus by Category\n",
    "\n",
    "Apa yang terjadi pada otak setelah kita meninggal duniaï¼Ÿ - BBC News Indonesia_Os98kzw3OU0\n",
    "\n",
    "```python\n",
    "# TODO: Import necessary libraries\n",
    "\n",
    "# TODO: Fetch the unique categories from Reuters corpus\n",
    "\n",
    "# TODO: For each unique category\n",
    "    # TODO: Get all documents related to the category\n",
    "\n",
    "    # TODO: Access and print the raw text of the first document in that category\n",
    "\n",
    "    # TODO: Print the categories of the first document in that category\n",
    "\n",
    "```\n",
    "\n",
    "I'll help create a script to explore each category in the Reuters corpus systematically:\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Fetch the unique categories from Reuters corpus\n",
    "unique_categories = set(reuters.categories())\n",
    "\n",
    "# For each unique category, explore its first document\n",
    "for category in sorted(unique_categories):\n",
    "    # Get all documents related to the category\n",
    "    category_docs = reuters.fileids(category)\n",
    "\n",
    "    # Skip if no documents found\n",
    "    if not category_docs:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n### Category: {category} ###\")\n",
    "    print(f\"Number of documents: {len(category_docs)}\")\n",
    "\n",
    "    # Access and print the raw text of the first document\n",
    "    first_doc_text = reuters.raw(category_docs[0])\n",
    "    print(\"\\nFirst document excerpt (first 200 characters):\")\n",
    "    print(first_doc_text[:200])\n",
    "\n",
    "    # Print the categories of the first document\n",
    "    first_doc_categories = reuters.categories(category_docs[0])\n",
    "    print(\"\\nCategories of this document:\")\n",
    "    print(first_doc_categories)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)  # Separator for readability\n",
    "```\n",
    "\n",
    "This enhanced version:\n",
    "\n",
    "1. Imports the Reuters corpus from NLTK\n",
    "\n",
    "2. Gets all unique categories using a set to avoid duplicates\n",
    "\n",
    "3. For each category:\n",
    "   * Retrieves all documents in that category\n",
    "   * Shows the total number of documents\n",
    "   * Displays the first 200 characters of the first document\n",
    "   * Lists all categories associated with that document\n",
    "   * Adds clear formatting for better readability\n",
    "\n",
    "4. Includes error handling (skips empty categories)\n",
    "\n",
    "5. Uses sorted() to process categories in alphabetical order\n",
    "\n",
    "The output will look like:\n",
    "```\n",
    "### Category: acq ###\n",
    "Number of documents: 719\n",
    "\n",
    "First document excerpt (first 200 characters):\n",
    "COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SALE\n",
    "  Computer Terminal Systems Inc said it\n",
    "has completed the sale of 200,000 shares of its common stock, and\n",
    "warrants to acquire an additional one mln sha\n",
    "\n",
    "Categories of this document:\n",
    "['acq']\n",
    "==================================================\n",
    "\n",
    "### Category: alum ###\n",
    "...\n",
    "```\n",
    "\n",
    "This gives a comprehensive overview of:\n",
    "* How many documents are in each category\n",
    "* The content type in each category\n",
    "* How categories overlap\n",
    "* The distribution of topics in the Reuters corpus\n",
    "\n",
    "The script is particularly useful for:\n",
    "* Understanding the corpus structure\n",
    "* Identifying major themes\n",
    "* Seeing how news stories were categorized\n",
    "* Finding relationships between different categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
