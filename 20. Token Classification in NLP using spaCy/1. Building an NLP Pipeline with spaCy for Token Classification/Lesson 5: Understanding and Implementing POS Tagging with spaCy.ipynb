{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 5: Understanding and Implementing POS Tagging with spaCy\n",
                                    "\n",
                                    "Here's the provided text converted to Markdown format:\n",
                                    "\n",
                                    "\n",
                                    "# Introduction to the Lesson\n",
                                    "\n",
                                    "Hello and welcome to this lesson on **Understanding and Implementing Part-of-speech (POS) Tagging with spaCy**!\n",
                                    "\n",
                                    "In this lesson, we'll discuss what POS tagging means, its importance in Natural Language Processing (NLP), and how we can effortlessly perform it using spaCy. By the end of this lesson, you should be able to process a text and tag each token (word) with its corresponding POS using spaCy.\n",
                                    "\n",
                                    "## Introduction to POS Tagging\n",
                                    "\n",
                                    "POS tagging is the process of assigning a part-of-speech label (noun, verb, adjective, etc.) to each token (word) in a given text. For example, in the sentence \"Sam eats quickly.\", \"Sam\" is a noun, \"eats\" is a verb, and \"quickly\" is an adverb. This is important because the meaning of a sentence can significantly be determined by the POS of the words in the sentence.\n",
                                    "\n",
                                    "When we perform POS tagging, it not only identifies the POS of a word, but also its grammatical use within the sentence. For instance, \"book\" can be a noun (\"Sam reads a book.\") or a verb (\"Book a ticket for me.\"), and POS tagging helps in distinguishing between these uses.\n",
                                    "\n",
                                    "In NLP tasks like parsing, text-to-speech conversion, machine translation, and extraction of relationships and entities, POS tagging plays a crucial role. For example, in information extraction, if you want to extract all named entities that are 'organizations' from some text, knowing that a word is a proper noun (NNP in the detailed Penn Treebank POS tags set) may not be enough; you would need its context among other words in the text.\n",
                                    "\n",
                                    "## Understanding POS Tagging Implementation with spaCy\n",
                                    "\n",
                                    "Implementing POS tagging in spaCy is pretty straightforward. However, it's important to note that POS tagging in spaCy is statistical, meaning it is based on statistical models that consider the context of the words in the text. When we process a text with the `nlp` object, spaCy tokenizes the text to create a `Doc` object. This `Doc` object carries all the computed attributes and properties that we can delve into. For POS tagging, we focus on two token attributes:\n",
                                    "\n",
                                    "- **pos_**: This is the simple part-of-speech tag, using the Universal POS tag set. It provides a general POS tag, like 'NOUN', 'VERB', 'ADV', etc.\n",
                                    "- **tag_**: This is the detailed part-of-speech tag using the Penn Treebank POS tag set. It provides detailed POS information, like 'VBZ' (verb, 3rd person singular present), 'RB' (adverb), etc.\n",
                                    "\n",
                                    "## Performing POS Tagging on a Sample Text Using spaCy\n",
                                    "\n",
                                    "The power of any learning lies in the doing. Let's roll up our sleeves and dive into some code. For POS tagging with spaCy, we need to process the text and loop through the token properties of the processed `Doc` object. Let's look at how to do that.\n",
                                    "\n",
                                    "First, we import the spaCy library and load the English language model.\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "Define a sentence of English text that we want to perform POS tagging on:\n",
                                    "\n",
                                    "text = \"I am learning NLP and using spaCy for POS tagging.\"\n",
                                    "\n",
                                    "Process this text using the `nlp` function to create a `Doc` object:\n",
                                    "\n",
                                    "doc = nlp(text)\n",
                                    "\n",
                                    "Perform POS tagging on each token in the `Doc` object using a for loop:\n",
                                    "\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text:{10}} {token.lemma_:{10}} {token.pos_:{10}} {token.tag_:{10}}\")\n",
                                    "```\n",
                                    "\n",
                                    "This small piece of code will give you the POS tagging information for each word in the text, providing the word (`token.text`), its base form (`token.lemma_`), simple POS (`token.pos_`), and detailed POS tag (`token.tag_`).\n",
                                    "\n",
                                    "## Understanding the Output and Next Steps\n",
                                    "\n",
                                    "The output of the above code will be:\n",
                                    "\n",
                                    "```sh\n",
                                    "I          I          PRON       PRP       \n",
                                    "am         be         AUX        VBP       \n",
                                    "learning   learn      VERB       VBG       \n",
                                    "NLP        NLP        PROPN      NNP       \n",
                                    "and        and        CCONJ      CC        \n",
                                    "using      use        VERB       VBG       \n",
                                    "spaCy      spacy      NOUN       NN        \n",
                                    "for        for        ADP        IN        \n",
                                    "POS        POS        PROPN      NNP       \n",
                                    "tagging    tagging    NOUN       NN        \n",
                                    ".          .          PUNCT      .         \n",
                                    "```\n",
                                    "\n",
                                    "This output illustrates the tokenization and POS tagging of a text. Each row represents a token (word or punctuation) from the original text. The columns include the token itself, its lemma (base form), its simple POS tag, and its detailed POS tag. This tagging helps in understanding not just the role of each word in the sentence, but also its base form, which is crucial for many NLP tasks.\n",
                                    "\n",
                                    "## Lesson Summary and Upcoming Tasks\n",
                                    "\n",
                                    "What a journey that was! We moved from understanding POS tagging to implementing it using spaCy and working through a practical example. You've now gained a significant NLP skill and you should be able to process a text using spaCy and perform POS tagging on it.\n",
                                    "\n",
                                    "As we transition into the practice tasks, it's important to remember that learning is an iterative process. The more you do, the more you learn and reinforce that learning. The next practice tasks will allow you to apply what you've learnt today and cement this new knowledge. Happy tagging!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Refining Output Format of POS Tagging\n",
                                    "\n",
                                    "Great job, Stellar Navigator! Your next task is to change the printed output format. Currently, it prints each token's details on one line. Modify it so that each token's details are printed in a more intuitive sentence structure like: \"The word X is a Part of Speech. In its base form, it is Lemma. The detailed POS tag for X is POS Tag.\"\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "text = \"I am learning NLP and using spaCy for POS tagging.\"\n",
                                    "doc = nlp(text)\n",
                                    "\n",
                                    "for token in doc:\n",
                                    "    print(f\"{token.text} {token.lemma_} {token.pos_} {token.tag_}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "text = \"I am learning NLP and using spaCy for POS tagging.\"\n",
                                    "doc = nlp(text)\n",
                                    "\n",
                                    "for token in doc:\n",
                                    "    print(f\"The word '{token.text}' is a '{token.pos_}'. In its base form, it is '{token.lemma_}'. The detailed POS tag for '{token.text}' is '{token.tag_}'.\")\n",
                                    "```\n",
                                    "\n",
                                    "This version prints the details of each token in the specified sentence structure without filtering any tokens. Let me know if you need any further adjustments!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## POS tagging on a Real-world Text Document\n",
                                    "\n",
                                    "Awesome work, Stellar Navigator! Now, let's use the same process to perform POS tagging on a real-world text. Your task is to replace the simple English string with the first document from nltk.corpus.reuters. Remember to import the reuters dataset from nltk.corpus and then process the document as a single string. Observe the output of the POS tags on a more complex text.\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "text_string = \"I am learning NLP and using spaCy for POS tagging.\"\n",
                                    "\n",
                                    "doc = nlp(text_string)\n",
                                    "\n",
                                    "for token in doc:\n",
                                    "    print(f'The word \"{token.text}\" is a {token.pos_}. In its base form, it is \"{token.lemma_}\". The detailed POS tag for \"{token.text}\" is {token.tag_}.')\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Thank you for the clarification! Here’s the corrected code without the condition to filter out stop words and punctuation:\n",
                                    "\n",
                                    "Here's the updated code that uses the first document from the `nltk.corpus.reuters` dataset for POS tagging. This will allow you to observe the output of the POS tags on a more complex text:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "import nltk\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Download the Reuters dataset if not already downloaded\n",
                                    "nltk.download('reuters', quiet=True)\n",
                                    "\n",
                                    "# Load the English language model from spaCy\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "# Get the raw text from the first document in the Reuters dataset\n",
                                    "text_string = reuters.raw(reuters.fileids()[0])\n",
                                    "\n",
                                    "# Process the document as a single string\n",
                                    "doc = nlp(text_string)\n",
                                    "\n",
                                    "# Print the POS tagging details for each token\n",
                                    "for token in doc:\n",
                                    "    print(f'The word \"{token.text}\" is a {token.pos_}. In its base form, it is \"{token.lemma_}\". The detailed POS tag for \"{token.text}\" is {token.tag_}.')\n",
                                    "```\n",
                                    "\n",
                                    "In this code:\n",
                                    "\n",
                                    "1. The Reuters dataset is imported and downloaded if necessary.\n",
                                    "2. The first document from the Reuters dataset is retrieved and processed as a single string.\n",
                                    "3. The POS tagging details for each token in the document are printed in the specified format.\n",
                                    "\n",
                                    "This will give you a more complex text to analyze. Let me know if you need any further assistance!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Analyzing Verb Usage in Reuters News\n",
                                    "\n",
                                    "Superb work, Stellar Navigator! Are you ready to make the code encounter verbs in the Reuters news? Fill in the blanks (____) to load the spaCy model, preprocess the text, iterate through the text, tag it, and, finally, print out the top five most frequently used verbs. Let's analyze the Reuters corpus!\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "import pandas as pd\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = ____\n",
                                    "\n",
                                    "text_list = [' '.join(reuters.words(file_id)) for file_id in reuters.fileids()[:5]]\n",
                                    "text = ____ \n",
                                    "\n",
                                    "doc = ____\n",
                                    "\n",
                                    "tokens = []\n",
                                    "lemmas = []\n",
                                    "pos_tags = []\n",
                                    "detailed_pos_tags = []\n",
                                    "\n",
                                    "for token in ____:\n",
                                    "    tokens.append(____)\n",
                                    "    lemmas.append(____)\n",
                                    "    pos_tags.append(____)\n",
                                    "    detailed_pos_tags.append(____)\n",
                                    "\n",
                                    "df = pd.DataFrame(list(zip(tokens, lemmas, pos_tags, detailed_pos_tags)), \n",
                                    "                  columns = ['Token', 'Lemma', 'POS', 'Detailed_POS'])\n",
                                    "\n",
                                    "top_verbs =(df[df['POS'] == 'VERB']['Lemma'].value_counts().head(5))\n",
                                    "print(____)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here's the completed code to load the spaCy model, preprocess the text, iterate through it, tag it, and print out the top five most frequently used verbs from the Reuters corpus:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "import pandas as pd\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "text_list = [' '.join(reuters.words(file_id)) for file_id in reuters.fileids()[:5]]\n",
                                    "text = ' '.join(text_list)\n",
                                    "\n",
                                    "doc = nlp(text)\n",
                                    "\n",
                                    "tokens = []\n",
                                    "lemmas = []\n",
                                    "pos_tags = []\n",
                                    "detailed_pos_tags = []\n",
                                    "\n",
                                    "for token in doc:\n",
                                    "    tokens.append(token.text)\n",
                                    "    lemmas.append(token.lemma_)\n",
                                    "    pos_tags.append(token.pos_)\n",
                                    "    detailed_pos_tags.append(token.tag_)\n",
                                    "\n",
                                    "df = pd.DataFrame(list(zip(tokens, lemmas, pos_tags, detailed_pos_tags)), \n",
                                    "                  columns = ['Token', 'Lemma', 'POS', 'Detailed_POS'])\n",
                                    "\n",
                                    "top_verbs = df[df['POS'] == 'VERB']['Lemma'].value_counts().head(5)\n",
                                    "print(top_verbs)\n",
                                    "```\n",
                                    "\n",
                                    "In this code:\n",
                                    "\n",
                                    "- The spaCy model is loaded with `spacy.load(\"en_core_web_sm\")`.\n",
                                    "- The text is created by joining the words from the first five Reuters articles.\n",
                                    "- The `doc` variable processes the text with spaCy.\n",
                                    "- The loop iterates through the tokens in the `doc`, collecting their text, lemmas, part-of-speech tags, and detailed tags.\n",
                                    "- Finally, the top five most frequently used verbs are printed.\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Frequency Analysis on Adjectives Using POS Tagging\n",
                                    "\n",
                                    "Great work so far, Stellar Navigator! Are you ready for the next step? Your task is to create a DataFrame containing only adjectives. Then, find and print the top five most frequently used adjectives in our text document. I can't wait to see your result!\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "text_list = reuters.words(reuters.fileids()[0])\n",
                                    "text = ' '.join(word for word in text_list)\n",
                                    "\n",
                                    "doc = nlp(text)\n",
                                    "\n",
                                    "tokens = []\n",
                                    "lemmas = []\n",
                                    "pos_tags = []\n",
                                    "detailed_pos_tags = []\n",
                                    "\n",
                                    "for token in doc:\n",
                                    "    tokens.append(token.text)\n",
                                    "    lemmas.append(token.lemma_)\n",
                                    "    pos_tags.append(token.pos_)\n",
                                    "    detailed_pos_tags.append(token.tag_)\n",
                                    "\n",
                                    "df = pd.DataFrame(list(zip(tokens, lemmas, pos_tags, detailed_pos_tags)), \n",
                                    "                  columns =['Token', 'Lemma', 'POS', 'Detailed_POS'])\n",
                                    "\n",
                                    "# TODO: Create dataframe of adjectives only\n",
                                    "# TODO: Print the 5 most common adjectives\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here's the completed code to create a DataFrame containing only adjectives and to find and print the top five most frequently used adjectives from the Reuters corpus:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "text_list = reuters.words(reuters.fileids()[0])\n",
                                    "text = ' '.join(word for word in text_list)\n",
                                    "\n",
                                    "doc = nlp(text)\n",
                                    "\n",
                                    "tokens = []\n",
                                    "lemmas = []\n",
                                    "pos_tags = []\n",
                                    "detailed_pos_tags = []\n",
                                    "\n",
                                    "for token in doc:\n",
                                    "    tokens.append(token.text)\n",
                                    "    lemmas.append(token.lemma_)\n",
                                    "    pos_tags.append(token.pos_)\n",
                                    "    detailed_pos_tags.append(token.tag_)\n",
                                    "\n",
                                    "df = pd.DataFrame(list(zip(tokens, lemmas, pos_tags, detailed_pos_tags)), \n",
                                    "                  columns=['Token', 'Lemma', 'POS', 'Detailed_POS'])\n",
                                    "\n",
                                    "# Create dataframe of adjectives only\n",
                                    "adjectives_df = df[df['POS'] == 'ADJ']\n",
                                    "\n",
                                    "# Print the 5 most common adjectives\n",
                                    "top_adjectives = adjectives_df['Lemma'].value_counts().head(5)\n",
                                    "print(top_adjectives)\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation:\n",
                                    "\n",
                                    "1. **DataFrame Creation:**  \n",
                                    "   The code filters the original DataFrame `df` to create a new DataFrame `adjectives_df` that contains only the rows where the part-of-speech (POS) tag is 'ADJ' (for adjectives).\n",
                                    "\n",
                                    "2. **Finding Top Adjectives:**  \n",
                                    "   The code then counts the occurrences of each lemma in the `adjectives_df` and retrieves the top five most frequently used adjectives using `value_counts().head(5)`.\n",
                                    "\n",
                                    "3. **Output:**  \n",
                                    "   Finally, it prints the top five adjectives along with their counts.\n",
                                    "\n",
                                    "This code will help you analyze the frequency of adjectives in the text document from the Reuters corpus.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "36f03e6a",
                           "metadata": {},
                           "source": [
                                    "Well done, Stellar Navigator! Now, let's wrap it up with a fun exercise. Pick a word from the Reuters dataset and identify all the different parts of speech usages of this word. Let's explore the word '\"back\". Get started; words are waiting to be revealed!\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "word_to_explore = '____'\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "# TODO: Load the first 50 documents from the Reuters dataset\n",
                                    "\n",
                                    "# TODO: Perform POS tagging on the text documents just like you did before\n",
                                    "\n",
                                    "# TODO: Create a dataframe that reflects a POS tagging structure\n",
                                    "\n",
                                    "# TODO: Filter the dataframe to display POS tags for your selected word\n",
                                    "\n",
                                    "# TODO: Display the count of each POS category your word falls into\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here's the completed code to explore the word \"back\" in the Reuters dataset, identifying all the different parts of speech usages of this word:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "word_to_explore = 'back'\n",
                                    "\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "# Load the first 50 documents from the Reuters dataset\n",
                                    "text_list = [' '.join(reuters.words(file_id)) for file_id in reuters.fileids()[:50]]\n",
                                    "text = ' '.join(text_list)\n",
                                    "\n",
                                    "# Perform POS tagging on the text documents\n",
                                    "doc = nlp(text)\n",
                                    "\n",
                                    "tokens = []\n",
                                    "lemmas = []\n",
                                    "pos_tags = []\n",
                                    "detailed_pos_tags = []\n",
                                    "\n",
                                    "for token in doc:\n",
                                    "    tokens.append(token.text)\n",
                                    "    lemmas.append(token.lemma_)\n",
                                    "    pos_tags.append(token.pos_)\n",
                                    "    detailed_pos_tags.append(token.tag_)\n",
                                    "\n",
                                    "# Create a dataframe that reflects a POS tagging structure\n",
                                    "df = pd.DataFrame(list(zip(tokens, lemmas, pos_tags, detailed_pos_tags)), \n",
                                    "                  columns=['Token', 'Lemma', 'POS', 'Detailed_POS'])\n",
                                    "\n",
                                    "# Filter the dataframe to display POS tags for your selected word\n",
                                    "filtered_df = df[df['Token'].lower() == word_to_explore]\n",
                                    "\n",
                                    "# Display the count of each POS category your word falls into\n",
                                    "pos_count = filtered_df['POS'].value_counts()\n",
                                    "print(pos_count)\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation:\n",
                                    "\n",
                                    "1. **Word Selection:**  \n",
                                    "   The variable `word_to_explore` is set to \"back\".\n",
                                    "\n",
                                    "2. **Loading Documents:**  \n",
                                    "   The code loads the first 50 documents from the Reuters dataset and combines their words into a single text string.\n",
                                    "\n",
                                    "3. **POS Tagging:**  \n",
                                    "   The text is processed with spaCy to perform part-of-speech tagging.\n",
                                    "\n",
                                    "4. **DataFrame Creation:**  \n",
                                    "   A DataFrame `df` is created to store the tokens, lemmas, POS tags, and detailed POS tags.\n",
                                    "\n",
                                    "5. **Filtering for the Selected Word:**  \n",
                                    "   The DataFrame is filtered to create `filtered_df`, which contains only the rows where the token matches \"back\" (case insensitive).\n",
                                    "\n",
                                    "6. **Counting POS Categories:**  \n",
                                    "   Finally, the code counts the occurrences of each POS category for the word \"back\" and prints the results.\n",
                                    "\n",
                                    "This will allow you to explore the different usages of the word \"back\" in the context of the Reuters dataset.\n"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
