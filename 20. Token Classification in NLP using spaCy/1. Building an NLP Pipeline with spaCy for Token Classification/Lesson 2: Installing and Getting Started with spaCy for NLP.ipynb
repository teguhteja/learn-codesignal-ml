{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 2: Installing and Getting Started with spaCy for NLP\n",
                                    "\n",
                                    "### Lesson Overview\n",
                                    "\n",
                                    "In this lesson, we'll sharpen our understanding of tokenization, exploring more advanced aspects such as handling special token types and working with different cases. Using the Reuters dataset and spaCy, our versatile NLP library, we'll go beyond basic tokenization, implementing strategies to handle punctuation, numbers, non-alphabetical characters, and stopwords. This lesson aims to deepen our NLP expertise and make text preprocessing even more effective.\n",
                                    "\n",
                                    "### Tokenization and Its Role in NLP\n",
                                    "\n",
                                    "Firstly, let's revisit tokenization. In our previous lesson, we introduced tokenization as the process of splitting up text into smaller pieces, called tokens. These tokens work as the basic building blocks in NLP, enabling us to process and analyze text more efficiently. It's like slicing a cake into pieces to serve, where each slice or token represents a piece of the overall content (the cake).\n",
                                    "\n",
                                    "Different types of tokens exist, each serving a unique purpose in NLP. Today, we will explore four types: punctuation tokens, numerical tokens, non-alphabetic tokens, and stopword tokens.\n",
                                    "\n",
                                    "### Exploring Different Types of Tokens\n",
                                    "\n",
                                    "Knowing the types of tokens we are working with is fundamental for successful NLP tasks. Let's take a closer look at each one:\n",
                                    "\n",
                                    "- **Punctuation Tokens**: These are tokens composed of punctuation marks such as full stops, commas, exclamation marks, etc. Although often disregarded, punctuation can sometimes hold significant meaning, affecting the interpretation of the text.\n",
                                    "\n",
                                    "- **Numerical Tokens**: These represent numbers found in the text. Depending on the context, numerical tokens can provide valuable information or, alternatively, can serve as noise that you might want to filter out.\n",
                                    "\n",
                                    "- **Non-Alphabetic Tokens**: Such tokens consist of characters that are neither letters nor numbers. They include spaces, punctuation, symbols, etc.\n",
                                    "\n",
                                    "- **Stopword Tokens**: Generally, these are common words like 'is', 'at', 'which', 'on'. In many NLP tasks, stopwords are filtered out as they often provide little to no meaningful information.\n",
                                    "\n",
                                    "### Dealing with Special Cases in Tokenization\n",
                                    "\n",
                                    "Special tokens like those mentioned often need to be treated differently depending on the task at hand. For instance, while punctuation might be critical for sentiment analysis (imagine an exclamation mark to express excitement), you may wish to ignore it while performing tasks like topic identification.\n",
                                    "\n",
                                    "spaCy provides us with simple and efficient methods to handle these token types. For instance, with `token.is_punct`, we can filter out all punctuation tokens from our token list. Similarly, we can use `token.like_num` to filter numerical tokens, `token.is_alpha` to filter out non-alphabetic tokens, and `token.is_stop` to identify stopword tokens.\n",
                                    "\n",
                                    "### Interactive Code Run: Classifying Tokens with spaCy\n",
                                    "\n",
                                    "Let's now run our example code and see these methods in action.\n",
                                    "\n",
                                    "```python\n",
                                    "# Import necessary modules\n",
                                    "import spacy\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Load English tokenizer\n",
                                    "nlp = spacy.load(\"en_core_web_sm\")\n",
                                    "\n",
                                    "# Get the raw document from reuters dataset\n",
                                    "doc_id = reuters.fileids()[0]\n",
                                    "doc_text = reuters.raw(doc_id)\n",
                                    "\n",
                                    "# Pass the raw document to the nlp object\n",
                                    "doc = nlp(doc_text)\n",
                                    "\n",
                                    "# Get all punctuation tokens in the document\n",
                                    "punctuation_tokens = [token.text for token in doc if token.is_punct]\n",
                                    "print('Punctuation Tokens: ', punctuation_tokens)\n",
                                    "\n",
                                    "# Get all numerical tokens in the document\n",
                                    "numerical_tokens = [token.text for token in doc if token.like_num]\n",
                                    "print('Numerical Tokens: ', numerical_tokens)\n",
                                    "\n",
                                    "# Get all non-alphabetical tokens in the document\n",
                                    "non_alpha_tokens = [token.text for token in doc if not token.is_alpha]\n",
                                    "print('Non-Alphabetic Tokens: ', non_alpha_tokens)\n",
                                    "\n",
                                    "# Get all stopword tokens in the document\n",
                                    "stopword_tokens = [token.text for token in doc if token.is_stop]\n",
                                    "print('Stopword Tokens: ', stopword_tokens)\n",
                                    "\n",
                                    "# Let's extract the unique non-stopword, non-punctuation alpha tokens from the document\n",
                                    "non_stop_alpha_tokens = list(set([token.text for token in doc if not token.is_stop and\n",
                                    "                                  token.is_alpha and not token.is_punct]))\n",
                                    "\n",
                                    "print('\\nUnique non-stopword, non-punctuation alphanumeric tokens: ', non_stop_alpha_tokens)\n",
                                    "```\n",
                                    "The output of the above code will be:\n",
                                    "\n",
                                    "```sh\n",
                                    "Punctuation Tokens:  ['.', '.', '.', ',', '.', ',', ',', ',', ',', '.', ...] (truncated for brevity)\n",
                                    "Numerical Tokens:  ['1986', '15', 'April', '5', '5', '15']\n",
                                    "Non-Alphabetic Tokens:  ['1986', '15', '.', '.', 'April', '5', ',', '5', '15', ',', ...] (truncated for brevity)\n",
                                    "Stopword Tokens:  ['for', 'the', 'to', 'of', 'and', ...] (truncated for brevity)\n",
                                    "\n",
                                    "Unique non-stopword, non-punctuation alphanumeric tokens:  ['Lorem', 'Ipsum', 'Dolor', 'Sit', 'Amet', ...] (truncated for brevity)\n",
                                    "```\n",
                                    "\n",
                                    "This output demonstrates the classification of tokens into different categories using spaCy: punctuation, numerical, non-alphabetical, and stopword tokens. It also showcases the extraction of unique non-stopword, non-punctuation alphanumeric tokens, illustrating a common preprocessing step in NLP.\n",
                                    "\n",
                                    "### Summary and Upcoming Practice Tasks\n",
                                    "\n",
                                    "Great work getting through this lesson! Today, we boosted our understanding of tokenization in NLP, exploring different types of tokens and strategies to handle them. We also dove deep into special token types such as punctuation, numerical tokens, non-alphabetic tokens, and stopwords, understanding why and when they matter in NLP applications.\n",
                                    "\n",
                                    "Now, it's time to cement this knowledge with some hands-on practice. Up next are exercises that will require you to implement the techniques we covered today. Don't worry - solving these tasks will be crucial for mastering token classification and building a strong foundation for more complex NLP tasks. Let's dive in!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Changing the String for Tokenization\n",
                                    "\n",
                                    "Great job, Stellar Navigator! Let's delve deeper into token analysis. For this task, update the code to filter out tokens that are both stop words and non-alphabetic. Create a new list, non_alpha_stopword_tokens, store the required tokens in it and print out the result.\n",
                                    "\n",
                                    "```python\n",
                                    "# Firstly, let's import spaCy and load the English language model\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# import the Reuters Corpus\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Then select a document from the Reuters Corpus\n",
                                    "doc_id = reuters.fileids()[9]\n",
                                    "doc_text = reuters.raw(doc_id)\n",
                                    "\n",
                                    "# And Process the text with spaCy\n",
                                    "doc = nlp(doc_text)\n",
                                    "\n",
                                    "# Now, let's analyze these tokens more\n",
                                    "punctuation_tokens = [token.text for token in doc if token.is_punct]\n",
                                    "print('Punctuation tokens: ', punctuation_tokens)\n",
                                    "\n",
                                    "numerical_tokens = [token.text for token in doc if token.like_num]\n",
                                    "print('Numerical tokens: ', numerical_tokens)\n",
                                    "\n",
                                    "non_alpha_tokens = [token.text for token in doc if not token.is_alpha]\n",
                                    "print('Non-alphabetic tokens: ', non_alpha_tokens)\n",
                                    "\n",
                                    "stopword_tokens = [token.text for token in doc if token.is_stop]\n",
                                    "print('Stopword tokens: ', stopword_tokens)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here's the updated code that filters out tokens that are both stop words and non-alphabetic. A new list, `non_alpha_stopword_tokens`, is created to store these tokens, and the result is printed out.\n",
                                    "\n",
                                    "```python\n",
                                    "# Firstly, let's import spaCy and load the English language model\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Import the Reuters Corpus\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Then select a document from the Reuters Corpus\n",
                                    "doc_id = reuters.fileids()[9]\n",
                                    "doc_text = reuters.raw(doc_id)\n",
                                    "\n",
                                    "# And process the text with spaCy\n",
                                    "doc = nlp(doc_text)\n",
                                    "\n",
                                    "# Now, let's analyze these tokens more\n",
                                    "punctuation_tokens = [token.text for token in doc if token.is_punct]\n",
                                    "print('Punctuation tokens: ', punctuation_tokens)\n",
                                    "\n",
                                    "numerical_tokens = [token.text for token in doc if token.like_num]\n",
                                    "print('Numerical tokens: ', numerical_tokens)\n",
                                    "\n",
                                    "non_alpha_tokens = [token.text for token in doc if not token.is_alpha]\n",
                                    "print('Non-alphabetic tokens: ', non_alpha_tokens)\n",
                                    "\n",
                                    "stopword_tokens = [token.text for token in doc if token.is_stop]\n",
                                    "print('Stopword tokens: ', stopword_tokens)\n",
                                    "\n",
                                    "# Create a new list for tokens that are both stop words and non-alphabetic\n",
                                    "non_alpha_stopword_tokens = [token.text for token in doc if not token.is_alpha and token.is_stop]\n",
                                    "print('Non-alphabetic Stopword tokens: ', non_alpha_stopword_tokens)\n",
                                    "```\n",
                                    "\n",
                                    "This code will now output the tokens that are both stop words and non-alphabetic, allowing for a more refined analysis of the text."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Tokenize Sentences with Missing Code\n",
                                    "\n",
                                    "Nice work, Stellar Navigator! It's time to use more built-in token functions. Use the is_oov function to find out-of-vocabulary tokens (words that are not in the predefined vocabulary or word list of a language model) and is_digit to identify digital tokens. Fill in the blanks (____) in the code to accomplish this.\n",
                                    "\n",
                                    "```python\n",
                                    "# Firstly, import spaCy and load the English language model\n",
                                    "# We are using medium model for `is_oov` function\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_md')\n",
                                    "\n",
                                    "# Secondly, import the Reuters Corpus\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Select a document from the Reuters Corpus\n",
                                    "doc_id = reuters.fileids()[9]\n",
                                    "doc_text = reuters.raw(doc_id)\n",
                                    "\n",
                                    "# Process the text with spaCy\n",
                                    "doc = nlp(doc_text)\n",
                                    "\n",
                                    "# Let's analyze these tokens more!\n",
                                    "oov_tokens = [token.text for token in doc if ____]\n",
                                    "print('Out-of-vocabulary tokens: ', oov_tokens)\n",
                                    "\n",
                                    "digit_tokens = [token.text for token in doc if ____]\n",
                                    "print('Digit tokens: ', digit_tokens)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here's the completed code with the appropriate functions filled in to identify out-of-vocabulary tokens and digit tokens:\n",
                                    "\n",
                                    "```python\n",
                                    "# Firstly, import spaCy and load the English language model\n",
                                    "# We are using medium model for `is_oov` function\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_md')\n",
                                    "\n",
                                    "# Secondly, import the Reuters Corpus\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Select a document from the Reuters Corpus\n",
                                    "doc_id = reuters.fileids()[9]\n",
                                    "doc_text = reuters.raw(doc_id)\n",
                                    "\n",
                                    "# Process the text with spaCy\n",
                                    "doc = nlp(doc_text)\n",
                                    "\n",
                                    "# Let's analyze these tokens more!\n",
                                    "oov_tokens = [token.text for token in doc if token.is_oov]\n",
                                    "print('Out-of-vocabulary tokens: ', oov_tokens)\n",
                                    "\n",
                                    "digit_tokens = [token.text for token in doc if token.is_digit]\n",
                                    "print('Digit tokens: ', digit_tokens)\n",
                                    "```\n",
                                    "\n",
                                    "In this code, `token.is_oov` is used to find out-of-vocabulary tokens, and `token.is_digit` is used to identify digital tokens. This will help you analyze the text more effectively!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Tokenizing First Reuters Document with spaCy\n",
                                    "\n",
                                    "You're doing great, Stellar Navigator!\n",
                                    "\n",
                                    "Let's make it a bit more challenging, shall we? Fill in the blank spots (____) to make the code work.\n",
                                    "\n",
                                    "It should count the number of stop words in the text.\n",
                                    "Print the count of stop word tokens.\n",
                                    "\n",
                                    "```python\n",
                                    "# Firstly, import spaCy and load the English language model\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Secondly, import the Reuters Corpus\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Select a document from the Reuters Corpus\n",
                                    "doc_id = reuters.fileids()[11]\n",
                                    "doc_text = reuters.raw(doc_id)\n",
                                    "\n",
                                    "# Process the text with spaCy\n",
                                    "doc = nlp(doc_text)\n",
                                    "\n",
                                    "# Let's analyze these tokens more!\n",
                                    "stopword_tokens = [token.text for token in doc if ____]\n",
                                    "print('Number of stopword tokens: ', ____)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here's the completed code with the appropriate functions filled in to count the number of stop words in the text:\n",
                                    "\n",
                                    "```python\n",
                                    "# Firstly, import spaCy and load the English language model\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Secondly, import the Reuters Corpus\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Select a document from the Reuters Corpus\n",
                                    "doc_id = reuters.fileids()[11]\n",
                                    "doc_text = reuters.raw(doc_id)\n",
                                    "\n",
                                    "# Process the text with spaCy\n",
                                    "doc = nlp(doc_text)\n",
                                    "\n",
                                    "# Let's analyze these tokens more!\n",
                                    "stopword_tokens = [token.text for token in doc if token.is_stop]\n",
                                    "print('Number of stopword tokens: ', len(stopword_tokens))\n",
                                    "```\n",
                                    "\n",
                                    "In this code, `token.is_stop` is used to filter the stop words, and `len(stopword_tokens)` counts the number of stop word tokens. This will print the total count of stop words in the selected document."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Calculating Unique Tokens in Document\n",
                                    "\n",
                                    "Great job, Celestial Traveler!\n",
                                    "\n",
                                    "For this task, you are required to complete the print statements to display the uppercase and lowercase tokens in the document. Utilize what you have learned in the lesson to analyze the tokens!\n",
                                    "\n",
                                    "```python\n",
                                    "# Firstly, import spaCy and load the English language model\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Secondly, import the Reuters Corpus\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Select a document from the Reuters Corpus\n",
                                    "doc_id = reuters.fileids()[2]\n",
                                    "doc_text = reuters.raw(doc_id)\n",
                                    "\n",
                                    "# Process the text with spaCy\n",
                                    "doc = nlp(doc_text)\n",
                                    "\n",
                                    "# Let's analyze these tokens more!\n",
                                    "capitalized_tokens = [token.text for token in doc if token.is_title]\n",
                                    "print('Capitalized tokens: ', capitalized_tokens)\n",
                                    "\n",
                                    "# TODO: Print uppercase tokens\n",
                                    "\n",
                                    "# TODO: Print lowercase tokens\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here's the completed code with the print statements filled in to display the uppercase and lowercase tokens in the document:\n",
                                    "\n",
                                    "```python\n",
                                    "# Firstly, import spaCy and load the English language model\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Secondly, import the Reuters Corpus\n",
                                    "from nltk.corpus import reuters\n",
                                    "\n",
                                    "# Select a document from the Reuters Corpus\n",
                                    "doc_id = reuters.fileids()[2]\n",
                                    "doc_text = reuters.raw(doc_id)\n",
                                    "\n",
                                    "# Process the text with spaCy\n",
                                    "doc = nlp(doc_text)\n",
                                    "\n",
                                    "# Let's analyze these tokens more!\n",
                                    "capitalized_tokens = [token.text for token in doc if token.is_title]\n",
                                    "print('Capitalized tokens: ', capitalized_tokens)\n",
                                    "\n",
                                    "# Print uppercase tokens\n",
                                    "uppercase_tokens = [token.text for token in doc if token.is_upper]\n",
                                    "print('Uppercase tokens: ', uppercase_tokens)\n",
                                    "\n",
                                    "# Print lowercase tokens\n",
                                    "lowercase_tokens = [token.text for token in doc if token.is_lower]\n",
                                    "print('Lowercase tokens: ', lowercase_tokens)\n",
                                    "```\n",
                                    "\n",
                                    "In this code, `token.is_upper` is used to filter uppercase tokens, and `token.is_lower` is used to filter lowercase tokens. The results will display the respective tokens from the selected document."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Tokenizing Multiple Reuters Documents with spaCy\n",
                                    "\n",
                                    "Fantastic effort, Stellar Navigator! Now, for the final challenge: Write a simple pipeline to tokenize a sentence, filter out punctuation, stop words, and non-alphabetic tokens, and then print out the remaining tokens. Remember, practice is the key to mastering any skill!\n",
                                    "\n",
                                    "```python\n",
                                    "# Firstly, import spaCy and load the English language model\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# TODO: Define a sentence to process\n",
                                    "\n",
                                    "\n",
                                    "# TODO: Process the sentence with spaCy\n",
                                    "\n",
                                    "\n",
                                    "# TODO: Step1: Filter out punctuation\n",
                                    "\n",
                                    "\n",
                                    "# TODO: Step2: Filter out stop words\n",
                                    "\n",
                                    "\n",
                                    "# TODO: Step3: Filter out non-alphabetic tokens\n",
                                    "\n",
                                    "\n",
                                    "# TODO: Print the remaining tokens\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here's the completed code for a simple pipeline that tokenizes a sentence, filters out punctuation, stop words, and non-alphabetic tokens, and then prints out the remaining tokens:\n",
                                    "\n",
                                    "```python\n",
                                    "# Firstly, import spaCy and load the English language model\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Define a sentence to process\n",
                                    "sentence = \"The quick brown fox jumps over the lazy dog!\"\n",
                                    "\n",
                                    "# Process the sentence with spaCy\n",
                                    "doc = nlp(sentence)\n",
                                    "\n",
                                    "# Step 1: Filter out punctuation\n",
                                    "tokens_no_punct = [token for token in doc if not token.is_punct]\n",
                                    "\n",
                                    "# Step 2: Filter out stop words\n",
                                    "tokens_no_stop = [token for token in tokens_no_punct if not token.is_stop]\n",
                                    "\n",
                                    "# Step 3: Filter out non-alphabetic tokens\n",
                                    "remaining_tokens = [token.text for token in tokens_no_stop if token.is_alpha]\n",
                                    "\n",
                                    "# Print the remaining tokens\n",
                                    "print('Remaining tokens: ', remaining_tokens)\n",
                                    "```\n",
                                    "\n",
                                    "In this code, we define a sentence, process it with spaCy, and then filter out punctuation, stop words, and non-alphabetic tokens step by step. Finally, the remaining tokens are printed out. This pipeline effectively demonstrates the tokenization and filtering process!"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
