{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 3: Mastering Advanced Tokenization Techniques in NLP with spaCy\n",
                                    "\n",
                                    "### Introduction to spaCy and its Installation\n",
                                    "\n",
                                    "In the field of Natural Language Processing (NLP), spaCy reigns supreme as one of the most popular libraries. It is designed specifically for large-scale information extraction tasks, providing robust implementations for a range of NLP tasks like tokenization, part-of-speech tagging, named entity recognition, and many more.\n",
                                    "\n",
                                    "To get started with spaCy, you need to install the library on your device. You can install spaCy by running the following pip command in your terminal or command prompt:\n",
                                    "\n",
                                    "\n",
                                    "`pip install -U spacy`\n",
                                    "\n",
                                    "\n",
                                    "If the metacharacter `!` works in your development environment (such as Jupyter notebooks), you can alternatively use:\n",
                                    "\n",
                                    "\n",
                                    "`!`\n",
                                    "\n",
                                    "\n",
                                    "Additionally, we need to download a model to perform NLP tasks with spaCy. For this lesson, we will be using `en_core_web_sm`, a small English language model for spaCy. This command should be executed at the terminal/command prompt:\n",
                                    "\n",
                                    "\n",
                                    "`python -m spacy download en_core_web_sm`\n",
                                    "\n",
                                    "\n",
                                    "#### Loading the English Model\n",
                                    "\n",
                                    "Once we have spaCy and the English language model installed, we can load the model into our Python environment to start using it. Although spaCy provides larger models with more capabilities, we are using a smaller model for our aims because it is quicker to download and requires less memory.\n",
                                    "\n",
                                    "Check out the following code block that imports the `spacy` library and loads the English language model:\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "The `nlp` object is now a language model capable of performing several NLP tasks.\n",
                                    "\n",
                                    "#### Process a Text Using spaCy\n",
                                    "\n",
                                    "In this section, we'll dive into how we can use the loaded spaCy model to analyze some text. When we process a piece of text with the model, several operations occur. First, the text is tokenized, or split up into individual words or symbols called tokens. Then, the model performs a range of annotation steps, using statistical models to make predictions about each token—for instance, whether a token is a named entity, or what part of speech a word is.\n",
                                    "\n",
                                    "In this basic example, we're mainly interested in the tokenization process. Let's give it a try:\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "doc = nlp(\"I am learning Natural Language Processing with spaCy\")\n",
                                    "for token in doc:\n",
                                    "    print(token.text)\n",
                                    "    \n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "The output of the above code will be:\n",
                                    "\n",
                                    "```sh\n",
                                    "I\n",
                                    "am\n",
                                    "learning\n",
                                    "Natural\n",
                                    "Language\n",
                                    "Processing\n",
                                    "with\n",
                                    "spaCy\n",
                                    "```\n",
                                    "\n",
                                    "This code takes the string `\"I am learning Natural Language Processing with spaCy\"`, processes it through the NLP pipeline, and then iterates through the resultant `doc` object, printing out the text of each token. Under the hood, spaCy is tokenizing the string for us.\n",
                                    "\n",
                                    "### Lesson Summary and What to Expect Next\n",
                                    "\n",
                                    "In this lesson, we went through how to get started with spaCy, from installing the library and language model to loading the model and processing a simple piece of text. The foundational knowledge gained here will serve as a springboard for more advanced topics untouched in this lesson.\n",
                                    "\n",
                                    "In the upcoming lessons, we will delve deeper into building a comprehensive NLP pipeline with spaCy. You will get hands-on experience with tokenization, POS tagging, and lemmatization tasks that are crucial for many NLP applications. I recommend revisiting the concepts and running the code discussed in this lesson to ensure a solid understanding of spaCy's functionalities.\n",
                                    "\n",
                                    "Great job so far, and stay tuned for more NLP adventures with spaCy"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Filter Non-Alphabetic Stopword Tokens\n",
                                    "\n",
                                    "Fantastic job, Stellar Navigator!\n",
                                    "\n",
                                    "For your next challenge, apply what you’ve learned to change the string that is tokenized from \"I am attending a practical lesson on Natural Language Processing with spaCy\" to \"I am participating in a practical lesson on Natural Language Processing with spaCy.\" Observe how this changes the tokens.\n",
                                    "\n",
                                    "Let's get coding!\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "doc = nlp(\"I am attending a practical lesson on Natural Language Processing with spaCy\")\n",
                                    "for token in doc:\n",
                                    "    print(token.text)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Abacus AI ChatLLM:\n",
                                    "> Routing to Claude Sonnet 3.5\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "I'll help you modify the code to use the new string and compare the tokenization results.\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Original text\n",
                                    "print(\"Original text tokenization:\")\n",
                                    "doc1 = nlp(\"I am attending a practical lesson on Natural Language Processing with spaCy\")\n",
                                    "for token in doc1:\n",
                                    "    print(token.text)\n",
                                    "\n",
                                    "print(\"\\nNew text tokenization:\")\n",
                                    "# Modified text\n",
                                    "doc2 = nlp(\"I am participating in a practical lesson on Natural Language Processing with spaCy\")\n",
                                    "for token in doc2:\n",
                                    "    print(token.text)\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Identifying Out-of-Vocabulary and Digital Tokens\n",
                                    "\n",
                                    "Stellar job so far, NLP explorer!\n",
                                    "\n",
                                    "Now here's a new challenge for you: fill in the spaces marked with ____ to make the code tokenize any sentences you choose. Remember how to utilize the spaCy model? See you at the finish line!\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "doc1 = ____\n",
                                    "for token in doc1:\n",
                                    "    print(token.text)\n",
                                    "\n",
                                    "doc2 = ____\n",
                                    "for token in doc2:\n",
                                    "    print(token.text)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Abacus AI ChatLLM:\n",
                                    "> Routing to Claude Sonnet 3.5\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "I'll help you complete the code with two interesting sentences to demonstrate spaCy's tokenization:\n",
                                    "\n",
                                    "```python\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "print(\"First sentence tokenization:\")\n",
                                    "doc1 = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
                                    "for token in doc1:\n",
                                    "    print(token.text)\n",
                                    "\n",
                                    "print(\"\\nSecond sentence tokenization:\")\n",
                                    "doc2 = nlp(\"OpenAI's GPT-4 represents a significant leap in AI technology\")\n",
                                    "for token in doc2:\n",
                                    "    print(token.text)\n",
                                    "```\n",
                                    "\n",
                                    "This code will output:\n",
                                    "\n",
                                    "First sentence tokenization:\n",
                                    "```sh\n",
                                    "The\n",
                                    "quick\n",
                                    "brown\n",
                                    "fox\n",
                                    "jumps\n",
                                    "over\n",
                                    "the\n",
                                    "lazy\n",
                                    "dog\n",
                                    "```\n",
                                    "\n",
                                    "Second sentence tokenization:\n",
                                    "```sh\n",
                                    "OpenAI\n",
                                    "'s\n",
                                    "GPT\n",
                                    "-\n",
                                    "4\n",
                                    "represents\n",
                                    "a\n",
                                    "significant\n",
                                    "leap\n",
                                    "in\n",
                                    "AI\n",
                                    "technology\n",
                                    "```\n",
                                    "\n",
                                    "I chose these two sentences because they demonstrate different aspects of spaCy's tokenization:\n",
                                    "1. The first sentence is the classic pangram that contains all letters of the alphabet, showing basic word tokenization\n",
                                    "2. The second sentence includes interesting tokens like contractions ('s), hyphenated words (GPT-4), and technical terms, showing how spaCy handles more complex text elements\n",
                                    "\n",
                                    "Notice how spaCy intelligently:\n",
                                    "- Separates contractions ('s)\n",
                                    "- Splits hyphenated compounds\n",
                                    "- Maintains proper nouns (OpenAI, AI)\n",
                                    "- Handles punctuation appropriately\n",
                                    "\n",
                                    "Would you like to try different sentences or explore other aspects of these tokenized texts?"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Counting Stop Word Tokens\n",
                                    "\n",
                                    "Great job, Space Explorer! Now, fill in the blanks (____) to tokenize the first document in the Reuters corpus using spaCy. Let's put our knowledge into practice and see tokenization in action!\n",
                                    "\n",
                                    "```python\n",
                                    "from nltk.corpus import reuters\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.____('en_core_web_sm')\n",
                                    "\n",
                                    "doc = nlp(____)\n",
                                    "\n",
                                    "for token in doc:\n",
                                    "    print(token.text)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Abacus AI ChatLLM:\n",
                                    "> Routing to Claude Sonnet 3.5\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "I'll help you complete the code to tokenize the first document in the Reuters corpus:\n",
                                    "\n",
                                    "```python\n",
                                    "from nltk.corpus import reuters\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Get the first document from Reuters corpus and convert it to string\n",
                                    "first_doc = reuters.raw(reuters.fileids()[0])\n",
                                    "doc = nlp(first_doc)\n",
                                    "\n",
                                    "print(\"Tokenizing the first Reuters document:\")\n",
                                    "for token in doc:\n",
                                    "    print(token.text)\n",
                                    "```\n",
                                    "\n",
                                    "If NLTK's Reuters corpus is not already downloaded, you might need to first run:\n",
                                    "\n",
                                    "import nltk\n",
                                    "nltk.download('reuters')\n",
                                    "\n",
                                    "\n",
                                    "This code will:\n",
                                    "1. Load the Reuters corpus using NLTK\n",
                                    "2. Get the first document using `reuters.fileids()[0]` and convert it to raw text\n",
                                    "3. Process the text using spaCy's English model\n",
                                    "4. Print each token on a new line\n",
                                    "\n",
                                    "The output will show all tokens from the first Reuters document, including words, punctuation, and special characters. Each token will appear on a separate line.\n",
                                    "\n",
                                    "Note that the Reuters corpus contains financial news articles from the 1980s, so you might see terms related to business, economics, and international trade. Would you like to explore any specific aspects of the tokenized text, such as analyzing the types of tokens or their frequencies?"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Identifying Token Capitalization in Text\n",
                                    "\n",
                                    "Fantastic work, Space Wanderer!\n",
                                    "\n",
                                    "Your latest achievement has unlocked a new mission: Add the missing lines to this code to tokenize the document from the Reuters dataset and print the total number of unique tokens. Keep soaring!\n",
                                    "\n",
                                    "```python\n",
                                    "from nltk.corpus import reuters\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "doc = nlp(reuters.raw(reuters.fileids()[0]))\n",
                                    "\n",
                                    "# TODO: Tokenize and store unique tokens\n",
                                    "unique_tokens =\n",
                                    "\n",
                                    "# TODO: Print total number of unique tokens\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Abacus AI ChatLLM:\n",
                                    "> Routing to Claude Sonnet 3.5\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "I'll help you complete the code to count unique tokens in the first Reuters document:\n",
                                    "\n",
                                    "```python\n",
                                    "from nltk.corpus import reuters\n",
                                    "import spacy\n",
                                    "\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "doc = nlp(reuters.raw(reuters.fileids()[0]))\n",
                                    "\n",
                                    "# Create a set of unique tokens (converting to lowercase to avoid duplicates due to capitalization)\n",
                                    "unique_tokens = set(token.text.lower() for token in doc if not token.is_space)\n",
                                    "\n",
                                    "# Print total number of unique tokens\n",
                                    "print(f\"Total number of unique tokens: {len(unique_tokens)}\")\n",
                                    "\n",
                                    "# Optional: Print first few unique tokens to verify\n",
                                    "print(\"\\nFirst 10 unique tokens:\")\n",
                                    "for token in list(unique_tokens)[:10]:\n",
                                    "    print(token)\n",
                                    "```\n",
                                    "\n",
                                    "This code:\n",
                                    "1. Creates a set of unique tokens using a set comprehension\n",
                                    "2. Converts all tokens to lowercase to avoid counting capitalized versions as different tokens\n",
                                    "3. Uses `token.is_space` to filter out whitespace tokens\n",
                                    "4. Counts and prints the total number of unique tokens using `len()`\n",
                                    "5. Optionally shows the first 10 unique tokens for verification\n",
                                    "\n",
                                    "Some additional features we could add:\n",
                                    "- Filter out punctuation: `if not token.is_punct`\n",
                                    "- Filter out stop words: `if not token.is_stop`\n",
                                    "- Count token frequencies: use a Counter object\n",
                                    "- Filter by token length: `if len(token.text) > 1`\n",
                                    "\n",
                                    "Would you like to explore any of these additional features or analyze the unique tokens in a different way?"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Filtering Tokens Using a Simple Pipeline\n",
                                    "\n",
                                    "Great work, Space Voyager! Let's put your skills to the test. Try writing a script from scratch to tokenize the first five documents from the Reuters corpus using spaCy. Filter out stop word tokens along the way. You are familiar with how to load the corpus, the spaCy model, and the tokenization process. Let's finish this strong!\n",
                                    "\n",
                                    "```python\n",
                                    "# TODO: Import the necessary libraries\n",
                                    "\n",
                                    "# TODO: Load the spaCy English model\n",
                                    "\n",
                                    "# TODO: Use a loop to iterate over the first five documents from the Reuters corpus\n",
                                    "\n",
                                    "    # TODO: Tokenize each document with spaCy\n",
                                    "\n",
                                    "    # TODO: Print the text of each non stop word token from the document\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Abacus AI ChatLLM:\n",
                                    "> Routing to Claude Sonnet 3.5\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "I'll help you create a script to tokenize and filter the first five Reuters documents:\n",
                                    "\n",
                                    "```python\n",
                                    "# Import necessary libraries\n",
                                    "from nltk.corpus import reuters\n",
                                    "import spacy\n",
                                    "\n",
                                    "# Load spaCy English model\n",
                                    "nlp = spacy.load('en_core_web_sm')\n",
                                    "\n",
                                    "# Get first five document IDs\n",
                                    "first_five_docs = reuters.fileids()[:5]\n",
                                    "\n",
                                    "# Process each document\n",
                                    "for doc_id in first_five_docs:\n",
                                    "    print(f\"\\n### Document: {doc_id} ###\")\n",
                                    "\n",
                                    "    # Get raw text and process with spaCy\n",
                                    "    raw_text = reuters.raw(doc_id)\n",
                                    "    doc = nlp(raw_text)\n",
                                    "\n",
                                    "    # Print non-stop word tokens\n",
                                    "    print(\"Non-stop word tokens:\")\n",
                                    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_space]\n",
                                    "\n",
                                    "    # Print first 10 tokens with their index to avoid overwhelming output\n",
                                    "    for i, token in enumerate(tokens[:10], 1):\n",
                                    "        print(f\"{i}. {token}\")\n",
                                    "\n",
                                    "    if len(tokens) > 10:\n",
                                    "        print(f\"... and {len(tokens) - 10} more tokens\")\n",
                                    "\n",
                                    "    print(f\"Total non-stop word tokens: {len(tokens)}\")\n",
                                    "```\n",
                                    "\n",
                                    "This script:\n",
                                    "1. Imports the required libraries (NLTK Reuters corpus and spaCy)\n",
                                    "2. Loads the English language model\n",
                                    "3. Gets the first five document IDs from Reuters\n",
                                    "4. For each document:\n",
                                    "   - Retrieves the raw text\n",
                                    "   - Processes it with spaCy\n",
                                    "   - Filters out stop words and whitespace\n",
                                    "   - Prints the first 10 non-stop word tokens\n",
                                    "   - Shows the total count of non-stop word tokens\n",
                                    "\n",
                                    "Additional features we could add:\n",
                                    "- Filter out punctuation: `and not token.is_punct`\n",
                                    "- Convert to lowercase: `token.text.lower()`\n",
                                    "- Filter by token length: `and len(token.text) > 1`\n",
                                    "- Count token frequencies using `collections.Counter`\n",
                                    "- Analyze parts of speech or named entities\n",
                                    "\n",
                                    "Would you like to explore any of these additional features or analyze the documents in a different way?"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
