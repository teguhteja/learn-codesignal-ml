{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 6: Mastering Model Evaluation: Performance Metrics & Selection in Machine Learning\n",
                                    "\n",
                                    "# Mastering Model Evaluation: Performance Metrics & Selection in Machine Learning\n",
                                    "\n",
                                    "## Lesson Overview\n",
                                    "In this final lesson on Model Evaluation Post-Optimization, we navigate through logistic regression, decision trees, and explore ensemble techniques to enhance accuracy. We tie it all together by comparing models post-optimization and making an informed selection to ensure reliable predictions.\n",
                                    "\n",
                                    "## Model Evaluation and Selection: A Practical Approach\n",
                                    "Post-optimization model evaluation transcends mere accuracy comparisons. It involves analyzing performance metrics such as precision, recall, and f1-score to choose a model that truly understands the data, ensuring selection of a model that generalizes well on unseen data.\n",
                                    "\n",
                                    "### Logistic Regression Optimization\n",
                                    "We begin by fine-tuning our Logistic Regression model:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.model_selection import GridSearchCV\n",
                                    "from sklearn.linear_model import LogisticRegression\n",
                                    "\n",
                                    "# Hyperparameter tuning with GridSearchCV\n",
                                    "lr_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l2']}\n",
                                    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
                                    "clf_lr = GridSearchCV(lr, lr_params, cv=5)\n",
                                    "clf_lr.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Evaluating optimized Logistic Regression\n",
                                    "lr_pred = clf_lr.predict(X_test)\n",
                                    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, lr_pred))\n",
                                    "```\n",
                                    "\n",
                                    "The best parameters found are `{'C': 1, 'penalty': 'l2'}`, significantly enhancing our model performance. Evaluating the optimized Logistic Regression yields:\n",
                                    "\n",
                                    "```sh\n",
                                    "Logistic Regression Classification Report:\n",
                                    "               precision    recall  f1-score   support\n",
                                    "\n",
                                    "           0       0.97      0.98      0.98        63\n",
                                    "           1       0.99      0.98      0.99       108\n",
                                    "\n",
                                    "    accuracy                           0.98       171\n",
                                    "   macro avg       0.98      0.98      0.98       171\n",
                                    "weighted avg       0.98      0.98      0.98       171\n",
                                    "```\n",
                                    "\n",
                                    "### Random Forest & Gradient Boosting Performance\n",
                                    "Next, we evaluate Random Forest and Gradient Boosting:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                                    "\n",
                                    "# Random Forest\n",
                                    "rf = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
                                    "rf_pred = rf.predict(X_test)\n",
                                    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n",
                                    "\n",
                                    "# Gradient Boosting\n",
                                    "gb = GradientBoostingClassifier(random_state=42).fit(X_train, y_train)\n",
                                    "gb_pred = gb.predict(X_test)\n",
                                    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, gb_pred))\n",
                                    "```\n",
                                    "\n",
                                    "- **Random Forest Classifier Accuracy:** `0.9707602339181286`\n",
                                    "- **Gradient Boosting Classifier Accuracy:** `0.9590643274853801`\n",
                                    "\n",
                                    "These comparisons illuminate the balanced performance of Logistic Regression across all metrics, overtaking the ensemble techniques in our scenario.\n",
                                    "\n",
                                    "### Final Model Selection and Evaluation\n",
                                    "After optimizing and evaluating our models, the crucial step is to select the best-performing model. This decision is made not solely on accuracy but considering a holistic view of performance metrics.\n",
                                    "\n",
                                    "Given the comprehensive analysis, the optimized Logistic Regression emerges as the preferred choice. Adopting it as our final model, we conduct an ultimate evaluation:\n",
                                    "\n",
                                    "```python\n",
                                    "# Final Model Selection and Evaluation\n",
                                    "# Based on the accuracy and classification report, choose the best model\n",
                                    "final_model = clf_lr.best_estimator_\n",
                                    "final_predictions = final_model.predict(X_test)\n",
                                    "print(\"Final Model Accuracy:\", accuracy_score(y_test, final_predictions))\n",
                                    "print(\"Final Model Classification Report:\\n\", classification_report(y_test, final_predictions))\n",
                                    "```\n",
                                    "\n",
                                    "```sh\n",
                                    "Final Model Accuracy: 0.9824561403508771\n",
                                    "Final Model Classification Report:\n",
                                    "               precision    recall  f1-score   support\n",
                                    "\n",
                                    "           0       0.97      0.98      0.98        63\n",
                                    "           1       0.99      0.98      0.99       108\n",
                                    "\n",
                                    "    accuracy                           0.98       171\n",
                                    "   macro avg       0.98      0.98      0.98       171\n",
                                    "weighted avg       0.98      0.98      0.98       171\n",
                                    "```\n",
                                    "\n",
                                    "Given its superior precision, recall, and f1-score alongside outstanding accuracy, our final model choice is justified.\n",
                                    "\n",
                                    "## Key Takeaways\n",
                                    "The process of model selection post-optimization is pivotal, requiring a careful balance between different performance metrics. Our effort with Logistic Regression demonstrates that meticulously tuning and evaluating models can significantly enhance their predictive capabilities. Remember, the essence of machine learning lies in experimenting, evaluating, and selecting the best model tailored to the specific needs of the dataset at hand.\n",
                                    "\n",
                                    "**Wow, another course almost done! You're getting so good at this! Let's do some more practice and call it complete.**\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Scaling Features for Improved Accuracy"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Performance Boost with Data Scaling"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
