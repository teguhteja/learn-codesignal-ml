{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Strategies for Effective Feature Selection in Machine Learning\n",
    "\n",
    "Strategies for Effective Feature Selection in Machine Learning\n",
    "Topic Overview\n",
    "Hello and welcome! Today, we will delve into the captivating domain of Machine Learning, focusing specifically on Varying Strategies for Feature Selection. In this lesson, we aim to demystify and explore the various strategies involved in selecting informative features from our dataset. This is an essential step in building robust machine learning models.\n",
    "\n",
    "Feature Selection is akin to cherry-picking the most relevant columns (features) from a table (dataset). It contributes significantly to a model's performance, simplifying it, reducing computational costs, and most importantly, improving its accuracy. For instance, in the context of the UCI's Abalone Dataset, we have features such as Sex, Length, Diameter, etc. Our goal is to identify which of these hold the most relevance to our targeted prediction: the age of an Abalone.\n",
    "\n",
    "Now let's dive into Feature Selection strategies: Filter Method, Wrapper Method, and Embedded Method. We'll apply these on the UCI's Abalone Dataset to gain practical understanding.\n",
    "\n",
    "Understand the Concept of Feature Selection\n",
    "Let's explore the essence of Feature Selection in Machine Learning. This central process involves identifying and selecting the most relevant variables (features) for your predictive modeling task.\n",
    "\n",
    "Visualize a dataset as a cluttered work table, where each feature is a tool. Feature Selection resembles the process of selecting the most suitable tools to complete a task. In the context of our Abalone Dataset, imagine an array of features describing each Abalone. Feature Selection helps us ascertain which ones are crucial in predicting the age of an Abalone.\n",
    "\n",
    "Implications of Feature Selection\n",
    "So, why is a carefully conducted Feature Selection process so vital?\n",
    "\n",
    "Consider a scenario where you're building a house—would you use every tool in the toolbox, or would you choose the ones most suitable for each job? Plausibly, using an inappropriate tool or an excessive number of tools could lead to mistakes and inefficiencies.\n",
    "\n",
    "In the context of the Abalone Dataset, suppose we have a feature that inaccurately records a measurement. This unwanted 'noise' could confuse our model and lead to interpretational errors that could harm our model's performance. As such, a thoughtful and thorough Feature Selection process is indispensable.\n",
    "\n",
    "Varying Strategies for Feature Selection\n",
    "Now let's deep-dive into the primary categories of feature selection algorithms: Filter Methods, Wrapper Methods, and Embedded Methods. In each case, we will supplement our analysis with illustrative Python code, using the UCI's Abalone Dataset.\n",
    "\n",
    "Filter Methods\n",
    "Filter Methods examine the relevance of features based on their properties. It's akin to sifting gold: you rinse the dirt and rock off and keep the gold based on its intrinsic value.\n",
    "\n",
    "For instance, with the Abalone Dataset, we might want to select features that are strongly correlated with age. Below is an illustrative code snippet using Chi-Square:\n",
    "\n",
    "Python\n",
    "Copy\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "#One-hot encode \"Sex\" feature\n",
    "X = pd.get_dummies(X)\n",
    "#Convert y to 1D array\n",
    "y = y.values.ravel()\n",
    "\n",
    "# Assume X and y to be the feature set and target variable\n",
    "best_features = SelectKBest(score_func=chi2, k=3) # We choose top 3 features\n",
    "fit = best_features.fit(X,y)\n",
    "print(fit.get_feature_names_out()) #prints ['Whole_weight' 'Sex_F' 'Sex_I'] \n",
    "This demonstrates how this filter method selects the 3 features with the highest Chi-Square correlation\n",
    "\n",
    "Wrapper Methods\n",
    "Wrapper Methods assess a set of features as a search problem, evaluating, comparing, and selecting different combinations. It's like picking a team for a relay race where the combination of team members is even more important than their individual strengths. Here is a sample Recursive Feature Elimination (RFE) code:\n",
    "\n",
    "Python\n",
    "Copy\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=250)\n",
    "rfe = RFE(model, n_features_to_select= 3) # We choose top 3 features\n",
    "fit = rfe.fit(X, y)\n",
    "print(fit.get_feature_names_out())#prints ['Whole_weight' 'Shucked_weight' 'Shell_weight']\n",
    "Notice how this wrapper method selects different features than the filter method because the wrapper method uses LogisticRegression instead of SelectKBest.\n",
    "\n",
    "Embedded Methods\n",
    "Embedded Methods combine the benefits of both Filter Methods and Wrapper Methods by performing feature selection and model training simultaneously. It's like being in a reality show where participants are eliminated based on their performance in each round. Here, we've showcased an example using the Lasso (L1 regularization) embedded method:\n",
    "\n",
    "Python\n",
    "Copy\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso = LassoCV(cv=5).fit(X, y)\n",
    "sfm = SelectFromModel(lasso)\n",
    "fit = sfm.fit(X,y) \n",
    "print(fit.get_feature_names_out()) \n",
    "#prints ['Diameter' 'Height' 'Whole_weight' 'Shucked_weight' 'Viscera_weight' 'Shell_weight' 'Sex_I' 'Sex_M']\n",
    "In all these methods, the chosen subset can be accessed using fit.get_support(indices=True) and fit.get_feature_names_out().\n",
    "\n",
    "Lesson Summary and Practice\n",
    "Great work! You've successfully understood what Feature Selection is and learned about the different strategies such as Filter, Wrapper, and Embedded methods in feature selection. This knowledge will be invaluable as you advance in your Machine Learning journey.\n",
    "\n",
    "Now it's time to put your knowledge into practice. Prepare for some hands-on exercises designed to solidify your understanding. Remember, skills are not just acquired—they need to be practiced to perfection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing With Wrapper Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Embedded Feature Selection Method using Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining Feature Selection with f_classif"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
