{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: Unveiling the Power of Feature Interaction in Machine Learning Model Accuracy\n",
    "\n",
    "Unveiling the Power of Feature Interaction in Machine Learning Model Accuracy\n",
    "Overview\n",
    "Welcome to a fresh chapter of our Feature Engineering for Machine Learning! Today, we'll unravel an insightful element of machine learning models: feature interaction. Using our trusty sidekick, the UCI Abalone Dataset, we'll traverse the fascinating world of feature interaction and discover its unparalleled influence on model accuracy.\n",
    "\n",
    "Feature interaction plays a vital role, especially in the world of machine learning. When multiple attributes jointly influence the target in a way that individual features cannot capture, they are said to \"interact\". By recognizing and leveraging these interactions, we can guide our machine-learning models to make more accurate predictions.\n",
    "\n",
    "Understanding Feature Interaction\n",
    "In a machine-learning context, feature interaction can be divided into additive and multiplicative interactions. An additive interaction means that the effects of two or more individual features combine, contributing to the target variable. Conversely, multiplicative interaction implies that features enhance or dampen each other's impact.\n",
    "\n",
    "Consider a real-life scenario. Predicting an individual's happiness isn't solely dependent on their personal life or work life. Instead, it's an interaction of both. A balance between a satisfying personal and work life leads to a happy individual.\n",
    "\n",
    "Feature Interaction in the UCI Abalone Dataset\n",
    "Let's unravel the potential interactions hidden within our UCI Abalone Dataset. We'll engineer new features based on our conjectures and assess their impact.\n",
    "\n",
    "Python\n",
    "Copy\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect the UCI Abalone dataset\n",
    "abalone = fetch_ucirepo(id=1)\n",
    "X = abalone.data.features\n",
    "y = abalone.data.targets\n",
    "\n",
    "# Engineering new features which multiply Shucked weight and height.\n",
    "X['Shucked_weight*Height'] = X['Shucked_weight'] * X['Height']\n",
    "\n",
    "# Exclude the categorical feature 'Sex' before computing the correlation matrix\n",
    "numerical_features = X.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Creating a correlation matrix for numerical features\n",
    "correlation_matrix = numerical_features.corr()\n",
    "\n",
    "# Display correlation matrix as a heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Displaying the correlation matrix as a heatmap provides us with a holistic view of how each feature relates to the others. The color intensity indicates the correlation's strength, painting a more vivid picture.\n",
    "\n",
    "Measuring the Impact of Feature Interaction on Model Accuracy\n",
    "Model accuracy reflects the proportion of correct predictions made by the machine learning model. It precisely communicates how the model performs across a series of tests or holdouts.\n",
    "\n",
    "Let's witness the impact of feature interaction by creating a machine-learning model with and without our engineered feature:\n",
    "\n",
    "Python\n",
    "Copy\n",
    "# Importing necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Perform one-hot encoding on the 'Sex' column\n",
    "X_encoded = pd.get_dummies(X, columns=['Sex'])\n",
    "\n",
    "# Baseline model excluding the engineered feature\n",
    "X_base = X_encoded.drop('Shucked_weight*Height', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_base, y, test_size=0.2, random_state=42)\n",
    "lr_base = LinearRegression()\n",
    "lr_base.fit(X_train, y_train)\n",
    "y_pred_base = lr_base.predict(X_test)\n",
    "mse_base = mean_squared_error(y_test, y_pred_base)\n",
    "\n",
    "# Model including the engineered feature\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Displaying the model performance\n",
    "print(f\"MSE for Baseline model: {mse_base}\")\n",
    "print(f\"MSE for Model with engineered feature: {mse}\")\n",
    "output\n",
    "\n",
    "Markdown\n",
    "Copy\n",
    "MSE for Baseline model: 4.891232447128562\n",
    "MSE for Model with engineered feature: 4.847949790041983\n",
    "In this case, our model is evaluated based on the Mean Square Error(MSE). A lower MSE implies that the predicted values deviate less from the actual values, indicating a model that excels in predictions. While the impact of the feature is not very meaningful, we still see an improvement in prediction quality.\n",
    "\n",
    "Lesson Summary and Practice\n",
    "Kudos on journeying through the intriguing landscape of feature interaction! You've gained a comprehensive understanding of feature interaction, engineered features based on interactions, and observed their effect on model accuracy.\n",
    "\n",
    "Feature interaction paves new pathways, enabling machine learning models to capture more complex relationships and enhance their predictive power.\n",
    "\n",
    "Before we wrap up, you'll encounter several practice exercises designed to reinforce and apply the skills and knowledge you've gained today. Enjoy exploring multiple feature combinations and the unique impacts they project. Remember, the more you practice, the more you learn. Happy engineering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Impact of the 'Viscera_Shell' Feature on Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alter the Linear Regression Model for a Different Feature Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging 'Viscera_Shell' Feature"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
