{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 4: Math Behind Neural Networks\n",
                                    "\n",
                                    "## Math of Neural Networks and the Universal Approximation Theorem\n",
                                    "\n",
                                    "Neural networks are computational systems inspired by the biological neural networks found in human and animal brains. At their core, these networks consist of layers of nodes, or \"neurons,\" each of which applies a simple computation to its inputs. The Universal Approximation Theorem provides the theoretical foundation for these systems, offering assurance that neural networks have the capacity to model a wide variety of functions given sufficient complexity and proper configuration.\n",
                                    "\n",
                                    "### Mathematical Representation of a Neural Network\n",
                                    "\n",
                                    "At the simplest level, a neural network can be thought of as a function \\( f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m \\) where \\( n \\) is the dimensionality of the input vector and \\( m \\) is the dimensionality of the output vector. A basic feed-forward neural network with one hidden layer can be mathematically represented as:\n",
                                    "\n",
                                    "\\[\n",
                                    "f(x) = \\sigma(W_2 \\cdot \\sigma(W_1 \\cdot x + b_1) + b_2)\n",
                                    "\\]\n",
                                    "\n",
                                    "Where:\n",
                                    "- \\( x \\) is the input vector.\n",
                                    "- \\( W_1 \\) and \\( W_2 \\) are matrices representing the weights of the first and second layers, respectively.\n",
                                    "- \\( b_1 \\) and \\( b_2 \\) are vectors representing the biases of the first and second layers, respectively.\n",
                                    "- \\( \\sigma \\) represents the activation function applied element-wise. Common choices for \\( \\sigma \\) include the sigmoid function, ReLU (Rectified Linear Unit), and tanh (hyperbolic tangent).\n",
                                    "\n",
                                    "### The Role of the Activation Function\n",
                                    "\n",
                                    "The activation function is a vital component of a neural network. As its name implies, it governs the output, or 'activation,' of a neuron. Its importance lies in its unique ability to introduce non-linearity into the model, which broadens the range and complexity of functions the network can represent.\n",
                                    "\n",
                                    "Without activation functions, a neural network comprising many layers would merely apply a sequence of linear transformations on the input data. Regardless of how many times you apply them, a composition of linear transformations results in another linear transformation. Thus, a neural network without any activation functions, no matter how many layers it has, behaves similarly to linear regression, performing a linear transformation on the input data.\n",
                                    "\n",
                                    "By introducing non-linearity via activation functions, the network is empowered to learn from and represent much more complex patterns in the data.\n",
                                    "\n",
                                    "#### Common Activation Functions\n",
                                    "\n",
                                    "- **Sigmoid function**: Outputs a value between 0 and 1, making it useful for binary classification problems to represent probabilities. However, it suffers from the vanishing gradients problem, limiting its use in deep networks.\n",
                                    "  \n",
                                    "  \\[\n",
                                    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
                                    "  \\]\n",
                                    "\n",
                                    "- **Hyperbolic Tangent (tanh)**: Outputs a value between -1 and 1. It's a scaled version of the sigmoid function and, like sigmoid, suffers from the risk of vanishing gradients.\n",
                                    "\n",
                                    "  \\[\n",
                                    "  \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
                                    "  \\]\n",
                                    "\n",
                                    "- **Rectified Linear Unit (ReLU)**: Keeps positive inputs unchanged and outputs 0 for negative inputs. It's simple, computationally efficient, and widely used in many neural networks. However, it may cause dead neurons which never get activated.\n",
                                    "\n",
                                    "  \\[\n",
                                    "  \\text{ReLU}(x) = \\max(0, x)\n",
                                    "  \\]\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "\n",
                                    "def sigmoid(x):\n",
                                    "    return 1 / (1 + np.exp(-x))\n",
                                    "\n",
                                    "def tanh(x):\n",
                                    "    return np.tanh(x)\n",
                                    "\n",
                                    "def relu(x):\n",
                                    "    return np.maximum(0, x)\n",
                                    "```\n",
                                    "\n",
                                    "These functions enable the neural network to model a diversity of complex, non-linear phenomena, making them indispensable in the world of deep learning.\n",
                                    "\n",
                                    "### The Universal Approximation Theorem - Simplified Explanation and Code\n",
                                    "\n",
                                    "The Universal Approximation Theorem (UAT) is a key mathematical concept guiding the functionality of neural networks. UAT declares that a neural network with just one hidden layer - a layer between the input and output - containing a finite number of neurons (nodes where computation takes place), can nearly replicate or mimic any sort of continuous function.\n",
                                    "\n",
                                    "Imagine the role of a hidden layer as a talented ensemble of artists. If you have a picture (a function) that you'd like them to recreate, they can do it with their collective skill set. Each artist (neuron) specializes in a different type of stroke or style, and together, they combine their talents to reproduce the image. To replicate more complex pictures (functions), you might need more artists (neurons) or an artist capable of a broader range of styles (non-linear activation function). However, as the Universal Approximation Theorem insists, they will always be able to recreate the picture to the desired level of accuracy.\n",
                                    "\n",
                                    "Here, the artist's style is analogous to the activation function in a neural network, which is typically a non-linear function that transforms the input they receive. The Universal Approximation Theorem does come with a small caveat - it specifies that the activation function must be a non-constant, bounded, and increasing function.\n",
                                    "\n",
                                    "To implement the concept in code and understand it better, let's explore a simple example:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Define a target function \n",
                                    "def target_function(x):\n",
                                    "    return x * np.sin(x)\n",
                                    "\n",
                                    "# Define the points where the function will be evaluated\n",
                                    "x = np.linspace(0, 10, 100)\n",
                                    "\n",
                                    "# Apply the target function \n",
                                    "y = target_function(x) \n",
                                    "\n",
                                    "# Plot the target function\n",
                                    "plt.plot(x, y, label=\"Target Function: $f(x) = x*\\sin(x)$\")\n",
                                    "\n",
                                    "# Let's simulate an approximation using a neural network \n",
                                    "n_neurons = 10\n",
                                    "np.random.seed(42) \n",
                                    "\n",
                                    "# Simulate random weights and biases for each neuron\n",
                                    "weights = np.random.rand(n_neurons)\n",
                                    "biases = np.random.rand(n_neurons)\n",
                                    "\n",
                                    "# Simulate neurons\n",
                                    "neurons = np.tanh(weights * x.reshape(-1, 1) + biases)\n",
                                    "\n",
                                    "# Learn the weighting of the neurons\n",
                                    "coefficients = np.linalg.lstsq(neurons, y, rcond=None)[0]\n",
                                    "\n",
                                    "# approximate function\n",
                                    "y_approx = neurons @ coefficients\n",
                                    "\n",
                                    "plt.plot(x, y_approx, label=\"Neural Network Approximation\")\n",
                                    "plt.legend()\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "Thus, with just 10 neurons and the tanh activation function, you can see that our network does a decent job approximating the target function \\( f(x) = x \\cdot \\sin(x) \\). Of course, more complex functions may require more hidden neurons or additional layers. However, according to the Universal Approximation Theorem, they can still be approximated by a neural network!\n",
                                    "\n",
                                    "### Deep Neural Networks & The Universal Approximation Theorem\n",
                                    "\n",
                                    "The Universal Approximation Theorem (UAT) in its original form pertains to neural networks with just a single hidden layer. However, in practice, we often encounter many more layers, which constitutes what we call Deep Neural Networks.\n",
                                    "\n",
                                    "In the world of Deep Learning, these deep networks have proven to make a significant difference. When you add more hidden layers, what you're essentially doing is introducing a hierarchy of concepts learned by the neural network. For example, in a deep neural network designed for image recognition, the initial layers might learn to recognize simple patterns like edges, the middle layers may combine these patterns to recognize slightly more complex shapes, and the last layers might identify high-level features such as an entire object.\n",
                                    "\n",
                                    "Interestingly, while the original UAT does not directly apply to deep networks, subsequent research and extensions of the theorem do indicate that deep networks can be more efficient at approximating complex functions compared to shallow networks. Specifically, certain functions that could be compactly represented in a deep network might require exponentially more neurons to be represented in a shallow network.\n",
                                    "\n",
                                    "Let's revisit our previous example, using a deeper network this time:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Define a target function \n",
                                    "def target_function(x):\n",
                                    "    return x * np.sin(x)\n",
                                    "\n",
                                    "# Define the points where the function will be evaluated\n",
                                    "x = np.linspace(0, 10, 100)\n",
                                    "\n",
                                    "# Apply the target function \n",
                                    "y = target_function(x) \n",
                                    "\n",
                                    "# Plot the target function\n",
                                    "plt.plot(x, y, label=\"Target Function: $f(x) = x*\\sin(x)$\")\n",
                                    "\n",
                                    "# Let's simulate an approximation using a deeper neural network \n",
                                    "np.random.seed(42) \n",
                                    "\n",
                                    "# Simulate random weights and biases for each neuron in two layers\n",
                                    "weights_1 = np.random.rand(10)\n",
                                    "biases_1 = np.random.rand(10)\n",
                                    "weights_2 = np.random.rand(10)\n",
                                    "biases_2 = np.random.rand(10)\n",
                                    "\n",
                                    "# Simulate the first layer of neurons\n",
                                    "neurons_1 = np.tanh(weights_1 * x.reshape(-1, 1) + biases_1)\n",
                                    "\n",
                                    "# The output of the first layer of neurons feeds into the second layer\n",
                                    "neurons_2 = np.tanh(weights_2 * neurons_1 + biases_2)\n",
                                    "\n",
                                    "# Learn the weighting of the neurons\n",
                                    "coefficients = np.linalg.lstsq(neurons_2, y, rcond=None)[0]\n",
                                    "\n",
                                    "# approximate function\n",
                                    "y_approx = neurons_2 @ coefficients\n",
                                    "\n",
                                    "plt.plot(x, y_approx, label=\"Deep Neural Network Approximation\")\n",
                                    "plt.legend()\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "With more hidden layers (each simulating a group of artists working on each detail level), our deep network could achieve a high degree of accuracy while approximating a complex function. This power of deep networks to build up layers of abstraction is why they're successful in tasks like image recognition, speech recognition, and natural language processing.\n",
                                    "\n",
                                    "## Summary and How TensorFlow Hides Away the Math Complexity\n",
                                    "\n",
                                    "Neural networks, though theoretically simple, become intricate as the layers deepen and connections increase. The Universal Approximation Theorem guarantees that, given a sufficient number of neurons, these networks can approximate a wide array of functions. However, the practicality of this often requires sophisticated software tools.\n",
                                    "\n",
                                    "TensorFlow is a powerful tool for designing, training, and deploying neural networks. TensorFlow abstracts away much of the mathematical complexity involved in neural network implementation, allowing researchers and developers to focus more on architecture and problem-solving rather than the underlying calculus and linear algebra.\n",
                                    "\n",
                                    "### Here's a simplified breakdown of how TensorFlow achieves this:\n",
                                    "\n",
                                    "1. **Automatic Differentiation**: TensorFlow automatically computes gradients, a key step in training neural networks, especially for backpropagation.\n",
                                    "  \n",
                                    "2. **Graph Computations**: TensorFlow represents computations as data flow graphs, optimizing performance, particularly for large-scale models.\n",
                                    "\n",
                                    "3. **Tensor Abstraction**: Operations in TensorFlow revolve around 'tensors,' n-dimensional arrays that generalize matrices, hiding the intricate mathematical manipulations.\n",
                                    "\n",
                                    "4. **High-Level APIs**: TensorFlow provides high-level APIs like Keras that make defining, training, and deploying models user-friendly without deep dives into the underlying mathematics.\n",
                                    "\n",
                                    "In essence, TensorFlow and similar tools simplify the complexities of neural networks, enabling widespread adoption and application even by those without extensive mathematical backgrounds.\n",
                                    "\n",
                                    "```python\n",
                                    "import tensorflow as tf\n",
                                    "from tensorflow.keras import layers, models\n",
                                    "\n",
                                    "# Create a simple feed-forward network in TensorFlow/Keras\n",
                                    "model = models.Sequential([\n",
                                    "    layers.Dense(10, activation='tanh', input_shape=(1,)),\n",
                                    "    layers.Dense(10, activation='tanh'),\n",
                                    "    layers.Dense(1)  # output layer\n",
                                    "])\n",
                                    "\n",
                                    "# Compile the model\n",
                                    "model.compile(optimizer='adam', loss='mse')\n",
                                    "\n",
                                    "# Train the model (using dummy data)\n",
                                    "x_train = np.linspace(0, 10, 100)\n",
                                    "y_train = x_train * np.sin(x_train)\n",
                                    "model.fit(x_train, y_train, epochs=500, verbose=0)\n",
                                    "\n",
                                    "# Predict the function values using the trained model\n",
                                    "y_pred = model.predict(x_train)\n",
                                    "\n",
                                    "# Plot the prediction\n",
                                    "plt.plot(x_train, y_train, label='True function')\n",
                                    "plt.plot(x_train, y_pred, label='Neural network approximation')\n",
                                    "plt.legend()\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "In this code snippet, a simple feed-forward neural network is implemented using TensorFlow/Keras. The model is trained to approximate the function \\( f(x) = x \\cdot \\sin(x) \\). With just a few lines of code, TensorFlow handles the gradient calculations, optimizes the network parameters, and trains the model, making neural network implementation accessible and manageable.\n",
                                    "\n",
                                    "**Conclusion**: Neural networks are powerful tools for modeling complex functions, supported by the Universal Approximation Theorem. While the underlying math can be intricate, tools like TensorFlow significantly reduce the complexity, making it possible to build and train neural networks with ease. As a result, neural networks have become the backbone of many modern AI applications, capable of solving a diverse array of problems.\n",
                                    "\n",
                                    "\n",
                                    "## ðŸ“Š Summary: Math Behind Neural Networks\n",
                                    "\n",
                                    "### ðŸ§  **Neural Networks Basics**\n",
                                    "1. **Core Concept**: Neural networks are inspired by biological systems, consisting of layers of nodes (neurons).\n",
                                    "2. **Mathematical Foundation**: A simple neural network can be represented as \\( f(x) = \\sigma(W_2 \\cdot \\sigma(W_1 \\cdot x + b_1) + b_2) \\).\n",
                                    "\n",
                                    "### ðŸ“ˆ **Activation Functions**\n",
                                    "1. **Purpose**: Introduce non-linearity to broaden the range of functions a network can model.\n",
                                    "2. **Common Functions**:\n",
                                    "   - **Sigmoid**: \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)\n",
                                    "   - **tanh**: \\( \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)\n",
                                    "   - **ReLU**: \\( \\text{ReLU}(x) = \\max(0, x) \\)\n",
                                    "\n",
                                    "### ðŸ“š **Universal Approximation Theorem (UAT)**\n",
                                    "1. **Theorem Summary**: A neural network with one hidden layer can approximate any continuous function with enough neurons.\n",
                                    "2. **Implication**: Provides the theoretical backing for the power of neural networks in function approximation.\n",
                                    "\n",
                                    "### ðŸ›  **TensorFlowâ€™s Role**\n",
                                    "1. **Abstraction**: Simplifies neural network implementation by hiding mathematical complexities.\n",
                                    "2. **Features**:\n",
                                    "   - **Automatic Differentiation**: For gradient computation.\n",
                                    "   - **Graph Computations**: Efficient performance for large models.\n",
                                    "   - **Tensor Abstraction**: Manages n-dimensional arrays.\n",
                                    "   - **High-Level APIs**: Eases model definition and training.\n",
                                    "\n",
                                    "### ðŸŽ¯ **Conclusion**\n",
                                    "Neural networks, supported by the Universal Approximation Theorem, are powerful tools for approximating complex functions. TensorFlow further simplifies their implementation, making neural network modeling accessible and efficient for a wide range of applications.\n",
                                    "\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implementing the Sigmoid Activation Function"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implementing the ReLU Activation Function"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Neuron Output Calculation Fix"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Defining Second Layer of Neurons"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
