{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Exploring Sklearn: Introduction to Machine Learning Basics\n",
    "\n",
    "Exploring Sklearn: Introduction to Machine Learning Basics\n",
    "Machine Learning and Sklearn: An Introduction\n",
    "Welcome! This lesson paves your path toward understanding machine learning and the powerful Python library, sklearn. Machine learning, an application of artificial intelligence, enables systems to learn and improve without being explicitly programmed. It plays a key role in various sectors, such as autonomous vehicles, voice recognition systems, and recommendation engines.\n",
    "\n",
    "Suppose you aim to predict housing prices as an illustration. This scenario constitutes a standard supervised learning problem wherein you train your model using past data. With sklearn, you can import the data, preprocess it, select an algorithm (like linear regression), train the model with the training data, and make predictions. All these steps can be accomplished without manually implementing algorithms.\n",
    "\n",
    "Importing the Iris Dataset\n",
    "Datasets form the backbone of machine learning. In this course, we'll use the Iris dataset, which consists of measurements — namely, sepal length, sepal width, petal length, and petal width — for 150 flowers representing three species of iris.\n",
    "\n",
    "Sklearn provides an easy-to-use load_iris function to import the Iris dataset. Let's see how it works:\n",
    "\n",
    "Python\n",
    "Copy\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "Here, the load_iris() function loads the dataset and assigns it to the iris variable. We then separate the dataset into X for features and y for the target.\n",
    "\n",
    "Furthermore, you can print the description of the dataset for more detailed insight using the DESCR attribute as follows:\n",
    "\n",
    "Python\n",
    "Copy\n",
    "print(iris.DESCR)\n",
    "Output:\n",
    "\n",
    "Markdown\n",
    "Copy\n",
    ".. _iris_dataset:\n",
    "\n",
    "Iris plants dataset\n",
    "--------------------\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    "    :Number of Instances: 150 (50 in each of three classes)\n",
    "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
    "    :Attribute Information:\n",
    "        - sepal length in cm\n",
    "        - sepal width in cm\n",
    "        - petal length in cm\n",
    "        - petal width in cm\n",
    "        - class:\n",
    "                - Iris-Setosa\n",
    "                - Iris-Versicolour\n",
    "                - Iris-Virginica\n",
    "                \n",
    "    :Missing Attribute Values: None\n",
    "    :Class Distribution: 33.3% for each of 3 classes.\n",
    "    :Creator: R.A. Fisher\n",
    "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
    "    :Date: July, 1988\n",
    "\n",
    "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
    "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
    "Machine Learning Repository, which has two wrong data points.\n",
    "\n",
    "The mean sepal length is 5.843, which is close to the mean of all other\n",
    "attributes. The petal width varies from 0.1cm to 2.5cm, indicating a large\n",
    "range.\n",
    "\n",
    "This is perhaps the best-known database to be found in the\n",
    "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
    "is referenced frequently to this day. \n",
    "This code prints a detailed description of the dataset and its attributes.\n",
    "\n",
    "Exploring Sklearn's Functionality\n",
    "After the data loading, let's delve into how Python and sklearn enable us to explore it. 'Features’ and 'Target' are two critical terms related to the dataset. Here, 'Features' refer to the attributes of the Iris flower: sepal length, sepal width, petal length, and petal width. 'Target', on the other hand, refers to the species of the Iris flower, which we aim to predict based on these features.\n",
    "\n",
    "The data and target attributes of the iris object hold the feature matrix and the response vector, respectively. The shape property gives information about their dimensionality - how many examples we have and how many features each example consists of.\n",
    "\n",
    "Python\n",
    "Copy\n",
    "print(\"Data shape: \", iris.data.shape)  # Prints (150, 4)\n",
    "print(\"Targets shape: \", iris.target.shape)  # Prints (150,)\n",
    "Output:\n",
    "\n",
    "Markdown\n",
    "Copy\n",
    "Data shape:  (150, 4)\n",
    "Targets shape:  (150,)\n",
    "Preparing Dataset for Model Training\n",
    "Before feeding our data to the machine learning model, we must split it into a training set and a test set. The training set teaches our model, while the test set evaluates its performance. Sklearn allows for the convenient split of these datasets using the train_test_split function from the model_selection module.\n",
    "\n",
    "Python\n",
    "Copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size: \", len(X_train))  # Prints 120\n",
    "print(\"Test set size: \", len(X_test))  # Prints 30\n",
    "Output:\n",
    "\n",
    "Markdown\n",
    "Copy\n",
    "Training set size:  120\n",
    "Test set size:  30\n",
    "Here, the train_test_split function has divided our data into a training set — 80% of the original data, and a test set — the remaining 20%.\n",
    "\n",
    "Quick Look at Sklearn's Model Structure\n",
    "Each machine learning model in sklearn is represented as a Python class. These classes offer an interface that includes methods for building the model (fit), making predictions (predict), and evaluating the model's performance (score).\n",
    "\n",
    "In the next, more concrete lesson, you'll see how to apply these methods after selecting a specific type of machine learning model. For now, understand that the procedure of using these models would look something as follows:\n",
    "\n",
    "Python\n",
    "Copy\n",
    "# model = SomeModel(args)\n",
    "# model.fit(X_train, y_train)\n",
    "# predictions = model.predict(X_test)\n",
    "# score = model.score(X_test, y_test)\n",
    "Congratulations! With the knowledge acquired from this lesson, you now understand what sklearn is, how to import data using it, the process of preparing data for machine learning tasks, and the rudimentary structure of sklearn models. The upcoming sessions will build upon this fundamental understanding by introducing you to more specific machine-learning models and optimization tricks. Keep practicing and continue learning as we're taking the first steps into the exciting world of machine learning! Keep up the good work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Shape of the Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Iris Dataset from Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the Dataset Splitting Code"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
