{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 1 : Creating a Document Processor for Contextual Retrieval\n",
                                    "\n",
                                    "## Creating a Document Processor for Contextual Retrieval\n",
                                    "\n",
                                    "Welcome to the first lesson of our course on building a RAG-powered chatbot with LangChain and Python! In this course, we’ll be creating a complete Retrieval-Augmented Generation (RAG) system that can intelligently answer questions based on your documents.\n",
                                    "\n",
                                    "At the heart of any RAG system is the **document processor**. This component is responsible for taking your raw documents, processing them into a format that can be efficiently searched, and retrieving the most relevant information when a query is made. Think of it as the librarian of your RAG system—organizing information and fetching exactly what you need when you ask for it.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### Understanding the Document Processor\n",
                                    "\n",
                                    "The document processing pipeline we’ll build today consists of several key steps:\n",
                                    "\n",
                                    "1. **Loading documents** from files (like PDFs)\n",
                                    "2. **Splitting** these documents into smaller, manageable chunks\n",
                                    "3. **Creating vector embeddings** for each chunk\n",
                                    "4. **Storing** these embeddings in a vector database\n",
                                    "5. **Retrieving** the most relevant chunks when a query is made\n",
                                    "\n",
                                    "This document processor will serve as the foundation for our RAG chatbot. In later units, we’ll:\n",
                                    "\n",
                                    "* Build a chat engine that can maintain conversation history\n",
                                    "* Integrate both components into a complete RAG system\n",
                                    "\n",
                                    "By the end of this course, you’ll have a powerful chatbot that can answer questions based on your document collection with remarkable accuracy.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## 1. Setting Up the Document Processor Class\n",
                                    "\n",
                                    "First, we need to create a class that will handle all our document processing needs. This class will encapsulate functionality for loading, processing, and retrieving information from documents.\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_openai import OpenAIEmbeddings\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "    def __init__(self):\n",
                                    "        self.chunk_size = 1000\n",
                                    "        self.chunk_overlap = 100\n",
                                    "        self.embedding_model = OpenAIEmbeddings()\n",
                                    "        self.vectorstore = None\n",
                                    "```\n",
                                    "\n",
                                    "In this `__init__`:\n",
                                    "\n",
                                    "* **`chunk_size`**: Number of characters per chunk (default: 1000)\n",
                                    "* **`chunk_overlap`**: Overlap between chunks (default: 100)\n",
                                    "* **`embedding_model`**: OpenAI Embeddings instance for vectorizing text\n",
                                    "* **`vectorstore`**: Placeholder for our FAISS index\n",
                                    "\n",
                                    "These parameters can be adjusted based on your needs—e.g., increase `chunk_size`/`chunk_overlap` for more technical documents.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## 2. Implementing Document Loading and Chunking\n",
                                    "\n",
                                    "### 2.1 Loading Documents\n",
                                    "\n",
                                    "We’ll start by writing a method to load documents based on file type:\n",
                                    "\n",
                                    "```python\n",
                                    "def load_document(self, file_path):\n",
                                    "    \"\"\"Load a document based on its file type.\"\"\"\n",
                                    "    if file_path.endswith('.pdf'):\n",
                                    "        loader = PyPDFLoader(file_path)\n",
                                    "    else:\n",
                                    "        raise ValueError(\"Unsupported file format\")\n",
                                    "        \n",
                                    "    return loader.load()\n",
                                    "```\n",
                                    "\n",
                                    "Currently only PDFs are supported, but you can extend this to `.txt`, `.docx`, `.html`, etc.\n",
                                    "\n",
                                    "### 2.2 Processing and Chunking\n",
                                    "\n",
                                    "Next, implement a method to split a loaded document into chunks and add them to FAISS:\n",
                                    "\n",
                                    "```python\n",
                                    "def process_document(self, file_path):\n",
                                    "    \"\"\"Process a document and add it to the vector store.\"\"\"\n",
                                    "    # 1. Load\n",
                                    "    docs = self.load_document(file_path)\n",
                                    "    \n",
                                    "    # 2. Split\n",
                                    "    text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "        chunk_size=self.chunk_size, \n",
                                    "        chunk_overlap=self.chunk_overlap\n",
                                    "    )\n",
                                    "    split_docs = text_splitter.split_documents(docs)\n",
                                    "    \n",
                                    "    # 3. Create or update vector store\n",
                                    "    if self.vectorstore is None:\n",
                                    "        self.vectorstore = FAISS.from_documents(split_docs, self.embedding_model)\n",
                                    "    else:\n",
                                    "        self.vectorstore.add_documents(split_docs)\n",
                                    "```\n",
                                    "\n",
                                    "* **First document**: builds a new FAISS index\n",
                                    "* **Subsequent documents**: appends to existing index\n",
                                    "\n",
                                    "This incremental approach lets you grow your knowledge base without rebuilding from scratch.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## 3. Implementing Context Retrieval\n",
                                    "\n",
                                    "To fetch relevant information at query time, we need a retrieval method:\n",
                                    "\n",
                                    "```python\n",
                                    "def retrieve_relevant_context(self, query, k=3):\n",
                                    "    \"\"\"Retrieve relevant document chunks for a query.\"\"\"\n",
                                    "    if self.vectorstore is None:\n",
                                    "        return []\n",
                                    "        \n",
                                    "    return self.vectorstore.similarity_search(query, k=k)\n",
                                    "```\n",
                                    "\n",
                                    "* **`query`**: Question string\n",
                                    "* **`k`**: Number of top chunks to return (default: 3)\n",
                                    "\n",
                                    "If no documents have been processed yet, it safely returns an empty list.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## 4. Resetting the Vector Store\n",
                                    "\n",
                                    "A utility to clear your processor’s memory:\n",
                                    "\n",
                                    "```python\n",
                                    "def reset(self):\n",
                                    "    \"\"\"Reset the document processor (clear vector store).\"\"\"\n",
                                    "    self.vectorstore = None\n",
                                    "```\n",
                                    "\n",
                                    "Use this to start fresh with a new document collection.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## 5. Putting It All Together: A Complete RAG Workflow\n",
                                    "\n",
                                    "Here’s an end-to-end example that:\n",
                                    "\n",
                                    "1. Initializes the processor\n",
                                    "2. Processes a PDF\n",
                                    "3. Retrieves context for a query\n",
                                    "4. Invokes a chat model with RAG\n",
                                    "\n",
                                    "```python\n",
                                    "from document_processor import DocumentProcessor\n",
                                    "from langchain_openai import ChatOpenAI\n",
                                    "from langchain.prompts import ChatPromptTemplate\n",
                                    "\n",
                                    "# Initialize\n",
                                    "processor = DocumentProcessor()\n",
                                    "\n",
                                    "# Process a PDF (e.g., Sherlock Holmes)\n",
                                    "file_path = \"data/a_scandal_in_bohemia.pdf\"\n",
                                    "processor.process_document(file_path)\n",
                                    "\n",
                                    "# Prepare chat model\n",
                                    "chat = ChatOpenAI()\n",
                                    "\n",
                                    "# Define a query\n",
                                    "query = \"What is the main mystery in the story?\"\n",
                                    "\n",
                                    "# Retrieve context\n",
                                    "relevant_docs = processor.retrieve_relevant_context(query)\n",
                                    "context = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
                                    "\n",
                                    "# Build RAG prompt\n",
                                    "prompt_template = ChatPromptTemplate.from_template(\n",
                                    "    \"Answer the following question based on the provided context.\\n\\n\"\n",
                                    "    \"Context:\\n{context}\\n\\n\"\n",
                                    "    \"Question: {question}\"\n",
                                    ")\n",
                                    "prompt = prompt_template.format(context=context, question=query)\n",
                                    "\n",
                                    "# Invoke and print\n",
                                    "response = chat.invoke(prompt)\n",
                                    "print(f\"Question: {query}\")\n",
                                    "print(f\"Answer: {response.content}\")\n",
                                    "```\n",
                                    "\n",
                                    "**Expected output example:**\n",
                                    "\n",
                                    "> **Question:** What is the main mystery in the story?\n",
                                    "> **Answer:** The main mystery in \"A Scandal in Bohemia\" revolves around retrieving a compromising photograph that Irene Adler possesses…\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Summary and Next Steps\n",
                                    "\n",
                                    "In this lesson, you’ve learned to:\n",
                                    "\n",
                                    "* Create a **DocumentProcessor** class\n",
                                    "* **Load** and **chunk** documents\n",
                                    "* Build and manage a **FAISS** vector store\n",
                                    "* **Retrieve** relevant chunks for a query\n",
                                    "* Integrate with a chat model for a basic **RAG** workflow\n",
                                    "\n",
                                    "Next unit: we’ll craft a **chat engine** that maintains conversation history, then merge it with our document processor to build a fully conversational RAG chatbot. Stay tuned!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implementing Document Loading Logic\n",
                                    "\n",
                                    "Your first task is to implement the method to load a document in the DocumentProcessor class. This method should:\n",
                                    "\n",
                                    "Check if the file path ends with .pdf\n",
                                    "Use PyPDFLoader to load PDF documents\n",
                                    "Raise a ValueError for unsupported file formats\n",
                                    "Return the loaded documents\n",
                                    "Then in main.py, use this method to load a PDF file, print the content of the first page, and handle any errors that might occur.\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "        \n",
                                    "    # TODO: Implement a method to load a document that accepts a file path parameter\n",
                                    "        # TODO: Check if the file path ends with '.pdf'\n",
                                    "            # TODO: If so, create a PyPDFLoader instance with the file path\n",
                                    "        # TODO: If it's not a PDF, raise a ValueError with an appropriate message\n",
                                    "        # TODO: Return the loaded documents\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Here’s an example of how you can implement `load_document` in your `DocumentProcessor` and then use it in a simple `main.py`.\n",
                                    "\n",
                                    "```python\n",
                                    "# document_processor.py\n",
                                    "\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "    def load_document(self, file_path: str):\n",
                                    "        \"\"\"\n",
                                    "        Load a document based on its file type.\n",
                                    "\n",
                                    "        Args:\n",
                                    "            file_path (str): Path to the file.\n",
                                    "\n",
                                    "        Returns:\n",
                                    "            List[Document]: A list of LangChain Document objects.\n",
                                    "\n",
                                    "        Raises:\n",
                                    "            ValueError: If the file format is unsupported.\n",
                                    "        \"\"\"\n",
                                    "        # Check for PDF extension\n",
                                    "        if file_path.lower().endswith('.pdf'):\n",
                                    "            loader = PyPDFLoader(file_path)\n",
                                    "        else:\n",
                                    "            raise ValueError(f\"Unsupported file format: {file_path}. Only PDF files are supported.\")\n",
                                    "        \n",
                                    "        # Load and return the documents\n",
                                    "        return loader.load()\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "# main.py\n",
                                    "\n",
                                    "from document_processor import DocumentProcessor\n",
                                    "\n",
                                    "def main():\n",
                                    "    processor = DocumentProcessor()\n",
                                    "    file_path = \"data/a_scandal_in_bohemia.pdf\"  # adjust path as needed\n",
                                    "\n",
                                    "    try:\n",
                                    "        docs = processor.load_document(file_path)\n",
                                    "        if not docs:\n",
                                    "            print(\"No pages found in document.\")\n",
                                    "            return\n",
                                    "\n",
                                    "        # Print the content of the first page\n",
                                    "        first_page = docs[0].page_content\n",
                                    "        print(\"=== First Page Content ===\")\n",
                                    "        print(first_page.strip())\n",
                                    "\n",
                                    "    except ValueError as e:\n",
                                    "        # Handle unsupported file formats\n",
                                    "        print(f\"Error: {e}\")\n",
                                    "\n",
                                    "    except Exception as e:\n",
                                    "        # Catch-all for other errors (e.g., file not found, loader issues)\n",
                                    "        print(f\"An unexpected error occurred: {e}\")\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    main()\n",
                                    "```\n",
                                    "\n",
                                    "**Explanation:**\n",
                                    "\n",
                                    "1. **`load_document` method**\n",
                                    "\n",
                                    "   * Checks whether `file_path` ends with `.pdf` (case-insensitive).\n",
                                    "   * Uses `PyPDFLoader` to load PDF pages into a list of `Document` objects.\n",
                                    "   * Raises a `ValueError` if the extension is not `.pdf`.\n",
                                    "\n",
                                    "2. **`main.py`**\n",
                                    "\n",
                                    "   * Instantiates `DocumentProcessor` and attempts to load a PDF.\n",
                                    "   * If successful, prints the text of the first page.\n",
                                    "   * Catches and reports:\n",
                                    "\n",
                                    "     * `ValueError` for unsupported formats\n",
                                    "     * Any other exception (I/O issues, missing file, etc.)\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Initializing Vector Store and Retrieving Context\n",
                                    "\n",
                                    "You've done well setting up the method to load documents. Now, let's focus on initializing our vector store and retrieving context.\n",
                                    "\n",
                                    "Your task is to:\n",
                                    "\n",
                                    "Complete the constructor for the DocumentProcessor class by initializing a vectorstore attribute to None.\n",
                                    "Implement the missing method to retrieve relevant context by performing similarity searches:\n",
                                    "This method should accept a query parameter and an optional k parameter (default=3)\n",
                                    "Check if the vectorstore is None and return an empty list if so\n",
                                    "If vectorstore is not None, return the result of a similarity search\n",
                                    "After implementing these, test your code in main.py by using your retrieval method. Print the result to confirm that it returns an empty list, indicating that the vector store is not yet initialized.\n",
                                    "\n",
                                    "Keep going; you're building a strong foundation!\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "    def __init__(self):\n",
                                    "        # TODO: Initialize a vectorstore attribute to None\n",
                                    "        \n",
                                    "    def load_document(self, file_path):\n",
                                    "        \"\"\"Load a document based on its file type\"\"\"\n",
                                    "        if file_path.endswith('.pdf'):\n",
                                    "            loader = PyPDFLoader(file_path)\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Unsupported file format\")\n",
                                    "            \n",
                                    "        return loader.load()\n",
                                    "        \n",
                                    "    # TODO: Implement a method to retrieve relevant context\n",
                                    "    # This method should accept a query parameter and an optional k parameter (default=3)\n",
                                    "        # TODO: Check if the vectorstore is None and return an empty list if so\n",
                                    "        # TODO: If vectorstore is not None, return the result of a similarity search\n",
                                    "\n",
                                    "from document_processor import DocumentProcessor\n",
                                    "\n",
                                    "# Initialize the document processor\n",
                                    "processor = DocumentProcessor()\n",
                                    "\n",
                                    "# Define a query\n",
                                    "query = \"What is the main mystery in the story?\"\n",
                                    "\n",
                                    "# TODO: Use the method to retrieve context and store the result\n",
                                    "\n",
                                    "# TODO: Print the result to verify it's an empty list\n",
                                    "# \n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "# document_processor.py\n",
                                    "\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "    def __init__(self):\n",
                                    "        # Initialize the vectorstore to None\n",
                                    "        self.vectorstore = None\n",
                                    "\n",
                                    "    def load_document(self, file_path):\n",
                                    "        \"\"\"Load a document based on its file type.\"\"\"\n",
                                    "        if file_path.lower().endswith('.pdf'):\n",
                                    "            loader = PyPDFLoader(file_path)\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Unsupported file format: only PDF is supported.\")\n",
                                    "        return loader.load()\n",
                                    "\n",
                                    "    def retrieve_relevant_context(self, query: str, k: int = 3):\n",
                                    "        \"\"\"\n",
                                    "        Retrieve the top-k most relevant document chunks for a given query.\n",
                                    "        \n",
                                    "        Args:\n",
                                    "            query (str): The search query.\n",
                                    "            k (int): How many chunks to retrieve (default: 3).\n",
                                    "        \n",
                                    "        Returns:\n",
                                    "            List[Document]: The list of matching chunks, or empty if no vectorstore.\n",
                                    "        \"\"\"\n",
                                    "        if self.vectorstore is None:\n",
                                    "            return []\n",
                                    "        return self.vectorstore.similarity_search(query, k=k)\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "# main.py\n",
                                    "\n",
                                    "from document_processor import DocumentProcessor\n",
                                    "\n",
                                    "def main():\n",
                                    "    processor = DocumentProcessor()\n",
                                    "\n",
                                    "    query = \"What is the main mystery in the story?\"\n",
                                    "    results = processor.retrieve_relevant_context(query)\n",
                                    "\n",
                                    "    # Since we haven't processed any documents yet, vectorstore is None,\n",
                                    "    # so we expect an empty list.\n",
                                    "    print(\"Retrieved context:\", results)\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    main()\n",
                                    "```\n",
                                    "\n",
                                    "**What to expect when you run `main.py`:**\n",
                                    "\n",
                                    "```\n",
                                    "Retrieved context: []\n",
                                    "```\n",
                                    "\n",
                                    "This confirms that, before any documents are processed, calling `retrieve_relevant_context` returns an empty list.\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Processing Documents for Vector Storage\n",
                                    "\n",
                                    "Excellent work on setting up document loading and retrieval functionality! Now it's time to implement the heart of our document processor — the method that processes documents and adds them to the vector store.\n",
                                    "\n",
                                    "This method needs to:\n",
                                    "\n",
                                    "Load the document using your existing load\\_document method.\n",
                                    "Split the document into manageable chunks using a RecursiveCharacterTextSplitter.\n",
                                    "Create or update the vector store with these chunks.\n",
                                    "Then, in main.py, you'll put everything together by processing a document and retrieving relevant context for a query. This is where your RAG system really comes to life — once you complete this exercise, you'll have a working document processor that can find relevant information from your documents!\n",
                                    "\n",
                                    "````python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_openai import OpenAIEmbeddings\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "    def __init__(self):\n",
                                    "        self.chunk_size = 1000\n",
                                    "        self.chunk_overlap = 100\n",
                                    "        self.embedding_model = OpenAIEmbeddings()\n",
                                    "        self.vectorstore = None\n",
                                    "        \n",
                                    "    def load_document(self, file_path):\n",
                                    "        \"\"\"Load a document based on its file type\"\"\"\n",
                                    "        if file_path.endswith('.pdf'):\n",
                                    "            loader = PyPDFLoader(file_path)\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Unsupported file format\")\n",
                                    "            \n",
                                    "        return loader.load()\n",
                                    "        \n",
                                    "    # TODO: Implement a method to process a document and add it to the vector store\n",
                                    "        # TODO: Load the document using the load_document method\n",
                                    "        # TODO: Create a RecursiveCharacterTextSplitter with the chunk_size and chunk_overlap attributes\n",
                                    "        # TODO: Split the loaded documents into chunks using the text splitter\n",
                                    "        # TODO: Check if the vectorstore is None\n",
                                    "            # TODO: If it is, create a new FAISS vectorstore from the split documents\n",
                                    "        # TODO: If not, add the documents to the existing vectorstore\n",
                                    "        \n",
                                    "    def retrieve_relevant_context(self, query, k=3):\n",
                                    "        \"\"\"Retrieve relevant document chunks for a query\"\"\"\n",
                                    "        if self.vectorstore is None:\n",
                                    "            return []\n",
                                    "            \n",
                                    "        return self.vectorstore.similarity_search(query, k=k)\n",
                                    "\n",
                                    "\n",
                                    "from document_processor import DocumentProcessor\n",
                                    "\n",
                                    "# Initialize the document processor\n",
                                    "processor = DocumentProcessor()\n",
                                    "\n",
                                    "# Define the file path of the document\n",
                                    "file_path = \"data/a_scandal_in_bohemia.pdf\"\n",
                                    "\n",
                                    "# TODO: Process the document\n",
                                    "\n",
                                    "# Define a query\n",
                                    "query = \"What is the main mystery in the story?\"\n",
                                    "\n",
                                    "# Retrieve relevant context\n",
                                    "relevant_docs = processor.retrieve_relevant_context(query)\n",
                                    "\n",
                                    "# Check if any relevant documents were found\n",
                                    "if relevant_docs:\n",
                                    "    # Print the content of the first one\n",
                                    "    print(relevant_docs[0].page_content)\n",
                                    "else:\n",
                                    "    print(\"No relevant documents found.\")\n",
                                    "    \n",
                                    "```\n",
                                    "\n",
                                    "Here’s how you can implement the `process_document` method for the `DocumentProcessor` class to handle document processing, splitting, and updating the vector store:\n",
                                    "\n",
                                    "```python\n",
                                    "# document_processor.py\n",
                                    "\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_openai import OpenAIEmbeddings\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "    def __init__(self):\n",
                                    "        self.chunk_size = 1000\n",
                                    "        self.chunk_overlap = 100\n",
                                    "        self.embedding_model = OpenAIEmbeddings()\n",
                                    "        self.vectorstore = None\n",
                                    "        \n",
                                    "    def load_document(self, file_path):\n",
                                    "        \"\"\"Load a document based on its file type.\"\"\"\n",
                                    "        if file_path.lower().endswith('.pdf'):\n",
                                    "            loader = PyPDFLoader(file_path)\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Unsupported file format: only PDF is supported.\")\n",
                                    "        return loader.load()\n",
                                    "        \n",
                                    "    def process_document(self, file_path):\n",
                                    "        \"\"\"\n",
                                    "        Process a document, split it into chunks, and add it to the vector store.\n",
                                    "        \n",
                                    "        Args:\n",
                                    "            file_path (str): The path to the document to process.\n",
                                    "        \"\"\"\n",
                                    "        # Load the document\n",
                                    "        docs = self.load_document(file_path)\n",
                                    "        \n",
                                    "        # Split the document into chunks\n",
                                    "        text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "            chunk_size=self.chunk_size, \n",
                                    "            chunk_overlap=self.chunk_overlap\n",
                                    "        )\n",
                                    "        split_docs = text_splitter.split_documents(docs)\n",
                                    "        \n",
                                    "        # Check if vectorstore exists, and create or update it\n",
                                    "        if self.vectorstore is None:\n",
                                    "            self.vectorstore = FAISS.from_documents(split_docs, self.embedding_model)\n",
                                    "        else:\n",
                                    "            self.vectorstore.add_documents(split_docs)\n",
                                    "        \n",
                                    "    def retrieve_relevant_context(self, query, k=3):\n",
                                    "        \"\"\"Retrieve relevant document chunks for a query.\"\"\"\n",
                                    "        if self.vectorstore is None:\n",
                                    "            return []\n",
                                    "            \n",
                                    "        return self.vectorstore.similarity_search(query, k=k)\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation:\n",
                                    "\n",
                                    "1. **`process_document` method**:\n",
                                    "\n",
                                    "   * Loads a document using `load_document()`.\n",
                                    "   * Splits the document into chunks using `RecursiveCharacterTextSplitter` with the defined `chunk_size` and `chunk_overlap`.\n",
                                    "   * Checks if the `vectorstore` is `None`. If so, it creates a new FAISS vector store from the document chunks. If the vectorstore already exists, it adds the new chunks to it.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### Main Program (`main.py`)\n",
                                    "\n",
                                    "Now, let's put everything together in `main.py` to test the document processor.\n",
                                    "\n",
                                    "```python\n",
                                    "# main.py\n",
                                    "\n",
                                    "from document_processor import DocumentProcessor\n",
                                    "\n",
                                    "def main():\n",
                                    "    # Initialize the document processor\n",
                                    "    processor = DocumentProcessor()\n",
                                    "\n",
                                    "    # Define the file path of the document\n",
                                    "    file_path = \"data/a_scandal_in_bohemia.pdf\"  # Ensure the correct path to your PDF file\n",
                                    "\n",
                                    "    # Process the document (this will add the document chunks to the vector store)\n",
                                    "    processor.process_document(file_path)\n",
                                    "\n",
                                    "    # Define a query\n",
                                    "    query = \"What is the main mystery in the story?\"\n",
                                    "\n",
                                    "    # Retrieve relevant context from the vector store\n",
                                    "    relevant_docs = processor.retrieve_relevant_context(query)\n",
                                    "\n",
                                    "    # Check if any relevant documents were found\n",
                                    "    if relevant_docs:\n",
                                    "        # Print the content of the first relevant document\n",
                                    "        print(\"=== First Relevant Document ===\")\n",
                                    "        print(relevant_docs[0].page_content)\n",
                                    "    else:\n",
                                    "        print(\"No relevant documents found.\")\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    main()\n",
                                    "```\n",
                                    "\n",
                                    "### Expected Output:\n",
                                    "\n",
                                    "After running this program, if the document processing works as expected, you should see the content of the first relevant document chunk printed.\n",
                                    "\n",
                                    "```\n",
                                    "=== First Relevant Document ===\n",
                                    "The main mystery in \"A Scandal in Bohemia\" revolves around retrieving a compromising photograph that Irene Adler possesses...\n",
                                    "```\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### Summary:\n",
                                    "\n",
                                    "* **`process_document`** method splits documents into chunks and adds them to the vector store.\n",
                                    "* **`retrieve_relevant_context`** method performs a similarity search for a query and returns the most relevant document chunks.\n",
                                    "* The **main program** processes a document, queries the vector store, and prints the most relevant content based on the query.\n",
                                    "\n",
                                    "You now have a working Retrieval-Augmented Generation (RAG) system!\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Building a Multi-Document Knowledge Base\n",
                                    "\n",
                                    "Now that you've built a working document processor that can handle a single document, let's take it to the next level! One of the most powerful features of our vector store is its ability to store and search across multiple documents.\n",
                                    "\n",
                                    "In this exercise, you'll expand your document processor to work with multiple documents:\n",
                                    "\n",
                                    "Complete the code in main.py to process the second document (the file path is already provided)\n",
                                    "Run the code to see how the system retrieves relevant chunks from both documents\n",
                                    "Notice how the system automatically retrieves the most relevant information regardless of which document it's stored in\n",
                                    "This exercise demonstrates a key advantage of RAG systems: the ability to build a knowledge base incrementally by adding multiple documents to the same vector store. The similarity search will find the most relevant information across your entire document collection, not just within a single file.\n",
                                    "\n",
                                    "```python\n",
                                    "from document_processor import DocumentProcessor\n",
                                    "\n",
                                    "# Initialize the document processor\n",
                                    "processor = DocumentProcessor()\n",
                                    "\n",
                                    "# Process the first document\n",
                                    "first_file_path = \"data/a_scandal_in_bohemia.pdf\"\n",
                                    "processor.process_document(first_file_path)\n",
                                    "\n",
                                    "# TODO: Process the second document\n",
                                    "second_file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "\n",
                                    "# Create a query that might retrieve content from both documents\n",
                                    "query = \"What methods does Sherlock Holmes use to solve cases?\"\n",
                                    "\n",
                                    "# Retrieve relevant context\n",
                                    "print(f\"Query: {query}\")\n",
                                    "relevant_docs = processor.retrieve_relevant_context(query, k=4)\n",
                                    "\n",
                                    "# Print the results and check which document they came from\n",
                                    "for i, doc in enumerate(relevant_docs):\n",
                                    "    # Extract source file from metadata\n",
                                    "    source = doc.metadata.get('source', 'Unknown source')\n",
                                    "    \n",
                                    "    # Print chunk information\n",
                                    "    print(f\"\\nChunk {i+1} (from {source}):\")\n",
                                    "    print(f\"Content preview:\\n{doc.page_content[:150]}...\")\n",
                                    "\n",
                                    "\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_openai import OpenAIEmbeddings\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "    def __init__(self):\n",
                                    "        self.chunk_size = 1000\n",
                                    "        self.chunk_overlap = 100\n",
                                    "        self.embedding_model = OpenAIEmbeddings()\n",
                                    "        self.vectorstore = None\n",
                                    "        \n",
                                    "    def load_document(self, file_path):\n",
                                    "        \"\"\"Load a document based on its file type\"\"\"\n",
                                    "        if file_path.endswith('.pdf'):\n",
                                    "            loader = PyPDFLoader(file_path)\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Unsupported file format\")\n",
                                    "            \n",
                                    "        return loader.load()\n",
                                    "        \n",
                                    "    def process_document(self, file_path):\n",
                                    "        \"\"\"Process a document and add it to the vector store\"\"\"\n",
                                    "        # Load the document\n",
                                    "        docs = self.load_document(file_path)\n",
                                    "        \n",
                                    "        # Split the document into chunks\n",
                                    "        text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "            chunk_size=self.chunk_size, \n",
                                    "            chunk_overlap=self.chunk_overlap\n",
                                    "        )\n",
                                    "        split_docs = text_splitter.split_documents(docs)\n",
                                    "        \n",
                                    "        # Create or update the vector store\n",
                                    "        if self.vectorstore is None:\n",
                                    "            self.vectorstore = FAISS.from_documents(split_docs, self.embedding_model)\n",
                                    "        else:\n",
                                    "            self.vectorstore.add_documents(split_docs)\n",
                                    "        \n",
                                    "        return split_docs\n",
                                    "        \n",
                                    "    def retrieve_relevant_context(self, query, k=3):\n",
                                    "        \"\"\"Retrieve relevant document chunks for a query\"\"\"\n",
                                    "        if self.vectorstore is None:\n",
                                    "            return []\n",
                                    "            \n",
                                    "        return self.vectorstore.similarity_search(query, k=k)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To expand the document processor to handle multiple documents and integrate them into the same vector store, we just need to process the second document and ensure the system correctly retrieves relevant context from both documents.\n",
                                    "\n",
                                    "### Steps:\n",
                                    "\n",
                                    "1. **Process the second document** using the `process_document` method. This will add the second document's chunks to the existing vector store.\n",
                                    "2. **Perform a query** that could retrieve information from either of the two documents.\n",
                                    "3. **Ensure that each document chunk has metadata** that indicates its source so we can easily identify which document the chunk came from.\n",
                                    "\n",
                                    "### Complete Code:\n",
                                    "\n",
                                    "Here’s how to implement the solution:\n",
                                    "\n",
                                    "#### `main.py`\n",
                                    "\n",
                                    "```python\n",
                                    "from document_processor import DocumentProcessor\n",
                                    "\n",
                                    "# Initialize the document processor\n",
                                    "processor = DocumentProcessor()\n",
                                    "\n",
                                    "# Process the first document\n",
                                    "first_file_path = \"data/a_scandal_in_bohemia.pdf\"\n",
                                    "processor.process_document(first_file_path)\n",
                                    "\n",
                                    "# Process the second document\n",
                                    "second_file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "processor.process_document(second_file_path)\n",
                                    "\n",
                                    "# Create a query that might retrieve content from both documents\n",
                                    "query = \"What methods does Sherlock Holmes use to solve cases?\"\n",
                                    "\n",
                                    "# Retrieve relevant context\n",
                                    "print(f\"Query: {query}\")\n",
                                    "relevant_docs = processor.retrieve_relevant_context(query, k=4)\n",
                                    "\n",
                                    "# Print the results and check which document they came from\n",
                                    "for i, doc in enumerate(relevant_docs):\n",
                                    "    # Extract source file from metadata\n",
                                    "    source = doc.metadata.get('source', 'Unknown source')\n",
                                    "    \n",
                                    "    # Print chunk information\n",
                                    "    print(f\"\\nChunk {i+1} (from {source}):\")\n",
                                    "    print(f\"Content preview:\\n{doc.page_content[:150]}...\")\n",
                                    "```\n",
                                    "\n",
                                    "### Updates to `DocumentProcessor`\n",
                                    "\n",
                                    "We need to ensure that **metadata is included** when adding documents to the vector store. This way, when we perform a query, we can retrieve the source document for each chunk.\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_openai import OpenAIEmbeddings\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "    def __init__(self):\n",
                                    "        self.chunk_size = 1000\n",
                                    "        self.chunk_overlap = 100\n",
                                    "        self.embedding_model = OpenAIEmbeddings()\n",
                                    "        self.vectorstore = None\n",
                                    "        \n",
                                    "    def load_document(self, file_path):\n",
                                    "        \"\"\"Load a document based on its file type\"\"\"\n",
                                    "        if file_path.endswith('.pdf'):\n",
                                    "            loader = PyPDFLoader(file_path)\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Unsupported file format\")\n",
                                    "            \n",
                                    "        return loader.load()\n",
                                    "        \n",
                                    "    def process_document(self, file_path):\n",
                                    "        \"\"\"Process a document and add it to the vector store\"\"\"\n",
                                    "        # Load the document\n",
                                    "        docs = self.load_document(file_path)\n",
                                    "        \n",
                                    "        # Split the document into chunks\n",
                                    "        text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "            chunk_size=self.chunk_size, \n",
                                    "            chunk_overlap=self.chunk_overlap\n",
                                    "        )\n",
                                    "        split_docs = text_splitter.split_documents(docs)\n",
                                    "        \n",
                                    "        # Add metadata to each document chunk\n",
                                    "        for doc in split_docs:\n",
                                    "            doc.metadata['source'] = file_path\n",
                                    "        \n",
                                    "        # Create or update the vector store\n",
                                    "        if self.vectorstore is None:\n",
                                    "            self.vectorstore = FAISS.from_documents(split_docs, self.embedding_model)\n",
                                    "        else:\n",
                                    "            self.vectorstore.add_documents(split_docs)\n",
                                    "        \n",
                                    "        return split_docs\n",
                                    "        \n",
                                    "    def retrieve_relevant_context(self, query, k=3):\n",
                                    "        \"\"\"Retrieve relevant document chunks for a query\"\"\"\n",
                                    "        if self.vectorstore is None:\n",
                                    "            return []\n",
                                    "            \n",
                                    "        return self.vectorstore.similarity_search(query, k=k)\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation:\n",
                                    "\n",
                                    "1. **Processing the Second Document**:\n",
                                    "   The `process_document` method now processes both the first and the second document, adding chunks to the vector store as we go. The metadata is updated for each chunk to include the `source` (the file path) so we can track which document each chunk came from.\n",
                                    "\n",
                                    "2. **Querying Across Multiple Documents**:\n",
                                    "   Once both documents are processed, we can query for relevant context, and the system will return the most relevant chunks from any of the documents in the vector store. The source of each chunk is displayed to let us know which document the chunk came from.\n",
                                    "\n",
                                    "3. **Handling Metadata**:\n",
                                    "   Each chunk’s metadata contains the source document file path, allowing us to easily identify which document a chunk belongs to when displaying the results.\n",
                                    "\n",
                                    "### Running the Code:\n",
                                    "\n",
                                    "When you run this code, the output will show relevant chunks from both documents, and the source of each chunk will be displayed. For example:\n",
                                    "\n",
                                    "```\n",
                                    "Query: What methods does Sherlock Holmes use to solve cases?\n",
                                    "\n",
                                    "Chunk 1 (from data/a_scandal_in_bohemia.pdf):\n",
                                    "Content preview:\n",
                                    "Sherlock Holmes uses a variety of methods to solve cases, including observation, deduction...\n",
                                    "\n",
                                    "Chunk 2 (from data/the_adventure_of_the_blue_carbuncle.pdf):\n",
                                    "Content preview:\n",
                                    "Holmes relies heavily on his acute observation and reasoning skills to uncover the truth...\n",
                                    "\n",
                                    "Chunk 3 (from data/a_scandal_in_bohemia.pdf):\n",
                                    "Content preview:\n",
                                    "In \"A Scandal in Bohemia,\" Holmes's method revolves around studying the behavior of suspects...\n",
                                    "\n",
                                    "Chunk 4 (from data/the_adventure_of_the_blue_carbuncle.pdf):\n",
                                    "Content preview:\n",
                                    "Holmes deduces the culprit by carefully analyzing the evidence and understanding human behavior...\n",
                                    "```\n",
                                    "\n",
                                    "This demonstrates that the system is capable of processing and searching across multiple documents, retrieving relevant information regardless of the document source. This is one of the core benefits of building a RAG system with incremental document additions.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implementing Reset for Document Management\n",
                                    "\n",
                                    "Your document processor has been growing with each exercise, and now it's time to implement a way to clear the vector store when needed. This is particularly useful when you want to start fresh with a new set of documents or when testing different document collections.\n",
                                    "\n",
                                    "Your tasks are:\n",
                                    "\n",
                                    "Implement the reset method in the DocumentProcessor class that sets the vector store to None.\n",
                                    "\n",
                                    "Complete the main.py file to test this functionality by:\n",
                                    "\n",
                                    "Calling your reset method.\n",
                                    "Confirming that queries after the reset return empty results.\n",
                                    "This exercise will teach you proper state management — an essential skill when building systems that need to be flexible and maintainable over time.\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_openai import OpenAIEmbeddings\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "    def __init__(self):\n",
                                    "        self.chunk_size = 1000\n",
                                    "        self.chunk_overlap = 100\n",
                                    "        self.embedding_model = OpenAIEmbeddings()\n",
                                    "        self.vectorstore = None\n",
                                    "        \n",
                                    "    def load_document(self, file_path):\n",
                                    "        \"\"\"Load a document based on its file type\"\"\"\n",
                                    "        if file_path.endswith('.pdf'):\n",
                                    "            loader = PyPDFLoader(file_path)\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Unsupported file format\")\n",
                                    "            \n",
                                    "        return loader.load()\n",
                                    "        \n",
                                    "    def process_document(self, file_path):\n",
                                    "        \"\"\"Process a document and add it to the vector store\"\"\"\n",
                                    "        # Load the document\n",
                                    "        docs = self.load_document(file_path)\n",
                                    "        \n",
                                    "        # Split the document into chunks\n",
                                    "        text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "            chunk_size=self.chunk_size, \n",
                                    "            chunk_overlap=self.chunk_overlap\n",
                                    "        )\n",
                                    "        split_docs = text_splitter.split_documents(docs)\n",
                                    "        \n",
                                    "        # Create or update the vector store\n",
                                    "        if self.vectorstore is None:\n",
                                    "            self.vectorstore = FAISS.from_documents(split_docs, self.embedding_model)\n",
                                    "        else:\n",
                                    "            self.vectorstore.add_documents(split_docs)\n",
                                    "        \n",
                                    "        return split_docs\n",
                                    "        \n",
                                    "    def retrieve_relevant_context(self, query, k=3):\n",
                                    "        \"\"\"Retrieve relevant document chunks for a query\"\"\"\n",
                                    "        if self.vectorstore is None:\n",
                                    "            return []\n",
                                    "            \n",
                                    "        return self.vectorstore.similarity_search(query, k=k)\n",
                                    "        \n",
                                    "    # TODO: Implement the method to clear the vector store\n",
                                    "\n",
                                    "\n",
                                    "from document_processor import DocumentProcessor\n",
                                    "\n",
                                    "# Initialize the document processor\n",
                                    "processor = DocumentProcessor()\n",
                                    "\n",
                                    "# Process a document\n",
                                    "file_path = \"data/a_scandal_in_bohemia.pdf\"\n",
                                    "processor.process_document(file_path)\n",
                                    "\n",
                                    "# Define a query\n",
                                    "query = \"What is the main mystery in the story?\"\n",
                                    "\n",
                                    "# Verify the document was processed correctly\n",
                                    "print(\"Test 1: Retrieving context before reset\")\n",
                                    "relevant_docs = processor.retrieve_relevant_context(query)\n",
                                    "if relevant_docs:\n",
                                    "    print(f\"First chunk preview: {relevant_docs[0].page_content[:100]}...\")\n",
                                    "else:\n",
                                    "    print(\"No relevant documents found.\")\n",
                                    "\n",
                                    "# TODO: Call the method to clear the vector store\n",
                                    "\n",
                                    "# TODO: Verify that no documents are returned after reset\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here’s how you can add a simple `reset` method to your `DocumentProcessor` and then finish your `main.py` to test that it really clears out the vector store.\n",
                                    "\n",
                                    "```python\n",
                                    "# document_processor.py\n",
                                    "\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_openai import OpenAIEmbeddings\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "\n",
                                    "class DocumentProcessor:\n",
                                    "    def __init__(self):\n",
                                    "        self.chunk_size = 1000\n",
                                    "        self.chunk_overlap = 100\n",
                                    "        self.embedding_model = OpenAIEmbeddings()\n",
                                    "        self.vectorstore = None\n",
                                    "        \n",
                                    "    def load_document(self, file_path):\n",
                                    "        \"\"\"Load a document based on its file type\"\"\"\n",
                                    "        if file_path.endswith('.pdf'):\n",
                                    "            loader = PyPDFLoader(file_path)\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Unsupported file format\")\n",
                                    "            \n",
                                    "        return loader.load()\n",
                                    "        \n",
                                    "    def process_document(self, file_path):\n",
                                    "        \"\"\"Process a document and add it to the vector store\"\"\"\n",
                                    "        # Load the document\n",
                                    "        docs = self.load_document(file_path)\n",
                                    "        \n",
                                    "        # Split the document into chunks\n",
                                    "        text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "            chunk_size=self.chunk_size, \n",
                                    "            chunk_overlap=self.chunk_overlap\n",
                                    "        )\n",
                                    "        split_docs = text_splitter.split_documents(docs)\n",
                                    "        \n",
                                    "        # Create or update the vector store\n",
                                    "        if self.vectorstore is None:\n",
                                    "            self.vectorstore = FAISS.from_documents(split_docs, self.embedding_model)\n",
                                    "        else:\n",
                                    "            self.vectorstore.add_documents(split_docs)\n",
                                    "        \n",
                                    "        return split_docs\n",
                                    "        \n",
                                    "    def retrieve_relevant_context(self, query, k=3):\n",
                                    "        \"\"\"Retrieve relevant document chunks for a query\"\"\"\n",
                                    "        if self.vectorstore is None:\n",
                                    "            return []\n",
                                    "            \n",
                                    "        return self.vectorstore.similarity_search(query, k=k)\n",
                                    "        \n",
                                    "    def reset(self):\n",
                                    "        \"\"\"Clear the vector store so we can start fresh.\"\"\"\n",
                                    "        self.vectorstore = None\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "# main.py\n",
                                    "\n",
                                    "from document_processor import DocumentProcessor\n",
                                    "\n",
                                    "# Initialize the document processor\n",
                                    "processor = DocumentProcessor()\n",
                                    "\n",
                                    "# Process a document\n",
                                    "file_path = \"data/a_scandal_in_bohemia.pdf\"\n",
                                    "processor.process_document(file_path)\n",
                                    "\n",
                                    "# Define a query\n",
                                    "query = \"What is the main mystery in the story?\"\n",
                                    "\n",
                                    "# Verify the document was processed correctly\n",
                                    "print(\"Test 1: Retrieving context before reset\")\n",
                                    "relevant_docs = processor.retrieve_relevant_context(query)\n",
                                    "if relevant_docs:\n",
                                    "    print(f\"First chunk preview: {relevant_docs[0].page_content[:100]}...\")\n",
                                    "else:\n",
                                    "    print(\"No relevant documents found.\")\n",
                                    "\n",
                                    "# --- Now reset and test again ---\n",
                                    "processor.reset()\n",
                                    "\n",
                                    "print(\"\\nTest 2: Retrieving context after reset\")\n",
                                    "relevant_docs_after_reset = processor.retrieve_relevant_context(query)\n",
                                    "if not relevant_docs_after_reset:\n",
                                    "    print(\"✅ Vector store has been reset. No documents found.\")\n",
                                    "else:\n",
                                    "    print(\"❌ Found documents after reset, something went wrong.\")\n",
                                    "```\n",
                                    "\n",
                                    "With this in place:\n",
                                    "\n",
                                    "1. Calling `processor.reset()` will drop your existing FAISS index by setting `vectorstore` back to `None`.\n",
                                    "2. A subsequent call to `retrieve_relevant_context(...)` will correctly return an empty list, confirming your reset logic works.\n"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
