{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 1 : Loading and Splitting Documents with LangChain\n",
                                    "\n",
                                    "# Introduction to Document Processing with LangChain\n",
                                    "\n",
                                    "Welcome to the first lesson of **Document Processing and Retrieval with LangChain in Python**! In this course, you'll learn how to work with documents programmatically, extract valuable information from them, and build systems that intelligently interact with document content.\n",
                                    "\n",
                                    "Document processing is a fundamental task in many applications, from search engines to question-answering systems. A typical document processing pipeline consists of:\n",
                                    "\n",
                                    "- Loading documents from various sources\n",
                                    "- Splitting them into manageable chunks\n",
                                    "- Converting those chunks into numerical representations (embeddings)\n",
                                    "- Retrieving relevant information when needed\n",
                                    "\n",
                                    "In this lesson, we'll focus on loading documents and splitting them into appropriate chunks. These steps form the foundation for all subsequent document processing tasks.\n",
                                    "\n",
                                    "## Learning Objectives\n",
                                    "By the end of this lesson, you'll be able to:\n",
                                    "- Load documents from different file formats using LangChain\n",
                                    "- Split documents into manageable chunks for further processing\n",
                                    "- Understand document preparation for embedding and retrieval\n",
                                    "\n",
                                    "Let's begin by exploring document loaders in LangChain.\n",
                                    "\n",
                                    "## Setting Up PyPDF\n",
                                    "\n",
                                    "Ensure the `pypdf` package is installed. `pypdf` allows LangChain's `PyPDFLoader` to read text from PDFs effectively.\n",
                                    "\n",
                                    "```bash\n",
                                    "pip install pypdf\n",
                                    "```\n",
                                    "\n",
                                    "> Note: This package is pre-installed on CodeSignal.\n",
                                    "\n",
                                    "## LangChain Document Loaders\n",
                                    "\n",
                                    "LangChain simplifies document processing with specialized loaders for different file formats.\n",
                                    "\n",
                                    "### PDF Files\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "\n",
                                    "pdf_loader = PyPDFLoader(\"document.pdf\")\n",
                                    "```\n",
                                    "\n",
                                    "### Text Files\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import TextLoader\n",
                                    "\n",
                                    "text_loader = TextLoader(\"document.txt\")\n",
                                    "```\n",
                                    "\n",
                                    "### General Files\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
                                    "\n",
                                    "general_loader = UnstructuredFileLoader(\"document.docx\")\n",
                                    "```\n",
                                    "\n",
                                    "LangChain also provides loaders like `CSVLoader`, `JSONLoader`, and `WebBaseLoader`.\n",
                                    "\n",
                                    "## Loading a Document\n",
                                    "Example using a Sherlock Holmes PDF:\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "pdf_loader = PyPDFLoader(file_path)\n",
                                    "docs = pdf_loader.load()\n",
                                    "```\n",
                                    "\n",
                                    "## Inspecting Loaded Documents\n",
                                    "\n",
                                    "```python\n",
                                    "print(f\"Loaded {len(docs)} document chunks\")\n",
                                    "print(f\"\\nFirst 200 characters:\\n{docs[0].page_content[:200]}\")\n",
                                    "print(f\"\\nMetadata:\\n{docs[0].metadata}\")\n",
                                    "```\n",
                                    "\n",
                                    "## Document Splitting Techniques\n",
                                    "Documents often require splitting into smaller chunks. LangChain's `RecursiveCharacterTextSplitter` splits documents recursively by separators:\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "\n",
                                    "text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "    chunk_size=1000,\n",
                                    "    chunk_overlap=100\n",
                                    ")\n",
                                    "```\n",
                                    "\n",
                                    "- **chunk_size:** Maximum characters per chunk\n",
                                    "- **chunk_overlap:** Characters shared between chunks to maintain context\n",
                                    "\n",
                                    "## Splitting Documents into Chunks\n",
                                    "\n",
                                    "```python\n",
                                    "split_docs = text_splitter.split_documents(docs)\n",
                                    "\n",
                                    "print(f\"After splitting: {len(split_docs)} chunks\")\n",
                                    "print(f\"\\nFirst chunk content:\\n{split_docs[0].page_content}\")\n",
                                    "```\n",
                                    "\n",
                                    "This method preserves metadata and splits content based on specified parameters.\n",
                                    "\n",
                                    "## Optimizing Chunk Size and Overlap\n",
                                    "Effective chunking balances size and overlap:\n",
                                    "- Small chunks fragment ideas; large chunks exceed model limits.\n",
                                    "- Moderate overlap (50–100 characters) maintains context without redundancy.\n",
                                    "\n",
                                    "Adjust chunking parameters based on:\n",
                                    "- Document type\n",
                                    "- Task requirements\n",
                                    "- Model token limits\n",
                                    "\n",
                                    "## Review and Next Steps\n",
                                    "In this lesson, you:\n",
                                    "- Explored LangChain document loaders (`PyPDFLoader`, `TextLoader`, `UnstructuredFileLoader`).\n",
                                    "- Learned loading and inspecting document content and metadata.\n",
                                    "- Discussed the importance of document splitting.\n",
                                    "- Utilized `RecursiveCharacterTextSplitter`.\n",
                                    "\n",
                                    "Next lesson covers converting chunks into vector embeddings for semantic retrieval.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Loading and Inspecting PDF Documents\n",
                                    "\n",
                                    "Now that you've learned about different document loaders in LangChain, let's put that knowledge into practice! In this exercise, you'll work with the PyPDFLoader to load a Sherlock Holmes story and examine its contents.\n",
                                    "\n",
                                    "You'll complete a script that loads a PDF document and helps you understand what information is available after loading. This hands-on experience will show you exactly what happens when a document is loaded into LangChain.\n",
                                    "\n",
                                    "Your tasks are to:\n",
                                    "\n",
                                    "Create a PyPDFLoader instance for the provided PDF file.\n",
                                    "Load the document using the loader.\n",
                                    "Print the number of document chunks (pages) that were loaded.\n",
                                    "Print a sample of the content (the first 200 characters).\n",
                                    "Print the metadata to see what additional information is available.\n",
                                    "This exercise will give you a solid foundation in document loading — a critical first step before we move on to document splitting in the next section.\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "\n",
                                    "# Define the file path to our Sherlock Holmes story\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# TODO: Create a PDF loader for our document\n",
                                    "\n",
                                    "# TODO: Load the document\n",
                                    "\n",
                                    "# TODO: Print the number of document chunks loaded\n",
                                    "\n",
                                    "# TODO: Print the first 200 characters of the first chunk\n",
                                    "\n",
                                    "# TODO: Print the metadata of the first chunk\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here's your completed script for loading and inspecting PDF documents using `PyPDFLoader`:\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "\n",
                                    "# Define the file path to our Sherlock Holmes story\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# Create a PDF loader for our document\n",
                                    "pdf_loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document\n",
                                    "docs = pdf_loader.load()\n",
                                    "\n",
                                    "# Print the number of document chunks loaded\n",
                                    "print(f\"Loaded {len(docs)} document chunks\")\n",
                                    "\n",
                                    "# Print the first 200 characters of the first chunk\n",
                                    "print(f\"\\nFirst 200 characters of the first chunk:\\n{docs[0].page_content[:200]}\")\n",
                                    "\n",
                                    "# Print the metadata of the first chunk\n",
                                    "print(f\"\\nMetadata of the first chunk:\\n{docs[0].metadata}\")\n",
                                    "```\n",
                                    "\n",
                                    "**Explanation of each step:**\n",
                                    "\n",
                                    "* **Loader Creation:** Instantiates a `PyPDFLoader` object to handle PDF files.\n",
                                    "* **Loading Documents:** Reads the content of the PDF into `docs`, a list of document chunks (usually pages).\n",
                                    "* **Inspecting Content:** Provides an overview by displaying the number of chunks, a snippet of text content, and available metadata.\n",
                                    "\n",
                                    "This approach allows you to understand precisely what information is extracted when loading documents, ensuring effective downstream processing.\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Switching to Text File Loading\n",
                                    "\n",
                                    "\n",
                                    "Let's switch gears and load a new file format!\n",
                                    "\n",
                                    "Your task is to replace the PyPDFLoader, which was previously used to load a Sherlock Holmes PDF file, with a TextLoader to load \"Alice in Wonderland\" as a text file. This will allow you to handle the text file format appropriately.\n",
                                    "\n",
                                    "As you make this change, pay attention to any differences you notice between the metadata and structure of text files versus PDFs. While PDFs split by pages, text files typically load as a single document chunk with simpler metadata.\n",
                                    "\n",
                                    "This exercise will help you understand how LangChain handles different file formats with their specialized loaders.\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
                                    "\n",
                                    "# Define the file path to our text file\n",
                                    "file_path = \"data/alice_in_wonderland.txt\"\n",
                                    "\n",
                                    "# TODO: Create a Text loader for our document\n",
                                    "loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# Print the number of document chunks loaded\n",
                                    "print(f\"Loaded {len(docs)} document chunks\")\n",
                                    "\n",
                                    "# Print the content of the first chunk\n",
                                    "print(f\"\\nFirst 200 characters of the first chunk:\\n{docs[0].page_content[:200]}\")\n",
                                    "\n",
                                    "# Print the metadata of the first chunk\n",
                                    "print(f\"\\nMetadata of the first chunk:\\n{docs[0].metadata}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import TextLoader\n",
                                    "\n",
                                    "# Define the file path to our text file\n",
                                    "file_path = \"data/alice_in_wonderland.txt\"\n",
                                    "\n",
                                    "# Create a Text loader for our document\n",
                                    "loader = TextLoader(file_path)\n",
                                    "\n",
                                    "# Load the document\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# Print the number of document chunks loaded\n",
                                    "print(f\"Loaded {len(docs)} document chunks\")\n",
                                    "\n",
                                    "# Print the first 200 characters of the first chunk\n",
                                    "print(f\"\\nFirst 200 characters of the first chunk:\\n{docs[0].page_content[:200]}\")\n",
                                    "\n",
                                    "# Print the metadata of the first chunk\n",
                                    "print(f\"\\nMetadata of the first chunk:\\n{docs[0].metadata}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Differences to notice:\n",
                                    "\n",
                                    "* Text files typically load as **one single chunk**, whereas PDFs split content into multiple chunks, usually by pages.\n",
                                    "* Metadata for text files is simpler, primarily showing only basic file details (such as source and filename), unlike PDFs that include richer metadata like authors, dates, and page numbers.\n",
                                    "\n",
                                    "This change demonstrates clearly how LangChain handles file format specifics using specialized loaders.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Experiment with Document Splitting Parameters\n",
                                    "\n",
                                    "\n",
                                    "Well done loading and inspecting documents using LangChain. Now, let's dive into experimenting with document splitting!\n",
                                    "\n",
                                    "In this exercise, you'll work with the RecursiveCharacterTextSplitter to explore how different settings affect the way a document is divided into chunks. Specifically, you'll adjust the parameters to:\n",
                                    "\n",
                                    "Set the chunk_size to 500\n",
                                    "Set the chunk_overlap to 50\n",
                                    "By making these changes, you'll observe how they influence both the number of chunks and the content within each chunk.\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "\n",
                                    "# Define the file path\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# Create a loader for our document\n",
                                    "loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# Initialize the text splitter with a specified chunk size and overlap\n",
                                    "text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "    # TODO: Set the chunk_size value to 500\n",
                                    "    chunk_size=1000,\n",
                                    "    # TODO: Set the chunk_overlap value to 50\n",
                                    "    chunk_overlap=100\n",
                                    ")\n",
                                    "\n",
                                    "# Split the loaded document into chunks using the text splitter\n",
                                    "split_docs = text_splitter.split_documents(docs)\n",
                                    "\n",
                                    "# Print the number of chunks after splitting\n",
                                    "print(f\"After splitting: {len(split_docs)} chunks\")\n",
                                    "\n",
                                    "# Print the content of the first chunk\n",
                                    "print(f\"\\nFirst chunk content:\\n{split_docs[0].page_content}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Anda sudah sangat dekat! Anda hanya perlu menyesuaikan nilai `chunk_size` dan `chunk_overlap` pada inisialisasi `RecursiveCharacterTextSplitter` sesuai dengan instruksi.\n",
                                    "\n",
                                    "Berikut adalah kode yang telah diperbaiki:\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "\n",
                                    "# Define the file path\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# Create a loader for our document\n",
                                    "loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# Initialize the text splitter with a specified chunk size and overlap\n",
                                    "text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "    # Set the chunk_size value to 500\n",
                                    "    chunk_size=500,\n",
                                    "    # Set the chunk_overlap value to 50\n",
                                    "    chunk_overlap=50\n",
                                    ")\n",
                                    "\n",
                                    "# Split the loaded document into chunks using the text splitter\n",
                                    "split_docs = text_splitter.split_documents(docs)\n",
                                    "\n",
                                    "# Print the number of chunks after splitting\n",
                                    "print(f\"After splitting: {len(split_docs)} chunks\")\n",
                                    "\n",
                                    "# Print the content of the first chunk\n",
                                    "print(f\"\\nFirst chunk content:\\n{split_docs[0].page_content}\")\n",
                                    "```\n",
                                    "\n",
                                    "**Perubahan yang dilakukan:**\n",
                                    "\n",
                                    "1.  `chunk_size` diubah dari `1000` menjadi `500`.\n",
                                    "2.  `chunk_overlap` diubah dari `100` menjadi `50`.\n",
                                    "\n",
                                    "Dengan perubahan ini, Anda akan melihat bahwa:\n",
                                    "\n",
                                    "*   **Jumlah chunk kemungkinan akan bertambah** karena setiap chunk sekarang lebih kecil (`chunk_size=500` dibandingkan `1000`).\n",
                                    "*   **Konten setiap chunk akan lebih pendek.**\n",
                                    "*   **Overlap antar chunk akan lebih kecil** (`chunk_overlap=50` dibandingkan `100`), yang berarti lebih sedikit teks yang diulang antara akhir satu chunk dan awal chunk berikutnya.\n",
                                    "\n",
                                    "Eksperimen seperti ini sangat berguna untuk memahami bagaimana parameter `chunk_size` dan `chunk_overlap` memengaruhi proses pemecahan dokumen, yang merupakan langkah penting dalam banyak aplikasi RAG (Retrieval Augmented Generation) dan pemrosesan dokumen lainnya dengan LLM. Ukuran chunk dan overlap yang optimal dapat bervariasi tergantung pada jenis dokumen, model LLM yang digunakan, dan tugas spesifik yang ingin diselesaikan."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Exploring Different Text Splitters\n",
                                    "\n",
                                    "\n",
                                    "In this task, you'll explore how different text splitters affect document processing in LangChain. Your goal is to modify the existing code by replacing the RecursiveCharacterTextSplitter with CharacterTextSplitter.\n",
                                    "\n",
                                    "The two splitters differ in their approach:\n",
                                    "\n",
                                    "The Recursive Character Text Splitter uses a list of separators to recursively break down text until the chunks are below a specified size, making it flexible for handling complex text structures.\n",
                                    "The CharacterTextSplitter divides text based solely on character count, offering a simpler approach that may result in chunks cutting through sentences or words.\n",
                                    "After making this change, run the code and observe how the alternative splitter impacts the resulting document chunks. Pay attention to the number of chunks created and the content of the first chunk.\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
                                    "\n",
                                    "# Define the file path\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# Create a loader for our document\n",
                                    "loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# TODO: Replace RecursiveCharacterTextSplitter with CharacterTextSplitter\n",
                                    "text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "    chunk_size=1000,\n",
                                    "    chunk_overlap=100\n",
                                    ")\n",
                                    "\n",
                                    "# Split the loaded document into chunks using the text splitter\n",
                                    "split_docs = text_splitter.split_documents(docs)\n",
                                    "\n",
                                    "# Print the number of chunks after splitting\n",
                                    "print(f\"After splitting: {len(split_docs)} chunks\")\n",
                                    "\n",
                                    "# Print the content of the first chunk\n",
                                    "print(f\"\\nFirst chunk content:\\n{split_docs[0].page_content}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Here’s how you can swap out the `RecursiveCharacterTextSplitter` for a plain `CharacterTextSplitter` and see the difference:\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import CharacterTextSplitter\n",
                                    "\n",
                                    "# Define the file path\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# Create a loader for our document\n",
                                    "loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document (a list of Document objects, one per page)\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# Replace RecursiveCharacterTextSplitter with CharacterTextSplitter\n",
                                    "text_splitter = CharacterTextSplitter(\n",
                                    "    chunk_size=1000,    # max characters per chunk\n",
                                    "    chunk_overlap=100   # overlap between chunks\n",
                                    ")\n",
                                    "\n",
                                    "# Split the loaded document into chunks\n",
                                    "split_docs = text_splitter.split_documents(docs)\n",
                                    "\n",
                                    "# Inspect the results\n",
                                    "print(f\"After splitting: {len(split_docs)} chunks\\n\")\n",
                                    "print(\"First chunk content:\")\n",
                                    "print(split_docs[0].page_content)\n",
                                    "```\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## What to look for\n",
                                    "\n",
                                    "1. **Chunk count**\n",
                                    "\n",
                                    "   * **RecursiveCharacterTextSplitter** might give you dozens of small, sentence-friendly chunks (e.g. 50+).\n",
                                    "   * **CharacterTextSplitter** will simply slice every \\~1000 characters (minus the overlap), so you’ll typically see far **fewer** chunks (roughly total\\_chars ÷ (chunk\\_size–chunk\\_overlap)).\n",
                                    "\n",
                                    "2. **Chunk boundaries**\n",
                                    "\n",
                                    "   * With **recursive**, splits tend to respect sentence/paragraph breaks.\n",
                                    "   * With **character-based**, you’ll often cut right through the middle of a sentence or word at the 1,000th character.\n",
                                    "\n",
                                    "3. **First chunk content**\n",
                                    "\n",
                                    "   * You’ll notice that the first chunk ends exactly at your character limit (minus the overlap), even if that’s mid-sentence.\n",
                                    "   * Compare it side-by-side with what you got from the recursive splitter to see how “clean” or “jagged” it feels.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "### Example (hypothetical) output\n",
                                    "\n",
                                    "```text\n",
                                    "After splitting: 14 chunks\n",
                                    "\n",
                                    "First chunk content:\n",
                                    "The Adventure of the Blue Carbuncle\n",
                                    "Arthur Conan Doyle\n",
                                    "\n",
                                    "One foggy Christmas Eve … [continues up to around character 1000, cutting off mid-sentence]\n",
                                    "```\n",
                                    "\n",
                                    "By running both approaches you’ll get a clear sense of when you need sentence-aware splitting versus when a simple character-count splitter will do.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "9ee7431a",
                           "metadata": {},
                           "source": [
                                    "## Loading and Splitting PDF Documents\n",
                                    "\n",
                                    "You've done a great job learning about document loaders and splitting techniques. Now, let's put it all together!\n",
                                    "\n",
                                    "In this exercise, you'll work with a PDF document and practice the following steps:\n",
                                    "\n",
                                    "Create a PyPDFLoader instance to load the provided PDF file.\n",
                                    "Load the document using the loader.\n",
                                    "Initialize a RecursiveCharacterTextSplitter with a specified chunk size and overlap.\n",
                                    "Use the text splitter to split the document into chunks.\n",
                                    "Print the content of the first five chunks to see how the document is divided.\n",
                                    "This task will solidify your understanding of loading and splitting documents, setting you up for more advanced processing tasks. Dive in and see how these tools can make document handling a breeze!\n",
                                    "\n",
                                    "```python\n",
                                    "\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "\n",
                                    "# Define the file path\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# TODO: Create a loader for our document\n",
                                    "\n",
                                    "# TODO: Load the document\n",
                                    "\n",
                                    "# TODO: Initialize a RecursiveCharacterTextSplitter with a specified chunk size and overlap\n",
                                    "\n",
                                    "# TODO: Split the loaded document into chunks using the text splitter\n",
                                    "\n",
                                    "# TODO: Print the content of the first five chunks\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "\n",
                                    "# Define the file path\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# Create a loader for our document\n",
                                    "loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document (returns a list of Document objects, one per page)\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# Initialize a RecursiveCharacterTextSplitter with a chunk size of 1000 chars and 100-char overlap\n",
                                    "text_splitter = RecursiveCharacterTextSplitter(\n",
                                    "    chunk_size=1000,\n",
                                    "    chunk_overlap=100\n",
                                    ")\n",
                                    "\n",
                                    "# Split the loaded document into chunks\n",
                                    "split_docs = text_splitter.split_documents(docs)\n",
                                    "\n",
                                    "# Print the content of the first five chunks\n",
                                    "for idx, chunk in enumerate(split_docs[:5], start=1):\n",
                                    "    print(f\"\\n--- Chunk {idx} ---\\n\")\n",
                                    "    print(chunk.page_content)\n",
                                    "    print(\"\\n\" + \"-\" * 40)\n",
                                    "```\n",
                                    "\n",
                                    "**What this does:**\n",
                                    "\n",
                                    "1. **Loads** the PDF into page-based `Document` objects.\n",
                                    "2. **Splits** each page recursively at natural boundaries (paragraphs, sentences, etc.) until each chunk is ≤1000 characters, with 100 chars of context overlap.\n",
                                    "3. **Prints** the first five chunks so you can inspect how the text was divided.\n",
                                    "\n",
                                    "Run this script to see exactly how your PDF gets loaded and split — a key first step before you move on to embedding or semantic searching!\n"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
