{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 4 : Asking Questions with Retrieved Context and Templates\n",
                                    "\n",
                                    "# Asking Questions with Retrieved Context and Templates\n",
                                    "\n",
                                    "Welcome to the final lesson of this course! In this lesson, we will integrate context retrieval with a chat model using LangChain. This builds on the skills you've developed in previous lessons, where you learned about document embeddings and similarity search. Today, we'll focus on using templates to format messages with extra context, enabling you to ask questions and receive answers based on the retrieved document content. This lesson will bring together all the skills you've learned so far, culminating in a comprehensive understanding of document processing and retrieval with LangChain in Python.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Quick Reminder: Preparing Documents and Creating a Vector Store\n",
                                    "\n",
                                    "Let's quickly recap what we've learned in previous lessons about preparing documents and creating a vector store. We'll load and prepare our document, **The Adventure of the Blue Carbuncle** and generate embeddings to create a vector store. This process is essential for effective context retrieval.\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_openai import OpenAIEmbeddings\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "# Define the file path\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# Create a loader for our document\n",
                                    "loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# Split the document into chunks\n",
                                    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
                                    "split_docs = text_splitter.split_documents(docs)\n",
                                    "\n",
                                    "# Create a vector store for all the document chunks\n",
                                    "embedding_model = OpenAIEmbeddings()\n",
                                    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
                                    "```\n",
                                    "\n",
                                    "In this code, we:\n",
                                    "\n",
                                    "1. Load a PDF document.\n",
                                    "2. Split it into manageable text chunks.\n",
                                    "3. Generate embeddings for each chunk.\n",
                                    "4. Build a FAISS vector store for efficient similarity search.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Combining Retrieved Context\n",
                                    "\n",
                                    "Once our vector store is ready, we can retrieve relevant chunks based on a query and combine them into a single context.\n",
                                    "\n",
                                    "```python\n",
                                    "# Define a query\n",
                                    "query = \"From whom was the stone stolen?\"\n",
                                    "\n",
                                    "# Retrieve relevant documents\n",
                                    "retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
                                    "\n",
                                    "# Combine the content of retrieved documents\n",
                                    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
                                    "```\n",
                                    "\n",
                                    "* **`similarity_search(query, k=3)`** retrieves the top 3 most relevant chunks.\n",
                                    "* We then **join** their `page_content` with double line breaks to form the `context`.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Formatting Messages with Templates\n",
                                    "\n",
                                    "To communicate with the chat model effectively, we use a **prompt template**. Think of it as a fill-in-the-blank form where you insert the context and the question.\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain.prompts import ChatPromptTemplate\n",
                                    "\n",
                                    "# Create a prompt template for RAG\n",
                                    "prompt_template = ChatPromptTemplate.from_template(\n",
                                    "    \"Answer the following question based on the provided context.\\n\\n\"\n",
                                    "    \"Context:\\n{context}\\n\\n\"\n",
                                    "    \"Question: {question}\"\n",
                                    ")\n",
                                    "```\n",
                                    "\n",
                                    "* We define placeholders `{context}` and `{question}`.\n",
                                    "* The `\\n` characters introduce line breaks for readability.\n",
                                    "* Breaking the string into multiple quoted lines keeps it maintainable.\n",
                                    "\n",
                                    "Next, fill in the template:\n",
                                    "\n",
                                    "```python\n",
                                    "# Format the prompt with our context and query\n",
                                    "prompt = prompt_template.format(context=context, question=query)\n",
                                    "```\n",
                                    "\n",
                                    "This ensures the chat model receives a complete, well-structured message containing both the retrieved context and the user’s question.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Exploring the Formatted Prompt\n",
                                    "\n",
                                    "Let’s print the formatted prompt to verify its structure:\n",
                                    "\n",
                                    "```python\n",
                                    "# Print the formatted prompt\n",
                                    "print(prompt)\n",
                                    "```\n",
                                    "\n",
                                    "It will output something like:\n",
                                    "\n",
                                    "```\n",
                                    "Answer the following question based on the provided context.\n",
                                    "\n",
                                    "Context:\n",
                                    "the back yard and smoked a pipe and wondered\n",
                                    "what it would be best to do.\n",
                                    "“I had a friend once called Maudsley, who went\n",
                                    "to the bad, and has just been serving his time in\n",
                                    "Pentonville. One day he had met me...\n",
                                    "\n",
                                    "Question: From whom was the stone stolen?\n",
                                    "```\n",
                                    "\n",
                                    "By inspecting the printed template, you can confirm that:\n",
                                    "\n",
                                    "* **Context** is properly inserted.\n",
                                    "* **Question** placeholder is correctly replaced.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Asking a Question with Retrieved Context to a Chat Model\n",
                                    "\n",
                                    "With our prompt ready, we now interact with the chat model:\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_openai import ChatOpenAI\n",
                                    "\n",
                                    "# Initialize the chat model\n",
                                    "chat = ChatOpenAI()\n",
                                    "\n",
                                    "# Get the response from the model\n",
                                    "response = chat.invoke(prompt)\n",
                                    "\n",
                                    "# Print the question and the AI's answer\n",
                                    "print(f\"Question: {query}\")\n",
                                    "print(f\"Answer: {response.content}\")\n",
                                    "```\n",
                                    "\n",
                                    "Example output:\n",
                                    "\n",
                                    "```\n",
                                    "Question: From whom was the stone stolen?\n",
                                    "Answer: The stone was stolen from the Countess of Morcar.\n",
                                    "```\n",
                                    "\n",
                                    "The model uses the provided context to generate a relevant and accurate answer.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "## Summary and Next Steps\n",
                                    "\n",
                                    "* **Recapped** document loading, chunking, embedding, and vector store creation.\n",
                                    "* **Retrieved** relevant chunks with a similarity search.\n",
                                    "* **Formatted** messages using `ChatPromptTemplate`.\n",
                                    "* **Interacted** with a chat model to ask context-aware questions.\n",
                                    "\n",
                                    "You’ve now mastered integrating context retrieval with a chat model in LangChain. Experiment with different documents and queries to deepen your understanding. In upcoming courses, we’ll explore more advanced retrieval and generation techniques.\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Creating a Chat Prompt Template\n",
                                    "\n",
                                    "In the previous lesson, you learned the fundamentals of formatting messages with templates - now it's time to apply those skills hands-on!\n",
                                    "\n",
                                    "Your task is to create a simple chat prompt template using the ChatPromptTemplate from LangChain. Here's what you'll do:\n",
                                    "\n",
                                    "Write a template string that includes placeholders for both context and question.\n",
                                    "Use the format() method with the provided sample values to replace the placeholders in your template.\n",
                                    "Finally, print the formatted prompt to ensure everything is correctly replaced.\n",
                                    "This exercise is all about understanding how to format a template, so feel free to get creative with your template content. Dive in and see how fun and rewarding it can be to work with templates!\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain.prompts import ChatPromptTemplate\n",
                                    "\n",
                                    "# TODO: Create a prompt template with placeholders for context and question\n",
                                    "\n",
                                    "# Static sample values for context and question\n",
                                    "context = \"You are in a mysterious forest with talking animals.\"\n",
                                    "question = \"What would you ask the wise old owl?\"\n",
                                    "\n",
                                    "# TODO: Format the prompt with the static values\n",
                                    "\n",
                                    "# TODO: Print the formatted prompt\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Here’s a simple example of how you could set up and use a `ChatPromptTemplate` in LangChain. You can copy this into your own Python environment (with LangChain installed) to see it in action:\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain.prompts import ChatPromptTemplate\n",
                                    "\n",
                                    "# Static sample values for context and question\n",
                                    "context = \"You are in a mysterious forest with talking animals.\"\n",
                                    "question = \"What would you ask the wise old owl?\"\n",
                                    "\n",
                                    "# 1. Create a prompt template with placeholders\n",
                                    "prompt_template = ChatPromptTemplate.from_template(\n",
                                    "    \"You find yourself in a strange scenario:\\n\\n\"\n",
                                    "    \"Context:\\n{context}\\n\\n\"\n",
                                    "    \"Your task:\\nQuestion: {question}\"\n",
                                    ")\n",
                                    "\n",
                                    "# 2. Format the prompt with the static values\n",
                                    "formatted_prompt = prompt_template.format(\n",
                                    "    context=context,\n",
                                    "    question=question\n",
                                    ")\n",
                                    "\n",
                                    "# 3. Print the formatted prompt\n",
                                    "print(formatted_prompt)\n",
                                    "```\n",
                                    "\n",
                                    "**What this does:**\n",
                                    "\n",
                                    "1. **Defines** a template string with `{context}` and `{question}` as placeholders.\n",
                                    "2. **Calls** `.format(...)` on that template, injecting your `context` and `question` strings.\n",
                                    "3. **Prints** out the fully assembled prompt, ready to send to your chat model.\n",
                                    "\n",
                                    "When you run it, you’ll see output like:\n",
                                    "\n",
                                    "```\n",
                                    "You find yourself in a strange scenario:\n",
                                    "\n",
                                    "Context:\n",
                                    "You are in a mysterious forest with talking animals.\n",
                                    "\n",
                                    "Your task:\n",
                                    "Question: What would you ask the wise old owl?\n",
                                    "```\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Combining Document Chunks for Context\n",
                                    "\n",
                                    "Having just explored the use of templates, it's crucial to first grasp how to merge document chunks to create a coherent context for your questions. Let's dive into a practical exercise to solidify this understanding!\n",
                                    "\n",
                                    "Here's what you'll do:\n",
                                    "\n",
                                    "Combine the retrieved document chunks into a single context string using the delimiter \"\\n\\n\".\n",
                                    "Print the combined context to ensure that the chunks are correctly joined.\n",
                                    "This exercise will help you practice string manipulation and context formatting. Let's see how you can make the context more structured and informative!\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_openai import OpenAIEmbeddings\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "\n",
                                    "# Define the file path\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# Create a loader for our document\n",
                                    "loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# Split the document into chunks\n",
                                    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
                                    "split_docs = text_splitter.split_documents(docs)\n",
                                    "\n",
                                    "# Create a vector store for all the document chunks\n",
                                    "embedding_model = OpenAIEmbeddings()\n",
                                    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
                                    "\n",
                                    "# Define a query\n",
                                    "query = \"From whom was the stone stolen?\"\n",
                                    "\n",
                                    "# Retrieve relevant documents\n",
                                    "retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
                                    "\n",
                                    "# TODO: Combine the content of retrieved documents\n",
                                    "\n",
                                    "# TODO: Print the combined context\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Here’s how you can complete the exercise by merging the retrieved chunks into one context string and printing it:\n",
                                    "\n",
                                    "```python\n",
                                    "# TODO: Combine the content of retrieved documents\n",
                                    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
                                    "\n",
                                    "# TODO: Print the combined context\n",
                                    "print(context)\n",
                                    "```\n",
                                    "\n",
                                    "**What this does:**\n",
                                    "\n",
                                    "1. Iterates over each `doc` in `retrieved_docs` and pulls out its `.page_content`.\n",
                                    "2. Joins all the chunks with two line-breaks (`\"\\n\\n\"`) to keep them clearly separated.\n",
                                    "3. Prints the full `context` string so you can verify the combined content.\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Crafting a Prompt Template with Retrieved Context\n",
                                    "\n",
                                    "Cosmo\n",
                                    "Just now\n",
                                    "Read message aloud\n",
                                    "You've mastered combining retrieved document content to create context for your questions. Now, let's seamlessly integrate this into a structured prompt using a template.\n",
                                    "\n",
                                    "Here's what you'll do:\n",
                                    "\n",
                                    "Create a ChatPromptTemplate that includes placeholders for both context and question.\n",
                                    "Use the format() method to replace these placeholders with the provided context and query.\n",
                                    "Print the formatted prompt to ensure everything is correctly replaced.\n",
                                    "This exercise will help you practice using templates to format messages with context and questions. Dive in and see how rewarding it can be to work with templates!\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "from langchain.prompts import ChatPromptTemplate\n",
                                    "\n",
                                    "# Define the file path\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# Create a loader for our document\n",
                                    "loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# Split the document into chunks\n",
                                    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
                                    "split_docs = text_splitter.split_documents(docs)\n",
                                    "\n",
                                    "# Create a vector store for all the document chunks\n",
                                    "embedding_model = OpenAIEmbeddings()\n",
                                    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
                                    "\n",
                                    "# Define a query\n",
                                    "query = \"From whom was the stone stolen?\"\n",
                                    "\n",
                                    "# Retrieve relevant documents\n",
                                    "retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
                                    "\n",
                                    "# Combine the content of retrieved documents\n",
                                    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
                                    "\n",
                                    "# TODO: Create a prompt template with placeholders for context and question\n",
                                    "\n",
                                    "# TODO: Format the prompt with the context and query\n",
                                    "\n",
                                    "# TODO: Print the formatted prompt\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain.prompts import ChatPromptTemplate\n",
                                    "\n",
                                    "# TODO: Create a prompt template with placeholders for context and question\n",
                                    "prompt_template = ChatPromptTemplate.from_template(\n",
                                    "    \"Answer the following question based on the provided context:\\n\\n\"\n",
                                    "    \"Context:\\n{context}\\n\\n\"\n",
                                    "    \"Question: {question}\"\n",
                                    ")\n",
                                    "\n",
                                    "# TODO: Format the prompt with the context and query\n",
                                    "formatted_prompt = prompt_template.format(\n",
                                    "    context=context,\n",
                                    "    question=query\n",
                                    ")\n",
                                    "\n",
                                    "# TODO: Print the formatted prompt\n",
                                    "print(formatted_prompt)\n",
                                    "```\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Integrating Chat Model with Context\n",
                                    "\n",
                                    "You've just explored how to interact with a chat model using formatted prompts. Now, let's enhance this by structuring the model's invocation with SystemMessage and HumanMessage.\n",
                                    "\n",
                                    "Here's what you'll do:\n",
                                    "\n",
                                    "Set up a SystemMessage to instruct the model to answer questions based only on the provided context. If no context is given, it should state that it doesn't have enough information.\n",
                                    "Use HumanMessage to invoke the model twice:\n",
                                    "First, with a prompt containing context.\n",
                                    "Second, with a prompt without context.\n",
                                    "Print both responses to verify the model's behavior in each scenario.\n",
                                    "This exercise will help you see how to guide the model's responses based on the availability of context. Let's dive in and see how this setup can improve your interactions!\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
                                    "from langchain_community.document_loaders import PyPDFLoader\n",
                                    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                                    "from langchain_community.vectorstores import FAISS\n",
                                    "from langchain.prompts import ChatPromptTemplate\n",
                                    "from langchain.schema.messages import SystemMessage, HumanMessage\n",
                                    "\n",
                                    "# Define the file path\n",
                                    "file_path = \"data/the_adventure_of_the_blue_carbuncle.pdf\"\n",
                                    "\n",
                                    "# Create a loader for our document\n",
                                    "loader = PyPDFLoader(file_path)\n",
                                    "\n",
                                    "# Load the document\n",
                                    "docs = loader.load()\n",
                                    "\n",
                                    "# Split the document into chunks\n",
                                    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
                                    "split_docs = text_splitter.split_documents(docs)\n",
                                    "\n",
                                    "# Create a vector store for all the document chunks\n",
                                    "embedding_model = OpenAIEmbeddings()\n",
                                    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
                                    "\n",
                                    "# Define a query\n",
                                    "query = \"From whom was the stone stolen?\"\n",
                                    "\n",
                                    "# Retrieve relevant documents\n",
                                    "retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
                                    "\n",
                                    "# Combine the content of retrieved documents\n",
                                    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
                                    "\n",
                                    "# Create a prompt template\n",
                                    "prompt_template = ChatPromptTemplate.from_template(\n",
                                    "    \"Answer the following question based on the provided context.\\n\\n\"\n",
                                    "    \"Context:\\n{context}\\n\\n\"\n",
                                    "    \"Question: {question}\"\n",
                                    ")\n",
                                    "\n",
                                    "# Format the prompt with our context and query\n",
                                    "prompt_with_context = prompt_template.format(context=context, question=query)\n",
                                    "prompt_without_context = prompt_template.format(context=\"\", question=query)\n",
                                    "\n",
                                    "# Initialize the chat model\n",
                                    "chat = ChatOpenAI()\n",
                                    "\n",
                                    "# TODO: Create a SystemMessage that instructs the model to answer based only on provided context\n",
                                    "\n",
                                    "# TODO: Invoke the model with both system message and HumanMessage containing the prompt with context\n",
                                    "\n",
                                    "# TODO: Invoke the model with system message and HumanMessage containing the prompt without context\n",
                                    "\n",
                                    "# TODO: Print both responses to compare how the model behaves with and without context\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Here’s how you can fill in the TODOs to create a `SystemMessage` that forces the model to rely only on the supplied context, and then invoke it twice—once with context, once without:\n",
                                    "\n",
                                    "```python\n",
                                    "from langchain.schema.messages import SystemMessage, HumanMessage\n",
                                    "\n",
                                    "# 1. Create the SystemMessage\n",
                                    "system_message = SystemMessage(\n",
                                    "    content=(\n",
                                    "        \"You are an AI assistant that answers questions strictly based on the provided context. \"\n",
                                    "        \"If the context is empty or does not contain the answer, you must respond with \"\n",
                                    "        \"'I don't have enough information to answer that question.'\"\n",
                                    "    )\n",
                                    ")\n",
                                    "\n",
                                    "# 2. Invoke with context\n",
                                    "response_with_context = chat([\n",
                                    "    system_message,\n",
                                    "    HumanMessage(content=prompt_with_context)\n",
                                    "])\n",
                                    "\n",
                                    "# 3. Invoke without context\n",
                                    "response_without_context = chat([\n",
                                    "    system_message,\n",
                                    "    HumanMessage(content=prompt_without_context)\n",
                                    "])\n",
                                    "\n",
                                    "# 4. Print both responses\n",
                                    "print(\"=== With Context ===\")\n",
                                    "print(response_with_context.content)\n",
                                    "print(\"\\n=== Without Context ===\")\n",
                                    "print(response_without_context.content)\n",
                                    "```\n",
                                    "\n",
                                    "**What this does:**\n",
                                    "\n",
                                    "1. **`SystemMessage`**\n",
                                    "   Instructs the model that it must base its answers *only* on the `Context:` block, and to admit lack of information if the context is empty or insufficient.\n",
                                    "\n",
                                    "2. **`HumanMessage`**\n",
                                    "   Supplies the formatted prompt (once with real context, once with an empty context string).\n",
                                    "\n",
                                    "3. **`chat([...])`**\n",
                                    "   Sends the full message list (system + human) to the model and returns a response message.\n",
                                    "\n",
                                    "4. **Printing**\n",
                                    "   Allows you to compare how the model behaves when it *does* versus *doesn’t* have the necessary context.\n",
                                    "\n",
                                    "Now run this script and you’ll see the assistant answer correctly when context is provided, and gracefully decline when it isn’t.\n"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
