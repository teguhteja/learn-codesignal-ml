{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here's the content formatted in Markdown:\n",
    "\n",
    "---\n",
    "\n",
    "# Lesson Overview\n",
    "\n",
    "Welcome! In today's lesson, we will learn how to split a dataset into training and testing sets. This is a crucial step in preparing your data for machine learning models to ensure they generalize well to unseen data.\n",
    "\n",
    "**Lesson Goal:** By the end of this lesson, you will understand how to split financial datasets, such as Tesla's stock data, into training and testing sets using Python.\n",
    "\n",
    "## Revision of Preprocessing Steps\n",
    "\n",
    "Before we delve into splitting the dataset, let's briefly review the preprocessing steps we have covered so far. The dataset has been loaded, new features have been engineered, and the features have been scaled.\n",
    "\n",
    "Here's the code for those steps for a quick revision:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datasets\n",
    "\n",
    "# Loading and preprocessing the dataset (revision)\n",
    "data = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(data['train'])\n",
    "tesla_df['High-Low'] = tesla_df['High'] - tesla_df['Low']\n",
    "tesla_df['Price-Open'] = tesla_df['Close'] - tesla_df['Open']\n",
    "\n",
    "# Defining features and target\n",
    "features = tesla_df[['High-Low', 'Price-Open', 'Volume']].values\n",
    "# Target is the column that we are trying to predict\n",
    "target = tesla_df['Close'].values\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "```\n",
    "\n",
    "## Understanding the Importance of Splitting Datasets\n",
    "\n",
    "To avoid overfitting, where a model learns the training data too well and performs poorly on new, unseen data, it's important to evaluate your machine learning model on data it has never seen before. This is where splitting datasets into training and testing sets comes into play.\n",
    "\n",
    "### Why Split?\n",
    "\n",
    "- **Training Set:** Used to train the machine learning model.\n",
    "- **Testing Set:** Used to evaluate the model's performance and check its ability to generalize to unseen data.\n",
    "\n",
    "This ensures that your model's performance is not just tailored to the training data but can be generalized to new inputs.\n",
    "\n",
    "## Implementing Dataset Split with `train_test_split`\n",
    "\n",
    "The `train_test_split` function from `sklearn.model_selection` helps us easily split the data.\n",
    "\n",
    "**Parameters of `train_test_split`:**\n",
    "\n",
    "- `test_size`: The proportion of the dataset to include in the test split (e.g., 0.25 means 25% of the data will be used for testing).\n",
    "- `train_size`: The proportion of the dataset to include in the train split (optional if `test_size` is provided).\n",
    "- `random_state`: Controls the shuffling applied to the data before the split. Providing a fixed value ensures reproducibility.\n",
    "\n",
    "Let's split our scaled features and targets into training and testing sets:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
    "```\n",
    "\n",
    "The `train_test_split` function will split our dataset into training and testing sets:\n",
    "\n",
    "- `features_scaled` and `target` are the inputs.\n",
    "- `test_size=0.25` means 25% of the data goes to the test set.\n",
    "- `random_state=42` ensures reproducibility. The state can be any other number, too.\n",
    "\n",
    "## Verifying Shapes and Contents of the Split Data\n",
    "\n",
    "After splitting the dataset, it's important to verify the shapes and the contents of the resulting sets to ensure the split was done correctly.\n",
    "\n",
    "### Checking Shapes:\n",
    "\n",
    "Print the shapes of the training and testing sets to confirm the split ratio is as expected.\n",
    "\n",
    "### Inspecting Sample Rows:\n",
    "\n",
    "Print a few rows of the training and testing sets to visually inspect the data.\n",
    "\n",
    "Let's check our split data:\n",
    "\n",
    "```python\n",
    "# Verify splits\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"First 5 rows of training features: \\n{X_train[:5]}\")\n",
    "print(f\"First 5 training targets: {y_train[:5]}\\n\")\n",
    "\n",
    "print(f\"First 5 rows of testing features: \\n{X_test[:5]}\")\n",
    "print(f\"First 5 testing targets: {y_test[:5]}\")\n",
    "```\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```\n",
    "Training features shape: (2510, 3)\n",
    "Testing features shape: (837, 3)\n",
    "First 5 rows of training features: \n",
    "[[-4.66075964e-01  6.80184955e-02  3.11378946e-01]\n",
    " [ 4.01701510e+00  5.04529577e+00 -4.61555718e-02]\n",
    " [ 2.04723437e+00  3.09900603e+00  9.43022378e-04]\n",
    " [-5.30579018e-01 -2.30986178e-02 -5.67163058e-01]\n",
    " [-4.78854883e-01 -5.79376618e-02 -6.94451021e-01]]\n",
    "First 5 training targets: [ 17.288    355.666656 222.419998  15.000667  13.092   ]\n",
    "\n",
    "First 5 rows of testing features: \n",
    "[[-0.36226203  0.2087143   0.69346624]\n",
    " [ 1.27319589  1.04049732  0.58204785]\n",
    " [-0.53556882 -0.03231093 -0.86874821]\n",
    " [-0.49029475  0.07773304 -0.51784526]\n",
    " [ 3.0026057  -4.41816938 -0.31923731]]\n",
    "First 5 testing targets: [ 23.209333 189.606674  14.730667  16.763332 325.733337]\n",
    "```\n",
    "\n",
    "This output confirms that our dataset has been successfully split into training and testing sets, showing the shape of each set and giving us a glimpse into the rows of our features and targets post-split. It's an important validation step to ensure our data is ready for machine learning model training and evaluation.\n",
    "\n",
    "## Lesson Summary\n",
    "\n",
    "Great job! In this lesson, we:\n",
    "\n",
    "- Discussed the importance of splitting datasets to avoid overfitting.\n",
    "- Implemented `train_test_split` to divide the dataset into training and testing sets.\n",
    "- Verified the shapes and inspected sample rows of the resulting splits.\n",
    "\n",
    "These steps are crucial for ensuring that your machine learning models can generalize well to new data. Up next, you'll have some practice exercises to solidify your understanding and improve your data preparation skills. Keep going!\n",
    "\n",
    "--- \n",
    "\n",
    "This Markdown format is clean, well-structured, and suitable for lesson notes or documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust the Dataset Split Ratio\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "\n",
    "# Load the dataset\n",
    "data = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(data['train'])\n",
    "\n",
    "# Create new features\n",
    "tesla_df['High-Low'] = tesla_df['High'] - tesla_df['Low']\n",
    "tesla_df['Price-Open'] = tesla_df['Close'] - tesla_df['Open']\n",
    "\n",
    "# Define features and target\n",
    "features = tesla_df[['High-Low', 'Price-Open', 'Volume']].values\n",
    "target = tesla_df['Close'].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.30, random_state=42)\n",
    "\n",
    "# Verify splits\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"First 5 rows of training features: \\n{X_train[:5]}\")\n",
    "print(f\"First 5 training targets: {y_train[:5]}\\n\")\n",
    "\n",
    "print(f\"First 5 rows of testing features: \\n{X_test[:5]}\")\n",
    "print(f\"First 5 testing targets: {y_test[:5]}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the Dataset Split\n",
    "\n",
    "It looks like there's an issue with the variables returned by `train_test_split`. The `train_test_split` function from `sklearn.model_selection` returns four values: `X_train`, `X_test`, `y_train`, and `y_test`, but your code is assigning them incorrectly.\n",
    "\n",
    "Here's the corrected version of the code:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "\n",
    "# Load dataset\n",
    "data = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(data['train'])\n",
    "\n",
    "# Feature engineering\n",
    "tesla_df['High-Low'] = tesla_df['High'] - tesla_df['Low']\n",
    "tesla_df['Price-Open'] = tesla_df['Close'] - tesla_df['Open']\n",
    "\n",
    "# Define features and target\n",
    "features = tesla_df[['High-Low', 'Price-Open', 'Volume']].values\n",
    "target = tesla_df['Close'].values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Verify splits\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"First 5 rows of training features:\\n{X_train[:5]}\")\n",
    "print(f\"First 5 training targets: {y_train[:5]}\\n\")\n",
    "\n",
    "print(f\"First 5 rows of testing features:\\n{X_test[:5]}\")\n",
    "print(f\"First 5 testing targets: {y_test[:5]}\")\n",
    "```\n",
    "\n",
    "### Key Changes:\n",
    "- Changed `X_train, y_train, X_test, y_test` to `X_train, X_test, y_train, y_test` in the `train_test_split` function call.\n",
    "\n",
    "### Conclusion:\n",
    "This correction ensures that the features and targets are split into the appropriate training and testing sets, and the resulting shapes and sample values can be verified as expected. Happy debugging! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in the Blanks: Splitting and Scaling Data\n",
    "\n",
    "Here’s how you can fill in the blanks to ensure the code works as intended:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "\n",
    "# Loading and preprocessing the dataset (revision)\n",
    "data = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(data['train'])\n",
    "tesla_df['High-Low'] = tesla_df['High'] - tesla_df['Low']\n",
    "tesla_df['Price-Open'] = tesla_df['Close'] - tesla_df['Open']\n",
    "\n",
    "# Defining features and target\n",
    "features = tesla_df[['High-Low', 'Price-Open', 'Volume']].values\n",
    "target = tesla_df['Close'].values\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "# TODO: Fit and transform the features using StandardScaler\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Splitting the dataset\n",
    "# TODO: Split the preprocessed features and target into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Verify splits\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"First 5 rows of training features: \\n{X_train[:5]}\")\n",
    "print(f\"First 5 training targets: {y_train[:5]}\\n\")\n",
    "\n",
    "print(f\"First 5 rows of testing features: \\n{X_test[:5]}\")\n",
    "print(f\"First 5 testing targets: {y_test[:5]}\")\n",
    "```\n",
    "\n",
    "### Key Fill-ins:\n",
    "1. **Scaling the features**:\n",
    "   ```python\n",
    "   features_scaled = scaler.fit_transform(features)\n",
    "   ```\n",
    "   This scales the features using `StandardScaler`, ensuring that they have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Splitting the dataset**:\n",
    "   ```python\n",
    "   X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
    "   ```\n",
    "   This correctly splits the scaled features and target into training and testing sets.\n",
    "\n",
    "### Conclusion:\n",
    "This completes the data preprocessing steps and ensures the data is properly prepared for any further analysis or model training. Keep up the stellar exploration! 🚀✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into Training and Testing Sets\n",
    "\n",
    "The error occurs because the number of samples in `features_scaled` and `target` are inconsistent. This typically happens when you apply `dropna()` on the features but not on the target, resulting in a mismatch in the number of samples between them.\n",
    "\n",
    "To fix this, we need to ensure that both `features` and `target` have the same number of rows after dropping any rows with missing values.\n",
    "\n",
    "Here’s how to adjust the code:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "\n",
    "# Load the dataset\n",
    "data = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(data['train'])\n",
    "\n",
    "# Create new features\n",
    "tesla_df['5 Day Moving Avg'] = tesla_df['Close'].rolling(window=5).mean()\n",
    "tesla_df['10 Day Moving Avg'] = tesla_df['Close'].rolling(window=10).mean()\n",
    "\n",
    "# Drop rows with NaN values in both features and target to ensure consistency\n",
    "tesla_df = tesla_df.dropna(subset=['5 Day Moving Avg', '10 Day Moving Avg', 'Volume', 'Close'])\n",
    "\n",
    "# Define features and target\n",
    "features = tesla_df[['5 Day Moving Avg', '10 Day Moving Avg', 'Volume']].values\n",
    "target = tesla_df['Close'].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset into training and testing sets, using 25% for testing and random_state of 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Verify splits\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"First 5 rows of training features: \\n{X_train[:5]}\")\n",
    "print(f\"First 5 training targets: {y_train[:5]}\\n\")\n",
    "print(f\"First 5 rows of testing features: \\n{X_test[:5]}\")\n",
    "print(f\"First 5 testing targets: {y_test[:5]}\")\n",
    "```\n",
    "\n",
    "### Key Fix:\n",
    "- **Drop rows with `NaN` values**:\n",
    "  ```python\n",
    "  tesla_df = tesla_df.dropna(subset=['5 Day Moving Avg', '10 Day Moving Avg', 'Volume', 'Close'])\n",
    "  ```\n",
    "  This ensures that both the features and the target have consistent lengths.\n",
    "\n",
    "### Conclusion:\n",
    "This adjustment ensures that the `features` and `target` arrays have the same number of samples, avoiding the `ValueError`. Now, your code should work without any issues! 🌌🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and Split Tesla Stock Data\n",
    "\n",
    "Here's the complete code to accomplish the tasks outlined in the TODO comments:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "\n",
    "# TODO: Load the dataset 'codesignal/tsla-historic-prices' and convert it to a DataFrame\n",
    "data = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(data['train'])\n",
    "\n",
    "# TODO: Create new features 'SMA20' (20-day Simple Moving Average) and 'EMA20' (20-day Exponential Moving Average)\n",
    "tesla_df['SMA20'] = tesla_df['Close'].rolling(window=20).mean()\n",
    "tesla_df['EMA20'] = tesla_df['Close'].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "# TODO: Drop NaN values that were created by moving average\n",
    "tesla_df = tesla_df.dropna(subset=['SMA20', 'EMA20', 'Volume', 'Close'])\n",
    "\n",
    "# TODO: Define features and target\n",
    "# `features` include SMA20, EMA20, and Volume, `target` includes Close prices\n",
    "features = tesla_df[['SMA20', 'EMA20', 'Volume']].values\n",
    "target = tesla_df['Close'].values\n",
    "\n",
    "# TODO: Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# TODO: Split the dataset into training and testing sets using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# TODO: Verify splits by printing shapes and sample rows of training and testing sets\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"First 5 rows of training features: \\n{X_train[:5]}\")\n",
    "print(f\"First 5 training targets: {y_train[:5]}\\n\")\n",
    "print(f\"First 5 rows of testing features: \\n{X_test[:5]}\")\n",
    "print(f\"First 5 testing targets: {y_test[:5]}\")\n",
    "```\n",
    "\n",
    "### Breakdown of the Steps:\n",
    "1. **Loading the Dataset**:\n",
    "   ```python\n",
    "   data = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "   tesla_df = pd.DataFrame(data['train'])\n",
    "   ```\n",
    "   This loads the dataset and converts it into a DataFrame.\n",
    "\n",
    "2. **Creating New Features**:\n",
    "   ```python\n",
    "   tesla_df['SMA20'] = tesla_df['Close'].rolling(window=20).mean()\n",
    "   tesla_df['EMA20'] = tesla_df['Close'].ewm(span=20, adjust=False).mean()\n",
    "   ```\n",
    "   This creates the `SMA20` and `EMA20` features using rolling and exponential weighted averages.\n",
    "\n",
    "3. **Dropping NaN Values**:\n",
    "   ```python\n",
    "   tesla_df = tesla_df.dropna(subset=['SMA20', 'EMA20', 'Volume', 'Close'])\n",
    "   ```\n",
    "   This ensures that any rows with `NaN` values (caused by the moving averages) are removed.\n",
    "\n",
    "4. **Defining Features and Target**:\n",
    "   ```python\n",
    "   features = tesla_df[['SMA20', 'EMA20', 'Volume']].values\n",
    "   target = tesla_df['Close'].values\n",
    "   ```\n",
    "   This defines the `features` and `target` for model training.\n",
    "\n",
    "5. **Scaling the Features**:\n",
    "   ```python\n",
    "   scaler = StandardScaler()\n",
    "   features_scaled = scaler.fit_transform(features)\n",
    "   ```\n",
    "   The `StandardScaler` is used to normalize the features.\n",
    "\n",
    "6. **Splitting the Dataset**:\n",
    "   ```python\n",
    "   X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
    "   ```\n",
    "   This splits the data into training and testing sets.\n",
    "\n",
    "7. **Verifying the Splits**:\n",
    "   ```python\n",
    "   print(f\"Training features shape: {X_train.shape}\")\n",
    "   print(f\"Testing features shape: {X_test.shape}\")\n",
    "   print(f\"First 5 rows of training features: \\n{X_train[:5]}\")\n",
    "   print(f\"First 5 training targets: {y_train[:5]}\\n\")\n",
    "   print(f\"First 5 rows of testing features: \\n{X_test[:5]}\")\n",
    "   print(f\"First 5 testing targets: {y_test[:5]}\")\n",
    "   ```\n",
    "   This final step prints out the shapes and sample rows of the training and testing datasets to verify everything is in order.\n",
    "\n",
    "### Conclusion:\n",
    "With all these steps combined, you now have a fully preprocessed dataset ready for any further analysis or machine learning tasks. Keep reaching for the stars! 🌟🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
