{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 Feature Importance in Gradient Boosting Models\n",
    "\n",
    "\n",
    "Hello and welcome! Today's lesson focuses on **Feature Importance in Gradient Boosting Models**. We will explore how to determine which features in our dataset are most influential in predicting Tesla ($TSLA) stock prices. By understanding the importance of features, we can refine our models and make more informed trading decisions.\n",
    "\n",
    "## üìù Revision of Previous Steps\n",
    "\n",
    "Before diving into feature importance, let's quickly revise the previous steps to ensure we have a solid foundation.\n",
    "\n",
    "### Data Preparation and Feature Engineering:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load TSLA dataset\n",
    "tesla = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(tesla['train'])\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
    "\n",
    "# Feature Engineering: adding technical indicators as features\n",
    "tesla_df['SMA_5'] = tesla_df['Adj Close'].rolling(window=5).mean()\n",
    "tesla_df['SMA_10'] = tesla_df['Adj Close'].rolling(window=10).mean()\n",
    "tesla_df['EMA_5'] = tesla_df['Adj Close'].ewm(span=5, adjust=False).mean()\n",
    "tesla_df['EMA_10'] = tesla_df['Adj Close'].ewm(span=10, adjust=False).mean()\n",
    "\n",
    "# Drop NaN values created by moving averages\n",
    "tesla_df.dropna(inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "features = tesla_df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_5', 'SMA_10', 'EMA_5', 'EMA_10']].values\n",
    "target = tesla_df['Adj Close'].values\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "### Model Training:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "## üåü Understanding Feature Importance\n",
    "\n",
    "### What is Feature Importance?\n",
    "\n",
    "Feature importance refers to techniques that assign scores to input features based on their importance in predicting the target variable. In the context of a Gradient Boosting model, feature importance indicates how valuable each feature is in constructing the boosted decision trees.\n",
    "\n",
    "### Why is Feature Importance Useful?\n",
    "\n",
    "Understanding feature importance helps:\n",
    "\n",
    "- Identify and select the most influential features, potentially simplifying the model.\n",
    "- Gain insights into the factors driving your predictions.\n",
    "- Improve model interpretability and trustworthiness.\n",
    "\n",
    "### üß† Computing Feature Importance in Gradient Boosting\n",
    "\n",
    "Once the Gradient Boosting model is trained, we can easily access the feature importances. Let's walk through the steps:\n",
    "\n",
    "```python\n",
    "# Compute feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization of feature names alongside their importance\n",
    "feature_names = ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_5', 'SMA_10', 'EMA_5', 'EMA_10']\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances with names\n",
    "print(\"Feature importance:\\n\", feature_importance_df)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```sh\n",
    "Feature importance:\n",
    "   Feature    Importance\n",
    "3   Close  9.447889e-01\n",
    "1    High  3.668675e-02\n",
    "0    Open  9.142875e-03\n",
    "2     Low  8.464037e-03\n",
    "6  SMA_10  4.800413e-04\n",
    "7   EMA_5  2.992652e-04\n",
    "8  EMA_10  1.326235e-04\n",
    "5   SMA_5  5.195267e-06\n",
    "4  Volume  3.363300e-07\n",
    "```\n",
    "\n",
    "Here's what each step is doing:\n",
    "\n",
    "1. `model.feature_importances_`: Extracts the feature importance scores from the trained Gradient Boosting model.\n",
    "2. `feature_names = [...]`: Defines a list of feature names for better readability.\n",
    "3. `feature_importance_df = pd.DataFrame(...)`: Creates a DataFrame that links feature names with their respective importance scores.\n",
    "4. `feature_importance_df.sort_values(...)`: Sorts the DataFrame by feature importance in descending order for better interpretation.\n",
    "\n",
    "## üìä Visualizing Feature Importance\n",
    "\n",
    "Visualizing the importance of features helps interpret the results more effectively. We'll use Matplotlib to create a bar chart:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_importance_df = feature_importance_df.iloc[::-1]\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "The plot of the above code is a bar chart visually indicating the significance of each feature, making it easier to distinguish the most influential features. This visualization is crucial for understanding how different features contribute to the model's predictions.\n",
    "\n",
    "## üßê Interpreting the Results\n",
    "\n",
    "By examining the feature importance values and plot, you can determine which features have the most impact on the model's predictions. For instance, if `Adj Close` heavily relies on `SMA_10` and `Close`, we know they are critical factors in the stock's movement.\n",
    "\n",
    "### üõ†Ô∏è Insights and Next Steps:\n",
    "\n",
    "- **Focus on Key Features**: Emphasize the most important features in further analysis and model tuning.\n",
    "- **Feature Selection**: Consider removing less important features to simplify the model.\n",
    "- **Model Interpretation**: Use feature importance insights to explain model predictions to stakeholders.\n",
    "\n",
    "## üèÅ Lesson Summary\n",
    "\n",
    "In this lesson, you learned about the concept of feature importance in Gradient Boosting models and its practical application to predict Tesla ($TSLA) stock prices. You computed feature importances, visualized them using a bar chart, and interpreted the results to gain actionable insights.\n",
    "\n",
    "Understanding which features influence your model's predictions is crucial for refining your models and making informed trading decisions. Up next, practice these concepts to solidify your understanding and enhance your skillset in machine learning for financial trading.\n",
    "\n",
    "**Great job!**\n",
    "\n",
    "--- \n",
    "\n",
    "This markdown format organizes the content into clear sections, with proper code formatting and an easy-to-follow structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Volume Feature for Better Analysis\n",
    "To exclude the `Volume` feature from both the `features` array and `feature_names` array in your code for feature importance analysis, you can modify the code as follows:\n",
    "\n",
    "```python\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logging level setup\n",
    "import logging\n",
    "logging.getLogger('datasets').setLevel(logging.ERROR)\n",
    "\n",
    "# Load TSLA dataset\n",
    "tesla = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(tesla['train'])\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
    "\n",
    "# Feature Engineering: adding technical indicators as features\n",
    "tesla_df['SMA_5'] = tesla_df['Adj Close'].rolling(window=5).mean()\n",
    "tesla_df['SMA_10'] = tesla_df['Adj Close'].rolling(window=10).mean()\n",
    "tesla_df['EMA_5'] = tesla_df['Adj Close'].ewm(span=5, adjust=False).mean()\n",
    "tesla_df['EMA_10'] = tesla_df['Adj Close'].ewm(span=10, adjust=False).mean()\n",
    "\n",
    "# Drop NaN values created by moving averages\n",
    "tesla_df.dropna(inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "features = tesla_df[['Open', 'High', 'Low', 'Close', 'SMA_5', 'SMA_10', 'EMA_5', 'EMA_10']].values  # Exclude 'Volume'\n",
    "target = tesla_df['Adj Close'].values\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Compute feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization of feature names alongside their importance\n",
    "feature_names = ['Open', 'High', 'Low', 'Close', 'SMA_5', 'SMA_10', 'EMA_5', 'EMA_10']  # Adjusted feature names\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances with names\n",
    "print(\"Feature Importance:\\n\", feature_importance_df)\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(range(len(feature_importance)), feature_names, rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "In this modified code:\n",
    "\n",
    "- The `Volume` feature is removed from the `features` array by excluding it from the selection.\n",
    "- The `feature_names` array is adjusted to exclude `'Volume'`.\n",
    "- The rest of the code remains unchanged, ensuring that the model is trained and feature importance is computed based on the updated feature set. \n",
    "\n",
    "This adjustment allows you to analyze the importance of features other than `Volume` in predicting stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Calculation in Gradient Boosting Models\n",
    "\n",
    "The code is missing a critical step before fitting the model: standardizing the features using the `StandardScaler`. Without this step, the model might not perform optimally, especially if the features have different scales. Here's the corrected version of the code with the missing step included:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load TSLA dataset\n",
    "tesla = load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(tesla['train'])\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
    "\n",
    "# Feature Engineering: adding technical indicators as features\n",
    "tesla_df['SMA_5'] = tesla_df['Adj Close'].rolling(window=5).mean()\n",
    "tesla_df['SMA_10'] = tesla_df['Adj Close'].rolling(window=10).mean()\n",
    "tesla_df['EMA_5'] = tesla_df['Adj Close'].ewm(span=5, adjust=False).mean()\n",
    "tesla_df['EMA_10'] = tesla_df['Adj Close'].ewm(span=10, adjust=False).mean()\n",
    "\n",
    "# Drop NaN values created by moving averages\n",
    "tesla_df.dropna(inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "features = tesla_df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_5', 'SMA_10', 'EMA_5', 'EMA_10']].values\n",
    "target = tesla_df['Adj Close'].values\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# **Missing Step**: Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Compute feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization of feature names alongside their importance\n",
    "feature_names = ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_5', 'SMA_10', 'EMA_5', 'EMA_10']\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances with names\n",
    "print(\"Feature importance:\\n\", feature_importance_df)\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation='vertical')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Summary of the Changes:\n",
    "- **Added Standardization Step:** The missing step of standardizing the features using `StandardScaler` is added before fitting the model. This ensures that all features are on a comparable scale, which is important for gradient boosting models.\n",
    "\n",
    "### Conclusion:\n",
    "With the added standardization, the model should now perform better, as the features are scaled appropriately, reducing bias towards features with larger ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Visualize Feature Importance in Gradient Boosting Model\n",
    "\n",
    "Sure, let's fill in the missing parts of the code to complete the task of calculating and visualizing feature importance in a Gradient Boosting model:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppressing warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pandas')\n",
    "\n",
    "# Load TSLA dataset\n",
    "tesla = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(tesla['train'])\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
    "\n",
    "# Feature Engineering: adding Bollinger Bands as features\n",
    "tesla_df['Rolling_Mean'] = tesla_df['Adj Close'].rolling(window=20).mean()\n",
    "tesla_df['Bollinger_High'] = tesla_df['Rolling_Mean'] + 2 * tesla_df['Adj Close'].rolling(window=20).std()\n",
    "tesla_df['Bollinger_Low'] = tesla_df['Rolling_Mean'] - 2 * tesla_df['Adj Close'].rolling(window=20).std()\n",
    "\n",
    "# Drop NaN values created by rolling calculations\n",
    "tesla_df.dropna(inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "features = tesla_df[['Open', 'High', 'Low', 'Close', 'Bollinger_High', 'Bollinger_Low']].values\n",
    "target = tesla_df['Adj Close'].values\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# TODO: Instantiate and fit the Gradient Boosting model\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Compute feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization of feature names alongside their importance\n",
    "feature_names = ['Open', 'High', 'Low', 'Close', 'Bollinger_High', 'Bollinger_Low']\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# TODO: Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances with names\n",
    "print(\"Feature importance:\\n\", feature_importance_df)\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation of the Filled Parts:\n",
    "1. **Instantiate and fit the Gradient Boosting model:**\n",
    "   ```python\n",
    "   model = GradientBoostingRegressor(random_state=42)\n",
    "   model.fit(X_train, y_train)\n",
    "   ```\n",
    "   - Here, we instantiate the `GradientBoostingRegressor` model with a random state for reproducibility.\n",
    "   - We then fit the model to the training data (`X_train`, `y_train`).\n",
    "\n",
    "2. **Compute feature importance:**\n",
    "   ```python\n",
    "   feature_importance = model.feature_importances_\n",
    "   ```\n",
    "   - After fitting the model, we extract the feature importances using the `feature_importances_` attribute of the trained model.\n",
    "\n",
    "3. **Sort features by importance:**\n",
    "   ```python\n",
    "   feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "   ```\n",
    "   - To better visualize the importance of each feature, we sort the DataFrame `feature_importance_df` by the 'Importance' column in descending order.\n",
    "\n",
    "### Conclusion:\n",
    "This code now calculates and visualizes the importance of features in predicting stock prices using a Gradient Boosting model, providing insights into which features contribute most to the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing and Visualizing Feature Importance\n",
    "\n",
    "Sure, let's fill in the missing parts of the code to complete the task of calculating and visualizing feature importance in a Gradient Boosting model:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppressing warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pandas')\n",
    "\n",
    "# Load TSLA dataset\n",
    "tesla = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(tesla['train'])\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
    "\n",
    "# Feature Engineering: adding Bollinger Bands as features\n",
    "tesla_df['Rolling_Mean'] = tesla_df['Adj Close'].rolling(window=20).mean()\n",
    "tesla_df['Bollinger_High'] = tesla_df['Rolling_Mean'] + 2 * tesla_df['Adj Close'].rolling(window=20).std()\n",
    "tesla_df['Bollinger_Low'] = tesla_df['Rolling_Mean'] - 2 * tesla_df['Adj Close'].rolling(window=20).std()\n",
    "\n",
    "# Drop NaN values created by rolling calculations\n",
    "tesla_df.dropna(inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "features = tesla_df[['Open', 'High', 'Low', 'Close', 'Bollinger_High', 'Bollinger_Low']].values\n",
    "target = tesla_df['Adj Close'].values\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# TODO: Instantiate and fit the Gradient Boosting model\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Compute feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization of feature names alongside their importance\n",
    "feature_names = ['Open', 'High', 'Low', 'Close', 'Bollinger_High', 'Bollinger_Low']\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# TODO: Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances with names\n",
    "print(\"Feature importance:\\n\", feature_importance_df)\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation of the Filled Parts:\n",
    "1. **Instantiate and fit the Gradient Boosting model:**\n",
    "   ```python\n",
    "   model = GradientBoostingRegressor(random_state=42)\n",
    "   model.fit(X_train, y_train)\n",
    "   ```\n",
    "   - Here, we instantiate the `GradientBoostingRegressor` model with a random state for reproducibility.\n",
    "   - We then fit the model to the training data (`X_train`, `y_train`).\n",
    "\n",
    "2. **Compute feature importance:**\n",
    "   ```python\n",
    "   feature_importance = model.feature_importances_\n",
    "   ```\n",
    "   - After fitting the model, we extract the feature importances using the `feature_importances_` attribute of the trained model.\n",
    "\n",
    "3. **Sort features by importance:**\n",
    "   ```python\n",
    "   feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "   ```\n",
    "   - To better visualize the importance of each feature, we sort the DataFrame `feature_importance_df` by the 'Importance' column in descending order.\n",
    "\n",
    "### Conclusion:\n",
    "This code now calculates and visualizes the importance of features in predicting stock prices using a Gradient Boosting model, providing insights into which features contribute most to the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and Visualize Feature Importance in Gradient Boosting Model\n",
    "\n",
    "Let's go through the steps to compute and visualize feature importance using the Tesla dataset and a Gradient Boosting model. I'll fill in the missing parts of the code according to the provided TODOs.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "\n",
    "# Load TSLA dataset\n",
    "tesla = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
    "tesla_df = pd.DataFrame(tesla['train'])\n",
    "\n",
    "# TODO: Convert the Date column to datetime type\n",
    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
    "\n",
    "# TODO: Add new technical indicators to the DataFrame (Momentum, Daily Return, High-Low Difference)\n",
    "# Momentum_5: The change in the adjusted close price over the past 5 days\n",
    "tesla_df['Momentum_5'] = tesla_df['Adj Close'].diff(5)\n",
    "\n",
    "# Daily_Return: The daily percentage change in the adjusted close price\n",
    "tesla_df['Daily_Return'] = tesla_df['Adj Close'].pct_change()\n",
    "\n",
    "# High_Low_Diff: The difference between the highest and lowest prices of the day\n",
    "tesla_df['High_Low_Diff'] = tesla_df['High'] - tesla_df['Low']\n",
    "\n",
    "# TODO: Drop NaN values generated by the indicators\n",
    "tesla_df.dropna(inplace=True)\n",
    "\n",
    "# TODO: Select features and target for model training\n",
    "# The target will be the 'Adj Close' column\n",
    "features = tesla_df[['Open', 'High', 'Low', 'Close', 'Momentum_5', 'Daily_Return', 'High_Low_Diff']].values\n",
    "target = tesla_df['Adj Close'].values\n",
    "\n",
    "# TODO: Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# TODO: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# TODO: Train a Gradient Boosting Regressor model\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Compute and visualize feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization of feature names alongside their importance\n",
    "feature_names = ['Open', 'High', 'Low', 'Close', 'Momentum_5', 'Daily_Return', 'High_Low_Diff']\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances with names\n",
    "print(\"Feature importance:\\n\", feature_importance_df)\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation of the Added Steps:\n",
    "1. **Date Conversion:**\n",
    "   ```python\n",
    "   tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
    "   ```\n",
    "   - Converts the 'Date' column to a `datetime` type for easier manipulation.\n",
    "\n",
    "2. **Technical Indicators:**\n",
    "   - **Momentum_5:** Measures the change in the adjusted close price over the past 5 days.\n",
    "     ```python\n",
    "     tesla_df['Momentum_5'] = tesla_df['Adj Close'].diff(5)\n",
    "     ```\n",
    "   - **Daily_Return:** Calculates the daily percentage change in the adjusted close price.\n",
    "     ```python\n",
    "     tesla_df['Daily_Return'] = tesla_df['Adj Close'].pct_change()\n",
    "     ```\n",
    "   - **High_Low_Diff:** Computes the difference between the highest and lowest prices of the day.\n",
    "     ```python\n",
    "     tesla_df['High_Low_Diff'] = tesla_df['High'] - tesla_df['Low']\n",
    "     ```\n",
    "\n",
    "3. **Dropping NaNs:**\n",
    "   ```python\n",
    "   tesla_df.dropna(inplace=True)\n",
    "   ```\n",
    "   - Drops rows with NaN values generated by the new indicators.\n",
    "\n",
    "4. **Model Training and Feature Importance:**\n",
    "   - After standardizing the features, we train the `GradientBoostingRegressor` model and compute the feature importances.\n",
    "\n",
    "### Conclusion:\n",
    "This code calculates and visualizes the importance of various technical indicators and other features in predicting Tesla stock prices. The feature importances help us understand which features have the most impact on the model's predictions, aiding in better decision-making and feature engineering in future models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
