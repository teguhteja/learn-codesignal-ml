{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Greetings! Welcome to an exciting lesson on feature selection and dimensionality reduction, a foundational element in the realms of machine learning and data science. Today, we will delve into a variance-based approach for feature selection in high-dimensionality data. We will explore the importance of feature selection, understand the concept of variance, and implement feature selection using VarianceThreshold on a synthetic dataset.\n",
    "\n",
    "## Understanding Variance and VarianceThreshold\n",
    "The variance of a feature is a statistical measurement that describes the spread of data points in a data feature. It is one of the key metrics that carries top significance in statistical data analysis.\n",
    "\n",
    "In context of feature selection, if a feature has a low variance (close to zero), it likely carries less information. For instance, consider a dataset of students with a variable 'nationality' where 99% of students come from India, the 'nationality' feature will have very low variance as almost all observations are 'India'; it’s near-constant and therefore would not improve the model's performance.\n",
    "\n",
    "Variance based feature selection should be used in the cases when you suspect that some features are near-constant and may not be informative for the model.\n",
    "\n",
    "Scikit-learn provides the VarianceThreshold method to remove all features which variance doesn’t meet some threshold. By removing these low variance features, we can then decrease the number of input dimensions.\n",
    "\n",
    "## Generating Synthetic Data in Python\n",
    "To demonstrate our feature selection and dimensionality reduction concepts, let's start by generating a synthetic dataset. For many machine learning concepts, especially those related to data preprocessing and manipulation, synthetic datasets can be a useful tool for learning and exploration.\n",
    "\n",
    "First, we'll need to import pandas, numpy and VarianceThreshold from sklearn.feature_selection. We are going to use pandas and numpy to create a DataFrame with ten distinct features, each composed of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "np.random.seed(36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate a DataFrame with ten features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape:  (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(data={\n",
    "    \"feature_1\": np.random.rand(1000),\n",
    "    \"feature_2\": np.random.rand(1000) * 10,\n",
    "    \"feature_3\": np.random.rand(1000),\n",
    "    \"feature_4\": np.random.rand(1000) * 100,\n",
    "    \"feature_5\": np.random.rand(1000),\n",
    "    \"feature_6\": np.random.rand(1000) * 0.1,\n",
    "    \"feature_7\": np.random.rand(1000),\n",
    "    \"feature_8\": np.random.rand(1000) * 0.01,\n",
    "    \"feature_9\": np.random.rand(1000),\n",
    "    \"feature_10\": np.random.rand(1000) * 50,\n",
    "})\n",
    "\n",
    "print(\"Original data shape: \", data.shape) # (1000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above code will be 1000 rows and 10 columns.\n",
    "\n",
    "Here, we assume that all features in our data are numerical and there's consequently no missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying VarianceThreshold on Generated Data\n",
    "After generating the data, let's apply VarianceThreshold and see how it impacts the dimensionality of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced data shape:  (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# We use the VarianceThreshold to perform the feature selection.\n",
    "# We set the threshold to 0.1, meaning that if the variance of a feature is less than 0.1, it will be removed.\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "\n",
    "# Fit the data to the VarianceThreshold object\n",
    "data_values = data.values\n",
    "data_values_reduced = selector.fit_transform(data_values)\n",
    "\n",
    "# Print the shape of the reduced data\n",
    "print(\"Reduced data shape: \", data_values_reduced.shape) # (1000, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above code shows that the shape of the reduced data is (1000, 3) after applying the variance threshold.\n",
    "\n",
    "This indicates that the dimensionality of our dataset has been reduced from 10 features to 3, suggesting that only three features met the variance threshold and therefore were kept.\n",
    "\n",
    "## Identifying Retained Features\n",
    "Now, it would also be beneficial to know which features have been retained after the feature selection process. For this, we can utilize the get_support method of the VarianceThreshold object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept Features:  Index(['feature_2', 'feature_4', 'feature_10'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Get the names of the features that were kept. The get_support method returns a boolean mask of the features selected - True for selected features and False for removed features.\n",
    "kept_features = data.columns[selector.get_support(indices=True)]\n",
    "print(\"Kept Features: \", kept_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above code will be:\n",
    "\n",
    "```JSON\n",
    "Kept Features:  Index(['feature_2', 'feature_4', 'feature_10'], dtype='object')\n",
    "```\n",
    "\n",
    "This shows the names of the features that were kept after applying the variance threshold. It provides insight into which features contain enough variance to possibly improve the performance of a machine learning model.\n",
    "\n",
    "## Lesson Summary and Practice\n",
    "You've now learned how to implement VarianceThreshold for feature selection and dimensionality reduction. We've established the importance of dimensionality reduction, introduced feature selection, walked you through the concept of variance, and performed variance-based feature selection with VarianceThreshold using a synthetic dataset.\n",
    "\n",
    "Remember, to gain a good command over these concepts, practice is key! I would recommend you to experiment with different variance thresholds and observe how it affects the number and selection of features. This will bolster your understanding of implementing feature selection within your own data science and machine learning projects! Happy learning!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unveiling High Variance Features in Synthetic Data\n",
    "\n",
    "Curious about which features in our dataset have enough variance to be considered useful for a machine learning model? The given code employs the VarianceThreshold to filter out features with low variance from synthetic data. Let's embark on this space mission to identify the high-variance features that might significantly contribute to the predictive power of our models. Are you ready to see the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape:  (1000, 10)\n",
      "Reduced data shape:  (1000, 3)\n",
      "Kept Features:  Index(['feature_2', 'feature_4', 'feature_10'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Setting a fixed seed for reproducibility\n",
    "np.random.seed(36)\n",
    "\n",
    "# Generating synthetic data\n",
    "data = pd.DataFrame({\n",
    "    \"feature_1\": np.random.rand(1000),\n",
    "    \"feature_2\": np.random.rand(1000) * 10,\n",
    "    \"feature_3\": np.random.rand(1000),\n",
    "    \"feature_4\": np.random.rand(1000) * 100,\n",
    "    \"feature_5\": np.random.rand(1000),\n",
    "    \"feature_6\": np.random.rand(1000) * 0.1,\n",
    "    \"feature_7\": np.random.rand(1000),\n",
    "    \"feature_8\": np.random.rand(1000) * 0.01,\n",
    "    \"feature_9\": np.random.rand(1000),\n",
    "    \"feature_10\": np.random.rand(1000) * 50,\n",
    "})\n",
    "\n",
    "# Apply VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "data_values = data.values\n",
    "data_values_reduced = selector.fit_transform(data_values)\n",
    "\n",
    "# Identifying retained features\n",
    "kept_features = data.columns[selector.get_support(indices=True)]\n",
    "\n",
    "# Displaying shapes of original and reduced data\n",
    "print(\"Original data shape: \", data.shape)\n",
    "print(\"Reduced data shape: \", data_values_reduced.shape)\n",
    "\n",
    "# Displaying the names of the kept features\n",
    "print(\"Kept Features: \", kept_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the Variance Threshold\n",
    "\n",
    "Ready to tweak some code, Space Wanderer? Based on what you learned about reducing input dimensions, adjust the threshold in the VarianceThreshold method and observe the outcome. Will more or fewer features make the cut? Try changing the threshold from 0.15 to 0.5 in the starter code and watch the magic unfold!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape:  (1000, 5)\n",
      "Reduced data shape:  (1000, 3)\n",
      "Kept Features:  Index(['feature_1', 'feature_2', 'feature_5'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Initial data setup\n",
    "np.random.seed(10)\n",
    "\n",
    "# Generate synthetic data with modified feature values\n",
    "data = pd.DataFrame({\n",
    "    \"feature_1\": np.random.normal(0, 1, 1000),\n",
    "    \"feature_2\": np.random.normal(5, 2, 1000),\n",
    "    \"feature_3\": 0.5 * np.ones(1000),  # This feature has variance 0\n",
    "    \"feature_4\": np.random.binomial(1, 0.2, 1000), # Binary feature will have low variance if p is close to 0 or 1\n",
    "    \"feature_5\": np.random.normal(10, 5, 1000),\n",
    "})\n",
    "\n",
    "# Apply VarianceThreshold method to filter features\n",
    "selector = VarianceThreshold(threshold=0.50)  # The change should be made on this line\n",
    "filtered_data = selector.fit_transform(data)\n",
    "\n",
    "# Output the results\n",
    "print(\"Original data shape: \", data.shape) # Should print (1000, 5)\n",
    "print(\"Reduced data shape: \", filtered_data.shape)  # Output will vary based on threshold\n",
    "print(\"Kept Features: \", data.columns[selector.get_support(indices=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Variance Threshold\n",
    "Stellar work, Space Voyager! Now, to weld the armor of knowledge even tighter, there's a piece of code you need to add. Set the threshold for the variance in our machine learning feature selection technique and print the kept features list at the end. Stand ready with your decision!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape:  (1000, 10)\n",
      "Reduced data shape:  (1000, 7)\n",
      "Kept features:  ['feature_1', 'feature_2', 'feature_4', 'feature_5', 'feature_7', 'feature_8', 'feature_9']\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "np.random.seed(36)  # Ensures that the random numbers are the same each time we run.\n",
    "\n",
    "# Generating synthetic data with ten features\n",
    "data = pd.DataFrame({\n",
    "    \"feature_1\": np.random.normal(0, 1, 1000),  \n",
    "    \"feature_2\": np.random.normal(0, 2, 1000),  \n",
    "    \"feature_3\": np.full(1000, 1),              \n",
    "    \"feature_4\": np.random.normal(0, 4, 1000),  \n",
    "    \"feature_5\": np.random.normal(0, 1, 1000),  \n",
    "    \"feature_6\": np.full(1000, 0),              \n",
    "    \"feature_7\": np.random.normal(0, 1, 1000),  \n",
    "    \"feature_8\": np.random.normal(0, 1, 1000),  \n",
    "    \"feature_9\": np.random.normal(0, 2, 1000),  \n",
    "    \"feature_10\": np.full(1000, 3)              \n",
    "})\n",
    "\n",
    "print(\"Original data shape: \", data.shape)\n",
    "\n",
    "# Initialize the VarianceThreshold with threshold 0.1\n",
    "threshold = 0.1\n",
    "selector = VarianceThreshold(threshold=threshold)\n",
    "\n",
    "# Fit the selector and transform the data\n",
    "reduced_data = selector.fit_transform(data)\n",
    "print(\"Reduced data shape: \", reduced_data.shape)\n",
    "\n",
    "# Identify and print the names of the features that have been kept after variance thresholding\n",
    "kept_features = data.columns[selector.get_support()]\n",
    "print(\"Kept features: \", list(kept_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosmic Code Crafting: Feature Selection with Variance Threshold\n",
    "\n",
    "Ready for liftoff on your solo mission, Space Voyager? Here's your chance to prove your prowess with dimensionality reduction. Your task is to recreate the code to apply VarianceThreshold, and find out which features are left. Remember, the final destination should resemble the solution we previously crafted together. Have a stellar journey through the code cosmos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "np.random.seed(36)  # Set the seed for reproducibility\n",
    "\n",
    "# Generating synthetic data\n",
    "data = pd.DataFrame({\n",
    "    \"feature_1\": np.random.rand(1000),\n",
    "    \"feature_2\": np.random.rand(1000) * 10,\n",
    "    \"feature_3\": np.random.rand(1000),\n",
    "    \"feature_4\": np.random.rand(1000) * 100,\n",
    "    \"feature_5\": np.random.rand(1000),\n",
    "    \"feature_6\": np.random.rand(1000) * 0.1,\n",
    "    \"feature_7\": np.random.rand(1000),\n",
    "    \"feature_8\": np.random.rand(1000) * 0.01,\n",
    "    \"feature_9\": np.random.rand(1000),\n",
    "    \"feature_10\": np.random.rand(1000) * 50,\n",
    "})\n",
    "\n",
    "print(\"Original data shape: \", data.shape)\n",
    "\n",
    "# Select features using VarianceThreshold with a certain threshold\n",
    "threshold = 0.1\n",
    "selector = VarianceThreshold(threshold=threshold)\n",
    "\n",
    "# Transform the data using the fit_transform method of VarianceThreshold\n",
    "reduced_data = selector.fit_transform(data)\n",
    "print(\"Reduced data shape: \", reduced_data.shape)\n",
    "\n",
    "# Determine which features were kept after variance threshold\n",
    "kept_features = data.columns[selector.get_support()]\n",
    "print(\"Kept features: \", list(kept_features))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
