{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome! Today, we unfold the mysteries of fine-tuning Autoencoders. We learned about Autoencoders and their value in dimensionality reduction. Now, we'll delve into Hyperparameters — adjustable pre-training variables that optimize model performance. We'll experiment with different architectures (altering layers and activations) and training parameters (tweaking learning rates and batch sizes) of an Autoencoder using Python. Ready for the exploration voyage? Off we go!\n",
    "\n",
    "## Hyperparameters: Tuning Essentials\n",
    "Hyperparameters, serving as a model's adjustable knobs, influence how a machine learning model learns. Classified into architectural and learning types, they're vital for managing a model's complexity. Architectural hyperparameters encompass elements like hidden layers and units in a neural network. In contrast, learning hyperparameters include the learning rate, epochs, and batch sizes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with New Architectures\n",
    "Architectural Hyperparameters define layers and units in a network. Layers are computational constructs that transform input data, and units produce activations. Now, let's modify our Autoencoder and experiment with different activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:54:02.412751: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-27 08:54:02.412979: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-27 08:54:02.415853: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-27 08:54:02.452387: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-27 08:54:03.264147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Define the function to create an autoencoder\n",
    "def create_autoencoder(input_dim, encoded_dim, learning_rate):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    # Add a dense layer with 64 neurons and 'relu' activation\n",
    "    encoded = Dense(encoded_dim, activation='relu')(input_layer)\n",
    "    # Add another dense layer\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(Adam(learning_rate), loss='mean_squared_error')\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing Learning Hyperparameters\n",
    "Learning Hyperparameters, such as learning rate and batch size, significantly impact training. Let's measure their influence by tweaking them in our Autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0903 - val_loss: 0.0826\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0822 - val_loss: 0.0808\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0803 - val_loss: 0.0801\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0800 - val_loss: 0.0797\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0792 - val_loss: 0.0799\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0796 - val_loss: 0.0797\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0789 - val_loss: 0.0794\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0787 - val_loss: 0.0793\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0783 - val_loss: 0.0794\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0788 - val_loss: 0.0795\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0784 - val_loss: 0.0794\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0788 - val_loss: 0.0794\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0785 - val_loss: 0.0794\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0787 - val_loss: 0.0794\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0777 - val_loss: 0.0794\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0784 - val_loss: 0.0793\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0785 - val_loss: 0.0793\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0782 - val_loss: 0.0794\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0781 - val_loss: 0.0794\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0788 - val_loss: 0.0793\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0783 - val_loss: 0.0793\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0784 - val_loss: 0.0794\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0784 - val_loss: 0.0796\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0790 - val_loss: 0.0794\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0785 - val_loss: 0.0793\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0786 - val_loss: 0.0795\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0780 - val_loss: 0.0795\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0781 - val_loss: 0.0793\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0787 - val_loss: 0.0792\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0786 - val_loss: 0.0794\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0782 - val_loss: 0.0792\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0780 - val_loss: 0.0792\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0784 - val_loss: 0.0794\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0778 - val_loss: 0.0792\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0782 - val_loss: 0.0792\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0781 - val_loss: 0.0794\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0782 - val_loss: 0.0793\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0778 - val_loss: 0.0792\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0782 - val_loss: 0.0794\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0785 - val_loss: 0.0793\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0787 - val_loss: 0.0793\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0780 - val_loss: 0.0794\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0785 - val_loss: 0.0794\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0788 - val_loss: 0.0794\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0782 - val_loss: 0.0793\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0783 - val_loss: 0.0792\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0784 - val_loss: 0.0792\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0785 - val_loss: 0.0793\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0787 - val_loss: 0.0792\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0779 - val_loss: 0.0792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x75e730559bb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate training and testing data (randomly generated for the example)\n",
    "x_train = np.random.random((1000, 20))\n",
    "x_test = np.random.random((300, 20))\n",
    "\n",
    "# Training with a higher learning_rate\n",
    "learning_rate = 0.1\n",
    "autoencoder = create_autoencoder(20, 16, learning_rate)\n",
    "autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a slower learning rate with the same architecture and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0893 - val_loss: 0.0830\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0800\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0794 - val_loss: 0.0769\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0760 - val_loss: 0.0739\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0727 - val_loss: 0.0698\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0691 - val_loss: 0.0657\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0652 - val_loss: 0.0618\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0607 - val_loss: 0.0583\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0572 - val_loss: 0.0549\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0548 - val_loss: 0.0521\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0511 - val_loss: 0.0493\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0485 - val_loss: 0.0469\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0461 - val_loss: 0.0446\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0435 - val_loss: 0.0425\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0416 - val_loss: 0.0407\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0396 - val_loss: 0.0391\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0376 - val_loss: 0.0377\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0364 - val_loss: 0.0364\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0352 - val_loss: 0.0355\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0341 - val_loss: 0.0347\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0330 - val_loss: 0.0339\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0326 - val_loss: 0.0334\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0317 - val_loss: 0.0330\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0314 - val_loss: 0.0326\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0307 - val_loss: 0.0324\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0310 - val_loss: 0.0322\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0308 - val_loss: 0.0320\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0304 - val_loss: 0.0320\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0297 - val_loss: 0.0318\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0299 - val_loss: 0.0316\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0296 - val_loss: 0.0317\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0295 - val_loss: 0.0316\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0294 - val_loss: 0.0316\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0292 - val_loss: 0.0314\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0290 - val_loss: 0.0315\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0294 - val_loss: 0.0316\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0295 - val_loss: 0.0313\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0290 - val_loss: 0.0314\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0290 - val_loss: 0.0312\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0287 - val_loss: 0.0313\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0293 - val_loss: 0.0312\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0289 - val_loss: 0.0313\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0291 - val_loss: 0.0311\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0284 - val_loss: 0.0312\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0286 - val_loss: 0.0311\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0286 - val_loss: 0.0310\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0289 - val_loss: 0.0312\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0286 - val_loss: 0.0309\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0279 - val_loss: 0.0309\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0282 - val_loss: 0.0309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x75e72f109100>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training with a slower learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "autoencoder_slow = create_autoencoder(20, 16, learning_rate)\n",
    "autoencoder_slow.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Hyperparameters Impact\n",
    "Let's compare the model performances through reconstruction errors. Lower errors indicate better performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Reconstruction error (fast learning rate): 0.07921227763631411\n",
      "Reconstruction error (slow learning rate): 0.030933214013002582\n"
     ]
    }
   ],
   "source": [
    "decoded_imgs_fast = autoencoder.predict(x_test)\n",
    "decoded_imgs_slow = autoencoder_slow.predict(x_test)\n",
    "\n",
    "reconstruction_error_fast = np.mean((x_test - decoded_imgs_fast) ** 2)\n",
    "reconstruction_error_slow = np.mean((x_test - decoded_imgs_slow) ** 2)\n",
    "\n",
    "print(f\"Reconstruction error (fast learning rate): {reconstruction_error_fast}\") # ~0.07\n",
    "print(f\"Reconstruction error (slow learning rate): {reconstruction_error_slow}\") # ~0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code demonstrate the influence of learning rate and serve as a guided path to delve deeper into Hyperparameters tuning. We can compare the reconstruction error of the two models to see the impact of the learning rate on the performance of the autoencoder. The model trained with a slower learning rate has a lower reconstruction error, indicating better performance – note, that the ouptut values may vary due to randomness and library versions used.\n",
    "\n",
    "## Conclusion and Exercise\n",
    "Great job! Today, we explored fine-tuning Autoencoders through adjusting Hyperparameters. Your next step? Hands-on experimentation! Vary the settings and observe how they affect performance. Up next: a voyage into Loss Functions and Optimizers for Autoencoders. Keep exploring!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing Autoencoder Performance with Different Learning Rates\n",
    "\n",
    "Want to see an autoencoder in action? The given code defines an autoencoder with various learning rates and trains it on simulated data. Run the code to observe how the learning rates you've learned about in the lesson impact the model's performance. Watch the magic unfold without writing a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.1393 - val_loss: 0.0885\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0936 - val_loss: 0.0877\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0859 - val_loss: 0.0861\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0848 - val_loss: 0.0847\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0842 - val_loss: 0.0848\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0839 - val_loss: 0.0847\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0835 - val_loss: 0.0847\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0835 - val_loss: 0.0842\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0834 - val_loss: 0.0840\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0833 - val_loss: 0.0842\n",
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0845 - val_loss: 0.0830\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0819 - val_loss: 0.0813\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0797 - val_loss: 0.0787\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0766 - val_loss: 0.0757\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0731 - val_loss: 0.0723\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0700 - val_loss: 0.0700\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0678 - val_loss: 0.0680\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0655 - val_loss: 0.0655\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0630 - val_loss: 0.0627\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0602 - val_loss: 0.0604\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "0.08420919381286106\n",
      "0.06036949974059451\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the function to create an autoencoder\n",
    "def create_autoencoder(input_dim, encoded_dim, learning_rate):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(input_dim, activation='relu')(input_layer)\n",
    "    encoded = Dense(encoded_dim, activation='relu')(encoded)\n",
    "    decoded = Dense(input_dim, activation='relu')(encoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate), loss='mean_squared_error')\n",
    "    return autoencoder\n",
    "\n",
    "# Simulate training and testing data\n",
    "x_train = np.random.random((1000, 20))\n",
    "x_test = np.random.random((300, 20))\n",
    "\n",
    "# Training with a higher learning rate\n",
    "learning_rate_high = 0.3\n",
    "autoencoder_high = create_autoencoder(20, 16, learning_rate_high)\n",
    "autoencoder_high.fit(x_train, x_train, epochs=10, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
    "\n",
    "# Training with a slower learning rate\n",
    "learning_rate_slow = 0.01\n",
    "autoencoder_slow = create_autoencoder(20, 16, learning_rate_slow)\n",
    "autoencoder_slow.fit(x_train, x_train, epochs=10, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
    "\n",
    "# Compare the model performances\n",
    "decoded_imgs_high = autoencoder_high.predict(x_test)\n",
    "decoded_imgs_slow = autoencoder_slow.predict(x_test)\n",
    "reconstruction_error_high = np.mean(np.square(x_test - decoded_imgs_high))\n",
    "reconstruction_error_slow = np.mean(np.square(x_test - decoded_imgs_slow))\n",
    "print(reconstruction_error_high)\n",
    "print(reconstruction_error_slow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Activation Function Exploration\n",
    "\n",
    "Ready for a tweak in our cosmic code, Space Voyager? Let's fine-tune our Autoencoder by switching up the activation signals! Your mission is to change the activation function of the encoded layer from 'relu' to 'tanh', a variant you've learned in previous lessons. Observe the influence of this small but significant change on the model's learning journey.\n",
    "\n",
    "Dock into the code bay and start the recalibration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0948 - val_loss: 0.0836\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0837 - val_loss: 0.0840\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0830 - val_loss: 0.0843\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0831 - val_loss: 0.0838\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0828 - val_loss: 0.0840\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0829 - val_loss: 0.0838\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0827 - val_loss: 0.0835\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0826 - val_loss: 0.0835\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0826 - val_loss: 0.0835\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - val_loss: 0.0836\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0836\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0825 - val_loss: 0.0835\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0857 - val_loss: 0.0815\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0800 - val_loss: 0.0788\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0768 - val_loss: 0.0749\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0724 - val_loss: 0.0706\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0676 - val_loss: 0.0659\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0630 - val_loss: 0.0616\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0588 - val_loss: 0.0580\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0548 - val_loss: 0.0547\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0511 - val_loss: 0.0515\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0479 - val_loss: 0.0492\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0452 - val_loss: 0.0471\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0430 - val_loss: 0.0449\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0408 - val_loss: 0.0429\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0387 - val_loss: 0.0409\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0368 - val_loss: 0.0395\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0353 - val_loss: 0.0384\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0342 - val_loss: 0.0376\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0333 - val_loss: 0.0371\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0327 - val_loss: 0.0367\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0322 - val_loss: 0.0363\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0318 - val_loss: 0.0360\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0313 - val_loss: 0.0357\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0309 - val_loss: 0.0353\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0305 - val_loss: 0.0350\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0301 - val_loss: 0.0347\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0297 - val_loss: 0.0344\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0293 - val_loss: 0.0340\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0289 - val_loss: 0.0336\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0284 - val_loss: 0.0332\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0280 - val_loss: 0.0328\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0276 - val_loss: 0.0326\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0273 - val_loss: 0.0325\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0271 - val_loss: 0.0325\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0271 - val_loss: 0.0325\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0270 - val_loss: 0.0325\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0269 - val_loss: 0.0324\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0268 - val_loss: 0.0324\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0267 - val_loss: 0.0323\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0266 - val_loss: 0.0324\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0265 - val_loss: 0.0323\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0264 - val_loss: 0.0323\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0264 - val_loss: 0.0322\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0263 - val_loss: 0.0322\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0262 - val_loss: 0.0321\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0261 - val_loss: 0.0321\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0260 - val_loss: 0.0321\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0259 - val_loss: 0.0321\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0259 - val_loss: 0.0320\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0258 - val_loss: 0.0320\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0258 - val_loss: 0.0320\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0907 - val_loss: 0.0885\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0874 - val_loss: 0.0862\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0853 - val_loss: 0.0846\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0839 - val_loss: 0.0837\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0830 - val_loss: 0.0830\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0824 - val_loss: 0.0826\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0819 - val_loss: 0.0822\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0815 - val_loss: 0.0819\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0811 - val_loss: 0.0816\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0807 - val_loss: 0.0813\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0803 - val_loss: 0.0810\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0800 - val_loss: 0.0806\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0795 - val_loss: 0.0802\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0791 - val_loss: 0.0797\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0786 - val_loss: 0.0793\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0781 - val_loss: 0.0788\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0776 - val_loss: 0.0783\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0771 - val_loss: 0.0778\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0765 - val_loss: 0.0772\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0759 - val_loss: 0.0767\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0753 - val_loss: 0.0760\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0746 - val_loss: 0.0754\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0739 - val_loss: 0.0747\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0732 - val_loss: 0.0741\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0725 - val_loss: 0.0734\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0717 - val_loss: 0.0727\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0709 - val_loss: 0.0720\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0702 - val_loss: 0.0712\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0694 - val_loss: 0.0705\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0686 - val_loss: 0.0698\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0678 - val_loss: 0.0691\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0671 - val_loss: 0.0685\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0663 - val_loss: 0.0678\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0656 - val_loss: 0.0671\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0649 - val_loss: 0.0664\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0642 - val_loss: 0.0658\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0635 - val_loss: 0.0651\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0628 - val_loss: 0.0645\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0621 - val_loss: 0.0638\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0614 - val_loss: 0.0632\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0607 - val_loss: 0.0626\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0600 - val_loss: 0.0619\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0594 - val_loss: 0.0613\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0587 - val_loss: 0.0606\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0580 - val_loss: 0.0600\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0573 - val_loss: 0.0593\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0566 - val_loss: 0.0587\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0559 - val_loss: 0.0581\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0553 - val_loss: 0.0575\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0546 - val_loss: 0.0569\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0877 - val_loss: 0.0875\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0874 - val_loss: 0.0872\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0872 - val_loss: 0.0870\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0869 - val_loss: 0.0868\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0867 - val_loss: 0.0866\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0865 - val_loss: 0.0864\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0863 - val_loss: 0.0862\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0861 - val_loss: 0.0861\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0859 - val_loss: 0.0859\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0857 - val_loss: 0.0857\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0855 - val_loss: 0.0856\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0853 - val_loss: 0.0854\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0852 - val_loss: 0.0853\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0850 - val_loss: 0.0852\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0848 - val_loss: 0.0850\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0847 - val_loss: 0.0849\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0846 - val_loss: 0.0848\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0844 - val_loss: 0.0847\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0843 - val_loss: 0.0846\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0842 - val_loss: 0.0845\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0840 - val_loss: 0.0844\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0839 - val_loss: 0.0843\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0838 - val_loss: 0.0842\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0837 - val_loss: 0.0841\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0836 - val_loss: 0.0840\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0835 - val_loss: 0.0839\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0834 - val_loss: 0.0838\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0833 - val_loss: 0.0837\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0832 - val_loss: 0.0836\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0831 - val_loss: 0.0835\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0830 - val_loss: 0.0834\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0829 - val_loss: 0.0833\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0828 - val_loss: 0.0833\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0827 - val_loss: 0.0832\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0826 - val_loss: 0.0831\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0825 - val_loss: 0.0830\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0824 - val_loss: 0.0830\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0823 - val_loss: 0.0829\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0823 - val_loss: 0.0828\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0822 - val_loss: 0.0827\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0821 - val_loss: 0.0827\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0820 - val_loss: 0.0826\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0819 - val_loss: 0.0825\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0819 - val_loss: 0.0825\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0818 - val_loss: 0.0824\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0817 - val_loss: 0.0823\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0816 - val_loss: 0.0822\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0815 - val_loss: 0.0822\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0815 - val_loss: 0.0821\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0814 - val_loss: 0.0820\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppresses all TensorFlow logs, shows only errors\n",
    "\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Define the function to create an autoencoder\n",
    "def create_autoencoder(input_dim, encoded_dim, learning_rate):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    # Change the activation function from 'relu' to 'tanh'\n",
    "    encoded = Dense(encoded_dim, activation='tanh')(input_layer)\n",
    "    # Add another dense layer\n",
    "    decoded = Dense(input_dim, activation='relu')(encoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(Adam(learning_rate), loss='mean_squared_error')\n",
    "    return autoencoder\n",
    "\n",
    "# Simulate training and testing data (randomly generated for the example)\n",
    "x_train = np.random.random((1000, 32))\n",
    "x_test = np.random.random((300, 32))\n",
    "\n",
    "# Initialize list to store reconstruction errors\n",
    "reconstruction_errors = []\n",
    "\n",
    "# Train and test autoencoders with different learning rates\n",
    "for lr in [0.1, 0.01, 0.001, 0.0001]:\n",
    "    autoencoder = create_autoencoder(32, 20, lr)\n",
    "    autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
    "    decoded_imgs = autoencoder.predict(x_test)\n",
    "    reconstruction_error = np.mean((x_test - decoded_imgs) ** 2)\n",
    "    reconstruction_errors.append((lr, reconstruction_error))\n",
    "\n",
    "# Plot reconstruction error vs learning rates\n",
    "plt.plot([lr for lr, _ in reconstruction_errors], [error for _, error in reconstruction_errors], marker='o')\n",
    "plt.title('Reconstruction Error vs Learning Rate')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Autoencoder with Optimal Learning Rate\n",
    "\n",
    "Now, let's deepen our exploration into the effect of learning rates on autoencoder performance. Your task is to create two autoencoders with different learning rates and compare their reconstruction errors. This experiment will highlight how significant the choice of learning rate can be for the model's ability to learn effectively. Craft each autoencoder, train them on the same dataset, and let the reconstruction errors reveal the impact of your chosen learning rates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Reconstruction error with high learning rate (0.01): 0.044750296121086754\n",
      "Reconstruction error with low learning rate (0.001): 0.03732680474659947\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# Define the function to create an autoencoder\n",
    "def create_autoencoder(input_dim, encoded_dim, learning_rate):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(encoded_dim, activation='relu')(input_layer)\n",
    "    encoded = Dense(encoded_dim, activation='relu')(encoded)\n",
    "    decoded = Dense(input_dim, activation='relu')(encoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate), loss='mean_squared_error')\n",
    "    return autoencoder\n",
    "\n",
    "# Simulate training and testing data with input dimension 32\n",
    "x_train = np.random.random((1000, 32))\n",
    "x_test = np.random.random((300, 32))\n",
    "\n",
    "# Initialize two different learning rates for the experiment\n",
    "learning_rate_high = 0.01\n",
    "learning_rate_low = 0.001\n",
    "\n",
    "# Create the first autoencoder with a higher learning rate, train it, and calculate its reconstruction error with encoded dimension set to 24\n",
    "autoencoder_high_lr = create_autoencoder(input_dim=32, encoded_dim=24, learning_rate=learning_rate_high)\n",
    "autoencoder_high_lr.fit(x_train, x_train, epochs=50, batch_size=32, shuffle=True, verbose=0)\n",
    "reconstructed_high_lr = autoencoder_high_lr.predict(x_test)\n",
    "reconstruction_error_high_lr = np.mean(np.square(x_test - reconstructed_high_lr))\n",
    "\n",
    "# Create the second autoencoder with a lower learning rate, train it, and calculate its reconstruction error with the same setup as the previous one\n",
    "autoencoder_low_lr = create_autoencoder(input_dim=32, encoded_dim=24, learning_rate=learning_rate_low)\n",
    "autoencoder_low_lr.fit(x_train, x_train, epochs=50, batch_size=32, shuffle=True, verbose=0)\n",
    "reconstructed_low_lr = autoencoder_low_lr.predict(x_test)\n",
    "reconstruction_error_low_lr = np.mean(np.square(x_test - reconstructed_low_lr))\n",
    "\n",
    "# Compare and print the reconstruction errors to see the effect of learning rates on model performance\n",
    "print(f\"Reconstruction error with high learning rate ({learning_rate_high}): {reconstruction_error_high_lr}\")\n",
    "print(f\"Reconstruction error with low learning rate ({learning_rate_low}): {reconstruction_error_low_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
