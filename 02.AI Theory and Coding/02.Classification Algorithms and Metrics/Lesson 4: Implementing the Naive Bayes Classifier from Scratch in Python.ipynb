{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 4: Implementing the Naive Bayes Classifier from Scratch in Python\n",
                                    "\n",
                                    "## Introduction\n",
                                    "Welcome to our exploration tour of the Naive Bayes Classifier! This robust classification algorithm is renowned for its simplicity and effectiveness. We will implement it from scratch in Python, allowing you to leverage its sheer power without the need for any prebuilt libraries. Let's get started!\n",
                                    "\n",
                                    "## Recall\n",
                                    "Let's do a quick recall of probability theory.\n",
                                    "\n",
                                    "- **P(A)** usually denotes the likelihood of a certain event A occurring.\n",
                                    "- **P(A|B)**, on the other hand, indicates the probability of event A taking place, assuming event B has already happened.\n",
                                    "\n",
                                    "For instance, let's imagine there's a bag housing three marbles - one red and two blue. Denote A as the event where a red marble is picked, and B when a blue one is drawn. The probability of A, **P(A)**, is 1/3 in this case.\n",
                                    "\n",
                                    "Now, let's consider a scenario where a blue marble has already been drawn from the bag. This leaves us with one red and one blue marble in the bag. The probability of drawing a red marble (event A), given that a blue marble has already been extracted (event B), is denoted by **P(A|B)**. In this case, **P(A|B)** would be 1/2, highlighting a higher likelihood of drawing a red marble following the initial removal of a blue one.\n",
                                    "\n",
                                    "## The Principle of Naive Bayes\n",
                                    "The Naive Bayes algorithms rely on Bayes' theorem. Let's recall it quickly. This theorem calculates the probability of an event based on prior knowledge of potentially related events. It is represented mathematically as:\n",
                                    "\n",
                                    "\\[\n",
                                    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
                                    "\\]\n",
                                    "\n",
                                    "Where:\n",
                                    "\n",
                                    "- **P(A|B)** is the posterior probability of class **A** given predictor **B**. It's what we are trying to calculate.\n",
                                    "- **P(B|A)** is the likelihood, which is the probability of the predictor given a class.\n",
                                    "- **P(B)** is the marginal probability of the predictor.\n",
                                    "- **P(A)** is the prior probability of the class.\n",
                                    "\n",
                                    "This formula forms the backbone of the Naive Bayes classifier.\n",
                                    "\n",
                                    "The term 'naive' refers to the assumption that all variables in a dataset are independent of each other, which may not always be the case in real-life data. Nonetheless, it still offers robust performance and can be easily implemented.\n",
                                    "\n",
                                    "## Deriving the Naive Bayes Classifier Algorithm\n",
                                    "In the context of machine learning, the Naive Bayes Classifier uses the Bayes theorem to compute the posterior probability of a class given a set of features and then classifies the outcome based on the highest posterior probability.\n",
                                    "\n",
                                    "Assuming a binary class variable **Y** (binary means it can be equal to either 0 or 1) and features **X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>n</sub>**, our task is to compute the posterior probability **P(Y=1|X<sub>1</sub>=x<sub>1</sub>, X<sub>2</sub>=x<sub>2</sub>, ..., X<sub>n</sub>=x<sub>n</sub>)**. By shedding the denominator from Bayes' theorem (since it doesn't depend on **Y** and is constant for all classes), we are left with the task of maximizing the probability of **Y** and **X** happening together **P(Y,X) = P(X|Y) \\cdot P(Y)**, which forms the basis for Naive Bayes classification.\n",
                                    "\n",
                                    "## Implementing Naive Bayes Classifier\n",
                                    "We approach the implementation of the Naive Bayes Classifier by first calculating the prior probabilities of each class, and then the likelihood of each feature given a class:\n",
                                    "\n",
                                    "```python\n",
                                    "def calculate_prior_probabilities(y):\n",
                                    "    return y.value_counts(normalize=True)  # calculates the proportion of each class in the data\n",
                                    "\n",
                                    "def calculate_likelihoods(X, y):\n",
                                    "    likelihoods = {}\n",
                                    "    for class_ in y.unique():\n",
                                    "        for column in X.columns:\n",
                                    "            likelihoods[column + \"|\" + str(class_)] = X[y == class_][column].value_counts(normalize=True)\n",
                                    "    return likelihoods  # returns a dict with likelihood of each class given a feature\n",
                                    "```\n",
                                    "\n",
                                    "Armed with these utility functions, we can implement the Naive Bayes Classifier function:\n",
                                    "\n",
                                    "```python\n",
                                    "from collections import defaultdict\n",
                                    "\n",
                                    "def naive_bayes_classifier(X_test, priors, likelihoods):\n",
                                    "    class_probabilities = defaultdict(float)\n",
                                    "\n",
                                    "    for index, data_point in X_test.iterrows():\n",
                                    "        for class_ in priors.index:\n",
                                    "            class_likelihood = 1\n",
                                    "            for feature in X_test.columns:\n",
                                    "                class_likelihood *= likelihoods[feature + \"|\" + str(class_)].get(data_point[feature], 0)\n",
                                    "\n",
                                    "            class_probabilities[class_] += priors[class_] * class_likelihood\n",
                                    "\n",
                                    "    return max(class_probabilities, key=class_probabilities.get)\n",
                                    "```\n",
                                    "\n",
                                    "## Understanding and Handling Data Issues in Naive Bayes\n",
                                    "A recurring challenge in Naive Bayes is the handling of zero probabilities, i.e., when a category does not appear in the training data for a given class, resulting in a zero probability for that category. A known fix for this problem is applying Laplace or Add-1 smoothing, which adds a '1' to each category count to circumvent zero probabilities.\n",
                                    "\n",
                                    "You can integrate Laplace smoothing into the `calculate_likelihoods` function as follows:\n",
                                    "\n",
                                    "```python\n",
                                    "def calculate_likelihoods_with_smoothing(X, y):\n",
                                    "    likelihoods = {}\n",
                                    "    for class_ in y.unique():\n",
                                    "        for column in X.columns:\n",
                                    "            likelihoods[column + \"|\" + str(class_)] = (X[y == class_][column].value_counts() + 1) / (X[y == class_][column].count() + len(X[column].unique()))\n",
                                    "    return likelihoods  # returns a dict with likelihood of each class given a feature\n",
                                    "```\n",
                                    "\n",
                                    "The numerator is increased by 1 and the denominator by the count of unique categories to accommodate the added 1's.\n",
                                    "\n",
                                    "## Using Naive Bayes Classifier\n",
                                    "Here is a short example of predicting weather with our classifier:\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "data = {\n",
                                    "    'Temperature': ['Hot', 'Hot', 'Cold', 'Hot', 'Cold', 'Cold', 'Hot'],\n",
                                    "    'Humidity': ['High', 'High', 'Normal', 'Normal', 'High', 'Normal', 'Normal'],\n",
                                    "    'Weather': ['Sunny', 'Sunny', 'Snowy', 'Rainy', 'Snowy', 'Snowy', 'Sunny']\n",
                                    "}\n",
                                    "df = pd.DataFrame(data)\n",
                                    "\n",
                                    "# Split features and labels\n",
                                    "X = df[['Temperature', 'Humidity']]\n",
                                    "y = df['Weather']\n",
                                    "\n",
                                    "# Calculate prior probabilities\n",
                                    "priors = calculate_prior_probabilities(y)\n",
                                    "\n",
                                    "# Calculate likelihoods with smoothing\n",
                                    "likelihoods = calculate_likelihoods_with_smoothing(X, y)\n",
                                    "\n",
                                    "# New observation\n",
                                    "X_test = pd.DataFrame([{'Temperature': 'Hot', 'Humidity': 'Normal'}])\n",
                                    "\n",
                                    "# Make prediction\n",
                                    "prediction = naive_bayes_classifier(X_test, priors, likelihoods)\n",
                                    "print(\"Predicted Weather: \", prediction)  # Output: Predicted Weather:  Sunny\n",
                                    "```\n",
                                    "\n",
                                    "The Naive Bayes Classifier predicts a class label based on the observed features. Owing to its simplicity, power, and speed, this classifier lends itself to challenging scenarios, including text classification, spam detection, and sentiment analysis.\n",
                                    "\n",
                                    "## Lesson Summary and Practice\n",
                                    "Superb work! You've mastered the essentials of the Naive Bayes Classifier, from understanding its theory to crafting a Naive Bayes Classifier from scratch. The next phase is practice, which will consolidate your newly acquired skills. Enjoy the hands-on exercises lined up next. Delve deeper into your machine learning journey with the forthcoming lessons!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Predicting the Weather with Naive Bayes"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Forecast Predictor: Calculating Prior Probabilities"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Calculating Normalized Values in Weather Data Analysis"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Laplace Smoothing in the Naive Bayes Classifier"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Predict the Play Day with Naive Bayes"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
