{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 3: Implementing k-Nearest Neighbors Algorithm in Python\n",
                                    "\n",
                                    "\n",
                                    "## Introduction\n",
                                    "Welcome to our exploration of the k-Nearest Neighbors (k-NN) algorithm! This essential machine learning classifier is widely appreciated for its simplicity and effectiveness. This lesson will equip you with a clear understanding of the k-NN algorithm and its elements, including the concept and selection of 'k', as well as distance calculation using the Euclidean metric. We'll proceed to implement a k-NN classifier in Python. Intriguing, isn't it? Let's delve into k-NN!\n",
                                    "\n",
                                    "## k-Nearest Neighbors (k-NN) Algorithm\n",
                                    "The k-NN algorithm classifies data based on a data point's 'k' nearest neighbors from the training dataset. Consider a fruit classification scenario: if a new data point, or fruit, emerges and 'k' is set to 3, the new fruit is classified based on the majority within its three nearest neighbors. Essentially, k-NN takes advantage of the simplicity of voting to make decisions—the class that receives the most votes wins!\n",
                                    "\n",
                                    "Let's see this in action. Consider this dataset, where we have three fruits of Class 0 and three fruits of Class 1. We also have a query point, which is a fruit we aim to assign a class label to.\n",
                                    "\n",
                                    "The k-NN algorithm works on a basic principle: a data point is likely to be in the same category as the data points it is closest to. So, the model will identify the 'k' points nearest to our query point, and these 'k' points will vote on what Class the query should belong to. The class label with the most votes will be assigned to the query point. In this case, the query point will be assigned the Class 0 label.\n",
                                    "\n",
                                    "Note that choosing 'k' significantly impacts our model. A low 'k' might capture more noise in the data, whereas a high 'k' is computationally expensive. Therefore, running tests to identify the optimal 'k' is crucial.\n",
                                    "\n",
                                    "## Distance Metrics: Implementing Euclidean Distance in Python\n",
                                    "In k-NN, classification is determined by weighing the distance between data points. Euclidean distance is a frequently used metric that calculates the shortest straight-line distance \\((x_1 − x_2)^2 + (y_1 − y_2)^2\\) between two points \\((x_1, y_1)\\) and \\((x_2, y_2)\\) in a Euclidean space. This formula, rooted in the Pythagorean theorem, will be implemented next in Python:\n",
                                    "\n",
                                    "```python\n",
                                    "import math\n",
                                    "\n",
                                    "# The 'euclidean_distance' function computes the Euclidean distance between two points\n",
                                    "def euclidean_distance(point1, point2):\n",
                                    "    squares = [(p - q) ** 2 for p, q in zip(point1, point2)] # Calculate squared distance for each dimension\n",
                                    "    return math.sqrt(sum(squares)) # Return the square root of the sum of squares\n",
                                    "\n",
                                    "# Test it\n",
                                    "point1 = (1, 2) # The coordinates of the first point\n",
                                    "point2 = (4, 6) # The coordinates of the second point\n",
                                    "print(euclidean_distance(point1, point2)) # 5.0\n",
                                    "```\n",
                                    "\n",
                                    "This code calculates and outputs the Euclidean distance between `point1` and `point2`.\n",
                                    "\n",
                                    "## Implementing k-NN Classification\n",
                                    "Next, we will construct our k-NN algorithm. It must compute the distance between the test point and all data points, select the 'k' closest points, and designate the class based on the majority vote.\n",
                                    "\n",
                                    "```python\n",
                                    "from collections import Counter\n",
                                    "\n",
                                    "def k_nearest_neighbors(data, query, k, distance_fn):\n",
                                    "    neighbor_distances_and_indices = []\n",
                                    "    \n",
                                    "    # Compute distance from each training data point\n",
                                    "    for idx, label in enumerate(data):\n",
                                    "        distance = distance_fn(label[:-1], query)\n",
                                    "        neighbor_distances_and_indices.append((distance, idx))\n",
                                    "    \n",
                                    "    # Sort array by distance\n",
                                    "    sorted_neighbor_distances_and_indices = sorted(neighbor_distances_and_indices)\n",
                                    "    \n",
                                    "    # Select k closest data points\n",
                                    "    k_nearest_distances_and_indices = sorted_neighbor_distances_and_indices[:k]\n",
                                    "    \n",
                                    "    # Obtain class labels for those k data points\n",
                                    "    k_nearest_labels = [data[i][1] for distance, i in k_nearest_distances_and_indices]\n",
                                    "\n",
                                    "    # Majority vote\n",
                                    "    most_common = Counter(k_nearest_labels).most_common(1)\n",
                                    "    return most_common[0][0] # Return the label of the class that receives the majority vote\n",
                                    "```\n",
                                    "\n",
                                    "The input training data, query point, 'k', and a distance function are taken in this function, and the assigned class label is returned.\n",
                                    "\n",
                                    "Note that we can pass different distance functions in the algorithm. The most common Euclidean distance is used for points in continuous dimensions (like height), but in some cases, we might want to use different distance functions. For example, the Manhattan distance is used for non-comparable or non-continuous dimensions (like categories).\n",
                                    "\n",
                                    "## Using k-NN\n",
                                    "Here is how we can assign a class to a test data point using our algorithm:\n",
                                    "\n",
                                    "```python\n",
                                    "# Define the dataset (training set)\n",
                                    "# Each element of the dataset is a tuple (features, label)\n",
                                    "data = [\n",
                                    "    ((2, 3), 0),\n",
                                    "    ((5, 4), 0),\n",
                                    "    ((9, 6), 1),\n",
                                    "    ((4, 7), 0),\n",
                                    "    ((8, 1), 1),\n",
                                    "    ((7, 2), 1)\n",
                                    "]\n",
                                    "query = (5, 3)  # test point\n",
                                    "\n",
                                    "# Perform the classification\n",
                                    "predicted_label = k_nearest_neighbors(data, query, k=3, distance_fn=euclidean_distance)\n",
                                    "print(predicted_label)  # Expected class label is 0\n",
                                    "```\n",
                                    "\n",
                                    "## Lesson Summary and Practice\n",
                                    "You've successfully navigated the learning curve of the k-NN algorithm, fully grasping its work mechanism, distance functions, and Python implementation! Up next, practice exercises will solidify your grasp of these newly acquired concepts. Keep going and enjoy delving deeper into your Python learning journey!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adjusting k in k-Nearest Neighbors Classifier"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Manhattan Distance in k-NN Classifier"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Fruit Ripeness Classification Conundrum"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Classifying Fruit Ripeness with k-NN"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Navigating the Cosmos: Implementing k-NN Majority Vote"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
