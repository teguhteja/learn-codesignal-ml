{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 3: Backpropagation Unveiled: Understanding the Mathematics and Code Behind Neural Network Learning\n",
                                    "\n",
                                    "# Backpropagation Unveiled: Understanding the Mathematics and Code Behind Neural Network Learning\n",
                                    "\n",
                                    "## Introduction\n",
                                    "Hello! In this lesson, we'll thoroughly examine the inner workings of the crucial **backpropagation algorithm** in training neural networks and create it from scratch in Python.\n",
                                    "\n",
                                    "## The Structure of a Neural Network\n",
                                    "A neural network consists of an **input layer**, one or more **hidden layers**, and an **output layer**. Each layer houses neurons, or nodes interconnected through links attributed with **weights**. These weights and **bias** terms dictate the network's output. In our Python code, the size of the input layer adjusts according to the shape of `self.input`. The hidden layer hosts four neurons (`self.weights1`), and the output layer accommodates one neuron (`self.weights2`).\n",
                                    "\n",
                                    "## Understanding the Sigmoid Function\n",
                                    "Our activation function, the **sigmoid function**, transforms real-value numbers into a range between 0 and 1. Let's recall its mathematical definition:\n",
                                    "\n",
                                    "\\[\n",
                                    "\\text{sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n",
                                    "\\]\n",
                                    "\n",
                                    "The derivative of the sigmoid function plays an essential role in backpropagation for weight updates. It is represented as:\n",
                                    "\n",
                                    "\\[\n",
                                    "\\text{sigmoid\\_derivative}(x) = x * (1 - x)\n",
                                    "\\]\n",
                                    "\n",
                                    "These functions are implemented in Python as `sigmoid(x)` and `sigmoid_derivative(x)`.\n",
                                    "\n",
                                    "```python\n",
                                    "def sigmoid(x):\n",
                                    "    return 1.0 / (1 + np.exp(-x))\n",
                                    "\n",
                                    "def sigmoid_derivative(x):\n",
                                    "    return x * (1.0 - x)\n",
                                    "```\n",
                                    "\n",
                                    "## Defining a Neural Network\n",
                                    "The following methods will be defined in a class initialized like this:\n",
                                    "\n",
                                    "```python\n",
                                    "class NeuralNetwork:\n",
                                    "    def __init__(self, x, y, learning_rate=0.1):\n",
                                    "        self.input = x\n",
                                    "        self.weights1 = np.random.rand(self.input.shape[1], 4)\n",
                                    "        self.weights2 = np.random.rand(4, 1)\n",
                                    "        self.y = y\n",
                                    "        self.output = np.zeros(self.y.shape)\n",
                                    "        self.learning_rate = learning_rate\n",
                                    "```\n",
                                    "\n",
                                    "- `self.weights1` and `self.weights2` refer to the weights of connections from the input layer to the first hidden layer and from the first hidden layer to the output layer, respectively.\n",
                                    "- `self.y` stores the target data.\n",
                                    "- `self.output` is a Numpy array filled with zeroes to hold the neural network's output.\n",
                                    "\n",
                                    "## Feedforward Propagation\n",
                                    "Feedforward propagation involves data moving from the input layer to the output layer, passing through the hidden layers. The inputs and corresponding weights are multiplied, and the resultant values are processed through the activation function (the sigmoid function, in this case).\n",
                                    "\n",
                                    "```python\n",
                                    "def feedforward(self):\n",
                                    "    self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
                                    "    self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
                                    "```\n",
                                    "\n",
                                    "## The Essence of Backpropagation\n",
                                    "Backpropagation is crucial to the learning process of neural networks. It corrects the network's error by propagating the error from the output layer back to the input layer, adjusting the weights to minimize the discrepancy between predicted (`self.output`) and actual outputs (`self.y`). This process is mathematically presented as:\n",
                                    "\n",
                                    "\\[\n",
                                    "\\Delta w_{ij} = \\eta \\cdot e_j \\cdot x_i\n",
                                    "\\]\n",
                                    "\n",
                                    "Where:\n",
                                    "\n",
                                    "- \\(\\Delta w_{ij}\\) denotes the magnitude of weight adjustment,\n",
                                    "- \\(\\eta\\) represents the learning rate, dictating the pace at which our model learns,\n",
                                    "- \\(e_j\\) designates the error term for output unit \\(j\\), representing the difference between the predicted and actual output,\n",
                                    "- \\(x_i\\) identifies the input associated with the weight.\n",
                                    "\n",
                                    "In the `backprop` function, the error indicates the need for weight adjustments.\n",
                                    "\n",
                                    "## Implementing Backpropagation\n",
                                    "In the `backprop` function, the derivatives of weights (`d_weights2` and `d_weights1`) are computed using the error and the derivatives of the neuron outputs. Afterward, the weights are updated based on these derivatives.\n",
                                    "\n",
                                    "```python\n",
                                    "def backprop(self):\n",
                                    "    d_weights2 = np.dot(self.layer1.T, (2 * (self.y - self.output) * sigmoid_derivative(self.output)))\n",
                                    "    d_weights1 = np.dot(self.input.T, (np.dot(2 * (self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
                                    "\n",
                                    "    self.weights1 += self.learning_rate * d_weights1\n",
                                    "    self.weights2 += self.learning_rate * d_weights2\n",
                                    "```\n",
                                    "\n",
                                    "## Epochs in Neural Network Training\n",
                                    "An **epoch** signifies a complete pass through the entire training dataset. The `train` function applies backpropagation repeatedly over several epochs to adjust weights and minimize error. Multiple epochs provide the model with numerous opportunities to learn and correct its errors until it finds the optimal weights for predictions.\n",
                                    "\n",
                                    "```python\n",
                                    "def train(self, epochs):\n",
                                    "    for epoch in range(epochs):\n",
                                    "        self.feedforward()\n",
                                    "        self.backprop()\n",
                                    "```\n",
                                    "\n",
                                    "Now we can define the `predict` method.\n",
                                    "\n",
                                    "```python\n",
                                    "def predict(self, new_input):\n",
                                    "    layer1 = sigmoid(np.dot(new_input, self.weights1))\n",
                                    "    output = sigmoid(np.dot(layer1, self.weights2))\n",
                                    "    return output\n",
                                    "```\n",
                                    "\n",
                                    "The `predict` function computes outputs for given inputs by propagating inputs through layers using dot product operations and the sigmoid function as the activation function.\n",
                                    "\n",
                                    "## End-to-End Example: XOR Problem\n",
                                    "Let's implement these concepts for the **XOR (exclusive OR) problem**. In this problem, accurate results depend on the parity of the number of true inputs. We initialize our neural network with inputs `X`, corresponding outputs `Y`, and train it over 10,000 epochs. The weights adjust accordingly, enabling the correct prediction of the XOR problem.\n",
                                    "\n",
                                    "```python\n",
                                    "X = np.array([[0, 0, 1],\n",
                                    "              [0, 1, 1],\n",
                                    "              [1, 0, 1],\n",
                                    "              [1, 1, 1]])\n",
                                    "Y = np.array([[0], [1], [1], [0]])\n",
                                    "nn = NeuralNetwork(X, Y)\n",
                                    "\n",
                                    "nn.train(10000)\n",
                                    "print(\"\\nPredictions:\")\n",
                                    "for i, x in enumerate(X):\n",
                                    "    print(f\"Input: {x} ---> Prediction: {nn.predict(np.array([x]))}, Expected: {Y[i]}\")\n",
                                    "```\n",
                                    "\n",
                                    "## Lesson Summary and Practice\n",
                                    "Congratulations! You've dissected the fundamental **backpropagation algorithm**, comprehended the mathematics underpinning it, and manifested it from scratch in Python. Implement, learn, and observe the transformations they bring about in your neural network output. Keep exploring and enjoy your voyage through deep learning!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Training a Neural Network to Solve XOR"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implement Prediction in the Neural Network"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implement Weight Update in Neural Network"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
