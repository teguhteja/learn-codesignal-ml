{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 2: Understanding and Implementing Neural Network Activation Functions\n",
                                    "\n",
                                    "# Understanding and Implementing Neural Network Activation Functions\n",
                                    "\n",
                                    "## Introduction\n",
                                    "Welcome to our riveting exploration of **\"Understanding Activation Functions\"**. We'll traverse the realm of activation functions â€” crucial components in neural networks that guide the network's output. Embark with us as we delve deep into the theory and Python implementations of five specific activation functions:\n",
                                    "\n",
                                    "- Step function\n",
                                    "- Sigmoid function\n",
                                    "- Rectified Linear Unit (ReLU)\n",
                                    "- Hyperbolic tangent (tanh)\n",
                                    "- Softplus function\n",
                                    "\n",
                                    "Let's embark on this enlightening journey through the realm of neural networks.\n",
                                    "\n",
                                    "## Theoretical Understanding of Activation Functions\n",
                                    "Activation functions play a vital role in determining a neuron's output. Picturing them as computational gates can be helpful: these gates output results if the input crosses a threshold; otherwise, they remain silent. As we embark on our journey, we'll explore five types of activation functions.\n",
                                    "\n",
                                    "## Step Function Implementation\n",
                                    "At the start of our expedition, let's explore the **step function**, also known as the threshold function. This basic activation function works like a switch: if the input value is above or equal to a threshold value, the function returns `1`; otherwise, it returns `0`.\n",
                                    "\n",
                                    "Implementing this in Python is straightforward:\n",
                                    "\n",
                                    "```python\n",
                                    "def step_function(x):\n",
                                    "    return 1 if x >= 0 else 0\n",
                                    "```\n",
                                    "\n",
                                    "### Step Function Visualization\n",
                                    "To see this in practice, let's generate a descriptive visualization:\n",
                                    "\n",
                                    "```python\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "import numpy as np\n",
                                    "\n",
                                    "x = np.linspace(-10, 10, 100)\n",
                                    "y = [step_function(i) for i in x]\n",
                                    "plt.plot(x, y)\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "## Sigmoid Function Implementation\n",
                                    "The **sigmoid function** maps any value to a result between `0` and `1`, generating an S-shaped curve. It is particularly useful in binary classification problems.\n",
                                    "\n",
                                    "Here's its implementation in Python:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "\n",
                                    "def sigmoid_function(x):\n",
                                    "    return 1 / (1 + np.exp(-x))\n",
                                    "```\n",
                                    "\n",
                                    "### Sigmoid Function Visualization\n",
                                    "To study this function's behavior, let's sketch a plot:\n",
                                    "\n",
                                    "```python\n",
                                    "y = [sigmoid_function(i) for i in x]\n",
                                    "plt.plot(x, y)\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "## ReLU Function Implementation\n",
                                    "**ReLU** (Rectified Linear Unit) is designed to return the input value if it's positive; otherwise, it returns `0`. It helps resolve the vanishing gradient problem often seen in deep neural networks.\n",
                                    "\n",
                                    "ReLU function in Python:\n",
                                    "\n",
                                    "```python\n",
                                    "def relu_function(x):\n",
                                    "    return x if x > 0 else 0\n",
                                    "```\n",
                                    "\n",
                                    "### ReLU Function Visualization\n",
                                    "Visualizing the ReLU function:\n",
                                    "\n",
                                    "```python\n",
                                    "y = [relu_function(i) for i in x]\n",
                                    "plt.plot(x, y)\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "## Tanh Function Implementation\n",
                                    "The **tanh function** is an S-shaped curve like the sigmoid function, but it maps input values to a range between `-1` and `1`, making it suitable for mapping both positive and negative values.\n",
                                    "\n",
                                    "Defining this function in Python:\n",
                                    "\n",
                                    "```python\n",
                                    "def tanh_function(x):\n",
                                    "    return (2 / (1 + np.exp(-2*x))) - 1\n",
                                    "```\n",
                                    "\n",
                                    "### Tanh Function Visualization\n",
                                    "Illustrating this function's behavior through a plot:\n",
                                    "\n",
                                    "```python\n",
                                    "y = [tanh_function(i) for i in x]\n",
                                    "plt.plot(x, y)\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "## Softplus Function Implementation\n",
                                    "The **softplus function** is a smooth approximation to the ReLU function, addressing the dying ReLU problem, as it is differentiable at zero.\n",
                                    "\n",
                                    "Expressing it in Python:\n",
                                    "\n",
                                    "```python\n",
                                    "def softplus(x):\n",
                                    "    return np.log(1 + np.exp(x))\n",
                                    "```\n",
                                    "\n",
                                    "### Softplus Function Visualization\n",
                                    "Visualizing the softplus function:\n",
                                    "\n",
                                    "```python\n",
                                    "y = [softplus(i) for i in x]\n",
                                    "plt.plot(x, y)\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "## Lesson Summary and Practice\n",
                                    "We've successfully navigated through the theory and application of engaging activation functions. Thanks to the practical examples and vivid visualizations, we've comprehensively understood these essential functions that lay the foundation for neural networks.\n",
                                    "\n",
                                    "Are you excited for some hands-on practice with these functions? We hope you are. Let's delve into the practice!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Step Function in Action"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Comparing Function: Sigmoid and Tanh"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Sigmoid Activation Function Implementation"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
