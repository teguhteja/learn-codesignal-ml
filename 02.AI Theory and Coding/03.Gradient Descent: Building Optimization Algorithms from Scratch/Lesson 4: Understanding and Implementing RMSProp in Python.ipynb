{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 4: Understanding and Implementing RMSProp in Python\n",
                                    "\n",
                                    "Here's the content formatted in Markdown:\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "# Understanding and Implementing RMSProp in Python\n",
                                    "\n",
                                    "## Introduction to RMSProp\n",
                                    "Hello! Today, we will dive into **RMSProp** (Root Mean Square Propagation). This sophisticated optimization algorithm accelerates convergence by adapting the learning rate for each weight separately, addressing the limitations of previous techniques such as Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent, and momentum. Our focus today is understanding RMSProp and coding it from scratch in Python to optimize multivariable functions.\n",
                                    "\n",
                                    "## Recap on Gradient Descent Techniques\n",
                                    "Let's begin with a quick recap: SGD and Mini-Batch Gradient Descent can be sensitive to learning rates and may converge slowly. Even momentum, which mitigates these issues to an extent, has limitations. When a uniform learning rate is applied across all parameters, efficient optimization might not be achieved. This is where RMSProp steps in to offer a solution.\n",
                                    "\n",
                                    "## Understanding RMSProp\n",
                                    "RMSProp, an advanced optimization algorithm, adjusts the gradient descent step for each weight individually, accelerating training and allowing faster convergence. This optimization is achieved by RMSProp keeping track of a running average of the square of gradients and then using this information to scale the learning rate.\n",
                                    "\n",
                                    "### RMSProp Mathematically\n",
                                    "For RMSProp, we add another layer to the update rule of SGD. This additional layer scales each update with the inverse of the square root of the sum of squares of recent gradients. Here, gradients measure the quantity and direction of change for the weights. The mathematical expression is:\n",
                                    "\n",
                                    "$$\n",
                                    "s_{dw} = \\rho \\cdot s_{dw} + (1 - \\rho) \\cdot dw^2\n",
                                    "$$\n",
                                    "\n",
                                    "$$\n",
                                    "w = w - \\frac{\\alpha \\cdot dw}{\\sqrt{s_{dw}} + \\epsilon}\n",
                                    "$$\n",
                                    "\n",
                                    "Where:\n",
                                    "\n",
                                    "- **\\( w \\)** is the parameter vector,\n",
                                    "- **\\( dw \\)** is the gradient of the cost function with regards to the parameters at the current parameter value,\n",
                                    "- **\\( \\alpha \\)** is the learning rate,\n",
                                    "- **\\( s_{dw} \\)** is the running average of the square of the gradients (initialized to 0), and\n",
                                    "- **\\( \\rho \\)** is the momentum parameter (a new hyperparameter, generally set to 0.9).\n",
                                    "\n",
                                    "A higher **\\( \\rho \\)** will result in a faster convergence. The small additive constant **\\( \\epsilon \\)** ensures numerical stability by avoiding division by zero.\n",
                                    "\n",
                                    "## RMSProp in Python Code\n",
                                    "Let's now encapsulate the RMSProp concept into Python code. We will define an RMSProp function, which takes the learning rate, decay factor \\( \\rho \\), a small number \\( \\epsilon \\), gradient, and prior squared gradient (initialized to 0) as inputs and returns the updated parameters and updated squared gradients.\n",
                                    "\n",
                                    "```python\n",
                                    "def RMSProp(learning_rate, rho, epsilon, grad, s_prev):\n",
                                    "    # Update squared gradient\n",
                                    "    s = rho * s_prev + (1 - rho) * np.power(grad, 2)\n",
                                    "\n",
                                    "    # Calculate updates\n",
                                    "    updates = learning_rate * grad / (np.sqrt(s) + epsilon)\n",
                                    "    return updates, s\n",
                                    "```\n",
                                    "\n",
                                    "## Application of RMSProp on Multivariable Function Optimization\n",
                                    "Now let's apply RMSProp to find the minimum of a multivariable function \\( f(x, y) = x^2 + y^2 \\). Corresponding gradients are \\( \\frac{df}{dx} = 2x \\) and \\( \\frac{df}{dy} = 2y \\). We've set the initial starting point to \\( (x, y) = (5, 4) \\), and picked common choices for hyperparameters (\\( \\rho = 0.9 \\), \\( \\epsilon = 1e-6 \\), and \\( \\text{learning\\_rate} = 0.1 \\)), running our optimizer over 100 epochs.\n",
                                    "\n",
                                    "```python\n",
                                    "def f(x, y):\n",
                                    "    return x**2 + y**2\n",
                                    "\n",
                                    "def df(x, y):\n",
                                    "    return np.array([2*x, 2*y])\n",
                                    "\n",
                                    "coordinates = np.array([5.0, 4.0])\n",
                                    "learning_rate = 0.1\n",
                                    "rho = 0.9\n",
                                    "epsilon = 1e-6\n",
                                    "max_epochs = 100\n",
                                    "\n",
                                    "s_prev = np.array([0, 0])\n",
                                    "\n",
                                    "for epoch in range(max_epochs + 1):\n",
                                    "    grad = df(coordinates[0], coordinates[1])\n",
                                    "    updates, s_prev = RMSProp(learning_rate, rho, epsilon, grad, s_prev)\n",
                                    "    coordinates -= updates\n",
                                    "    if epoch % 20 == 0:\n",
                                    "        print(f\"Epoch {epoch}, current state: {coordinates}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Output:\n",
                                    "```sh\n",
                                    "Epoch 0, current state: [4.68377233 3.68377236]\n",
                                    "Epoch 20, current state: [2.3688824  1.47561697]\n",
                                    "Epoch 40, current state: [0.95903672 0.35004133]\n",
                                    "Epoch 60, current state: [0.13761293 0.00745214]\n",
                                    "Epoch 80, current state: [3.91649374e-04 3.12725069e-09]\n",
                                    "Epoch 100, current state: [-3.07701828e-17  2.18862195e-20]\n",
                                    "```\n",
                                    "As you can see, \\( x \\) and \\( y \\) quickly approach 0, which is indeed the minimum of the given function.\n",
                                    "\n",
                                    "## Evaluation of RMSProp Over Other Gradient Descent Techniques\n",
                                    "Lastly, we can compare the performance of RMSProp with SGD, Mini-Batch Gradient Descent, or Momentum-based Gradient Descent by examining how efficiently each one arrives at the global minimum of a cost function. For a two-variable function like in the example, RMSProp may not show significant differences, but it is known for its high efficiency in handling complex and large-scale machine learning tasks.\n",
                                    "\n",
                                    "It reduces the oscillations and high variance in parameter updates by introducing the moving average into the gradient, often leading to quicker convergence and improved stability in the learning process. This makes it particularly useful for handling complex models and large datasets in deep learning applications.\n",
                                    "\n",
                                    "## Conclusion\n",
                                    "Well done! Now, you comprehend RMSProp and can code it in Python. As an advanced optimization technique, RMSProp allows for faster convergence, making it a robust tool in your machine learning toolbox.\n",
                                    "\n",
                                    "Next, we will have hands-on exercises for you to practice and reinforce these new concepts. Remember, practice strengthens learning and expands understanding. Happy coding!\n",
                                    "\n",
                                    "---"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## RMSProp Assisted Space Navigation"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Scaling the Optimizer: Adjusting RMSProp with Gamma"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adjust the Decay Rate in RMSProp Algorithm"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implement RMSProp Update"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implement RMSProp's Squared Gradient Update"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
