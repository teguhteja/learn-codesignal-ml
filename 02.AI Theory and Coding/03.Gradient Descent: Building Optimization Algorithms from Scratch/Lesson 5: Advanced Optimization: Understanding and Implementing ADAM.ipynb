{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 5: Advanced Optimization: Understanding and Implementing ADAM\n",
                                    "\n",
                                    "\n",
                                    "# Advanced Optimization: Understanding and Implementing ADAM\n",
                                    "\n",
                                    "## Introduction to ADAM\n",
                                    "Hello! Today, we will explore the ADAM (Adaptive Moment Estimation) algorithm. This advanced optimization algorithm is a favorite among machine learning practitioners as it combines the advantages of two other extensions of Stochastic Gradient Descent (SGD): Root Mean Square Propagation (RMSprop) and Adaptive Gradient Algorithm (AdaGrad). Our primary focus today is understanding ADAM, and we will also build it from scratch in Python to optimize multivariable functions.\n",
                                    "\n",
                                    "## Understanding ADAM\n",
                                    "Before we dive into ADAM, let us recall that classic gradient descent methods like SGD and even sophisticated versions like Momentum and RMSProp have some limitations. These limitations relate to sensitivity to learning rates, the issue of vanishing gradients, and the absence of individual adaptive learning rates for different parameters.\n",
                                    "\n",
                                    "ADAM, a promising choice for an optimization algorithm, combines the merits of RMSProp and AdaGrad. It maintains a per-parameter learning rate adapted based on the average of recent magnitudes of the gradients for the weights (similar to RMSProp) and the average of recent gradients (like Momentum). This mechanism enables the algorithm to traverse quickly over the low gradient regions and slow down near the optimal points.\n",
                                    "\n",
                                    "## ADAM Mathematically\n",
                                    "For ADAM, we modify the update rule of SGD, introducing two additional hyperparameters, beta1 and beta2. The hyperparameter beta1 controls the exponential decay rate for the first-moment estimates (similar to Momentum), while beta2 controls the exponential decay rate for the second-moment estimates (similar to RMSProp). The mathematical expression can be formulated as follows:\n",
                                    "\n",
                                    "\\[\n",
                                    "m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot \\text{grad}\n",
                                    "\\]\n",
                                    "\n",
                                    "\\[\n",
                                    "v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot \\text{grad}^2\n",
                                    "\\]\n",
                                    "\n",
                                    "\\[\n",
                                    "w = w - \\alpha \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n",
                                    "\\]\n",
                                    "\n",
                                    "Here, \\( m_t \\) and \\( v_t \\) are estimates of the gradients' first moment (the mean) and the second moment (the uncentered variance), respectively, while \"grad\" represents the gradient. We also use an epsilon constant to maintain numerical stability and prevent division by zero, as in RMSProp.\n",
                                    "\n",
                                    "## ADAM in Python Code\n",
                                    "Let's now consolidate the ADAM concept into Python code. We will define an ADAM function, which takes the gradients, the decay rates beta1 and beta2, a numerical constant epsilon, the learning rate, and previous estimates of \\( m \\) and \\( v \\) (initialized to 0) as input and returns the updated parameters, along with the updated \\( m \\) and \\( v \\).\n",
                                    "\n",
                                    "```python\n",
                                    "def ADAM(beta1, beta2, epsilon, grad, m_prev, v_prev, learning_rate):\n",
                                    "    # Update biased first-moment estimate\n",
                                    "    m = beta1 * m_prev + (1 - beta1) * grad\n",
                                    "\n",
                                    "    # Update biased second raw moment estimate\n",
                                    "    v = beta2 * v_prev + (1 - beta2) * np.power(grad, 2)\n",
                                    "\n",
                                    "    # Calculate updates\n",
                                    "    updates = learning_rate * m / (np.sqrt(v) + epsilon)\n",
                                    "    return updates, m, v\n",
                                    "```\n",
                                    "\n",
                                    "\\( v \\) and \\( m \\) are initialized with zeros and therefore they are biased towards zero at the start of the optimization, especially when the decay rates are small (beta1 and beta2 close to 1).\n",
                                    "\n",
                                    "To counteract these biases, Adam also usually includes the correction terms \\( \\hat{m} \\) and \\( \\hat{v} \\). These terms adjust \\( m \\) and \\( v \\) by an amount that lessens as the number of time steps increases:\n",
                                    "\n",
                                    "```python\n",
                                    "m_hat = m / (1 - np.power(beta1, epoch+1))  # Correcting the bias for the first moment\n",
                                    "v_hat = v / (1 - np.power(beta2, epoch+1))  # Correcting the bias for the second moment\n",
                                    "\n",
                                    "updates = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
                                    "return updates, m, v\n",
                                    "```\n",
                                    "\n",
                                    "Note that we still return plain \\( m \\) and \\( v \\).\n",
                                    "\n",
                                    "## Application of ADAM on Multivariable Function Optimization\n",
                                    "Now, let's test ADAM slightly by finding the minimum of a multivariable function \\( f(x, y) = x^2 + y^2 \\). The corresponding gradients are \\( \\frac{\\partial f}{\\partial x} = 2x \\) and \\( \\frac{\\partial f}{\\partial y} = 2y \\). With an initial starting point at \\((x, y) = (3, 4)\\), selected reasonable values for \\( \\beta_1=0.9 \\), \\( \\beta_2=0.9999 \\), \\( \\epsilon=1e-8 \\), learning rate \\( \\alpha=0.02 \\), and an epoch size of 150, we can start minimizing our function.\n",
                                    "\n",
                                    "```python\n",
                                    "def f(x, y):\n",
                                    "    return x ** 2 + y ** 2\n",
                                    "\n",
                                    "def df(x, y):\n",
                                    "    return np.array([2 * x, 2 * y])\n",
                                    "\n",
                                    "coordinates = np.array([3.0, 4.0])\n",
                                    "learning_rate = 0.02\n",
                                    "beta1 = 0.9\n",
                                    "beta2 = 0.9999\n",
                                    "epsilon = 1e-8\n",
                                    "max_epochs = 150\n",
                                    "\n",
                                    "m_prev = np.array([0, 0])\n",
                                    "v_prev = np.array([0, 0])\n",
                                    "\n",
                                    "for epoch in range(max_epochs + 1):\n",
                                    "    grad = df(coordinates[0], coordinates[1])\n",
                                    "    updates, m_prev, v_prev = ADAM(beta1, beta2, epsilon, grad, m_prev, v_prev, learning_rate)\n",
                                    "    coordinates -= updates\n",
                                    "    if epoch % 30 == 0:\n",
                                    "        print(f\"Epoch {epoch}, current state: {coordinates}\")\n",
                                    "```\n",
                                    "\n",
                                    "The output of this code is the following:\n",
                                    "\n",
                                    "```plaintext\n",
                                    "Epoch 0, current state: [2.80000003 3.80000002]\n",
                                    "Epoch 30, current state: [ 0.27175946 -0.35494334]\n",
                                    "Epoch 60, current state: [-0.07373187 -0.06706317]\n",
                                    "Epoch 90, current state: [-0.02001478  0.0301726 ]\n",
                                    "Epoch 120, current state: [ 0.00082782 -0.0039881 ]\n",
                                    "Epoch 150, current state: [ 0.00094425 -0.00038352]\n",
                                    "```\n",
                                    "\n",
                                    "## ADAM vs Others\n",
                                    "ADAM (Adaptive Moment Estimation) optimizer is generally more efficient than many other optimization algorithms such as SGD (Stochastic Gradient Descent) or RMSprop.\n",
                                    "\n",
                                    "Overall, while the actual efficiency of ADAM compared to other optimizing algorithms can depend on the specific task or dataset, it often performs well in terms of both speed and accuracy across a variety of tasks.\n",
                                    "\n",
                                    "## Conclusion\n",
                                    "Congratulations! You've now understood ADAM and how to code it in Python. With its sound mathematical foundations and impressive empirical results, ADAM constitutes an excellent stepping-stone into the fascinating world of machine learning optimization.\n",
                                    "\n",
                                    "Remember, practice solidifies comprehension and consolidates understanding. Remember to attempt the upcoming hands-on exercises to reinforce these new burgeoning concepts. Until next time, happy coding!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Optimizing Robot Movements with ADAM Algorithm"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adjusting the Learning Rate in ADAM Optimization"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Optimize the Orbit: Tuning the ADAM Optimizer's Epsilon Parameter"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
