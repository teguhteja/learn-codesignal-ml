{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 2: Deep Dive into Random Forest: From Concepts to Real-World Application\n",
                                    "\n",
                                    "\n",
                                    "## Introduction\n",
                                    "Welcome to our journey into the heart of ensemble machine learning with the Random Forest algorithm. As an extension of decision trees, Random Forests operate a multitude of trees, creating a \"forest.\" This lesson will equip you to understand and implement a basic Random Forest in Python, focusing on the nuances of tree construction and aggregation within a forest. Let's get started!\n",
                                    "\n",
                                    "## Understanding the Random Forest\n",
                                    "Random Forest is a robust machine learning ensemble that builds upon many decision trees to solve regression and classification tasks. Each tree 'votes' for a particular class prediction, and the class with the majority votes becomes the final prediction of our model.\n",
                                    "\n",
                                    "Random Forests rely significantly on specific core hyperparameters:\n",
                                    "- **`n_trees`**: The number of trees in the forest. Increasing `n_trees` generally improves performance but adds computational cost.\n",
                                    "- **`max_depth`**: Controls the depth or levels of individual trees.\n",
                                    "- **`random_state`**: Introduces an element of randomness into the feature selection and bootstrapping processes when creating each tree.\n",
                                    "\n",
                                    "## Building Trees: Fostering Uniqueness\n",
                                    "A decision tree, the foundational building block of a Random Forest, embraces a structure akin to a flowchart, with branches that denote decision points and leaves that represent class outcomes. A Random Forest's strength lies in its trees' diversification, each tree constructed uniquely to ensure variety in the forest.\n",
                                    "\n",
                                    "## Implementing the Random Forest in Python\n",
                                    "Implementing our Random Forest begins by importing the necessary libraries:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from scipy import stats\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "```\n",
                                    "\n",
                                    "We initialize our `RandomForest` class with `__init__`, creating attributes for `n_trees`, `max_depth`, `random_state`, an empty list of trees, and a list of unique `random_states` for each tree:\n",
                                    "\n",
                                    "```python\n",
                                    "class RandomForest:\n",
                                    "    def __init__(self, n_trees=100, max_depth=None, random_state=None):\n",
                                    "        self.n_trees = n_trees\n",
                                    "        self.max_depth = max_depth\n",
                                    "        self.random_states = np.random.RandomState(random_state).randint(0, 10000, size=n_trees)\n",
                                    "        self.trees = []\n",
                                    "```\n",
                                    "\n",
                                    "## Bootstrapping: Creating Variety\n",
                                    "Bootstrapping is a statistical method for estimating the property of an estimator by resampling with replacement from an original data sample. It's used to assign measures of accuracy to sample estimates. Each tree is built on a separate bootstrapped dataset in a Random Forest, providing necessary randomness and variety. \n",
                                    "\n",
                                    "Hereâ€™s how bootstrapping is incorporated into our Random Forest:\n",
                                    "\n",
                                    "```python\n",
                                    "def bootstrapping(self, X, y):\n",
                                    "    n_samples = X.shape[0]\n",
                                    "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
                                    "    return X[idxs], y[idxs]\n",
                                    "```\n",
                                    "\n",
                                    "To 'fit' the model, we generate a bootstrapped dataset and fit a different decision tree to it with each iteration, appending each tree to our list:\n",
                                    "\n",
                                    "```python\n",
                                    "def fit(self, X, y):\n",
                                    "    for i in range(self.n_trees):\n",
                                    "        X_, y_ = self.bootstrapping(X, y)\n",
                                    "        tree = DecisionTreeClassifier(max_depth=self.max_depth, random_state=self.random_states[i])\n",
                                    "        tree.fit(X_, y_)\n",
                                    "        self.trees.append(tree)\n",
                                    "```\n",
                                    "\n",
                                    "Finally, the `predict` component of the `RandomForest` collects predictions from each tree, returning the class with the majority votes:\n",
                                    "\n",
                                    "```python\n",
                                    "def predict(self, X):\n",
                                    "    tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
                                    "    return stats.mode(tree_preds)[0][0]\n",
                                    "```\n",
                                    "\n",
                                    "## RandomForest in Action\n",
                                    "To validate our RandomForest's proficiency, let's use the widely employed Iris dataset as our testing ground:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn import datasets\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "iris = datasets.load_iris()\n",
                                    "X = iris.data\n",
                                    "y = iris.target\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
                                    "\n",
                                    "rf = RandomForest(n_trees=100, max_depth=2, random_state=42)\n",
                                    "rf.fit(X_train, y_train)\n",
                                    "y_pred = rf.predict(X_test)\n",
                                    "\n",
                                    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
                                    "```\n",
                                    "\n",
                                    "Here, we load the Iris dataset and split it into training and testing datasets. We train (or 'fit') the model using the training dataset and then predict the classes for the test dataset. The `accuracy_score` summarizes how well our model's predictions match the actual classes in the test data.\n",
                                    "\n",
                                    "## Lesson Summary and Practice\n",
                                    "Congratulations! We've delved deep into the heart of Random Forests, looked at the tree generation process, and engineered a basic Random Forest classifier from scratch using Python. Now it's time for practice to consolidate these concepts. After all, practice is the fuel for mastery! Happy coding!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Evaluating Random Forest Accuracy on Iris Dataset"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adjusting the Depth of Our RandomForest"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Seeding the Forest: Random State Initialization"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
