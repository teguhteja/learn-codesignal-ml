{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 2: Implementing Multiple Linear Regression from Scratch\n",
                                    "\n",
                                    "\n",
                                    "## Introduction\n",
                                    "\n",
                                    "Welcome to our exciting second class in the Regression and Gradient Descent series! In the previous lesson, we covered Simple Linear Regression. Now, we're transitioning toward Multiple Linear Regression, a powerful tool for examining the relationship between a dependent variable and several independent variables.\n",
                                    "\n",
                                    "Consider a case where we need to predict house prices, which undoubtedly depend on multiple factors, such as location, size, and the number of rooms. Multiple Linear Regression accounts for these simultaneous predictors. In today's lesson, you'll learn how to implement this concept in Python!\n",
                                    "\n",
                                    "## Multiple Linear Regression - The Concept\n",
                                    "\n",
                                    "Multiple Linear Regression builds upon the concept of Simple Linear Regression, accounting for more than one independent variable.\n",
                                    "\n",
                                    "Let's recall the Simple Linear Regression equation:\n",
                                    "\n",
                                    "\\[ y = \\beta_0 + \\beta_1 x \\]\n",
                                    "\n",
                                    "For Multiple Linear Regression, we add multiple independent variables, \\( x_1, x_2, \\dots, x_m \\):\n",
                                    "\n",
                                    "\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_m x_m \\]\n",
                                    "\n",
                                    "### Linear Algebra Behind: Dataset Representation\n",
                                    "\n",
                                    "Suppose we had `n` data points (equations), each with `m` features (x values). Then \\( X \\) would look like:\n",
                                    "\n",
                                    "\\[ X = \\begin{bmatrix} \n",
                                    "1 & x_{1,1} & x_{1,2} & \\dots & x_{1,m} \\\\\n",
                                    "1 & x_{2,1} & x_{2,2} & \\dots & x_{2,m} \\\\\n",
                                    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
                                    "1 & x_{n,1} & x_{n,2} & \\dots & x_{n,m} \\\\\n",
                                    "\\end{bmatrix} \\]\n",
                                    "\n",
                                    "Each row represents the `m` features for a single data point. Notice how we include a column of 1's to represent the intercept (also called bias) of each equation.\n",
                                    "\n",
                                    "For each row (equation), there is a corresponding `y` value. So \\( y \\) looks like:\n",
                                    "\n",
                                    "\\[ y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\]\n",
                                    "\n",
                                    "The normal equation results in a vector:\n",
                                    "\n",
                                    "\\[ \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_m \\end{bmatrix} \\]\n",
                                    "\n",
                                    "### Linear Algebra Behind: Making a Prediction\n",
                                    "\n",
                                    "Now, for any set of features \\( x_1 \\) through \\( x_m \\), we can predict the \\( \\hat{y} \\) value as:\n",
                                    "\n",
                                    "\\[ \\hat{y} = (1 \\cdot \\beta_0) + (\\beta_1 \\cdot x_1) + (\\beta_2 \\cdot x_2) + \\dots + (\\beta_m \\cdot x_m) \\]\n",
                                    "\n",
                                    "To calculate all the predictions at once, we take the dot product of \\( X \\) and \\( \\beta \\):\n",
                                    "\n",
                                    "\\[ y = X \\cdot \\beta \\]\n",
                                    "\n",
                                    "### Linear Algebra Behind: Math Solution\n",
                                    "\n",
                                    "To implement Multiple Linear Regression, we'll leverage some Linear Algebra concepts. Using the Normal Equation, we can calculate the coefficients for our regression equation:\n",
                                    "\n",
                                    "\\[ \\beta = (X^T X)^{-1} X^T y \\]\n",
                                    "\n",
                                    "Where \\( X \\) is a matrix of features and \\( y \\) is a vector of the target variable values. Like Simple Linear Regression, residuals (the differences between actual and predicted values) play a significant role. The smaller these residuals, the better the model fits.\n",
                                    "\n",
                                    "## Implementing Multiple Linear Regression from Scratch\n",
                                    "\n",
                                    "Let's roll up our sleeves and start coding! We'll primarily rely on NumPy to handle numerical operations and matrices.\n",
                                    "\n",
                                    "### Step 1: Set Up the Dataset\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "\n",
                                    "X = np.array([[73, 67, 43], \n",
                                    "              [91, 88, 64], \n",
                                    "              [87, 134, 58], \n",
                                    "              [102, 43, 37], \n",
                                    "              [69, 96, 70]], dtype='float32')\n",
                                    "\n",
                                    "y = np.array([56, 81, 119, 22, 103], dtype='float32')\n",
                                    "```\n",
                                    "\n",
                                    "### Step 2: Calculate Coefficients \\( \\beta \\)\n",
                                    "\n",
                                    "Enhance our feature matrix, \\( X \\), with an extra column of ones to account for the intercept.\n",
                                    "\n",
                                    "```python\n",
                                    "ones = np.ones(shape=(len(X), 1))\n",
                                    "X = np.append(ones, X, axis=1)\n",
                                    "```\n",
                                    "\n",
                                    "Compute the coefficients \\( \\beta \\) using the Normal Equation.\n",
                                    "\n",
                                    "```python\n",
                                    "beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
                                    "```\n",
                                    "\n",
                                    "Alternatively, you can use the `@` operator instead of `.dot`:\n",
                                    "\n",
                                    "```python\n",
                                    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
                                    "```\n",
                                    "\n",
                                    "## Model's Performance Evaluation\n",
                                    "\n",
                                    "After completing our model, we need to evaluate its performance. We employ the coefficient of determination (\\( R^2 \\) score) for that. It indicates how well our model fits the data. Let's recall it:\n",
                                    "\n",
                                    "\\[ R^2 = 1 - \\frac{SS_{residuals}}{SS_{total}} \\]\n",
                                    "\n",
                                    "Here, \\( SS_{residuals} \\) denotes the residual sum of squares, and \\( SS_{total} \\) is the total sum of squares:\n",
                                    "\n",
                                    "\\[ SS_{residuals} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\n",
                                    "\n",
                                    "where \\( y_i \\) represents the observed values, \\( \\hat{y}_i \\) represents the predicted values by the regression model.\n",
                                    "\n",
                                    "\\[ SS_{total} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\]\n",
                                    "\n",
                                    "where \\( y_i \\) represents the observed values, \\( \\bar{y} \\) stands for the mean value of observed data.\n",
                                    "\n",
                                    "A higher \\( R^2 \\) value (closer to 1) indicates a good model fit.\n",
                                    "\n",
                                    "```python\n",
                                    "predictions = X.dot(beta)\n",
                                    "ss_residuals = np.sum(np.square(y - predictions))\n",
                                    "ss_total = np.sum(np.square(y - np.mean(y)))\n",
                                    "r2_score = 1 - (ss_residuals / ss_total)\n",
                                    "\n",
                                    "print(\"R^2 Score:\", r2_score)  # Output: R^2 Score: 0.9992\n",
                                    "```\n",
                                    "\n",
                                    "The \\( R^2 \\) score is very close to one, meaning the obtained model is very accurate â€“ almost perfect!\n",
                                    "\n",
                                    "## Lesson Summary and Practice\n",
                                    "\n",
                                    "Congratulations on mastering Multiple Linear Regression! You've effectively bridged the gap from concept to implementation, designing a regression model in Python from scratch.\n",
                                    "\n",
                                    "Prepare for the upcoming lesson to delve more deeply into Regression Analysis. Meanwhile, make sure to practice and refine your newly acquired skills!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Determining House Prices with Multiple Features"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Predicting Housing Prices with Multiple Linear Regression"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Calculating Coefficients in Multiple Linear Regression"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## House Price Prediction with Multiple Linear Regression"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
