{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Lesson 4: Understanding Logistic Regression and Its Implementation Using Gradient Descent\n",
                                    "\n",
                                    "# Understanding Logistic Regression and Its Implementation Using Gradient Descent\n",
                                    "\n",
                                    "## Introduction\n",
                                    "Welcome to our new lesson on Logistic Regression and its implementation using the Gradient Descent technique. Having familiarized yourself with the fundamentals of Regression Analysis and the operation of Gradient Descent in optimizing regression models, we'll now address a different kind of problem: Classification. While Regression Analysis is suitable for predicting continuous variables, predicting categories such as whether an email is spam or not requires specially designed tools â€” one of them being Logistic Regression.\n",
                                    "\n",
                                    "In this lesson, we'll guide you through the basic concepts that define Logistic Regression, focusing on its unique components like the Sigmoid function and Log-Likelihood. Eventually, we'll utilize Python to engineer a straightforward Logistic Regression model using Gradient Descent. By the end of this lesson, you will have broadened your theoretical understanding of another vital machine learning concept and enhanced your practical Python coding skills.\n",
                                    "\n",
                                    "## Classification: From Linear Regression to Logistic Regression\n",
                                    "So far, we've dealt with tasks where a continuous output needs prediction based on one or more input variables - these tasks are known as regression tasks. There is, however, another category of tasks known as classification tasks, where the objective is to predict a categorical outcome. These categories are often binary, like \"spam\"/\"not spam\" for an email or \"malignant\"/\"benign\" for a tumor. The models we've studied so far are not optimal for predicting categorical outcomes - for example, it isn't intuitive to understand what it means for an email to be \"0.67\" spam. Enter Logistic Regression - a classification algorithm that can predict the probability of a binary outcome.\n",
                                    "\n",
                                    "## Sigmoid Function: The Heart of Logistic Regression\n",
                                    "While Linear Regression makes predictions by directly calculating the output, Logistic Regression does it differently. Instead of directly predicting the output, Logistic Regression calculates a raw model output, then transforms it using the sigmoid function, mapping it to a range between 0 and 1, thus making it a probability.\n",
                                    "\n",
                                    "The Sigmoid function is defined as:\n",
                                    "\n",
                                    "\\[\n",
                                    "S(x) = \\frac{1}{1 + e^{-x}}\n",
                                    "\\]\n",
                                    "\n",
                                    "We can implement it like this:\n",
                                    "\n",
                                    "```python\n",
                                    "def sigmoid(z):\n",
                                    "    return 1 / (1 + np.exp(-z))\n",
                                    "```\n",
                                    "\n",
                                    "It looks like this:\n",
                                    "\n",
                                    "![Sigmoid Function](https://via.placeholder.com/800x200.png)\n",
                                    "\n",
                                    "When providing a high positive input, the output of \\(S(x)\\) is close to 1, and for a large negative input, the output is close to 0. This feature of the Sigmoid function makes it a perfect fit when we want to classify emails into two categories: \"spam\" or \"not-spam\".\n",
                                    "\n",
                                    "## Understanding Logistic Regression\n",
                                    "The mathematical form of Logistic Regression can be expressed as follows:\n",
                                    "\n",
                                    "\\[\n",
                                    "P(Y = 1 \\mid x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\n",
                                    "\\]\n",
                                    "\n",
                                    "Where:\n",
                                    "- \\(P(Y = 1 \\mid x)\\) is the probability of event \\(Y = 1\\) given \\(x\\).\n",
                                    "- \\(\\beta_0\\) and \\(\\beta_1\\) are parameters of the model.\n",
                                    "- \\(x\\) is the input variable.\n",
                                    "- \\(\\beta_0 + \\beta_1 x\\) is the linear combination of parameters and feature(s).\n",
                                    "\n",
                                    "Log-Likelihood in Logistic Regression plays a similar role to the Least Squares method in Linear Regression. A maximum likelihood estimation method estimates parameters that maximize the likelihood of making the observations we collected. In Logistic Regression, we seek to maximize the log-likelihood.\n",
                                    "\n",
                                    "## The Cost Function in Logistic Regression\n",
                                    "We've seen the least squares cost function in Linear Regression. However, in Logistic Regression, the cost function is defined differently.\n",
                                    "\n",
                                    "The cost function for a single training instance can be expressed as:\n",
                                    "\n",
                                    "\\[\n",
                                    "- \\left[ y \\log(\\hat{p}) + (1 - y) \\log(1 - \\hat{p}) \\right]\n",
                                    "\\]\n",
                                    "\n",
                                    "Where \\(\\hat{p}\\) denotes the predicted probability.\n",
                                    "\n",
                                    "We can implement it like this:\n",
                                    "\n",
                                    "```python\n",
                                    "def cost_function(h, y):\n",
                                    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
                                    "```\n",
                                    "\n",
                                    "Let's plot it:\n",
                                    "\n",
                                    "![Cost Function](https://via.placeholder.com/800x200.png)\n",
                                    "\n",
                                    "This function makes sense because \\(-\\log(t)\\) approaches 0 when \\(t\\) approaches 1, so the cost will be close to 0 if the predicted probability is near the actual target. However, the cost will approach \\(\\infty\\) when \\(t\\) approaches 0, which coincides with predicting a probability close to 0 for a positive instance will be highly penalized. This peculiar feature of the cost function gives rise to another concern, the threshold selection. You might wonder why we often consider a probability of more than 0.5 as belonging to Category 1, and less than 0.5 as Category 0. This is simply a convention for binary classification and can be adjusted based on the problem at hand.\n",
                                    "\n",
                                    "## Implementing Logistic Regression with Gradient Descent\n",
                                    "As we already know, the Gradient Descent technique is highly efficient at finding the global minimum of a function. Logistic regression is used to calculate the values of parameters that result in the smallest cost. Here's a simple Python implementation of a Logistic Regression model:\n",
                                    "\n",
                                    "```python\n",
                                    "def logistic_regression(X, y, num_iterations, learning_rate):\n",
                                    "    # Add intercept to X\n",
                                    "    intercept = np.ones((X.shape[0], 1))\n",
                                    "    X = np.concatenate((intercept, X), axis=1)\n",
                                    "\n",
                                    "    # Weights initialization\n",
                                    "    theta = np.zeros(X.shape[1])\n",
                                    "\n",
                                    "    for i in range(num_iterations):\n",
                                    "        z = np.dot(X, theta)\n",
                                    "        h = sigmoid(z)\n",
                                    "        gradient = np.dot(X.T, (h - y)) / y.size\n",
                                    "        theta -= learning_rate * gradient\n",
                                    "\n",
                                    "        z = np.dot(X, theta)\n",
                                    "        h = sigmoid(z)\n",
                                    "        loss = cost_function(h, y)\n",
                                    "\n",
                                    "        if i % 10000 == 0:\n",
                                    "            print(f'Loss: {loss}\\t')\n",
                                    "\n",
                                    "    return theta\n",
                                    "```\n",
                                    "\n",
                                    "In this code:\n",
                                    "- The `sigmoid()` function computes the sigmoid of the input value.\n",
                                    "- The `cost_function()` computes the cost for given inputs and outputs using the weights.\n",
                                    "- The `logistic_regression()` applies Gradient Descent to Logistic Regression to find the optimum weights for minimizing the cost.\n",
                                    "\n",
                                    "This simple function can be a Logistic Regression model for classifying emails as \"spam\" or \"not-spam.\"\n",
                                    "\n",
                                    "## Applying Logistic Regression with Gradient Descent\n",
                                    "Now, we can define the `predict` function, which makes the prediction:\n",
                                    "\n",
                                    "```python\n",
                                    "def predict_prob(X, theta):\n",
                                    "    # Add intercept to X\n",
                                    "    intercept = np.ones((X.shape[0], 1))\n",
                                    "    X = np.concatenate((intercept, X), axis=1)\n",
                                    "    return sigmoid(np.dot(X, theta))\n",
                                    "\n",
                                    "def predict(X, theta, threshold=0.5):\n",
                                    "    return predict_prob(X, theta) >= threshold\n",
                                    "```\n",
                                    "\n",
                                    "## Lesson Summary and Practice\n",
                                    "That wraps up our lesson on the fundamentals of Logistic Regression and its Python implementation using Gradient Descent. Throughout this lesson, we've highlighted the differences between regression and classification tasks, introduced Logistic Regression as a classification algorithm, and elaborated on the components that define it.\n",
                                    "\n",
                                    "You'll have ample opportunities to refine these skills in our forthcoming practice exercises. Remember, the more you practice, the more fluent you'll become. So, practice away and have fun doing it!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Sigmoid Function: From Input to Probability\n",
                                    "\n",
                                    "Here's the modified code that includes the `Close` column in the feature selection. This will allow us to compare how including the `Close` feature impacts the model's Mean Squared Error (MSE):\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from sklearn.preprocessing import StandardScaler\n",
                                    "from datasets import load_dataset\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import GradientBoostingRegressor\n",
                                    "from sklearn.metrics import mean_squared_error\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "tesla = load_dataset('codesignal/tsla-historic-prices')\n",
                                    "tesla_df = pd.DataFrame(tesla['train'])\n",
                                    "\n",
                                    "# Convert 'Date' column to datetime format\n",
                                    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
                                    "\n",
                                    "# Adding technical indicators\n",
                                    "tesla_df['SMA_5'] = tesla_df['Adj Close'].rolling(window=5).mean()\n",
                                    "tesla_df['SMA_10'] = tesla_df['Adj Close'].rolling(window=10).mean()\n",
                                    "tesla_df['EMA_5'] = tesla_df['Adj Close'].ewm(span=5, adjust=False).mean()\n",
                                    "tesla_df['EMA_10'] = tesla_df['Adj Close'].ewm(span=10, adjust=False).mean()\n",
                                    "\n",
                                    "# Drop NaN values created by moving averages\n",
                                    "tesla_df.dropna(inplace=True)\n",
                                    "\n",
                                    "# Features and target selection including the 'Close' column\n",
                                    "features = tesla_df[['Close', 'Open', 'High', 'Low', 'Volume', 'SMA_5', 'SMA_10', 'EMA_5', 'EMA_10']].values\n",
                                    "target = tesla_df['Adj Close'].values\n",
                                    "\n",
                                    "# Standardizing features\n",
                                    "scaler = StandardScaler()\n",
                                    "features_scaled = scaler.fit_transform(features)\n",
                                    "\n",
                                    "# Splitting the dataset\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
                                    "\n",
                                    "# Train the model\n",
                                    "model = GradientBoostingRegressor(random_state=42)\n",
                                    "model.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Predict and evaluate\n",
                                    "predictions = model.predict(X_test)\n",
                                    "mse = mean_squared_error(y_test, predictions)\n",
                                    "print(\"Mean Squared Error with 'Close' feature:\", mse)\n",
                                    "\n",
                                    "# Visualizing the predictions\n",
                                    "plt.figure(figsize=(10, 6))\n",
                                    "plt.scatter(range(len(y_test)), y_test, label='Actual', alpha=0.7)\n",
                                    "plt.scatter(range(len(y_test)), predictions, label='Predicted', alpha=0.7)\n",
                                    "plt.title('Actual vs Predicted Values')\n",
                                    "plt.xlabel('Sample Index')\n",
                                    "plt.ylabel('Value')\n",
                                    "plt.legend()\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "### Changes Made:\n",
                                    "- **Included `Close` in Feature Selection**: Added `'Close'` to the features list in the `features` variable.\n",
                                    "- **Modified MSE Output**: Updated the print statement to specify that the MSE includes the `Close` feature.\n",
                                    "\n",
                                    "Now, when you run this code, it will standardize the `Close` column along with the other features and use it in training the model. This will help you determine the impact of the `Close` feature on the model's performance by observing the change in the Mean Squared Error (MSE)."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implementing the Sigmoid Function\n",
                                    "\n",
                                    "ðŸš€ Let's fix that bug so we can get the Gradient Boosting model running smoothly!\n",
                                    "\n",
                                    "### Identifying the Issue:\n",
                                    "The bug is in the line where the model is being trained:\n",
                                    "\n",
                                    "```python\n",
                                    "model.fit(X_test, y_test)\n",
                                    "```\n",
                                    "\n",
                                    "In the `fit` method, the model is supposed to learn from the training data, but here, it's incorrectly using the testing data (`X_test`, `y_test`) instead of the training data (`X_train`, `y_train`).\n",
                                    "\n",
                                    "### Fixing the Issue:\n",
                                    "We need to replace `X_test` and `y_test` with `X_train` and `y_train` in the `model.fit()` method.\n",
                                    "\n",
                                    "Hereâ€™s the corrected code:\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from sklearn.preprocessing import StandardScaler\n",
                                    "import datasets\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import GradientBoostingRegressor\n",
                                    "from sklearn.metrics import mean_squared_error\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "tesla = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
                                    "tesla_df = pd.DataFrame(tesla['train'])\n",
                                    "\n",
                                    "# Convert 'Date' to datetime format\n",
                                    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
                                    "\n",
                                    "# Calculate SMAs and EMAs\n",
                                    "tesla_df['SMA_5'] = tesla_df['Adj Close'].rolling(window=5).mean()\n",
                                    "tesla_df['SMA_10'] = tesla_df['Adj Close'].rolling(window=10).mean()\n",
                                    "tesla_df['EMA_5'] = tesla_df['Adj Close'].ewm(span=5, adjust=False).mean()\n",
                                    "tesla_df['EMA_10'] = tesla_df['Adj Close'].ewm(span=10, adjust=False).mean()\n",
                                    "\n",
                                    "# Drop NaN values\n",
                                    "tesla_df.dropna(inplace=True)\n",
                                    "\n",
                                    "# Features and target selection\n",
                                    "features = tesla_df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_5', 'SMA_10', 'EMA_5', 'EMA_10']].values\n",
                                    "target = tesla_df['Adj Close'].values\n",
                                    "\n",
                                    "# Standardizing features\n",
                                    "scaler = StandardScaler()\n",
                                    "features_scaled = scaler.fit_transform(features)\n",
                                    "\n",
                                    "# Splitting the dataset\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
                                    "\n",
                                    "# Instantiate and fit the Gradient Boosting Regressor\n",
                                    "model = GradientBoostingRegressor(random_state=42)\n",
                                    "model.fit(X_train, y_train)  # Use training data here\n",
                                    "\n",
                                    "# Predict and evaluate the model\n",
                                    "predictions = model.predict(X_test)\n",
                                    "mse = mean_squared_error(y_test, predictions)\n",
                                    "print(\"Mean Squared Error:\", mse)\n",
                                    "\n",
                                    "# Visualizing the actual vs predicted values\n",
                                    "plt.figure(figsize=(10, 6))\n",
                                    "plt.scatter(range(len(y_test)), y_test, label='Actual', alpha=0.7)\n",
                                    "plt.scatter(range(len(y_test)), predictions, label='Predicted', alpha=0.7)\n",
                                    "plt.title('Actual vs Predicted Values')\n",
                                    "plt.xlabel('Sample Index')\n",
                                    "plt.ylabel('Value')\n",
                                    "plt.legend()\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "### Summary:\n",
                                    "- **Bug**: The model was being trained on the testing data instead of the training data.\n",
                                    "- **Fix**: Corrected the `model.fit()` method to use `X_train` and `y_train` for training.\n",
                                    "\n",
                                    "Now the code should run correctly and give you the Mean Squared Error (MSE) along with a visualization of actual vs predicted values. ðŸš€"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Evaluating Spam Filter Accuracy with Logistic Regression\n",
                                    "\n",
                                    "Let's complete the code to standardize the features, train the Gradient Boosting Regressor model, and evaluate the model's performance.\n",
                                    "\n",
                                    "Here's the completed code:\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from sklearn.preprocessing import StandardScaler\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import GradientBoostingRegressor\n",
                                    "from sklearn.metrics import mean_squared_error\n",
                                    "import datasets\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "tesla = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
                                    "tesla_df = pd.DataFrame(tesla['train'])\n",
                                    "\n",
                                    "# Convert 'Date' column to datetime format\n",
                                    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
                                    "\n",
                                    "# Calculate SMA and EMA\n",
                                    "tesla_df['SMA_5'] = tesla_df['Adj Close'].rolling(window=5).mean()\n",
                                    "tesla_df['SMA_10'] = tesla_df['Adj Close'].rolling(window=10).mean()\n",
                                    "tesla_df['EMA_5'] = tesla_df['Adj Close'].ewm(span=5, adjust=False).mean()\n",
                                    "tesla_df['EMA_10'] = tesla_df['Adj Close'].ewm(span=10, adjust=False).mean()\n",
                                    "\n",
                                    "# Drop NaN values created by moving averages\n",
                                    "tesla_df.dropna(inplace=True)\n",
                                    "\n",
                                    "# Select features and target\n",
                                    "features = tesla_df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_5', 'SMA_10', 'EMA_5', 'EMA_10']].values\n",
                                    "target = tesla_df['Adj Close'].values\n",
                                    "\n",
                                    "# Standardize features\n",
                                    "scaler = StandardScaler()\n",
                                    "features_scaled = scaler.fit_transform(features)\n",
                                    "\n",
                                    "# Splitting the dataset\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
                                    "\n",
                                    "# Instantiate and fit the model\n",
                                    "model = GradientBoostingRegressor(random_state=42)\n",
                                    "model.fit(X_train, y_train)  # Fit the model with training data\n",
                                    "\n",
                                    "# Predict and evaluate error\n",
                                    "predictions = model.predict(X_test)\n",
                                    "mse = mean_squared_error(y_test, predictions)\n",
                                    "print(\"Mean Squared Error:\", mse)\n",
                                    "\n",
                                    "# Plotting predictions vs actual values\n",
                                    "plt.figure(figsize=(10, 6))\n",
                                    "plt.scatter(range(len(y_test)), y_test, label='Actual', alpha=0.7)\n",
                                    "plt.scatter(range(len(y_test)), predictions, label='Predicted', alpha=0.7)\n",
                                    "plt.title('Actual vs Predicted Values')\n",
                                    "plt.xlabel('Sample Index')\n",
                                    "plt.ylabel('Value')\n",
                                    "plt.legend()\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation of the Completed Code:\n",
                                    "1. **Standardize Features**:\n",
                                    "   - `features_scaled = scaler.fit_transform(features)` standardizes the features to have a mean of 0 and a standard deviation of 1.\n",
                                    "\n",
                                    "2. **Train the Model**:\n",
                                    "   - `model.fit(X_train, y_train)` fits the Gradient Boosting Regressor model using the training data.\n",
                                    "\n",
                                    "3. **Predict and Evaluate**:\n",
                                    "   - `predictions = model.predict(X_test)` generates predictions based on the test data.\n",
                                    "   - The Mean Squared Error (MSE) is calculated using `mean_squared_error(y_test, predictions)` to evaluate the model's performance.\n",
                                    "\n",
                                    "4. **Visualization**:\n",
                                    "   - The scatter plot compares the actual vs. predicted values, providing a visual representation of the model's accuracy.\n",
                                    "\n",
                                    "Now, your model is fully operational and ready to make predictions! ðŸš€ May the data be with you!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adding the Gradient to Logistic \n",
                                    "\n",
                                    "Thank you, Space Voyager! Let's complete the code and make sure it works flawlessly. I'll guide you through filling in the missing parts.\n",
                                    "\n",
                                    "Here's the complete code:\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from sklearn.preprocessing import StandardScaler\n",
                                    "import datasets\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import GradientBoostingRegressor\n",
                                    "from sklearn.metrics import mean_squared_error\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "tesla = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
                                    "tesla_df = pd.DataFrame(tesla['train'])\n",
                                    "\n",
                                    "# Convert the 'Date' column to datetime format\n",
                                    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
                                    "\n",
                                    "# Calculate technical indicators: Simple Moving Average (SMA), Exponential Moving Average (EMA), and Volatility\n",
                                    "tesla_df['SMA_5'] = tesla_df['Adj Close'].rolling(window=5).mean()\n",
                                    "tesla_df['EMA_5'] = tesla_df['Adj Close'].ewm(span=5, adjust=False).mean()\n",
                                    "tesla_df['Volatility'] = tesla_df['Close'].rolling(window=5).std()\n",
                                    "\n",
                                    "# Drop NaN values created by indicators\n",
                                    "tesla_df.dropna(inplace=True)\n",
                                    "\n",
                                    "# Selecting features and target\n",
                                    "# Configure all the features you need for training the model (excluding the target 'Adj Close')\n",
                                    "features = tesla_df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_5', 'EMA_5', 'Volatility']].values\n",
                                    "\n",
                                    "# Configure the target - 'Adj Close' column\n",
                                    "target = tesla_df['Adj Close'].values\n",
                                    "\n",
                                    "# Standardize features\n",
                                    "scaler = StandardScaler()\n",
                                    "features_scaled = scaler.fit_transform(features)\n",
                                    "\n",
                                    "# Splitting the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
                                    "\n",
                                    "# Instantiate the Gradient Boosting Regressor and fit the model to the training data\n",
                                    "model = GradientBoostingRegressor(random_state=42)\n",
                                    "model.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Predict the values for X_test and calculate the Mean Squared Error (MSE)\n",
                                    "predictions = model.predict(X_test)\n",
                                    "mse = mean_squared_error(y_test, predictions)\n",
                                    "print(\"Mean Squared Error:\", mse)\n",
                                    "\n",
                                    "# Visualizing the actual vs predicted values\n",
                                    "plt.figure(figsize=(10, 6))\n",
                                    "plt.scatter(range(len(y_test)), y_test, label='Actual', alpha=0.7)\n",
                                    "plt.scatter(range(len(y_test)), predictions, label='Predicted', alpha=0.7)\n",
                                    "plt.title('Actual vs Predicted Values')\n",
                                    "plt.xlabel('Sample Index')\n",
                                    "plt.ylabel('Value')\n",
                                    "plt.legend()\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation of the Completed Parts:\n",
                                    "1. **Features Selection**:\n",
                                    "   - `features = tesla_df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_5', 'EMA_5', 'Volatility']].values`\n",
                                    "   - These are the selected features for the model. Notice that we didn't include the target variable ('Adj Close') in the features.\n",
                                    "\n",
                                    "2. **Target Selection**:\n",
                                    "   - `target = tesla_df['Adj Close'].values`\n",
                                    "   - The target variable is the 'Adj Close' column, which we want to predict.\n",
                                    "\n",
                                    "3. **Standardizing Features**:\n",
                                    "   - `features_scaled = scaler.fit_transform(features)`\n",
                                    "   - This standardizes the features to ensure that they have a mean of 0 and a standard deviation of 1.\n",
                                    "\n",
                                    "4. **Model Training**:\n",
                                    "   - `model = GradientBoostingRegressor(random_state=42)`\n",
                                    "   - `model.fit(X_train, y_train)`\n",
                                    "   - We train the Gradient Boosting Regressor model using the training data.\n",
                                    "\n",
                                    "5. **Prediction and Evaluation**:\n",
                                    "   - `predictions = model.predict(X_test)`\n",
                                    "   - `mse = mean_squared_error(y_test, predictions)`\n",
                                    "   - We predict the test data and calculate the Mean Squared Error to evaluate model performance.\n",
                                    "\n",
                                    "Now you're ready to run the code and get insights from the model's predictions. ðŸš€ May the data be with you!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "a0e28ed1",
                           "metadata": {},
                           "source": [
                                    "Let's complete the full Gradient Boosting Model training workflow using Tesla stock data. I'll guide you through each step with the necessary code.\n",
                                    "\n",
                                    "Here's the full solution:\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from sklearn.preprocessing import StandardScaler\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import GradientBoostingRegressor\n",
                                    "from sklearn.metrics import mean_squared_error\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "import numpy as np\n",
                                    "import datasets\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "tesla = datasets.load_dataset('codesignal/tsla-historic-prices')\n",
                                    "tesla_df = pd.DataFrame(tesla['train'])\n",
                                    "\n",
                                    "# Convert 'Date' column to datetime format\n",
                                    "tesla_df['Date'] = pd.to_datetime(tesla_df['Date'])\n",
                                    "\n",
                                    "# Add technical indicators\n",
                                    "tesla_df['Price_Diff'] = tesla_df['Adj Close'] - tesla_df['Adj Close'].shift(1)\n",
                                    "tesla_df['Volatility'] = tesla_df['Adj Close'].rolling(window=5).std()\n",
                                    "tesla_df['Momentum'] = tesla_df['Adj Close'] - tesla_df['Adj Close'].shift(5)\n",
                                    "tesla_df['Log_Price'] = np.log(tesla_df['Adj Close'])\n",
                                    "\n",
                                    "# Drop rows with NaN values\n",
                                    "tesla_df.dropna(inplace=True)\n",
                                    "\n",
                                    "# Select features and target variable\n",
                                    "features = tesla_df[['Price_Diff', 'Volatility', 'Momentum', 'Log_Price']].values\n",
                                    "target = tesla_df['Adj Close'].values\n",
                                    "\n",
                                    "# Standardize the feature values\n",
                                    "scaler = StandardScaler()\n",
                                    "features_scaled = scaler.fit_transform(features)\n",
                                    "\n",
                                    "# Split the dataset into training and test sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.25, random_state=42)\n",
                                    "\n",
                                    "# Instantiate the Gradient Boosting Regressor and fit it to the training data\n",
                                    "model = GradientBoostingRegressor(random_state=42)\n",
                                    "model.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Make predictions and calculate the Mean Squared Error (MSE)\n",
                                    "predictions = model.predict(X_test)\n",
                                    "mse = mean_squared_error(y_test, predictions)\n",
                                    "print(\"Mean Squared Error:\", mse)\n",
                                    "\n",
                                    "# Visualize actual vs predicted values using scatter plots\n",
                                    "plt.figure(figsize=(10, 6))\n",
                                    "plt.scatter(range(len(y_test)), y_test, label='Actual', alpha=0.7)\n",
                                    "plt.scatter(range(len(y_test)), predictions, label='Predicted', alpha=0.7)\n",
                                    "plt.title('Actual vs Predicted Values')\n",
                                    "plt.xlabel('Sample Index')\n",
                                    "plt.ylabel('Value')\n",
                                    "plt.legend()\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation of Each Step:\n",
                                    "\n",
                                    "1. **Convert 'Date' to Datetime Format**:\n",
                                    "   - Ensures that the 'Date' column is properly formatted for any potential time-series analysis.\n",
                                    "\n",
                                    "2. **Add Technical Indicators**:\n",
                                    "   - `Price_Diff`: Measures the difference in the adjusted close price from the previous day.\n",
                                    "   - `Volatility`: Calculates the standard deviation of the adjusted close price over the last 5 days.\n",
                                    "   - `Momentum`: Measures the difference in the adjusted close price compared to 5 days ago.\n",
                                    "   - `Log_Price`: Takes the logarithm of the adjusted close price, useful for stabilizing variance.\n",
                                    "\n",
                                    "3. **Drop Rows with NaN Values**:\n",
                                    "   - Ensures that there are no missing values in the data before training the model.\n",
                                    "\n",
                                    "4. **Select Features and Target Variable**:\n",
                                    "   - The selected features are the ones we added above, and the target is the 'Adj Close' price.\n",
                                    "\n",
                                    "5. **Standardize Feature Values**:\n",
                                    "   - Standardizes the feature values to have a mean of 0 and a standard deviation of 1, which is important for the gradient boosting algorithm.\n",
                                    "\n",
                                    "6. **Split the Dataset**:\n",
                                    "   - Splits the data into training and test sets, where 25% of the data is used for testing.\n",
                                    "\n",
                                    "7. **Train the Gradient Boosting Model**:\n",
                                    "   - The model is trained using the training data.\n",
                                    "\n",
                                    "8. **Make Predictions and Evaluate**:\n",
                                    "   - Predictions are made on the test set, and the Mean Squared Error (MSE) is calculated to evaluate the model's performance.\n",
                                    "\n",
                                    "9. **Visualize Actual vs Predicted Values**:\n",
                                    "   - A scatter plot is used to visually compare the actual values against the predicted values.\n",
                                    "\n",
                                    "You can now run the code and analyze the results! ðŸš€"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "f3de4798",
                           "metadata": {},
                           "source": []
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
