{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Gradient Descent Optimization in Linear Regression\n",
                                    "\n",
                                    "## Introduction\n",
                                    "Hello and welcome to another session on \"Regression and Gradient Descent.\" In today's syllabus, we will construct and fit the gradient descent algorithm into a linear regression problem. Though linear regression has a direct solution, gradient descent is essential for computational efficiency, especially when handling larger datasets or complex models.\n",
                                    "\n",
                                    "## The Concept of Gradient Descent\n",
                                    "Gradient descent is an iterative optimization algorithm for minimizing a function, usually a loss function, quantifying the disparity between predicted and actual results. The goal of gradient descent is to find the parameters that minimize the value of the loss function. Importantly, gradient descent navigates its way to the minimum of the function by moving iteratively toward the direction of the steepest descent. To leverage gradient descent, the target function must be differentiable.\n",
                                    "\n",
                                    "### Taking Steps with Gradient Descent\n",
                                    "Gradient descent derives its name from its working mechanism: taking descents along the gradient. It operates in several iterative steps as follows:\n",
                                    "\n",
                                    "1. **Choose random values for initial parameters.**\n",
                                    "2. **Calculate the cost (the difference between actual and predicted value).**\n",
                                    "3. **Compute the gradient (the steepest slope of the function around that point).**\n",
                                    "4. **Update the parameters using the gradient.**\n",
                                    "5. **Repeat steps 2 to 4 until an acceptable error rate is reached or the maximum iterations are exhausted.**\n",
                                    "\n",
                                    "A vital component of gradient descent is the learning rate, which determines the size of the descent towards the optimum solution. If the learning rate is too high, we may overshoot the minimum; if it's too low, the convergence to the minimum may take too long.\n",
                                    "\n",
                                    "## Implementing Gradient Descent in Python: The Cost Function\n",
                                    "Let's implement gradient descent from scratch with a basic understanding of the algorithm. We need two functions: one for calculating the cost and another for calculating and applying the gradient to update our parameters. We'll also add an early stop mechanism to halt computations after a predefined number of iterations.\n",
                                    "\n",
                                    "The cost function is as follows:\n",
                                    "\n",
                                    "\\[\n",
                                    "J(X, y, \\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (X \\cdot \\theta - y_i)^2\n",
                                    "\\]\n",
                                    "\n",
                                    "Where:\n",
                                    "- \\( J \\) is the cost,\n",
                                    "- \\( X \\) is the data,\n",
                                    "- \\( y \\) are the actual values,\n",
                                    "- \\( \\theta \\) are the parameters,\n",
                                    "- \\( m \\) is the length of \\( y \\).\n",
                                    "\n",
                                    "This is the calculation of the mean squared error.\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "\n",
                                    "def cost(X, y, theta):\n",
                                    "    m = len(y)\n",
                                    "    predictions = X.dot(theta)\n",
                                    "    cost = (1/m) * np.sum(np.square(predictions-y))  # Compute mean square error\n",
                                    "    return cost\n",
                                    "```\n",
                                    "\n",
                                    "## Implementing Gradient Descent in Python: The Gradient Descent\n",
                                    "Next, for the gradient descent function, we follow the gradient descent update rule:\n",
                                    "\n",
                                    "\\[\n",
                                    "\\theta := \\theta - \\alpha \\frac{1}{m} X^T \\cdot (X \\cdot \\theta - y)\n",
                                    "\\]\n",
                                    "\n",
                                    "Where:\n",
                                    "- \\( \\alpha \\) is the learning rate,\n",
                                    "- \\( X^T \\) is the transpose of the data.\n",
                                    "\n",
                                    "Note that the derivative of the mean squared error usually includes a factor of 2, but we can incorporate this into the learning rate for simplicity.\n",
                                    "\n",
                                    "```python\n",
                                    "def gradient_descent(X, y, theta, alpha, iterations):\n",
                                    "    m = len(y)\n",
                                    "    cost_history = np.zeros(iterations)\n",
                                    "    theta_history = np.zeros((iterations, 2))\n",
                                    "    for i in range(iterations):  # Iterate until convergence\n",
                                    "        prediction = np.dot(X, theta)  # Matrix multiplication between X and theta\n",
                                    "        theta = theta - (1/m) * alpha * (X.T.dot((prediction - y)))  # Gradient update rule\n",
                                    "        theta_history[i, :] = theta.T\n",
                                    "        cost_history[i] = cost(X, y, theta)\n",
                                    "    return theta, cost_history, theta_history\n",
                                    "```\n",
                                    "\n",
                                    "## Applying Gradient Descent to Linear Regression\n",
                                    "Let's apply our gradient descent function to a simple linear regression problem. The form of linear regression is:\n",
                                    "\n",
                                    "\\[\n",
                                    "y = ax + b\n",
                                    "\\]\n",
                                    "\n",
                                    "Where:\n",
                                    "- \\( a \\) and \\( b \\) are the parameters \\( \\theta \\) that we need to learn.\n",
                                    "\n",
                                    "The following data has been generated based on this form with some noise.\n",
                                    "\n",
                                    "```python\n",
                                    "X = 2 * np.random.rand(100, 1)\n",
                                    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
                                    "\n",
                                    "lr = 0.01  # Learning Rate\n",
                                    "n_iter = 1000  # Max number of iterations\n",
                                    "theta = np.random.randn(2, 1)  # Randomly initialized parameters\n",
                                    "X_b = np.c_[np.ones((len(X), 1)), X]  # Add bias parameter to X\n",
                                    "theta, cost_history, theta_history = gradient_descent(X_b, y, theta, lr, n_iter)  # Gradient Descent\n",
                                    "```\n",
                                    "\n",
                                    "## Lesson Summary and Practice\n",
                                    "Congratulations! You have mastered implementing the gradient descent algorithm and its application to linear regression. We covered theoretical explanations, derived the math behind the cost function and the gradient descent update rule, and brought these concepts to life by coding in Python.\n",
                                    "\n",
                                    "It is now time to practice and solidify what you have learned. In the upcoming exercises, challenge yourself with different problems and experiment with varying parameters like the learning rate. Enjoy your journey into the world of gradients!\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adjust the Learning Rate"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Applying Gradient Descent in Real Estate Pricing"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implementing Gradient Descent in Real Estate Analysis"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Trying New Approach"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
