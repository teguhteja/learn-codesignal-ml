{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 4 Demystifying Stop Words in Natural Language Processing\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "# Lesson Overview\n",
                                    "\n",
                                    "Hello and welcome! In this lesson, we'll dive into a crucial step in text data preprocessing in **Natural Language Processing (NLP)** — removing stop words using Python and NLTK.\n",
                                    "\n",
                                    "**Stop words** usually refer to the most commonly used words in a language. However, despite their high frequency, these words carry little meaningful information and are often filtered out from text data. By the end of this lesson, you'll understand what stop words are, why they're essential in NLP, and how to remove them from your text data.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "# Understanding Stop Words in NLP\n",
                                    "\n",
                                    "The big question is — What exactly are stop words? You can look at stop words as **background noise** in your text data. They're words like *is*, *the*, *in*, and *and* — words that don't carry a lot of meaning on their own.\n",
                                    "\n",
                                    "So, why do we want to remove these stop words? Machine learning models look for signals to make decisions. If we leave in common words that are often in every document, we're not giving our model a lot of useful information. Hence, earlier in our pre-processing pipeline, we would probably remove these words to let our model focus on words that may indicate something more exceptional.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "# Identification and Removal of Stop Words\n",
                                    "\n",
                                    "To remove these stop words, we first have to identify them. To do this efficiently, we can leverage a resource from the NLTK library — the **NLTK's built-in English stop words list**. Before we can use this list, however, it's important to ensure that the `stopwords` package is downloaded. This is achieved using the command `nltk.download('stopwords')`. Once downloaded, we can access the list of commonly agreed-upon stop words in the English language by calling `nltk.corpus.stopwords.words('english')`. This function returns a comprehensive list that we can use to filter out stop words from our text data.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "# Practical Example: Code Explanation\n",
                                    "\n",
                                    "To better illustrate this, let's walk through the code block below:\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "from nltk.corpus import stopwords\n",
                                    "\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x))\n",
                                    "\n",
                                    "stop_words = stopwords.words('english')\n",
                                    "\n",
                                    "print(\"Some stop words:\", stop_words[:3])\n",
                                    "\n",
                                    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
                                    "\n",
                                    "print(df['filtered_tokens'].head())\n",
                                    "```\n",
                                    "\n",
                                    "In this block of code, we start by importing the necessary libraries and downloading the required NLTK packages. Next, we convert our `sms_spam` dataset into a pandas DataFrame, which makes it easier to handle.\n",
                                    "\n",
                                    "We then proceed to **tokenize** the `message` column of our DataFrame and store our tokens in a new column — `tokens`. Tokenizing involves breaking down our text data into individual components. At this point, we don't worry about any stop words; our primary focus is on breaking down the sentences into words.\n",
                                    "\n",
                                    "We then define our stop words using NLTK's built-in English stop words list and set this list to a variable — `stop_words`. Following that, we print 3 stop words as examples.\n",
                                    "\n",
                                    "Now that we have both our tokens and our stop words, we can proceed to remove any stop words from our tokens. We use a **lambda function** to compare each word in our tokens to our list of stop words. If the word is a stop word, we filter it out.\n",
                                    "\n",
                                    "We apply this process to our DataFrame, and our final output is a DataFrame with another new column — the `filtered_tokens` column. This column contains our tokenized messages, sans the stop words.\n",
                                    "\n",
                                    "The output will be:\n",
                                    "\n",
                                    "```\n",
                                    "Some stop words: ['i', 'me', 'my']\n",
                                    "0    [Go, jurong, point, ,, crazy, .., Available, b...\n",
                                    "1             [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
                                    "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
                                    "3    [U, dun, say, early, hor, ..., U, c, already, ...\n",
                                    "4    [Nah, I, n't, think, goes, usf, ,, lives, arou...\n",
                                    "Name: filtered_tokens, dtype: object\n",
                                    "```\n",
                                    "\n",
                                    "This output shows the first few entries of the `filtered_tokens` column from our DataFrame, demonstrating the result of removing stop words from the tokenized messages. Each entry corresponds to tokenized, filtered text from our initial dataset, showcasing how common stop words are excluded, leaving more meaningful words.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "# Lesson Summary and Practice\n",
                                    "\n",
                                    "Phew! We made it. You should now understand what stop words are, why they're important in **Natural Language Processing**, and why we remove them. You can now remove these stop words from your text data using Python and the NLTK library, which is a pretty neat skill to have.\n",
                                    "\n",
                                    "Remember to keep practicing. Challenge yourself with different text data and try to remove the stop words. There's no better way to learn than through constant hands-on experience. So, go ahead and start analyzing some data!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Stop Words Demystified in NLP\n",
                                    "\n",
                                    "Explore the removal of stop words in NLP by practicing with Python and NLTK on the SMS Spam Collection dataset. This task involves tokenizing messages and filtering out English stop words to highlight meaningful data. Simply Run the provided code to see the impact of this preprocessing step firsthand.\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "from datasets import load_dataset\n",
                                    "\n",
                                    "import nltk\n",
                                    "\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "\n",
                                    "from nltk.corpus import stopwords\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "# Tokenize the messages\n",
                                    "\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x))\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "# Define stop words\n",
                                    "\n",
                                    "stop_words = stopwords.words('english')\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "# Print some stop words\n",
                                    "\n",
                                    "print(\"Some stop words:\", stop_words[:3])\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "# Remove stop words from tokens\n",
                                    "\n",
                                    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "# Print filtered tokens\n",
                                    "\n",
                                    "print(df['filtered_tokens'].head())\n",
                                    "\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "from nltk.corpus import stopwords\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Tokenize the messages\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x))\n",
                                    "\n",
                                    "# Define stop words\n",
                                    "stop_words = stopwords.words('english')\n",
                                    "\n",
                                    "# Print some stop words\n",
                                    "print(\"Some stop words:\", stop_words[:3]) \n",
                                    "\n",
                                    "# Remove stop words from tokens\n",
                                    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
                                    "\n",
                                    "# Print filtered tokens\n",
                                    "print(df['filtered_tokens'].head())\n",
                                    "```\n",
                                    "Output of the code:\n",
                                    "```\n",
                                    "Some stop words: ['i', 'me', 'my']\n",
                                    "0    [Go, until, jurong, point, ,, crazy, .., Available, only, in, bugis, n, great, world, la, e, buffet, ..., Cine, got, amore, film, ,, great, world, 's, a, bugis, great, world, la, e]\n",
                                    "1                      [Ok, lar, ..., Joking, wif, u, r, non, pie, n, even, u, score, ., V, well, ..., Got, ur, go]\n",
                                    "2    [Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, ., Text, FA, 87121, receive, entry, question, (, std, txt, rate, ), T, C, 's, apply, 08452810075, over, 18]\n",
                                    "3                                  [U, dun, say, so, early, hor, ..., U, c, already, then, say, ...]\n",
                                    "4    [Nah, I, don, 't, think, goes, usf, ,, lives, around, here, though]\n",
                                    "Name: filtered_tokens, dtype: object\n",
                                    "```\n",
                                    "The code successfully demonstrates the removal of stop words from the SMS Spam Collection dataset using NLTK. \n",
                                    "\n",
                                    "Here's a breakdown of what happened:\n",
                                    "* The `sms-spam-collection` dataset was loaded.\n",
                                    "* Each message in the dataset was tokenized into individual words.\n",
                                    "* A list of common English stop words (like 'i', 'me', 'my') was defined.\n",
                                    "* A new column `filtered_tokens` was created, containing the tokens from which these stop words have been removed.\n",
                                    "\n",
                                    "By comparing the original messages (implicitly, through the process) and the `filtered_tokens` output, you can observe how words that are typically less meaningful for text analysis (e.g., articles, prepositions, common verbs) are removed, leaving behind words that are likely more significant for tasks like spam detection. This preprocessing step helps to reduce noise and focus on the most informative words in the text."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adapting Stop Words Removal for Spanish\n",
                                    "\n",
                                    "With the knowledge gained from understanding stop words in English, let's pivot and explore their impact in Spanish. This step involves modifying the given Python code to filter out stop words from a generated dataset containing both English and Spanish text. This task will test your understanding of applying text processing routines across different languages by altering the language parameter in the stop words function.\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import nltk\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "from nltk.corpus import stopwords\n",
                                    "\n",
                                    "# Generated dataset with English and Spanish words\n",
                                    "data = {\n",
                                    "    'message': [\n",
                                    "        'Hola cómo estás?',\n",
                                    "        'I am fine and you?',\n",
                                    "        'Estoy bien gracias por preguntar',\n",
                                    "        'Good morning have a nice day',\n",
                                    "        'Buenos días que tengas un buen día'\n",
                                    "    ]\n",
                                    "}\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "df = pd.DataFrame(data)\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x.lower()))\n",
                                    "\n",
                                    "# Define stop words\n",
                                    "stop_words = stopwords.words('english') # TODO: Change to use 'spanish' stop words\n",
                                    "\n",
                                    "# Print some stop words\n",
                                    "print(\"Some stop words:\", stop_words[:3]) \n",
                                    "\n",
                                    "# Remove stop words from tokens\n",
                                    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
                                    "print(df[['message', 'filtered_tokens']].head())\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To adapt the stop words removal for Spanish, you need to change the language parameter in the `stopwords.words()` function from `'english'` to `'spanish'`.\n",
                                    "\n",
                                    "Here's the modified code:\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import nltk\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "from nltk.corpus import stopwords\n",
                                    "\n",
                                    "# Generated dataset with English and Spanish words\n",
                                    "data = {\n",
                                    "    'message': [\n",
                                    "        'Hola cómo estás?',\n",
                                    "        'I am fine and you?',\n",
                                    "        'Estoy bien gracias por preguntar',\n",
                                    "        'Good morning have a nice day',\n",
                                    "        'Buenos días que tengas un buen día'\n",
                                    "    ]\n",
                                    "}\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "df = pd.DataFrame(data)\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x.lower()))\n",
                                    "\n",
                                    "# Define stop words\n",
                                    "stop_words = stopwords.words('spanish') # Changed to use 'spanish' stop words\n",
                                    "\n",
                                    "# Print some stop words\n",
                                    "print(\"Some stop words:\", stop_words[:3]) \n",
                                    "\n",
                                    "# Remove stop words from tokens\n",
                                    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
                                    "print(df[['message', 'filtered_tokens']].head())\n",
                                    "```\n",
                                    "\n",
                                    "**Output of the code:**\n",
                                    "\n",
                                    "```\n",
                                    "Some stop words: ['de', 'la', 'que']\n",
                                    "                           message                 filtered_tokens\n",
                                    "0             Hola cómo estás?            [hola, cómo, estás, ?]\n",
                                    "1           I am fine and you?           [i, am, fine, and, you, ?]\n",
                                    "2  Estoy bien gracias por preguntar  [estoy, bien, gracias, preguntar]\n",
                                    "3    Good morning have a nice day  [good, morning, have, a, nice, day]\n",
                                    "4  Buenos días que tengas un buen día      [buenos, días, tengas]\n",
                                    "```\n",
                                    "\n",
                                    "**Explanation of changes and impact:**\n",
                                    "\n",
                                    "By changing `stopwords.words('english')` to `stopwords.words('spanish')`, the code now loads the Spanish stop words list.\n",
                                    "\n",
                                    "* You can see the printed \"Some stop words:\" now shows Spanish stop words like 'de', 'la', 'que'.\n",
                                    "* In the `filtered_tokens` output, observe how Spanish stop words have been removed from the Spanish sentences (e.g., \"Estoy bien gracias por preguntar\" becomes `[estoy, bien, gracias, preguntar]` where \"por\" is removed), while English stop words remain in the English sentences. This demonstrates the successful adaptation of stop word removal for a different language."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Debugging Stop Words Removal\n",
                                    "\n",
                                    "Next, we'll tackle a common hiccup you might encounter along the way. Your task is to diagnose and rectify a bug within a given block of code. The code aims to tokenize text from our dataset and remove stop words; however, it's not functioning as intended. Identifying and fixing such bug is crucial for ensuring the reliability of your text preprocessing pipeline\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "from nltk.corpus import stopwords\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x))\n",
                                    "\n",
                                    "# Define stop words\n",
                                    "stop_words = stopwords.words('english')\n",
                                    "\n",
                                    "# This line should remove stop words but it's including only stop words instead.\n",
                                    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word in stop_words])\n",
                                    "print(df['filtered_tokens'].head())\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "The bug in the provided code lies in the line where stop words are supposed to be removed. Currently, it's including *only* stop words instead of filtering them out.\n",
                                    "\n",
                                    "The problematic line is:\n",
                                    "`df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word in stop_words])`\n",
                                    "\n",
                                    "To fix this, you need to change `if word in stop_words` to `if word not in stop_words`. This will ensure that only words that are *not* in the stop words list are kept.\n",
                                    "\n",
                                    "Here's the corrected code:\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "from nltk.corpus import stopwords\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x))\n",
                                    "\n",
                                    "# Define stop words\n",
                                    "stop_words = stopwords.words('english')\n",
                                    "\n",
                                    "# This line should remove stop words, so it should exclude words IN stop_words\n",
                                    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
                                    "print(df['filtered_tokens'].head())\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Setting the Stage for Stop Words Removal in Text Data\n",
                                    "\n",
                                    "Building on your newfound understanding of removing stop words, this practice aims to solidify your skills. You will fill in the blank in a Python script that prepares SMS data by tokenizing messages and filtering out stop words. By completing this exercise, you will become adept at cleaning textual data, ensuring it is primed for extracting insights.\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "from nltk.corpus import stopwords\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x))\n",
                                    "\n",
                                    "# TODO: Define stop words\n",
                                    "\n",
                                    "# Remove stop words from tokens\n",
                                    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
                                    "print(df['filtered_tokens'].head())\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To complete the script and define stop words, you need to use `stopwords.words('english')`. This will provide a list of common English stop words from the NLTK library.\n",
                                    "\n",
                                    "Here's the completed line for the `TODO` section:\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "from nltk.corpus import stopwords\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x))\n",
                                    "\n",
                                    "# TODO: Define stop words\n",
                                    "stop_words = set(stopwords.words('english'))\n",
                                    "\n",
                                    "# Remove stop words from tokens\n",
                                    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
                                    "print(df['filtered_tokens'].head())\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Mastering Stop Words Removal\n",
                                    "\n",
                                    "Your task now involves implementing the code to identify and omit stop words using the NLTK library. This final practice piece within this unit serves as a testament to your acquired skills in text preprocessing, which are crucial for NLP tasks. Let’s dive in, reinforcing your abilities and ensuring a strong grasp of the concepts.\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "from nltk.corpus import stopwords\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x))\n",
                                    "\n",
                                    "# TODO: Define and set the stop words\n",
                                    "\n",
                                    "# TODO: Remove stop words from the previously tokenized messages\n",
                                    "\n",
                                    "# TODO: Print the first five entries of the cleaned (stop words removed) tokens\n",
                                    "\n",
                                    "\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
