{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 3 Tokenizing Text Data in NLP with Python and NLTK\n",
                                    "\n",
                                    "# Lesson Overview\n",
                                    "\n",
                                    "Welcome to our new lesson on **Tokenization**. Tokenization is a form of textual data cleaning typically performed in **Natural Language Processing** (NLP). It transforms raw text into a more usable format by breaking it down into individual words or tokens. Our lesson uses Python, **NLTK (the Natural Language Toolkit)**, and the **pandas** library for data handling. We'll apply tokenization to the **SMS Spam Collection dataset** that you're already familiar with. Let's get started!\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "# Understanding Tokenization\n",
                                    "\n",
                                    "Tokenization is the process of converting a sequence of text into separate pieces called tokens, usually words. When reading a text, our brain automatically identifies words without spaces, punctuation, or other separators, and understands the context. For computers, the process isn't that straightforward. They need to be taught to understand language structures, and that's when tokenization comes into play.\n",
                                    "\n",
                                    "Tokenization plays a key role in various NLP tasks, including text classification, language modeling, and sentiment analysis. For instance, if we train a machine learning model to classify spam messages, tokenization helps split a message into individual words. Each word becomes a feature for our model to learn from.\n",
                                    "\n",
                                    "One of the challenges with tokenization can be handling contractions. For example, the word \"don't\" might get tokenized into \"don\", \"'\", and \"t\" with a traditional whitespace tokenizer, which is incorrect. To mitigate this, we might need additional steps to handle contractions appropriately.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "# Exploring the NLTK Library\n",
                                    "\n",
                                    "NLTK, or Natural Language Toolkit, is a Python library that provides tools for handling human language data. It supplies easy-to-use interfaces to over 50 corpora and lexical resources, such as the `nltk.tokenize` package, which offers several tokenizer functions including `word_tokenize`, `sent_tokenize`, and more.\n",
                                    "\n",
                                    "Before using `word_tokenize` for the first time, you might need to download the `punkt_tab` package using `nltk.download('punkt_tab', quiet=True)`. The `quiet=True` parameter suppresses output messages during the download process. This package includes a pre-trained model that helps NLTK effectively split ordinary text into tokens. It's especially tuned for splitting sentences into words, taking into account various language peculiarities and structures.\n",
                                    "\n",
                                    "```python\n",
                                    "import nltk\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "\n",
                                    "# Ensure necessary packages are downloaded for tokenization\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "```\n",
                                    "\n",
                                    "Downloading `punkt_tab` is necessary because `word_tokenize` relies on this model to distinguish between different parts of a sentence, such as words and punctuation, using an unsupervised machine learning algorithm. Without it, `word_tokenize` won't work.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "# Implementing Tokenization Using NLTK\n",
                                    "\n",
                                    "As you already know, the SMS Spam Collection dataset can be loaded directly into a pandas DataFrame for convenient handling.\n",
                                    "\n",
                                    "As a first step, let's convert all messages to lowercase to ensure uniformity, because NLP models treat \"hello\" and \"Hello\" differently.\n",
                                    "\n",
                                    "```python\n",
                                    "# Convert all messages to lowercase for uniformity\n",
                                    "df['processed_message'] = df['message'].apply(lambda x: x.lower())\n",
                                    "```\n",
                                    "\n",
                                    "Then we'll implement tokenization using the function `nltk.tokenize.word_tokenize()`.\n",
                                    "\n",
                                    "```python\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['processed_message'].apply(lambda x: word_tokenize(x))\n",
                                    "print(df['tokens'].head())\n",
                                    "```\n",
                                    "\n",
                                    "The output of the above code will be:\n",
                                    "\n",
                                    "```\n",
                                    "0    [go, until, jurong, point, ,, crazy, .., avail...\n",
                                    "1             [ok, lar, ..., joking, wif, u, oni, ...]\n",
                                    "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
                                    "3    [u, dun, say, so, early, hor, ..., u, c, alrea...\n",
                                    "4    [nah, i, do, n't, think, he, goes, to, usf, ,,...\n",
                                    "Name: tokens, dtype: object\n",
                                    "```\n",
                                    "\n",
                                    "This output clearly demonstrates tokenization in action, where each message is split into a list of components or \"tokens\". This step is critical for preparing text data for further analysis in NLP tasks.\n",
                                    "\n",
                                    "---\n",
                                    "\n",
                                    "# Lesson Summary\n",
                                    "\n",
                                    "Today, we learned about the concept of **tokenization** and its importance in the context of Natural Language Processing. Utilizing the power of the `nltk` library in Python, we explored how tokens, the individual pieces of text, can be extracted from raw text data for further processing. Now, it's your turn to practice and refine your tokenization skills with a series of exercises. Remember, the more you practice, the better you become at working with Natural Language Processing tasks! Happy learning!\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Efficient Text Preprocessing with NLTK\n",
                                    "\n",
                                    "n this practice, we'll tokenize text data from the SMS Spam Collection using Python and NLTK, lowercasing for uniform analysis and splitting messages into tokens. Run the existing code directly to see tokenization in action.\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "\n",
                                    "# Ensure necessary packages are downloaded for tokenization\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for easy handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Convert all messages to lowercase for uniformity\n",
                                    "df['processed_message'] = df['message'].apply(lambda x: x.lower())\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['processed_message'].apply(lambda x: word_tokenize(x))\n",
                                    "print(df['tokens'].head())\n",
                                    "\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```markdown\n",
                                    "## Efficient Text Preprocessing with NLTK\n",
                                    "\n",
                                    "In this practice, we'll tokenize text data from the SMS Spam Collection using Python and NLTK, lowercasing for uniform analysis and splitting messages into tokens. Run the existing code directly to see tokenization in action.\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "\n",
                                    "# Ensure necessary packages are downloaded for tokenization\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for easy handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Convert all messages to lowercase for uniformity\n",
                                    "df['processed_message'] = df['message'].apply(lambda x: x.lower())\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['processed_message'].apply(lambda x: word_tokenize(x))\n",
                                    "print(df['tokens'].head())\n",
                                    "```\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Streamlining Text Processing with NLTK\n",
                                    "\n",
                                    "In this exercise, you'll be diving into a common task within text data preprocessing—tokenization. Specifically, you are presented with a script designed to tokenize SMS text messages, a crucial step for analyzing such data. However, this script isn't functioning as expected due to a missing necessary step. Your mission is to identify and correct it to ensure the text messages are effectively tokenized.\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "\n",
                                    "# Ensure necessary packages are downloaded for tokenization\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for easy handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Convert all messages to lowercase for uniformity\n",
                                    "df['processed_message'] = df['message'].apply(lambda x: x.lower())\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['processed_message'].apply(lambda x: word_tokenize(x))\n",
                                    "print(df['tokens'].head())\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "## Streamlining Text Processing with NLTK\n",
                                    "\n",
                                    "In this exercise, you'll be diving into a common task within text data preprocessing—tokenization. Specifically, you are presented with a script designed to tokenize SMS text messages, a crucial step for analyzing such data. However, this script isn't functioning as expected due to a missing necessary step. Your mission is to identify and correct it to ensure the text messages are effectively tokenized.\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "\n",
                                    "# Ensure necessary packages are downloaded for tokenization\n",
                                    "# Fix: The 'punkt_tab' package needs to be downloaded for word_tokenize to work.\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for easy handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Convert all messages to lowercase for uniformity\n",
                                    "df['processed_message'] = df['message'].apply(lambda x: x.lower())\n",
                                    "\n",
                                    "# Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['processed_message'].apply(lambda x: word_tokenize(x))\n",
                                    "print(df['tokens'].head())\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Implementing Tokenization Basics\n",
                                    "\n",
                                    "Having familiarized yourself with the basics of tokenization and explored NLTK's toolkit, now you are tasked with completing a partially written Python script. Fill in the blank to add the missing part of the code to tokenize the messages.\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "\n",
                                    "# Ensure necessary packages are downloaded for tokenization\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Convert all messages to lowercase for uniformity\n",
                                    "df['processed_message'] = df['message'].apply(lambda x: x.lower())\n",
                                    "\n",
                                    "# TODO: Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['processed_message'].apply(lambda x: _____________(x))\n",
                                    "print(df['tokens'].head())\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "\n",
                                    "# Ensure necessary packages are downloaded for tokenization\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for convenient handling\n",
                                    "df = pd.DataFrame(sms_spam['train'])\n",
                                    "\n",
                                    "# Convert all messages to lowercase for uniformity\n",
                                    "df['processed_message'] = df['message'].apply(lambda x: x.lower())\n",
                                    "\n",
                                    "# TODO: Tokenize the messages into individual words\n",
                                    "df['tokens'] = df['processed_message'].apply(lambda x: word_tokenize(x))\n",
                                    "print(df['tokens'].head())\n",
                                    "```\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Mastering Tokenization with NLTK\n",
                                    "\n",
                                    "After delving into the complexities of tokenization and the versatility of the NLTK library, you are now well-equipped to take on this final exercise. This task requires you to manually implement the code for tokenization of the SMS messages. This step is fundamental for any text analysis endeavor in natural language processing (NLP).\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "\n",
                                    "# Ensure necessary packages are downloaded for tokenization\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection', split='train')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for easy handling\n",
                                    "df = pd.DataFrame(sms_spam)\n",
                                    "\n",
                                    "# TODO: Tokenize the messages into individual words using NLTK's word_tokenize\n",
                                    "\n",
                                    "# TODO: Print the first 5 entries of the tokens to verify the tokenization process\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "from datasets import load_dataset\n",
                                    "import nltk\n",
                                    "from nltk.tokenize import word_tokenize\n",
                                    "\n",
                                    "# Ensure necessary packages are downloaded for tokenization\n",
                                    "nltk.download('punkt_tab', quiet=True)\n",
                                    "\n",
                                    "# Load the SMS Spam Collection dataset\n",
                                    "sms_spam = load_dataset('codesignal/sms-spam-collection', split='train')\n",
                                    "\n",
                                    "# Convert to pandas DataFrame for easy handling\n",
                                    "df = pd.DataFrame(sms_spam)\n",
                                    "\n",
                                    "# Rename the column from 'sms_message' to 'message'\n",
                                    "df = df.rename(columns={'sms_message': 'message'})\n",
                                    "\n",
                                    "# Convert messages to lowercase and then tokenize into individual words using NLTK's word_tokenize\n",
                                    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(x.lower()))\n",
                                    "\n",
                                    "# Print the first 5 entries of the tokens to verify the tokenization process\n",
                                    "print(df['tokens'].head())\n",
                                    "\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
