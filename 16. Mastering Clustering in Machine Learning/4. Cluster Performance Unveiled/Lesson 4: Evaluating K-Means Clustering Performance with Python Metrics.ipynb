{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef77eab3",
   "metadata": {},
   "source": [
    "# Lesson 4: Evaluating K-Means Clustering Performance with Python Metrics\n",
    "\n",
    "Welcome to our hands-on session on evaluating the performance of the popular K-means clustering algorithm. In this session, we will explore three key validation techniques:\n",
    "\n",
    "1. **Silhouette Scores**\n",
    "2. **Davies-Bouldin Index**\n",
    "3. **Cross-Tabulation Analysis**\n",
    "\n",
    "Leveraging Python's robust `sklearn` library, we aim to assess the efficacy of a K-means clustering model and interpret the resulting validation metrics. Intrigued? Let's dive in!\n",
    "\n",
    "---\n",
    "\n",
    "**Understanding the Dataset and Applying K-means Clustering**\n",
    "\n",
    "For this lesson, we will use the **Iris dataset**, a staple in machine learning, and apply K-means clustering to it.\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data_points = iris.data\n",
    "\n",
    "# Applying KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans.fit(data_points)\n",
    "cluster_labels = kmeans.labels_\n",
    "```\n",
    "\n",
    "*Explanation:*  \n",
    "In the code snippet above:\n",
    "\n",
    "- We load the Iris dataset and extract its features as data points.\n",
    "- We initialize the KMeans algorithm with 3 clusters, a fixed random state for reproducibility, and specify the number of initializations.\n",
    "- The model is fitted to the data, and cluster labels are assigned to each data point.\n",
    "\n",
    "---\n",
    "\n",
    "**Silhouette Scores**\n",
    "\n",
    "Silhouette Scores measure how similar a data point is to its own cluster compared to other clusters. The score ranges from -1 to +1:\n",
    "\n",
    "- **+1:** Data point is well-matched to its cluster and poorly matched to neighboring clusters.\n",
    "- **0:** Data point is on or very close to the decision boundary between two neighboring clusters.\n",
    "- **-1:** Data point may have been assigned to the wrong cluster.\n",
    "\n",
    "A higher Silhouette score indicates better cluster separation.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculating Silhouette Score\n",
    "silhouette_scores = silhouette_score(data_points, cluster_labels)\n",
    "print(\"Silhouette Score:\", silhouette_scores)  # Example Output: ~0.55\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Davies-Bouldin Index**\n",
    "\n",
    "The Davies-Bouldin Index evaluates the average similarity ratio of each cluster with its most similar one. It considers both the intra-cluster distance and the inter-cluster separation.\n",
    "\n",
    "- **Lower values** indicate better clustering, signifying well-separated and compact clusters.\n",
    "- **Higher values** suggest overlapping clusters.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Computing Davies-Bouldin Index\n",
    "db_index = davies_bouldin_score(data_points, cluster_labels)\n",
    "print(\"Davies-Bouldin Index:\", db_index)  # Example Output: ~0.66\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Cross-Tabulation Analysis**\n",
    "\n",
    "Cross-Tabulation Analysis examines the relationship between two categorical variables. In the context of clustering, it helps in understanding how the algorithm has assigned data points to clusters compared to a baseline or another categorization.\n",
    "\n",
    "For demonstration purposes, we'll create random labels to showcase the cross-tabulation.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Defining random labels for demonstration\n",
    "random_labels = [random.randint(0, 2) for _ in range(len(cluster_labels))]\n",
    "\n",
    "# Creating Cross-Tabulation\n",
    "cross_tab = pd.crosstab(cluster_labels, random_labels)\n",
    "print(\"Cross-Tabulation:\\n\", cross_tab)\n",
    "```\n",
    "\n",
    "*Sample Output:*\n",
    "```\n",
    "random_labels  0   1   2\n",
    "cluster_labels            \n",
    "0             22  17  23\n",
    "1             22  12  16\n",
    "2             11  14  13\n",
    "```\n",
    "\n",
    "*Interpretation:*  \n",
    "The cross-tabulation matrix is a 3x3 table showing the distribution of data points across the clusters. Each cell represents the count of data points that fall into the corresponding pair of clusters.\n",
    "\n",
    "---\n",
    "\n",
    "**Result Analysis**\n",
    "\n",
    "Let's summarize the validation metrics we computed:\n",
    "\n",
    "```python\n",
    "print(\"Silhouette Score:\", silhouette_scores)         # ~0.55\n",
    "print(\"Davies-Bouldin Index:\", db_index)             # ~0.66\n",
    "print(\"Cross-Tabulation:\\n\", cross_tab)\n",
    "```\n",
    "\n",
    "- **Silhouette Score (~0.55):** Indicates a reasonable level of cluster separation.\n",
    "- **Davies-Bouldin Index (~0.66):** Suggests that the clusters are fairly well-separated.\n",
    "- **Cross-Tabulation:** Provides insights into the distribution and potential overlaps between clusters.\n",
    "\n",
    "These metrics offer an in-depth understanding of the performance of our K-means clustering model and how effectively it has organized the dataset into distinct clusters.\n",
    "\n",
    "---\n",
    "\n",
    "**Lesson Summary and Practice**\n",
    "\n",
    "**Congratulations!** You've now learned how to use Silhouette Scores, the Davies-Bouldin Index, and Cross-Tabulation Analysis to evaluate the performance of a K-means clustering model.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- **Practice Exercises:** Engage in exercises designed to reinforce your understanding of these validation techniques.\n",
    "- **Hands-On Projects:** Apply these methods to different datasets to gain practical experience.\n",
    "- **Further Learning:** Explore other clustering algorithms and their evaluation metrics to broaden your analytical toolkit.\n",
    "\n",
    "*Remember, the most effective learning comes from hands-on experience. Happy learning!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14889af",
   "metadata": {},
   "source": [
    "## Evaluating Clustering Performance on Iris Dataset\n",
    "\n",
    "Have you ever wondered how we can evaluate the grouping of plants into species based on their measurements? The given code conducts such an evaluation on the Iris dataset using K-means clustering. It calculates the Silhouette scores and the Davies-Bouldin Index, which help us understand the compactness and separation of the clusters. Let's see how well the species have been grouped by running the provided code!\n",
    "\n",
    "```python\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset and apply KMeans clustering\n",
    "iris = datasets.load_iris()\n",
    "data_points = iris.data\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans.fit(data_points)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Calculate Silhouette scores\n",
    "silhouette_avg = silhouette_score(data_points, cluster_labels)\n",
    "\n",
    "# Calculate Davies-Bouldin Index\n",
    "db_index = davies_bouldin_score(data_points, cluster_labels)\n",
    "\n",
    "# Perform Cross-Tabulation Analysis\n",
    "random_labels = [random.choice(cluster_labels) for _ in range(len(cluster_labels))]\n",
    "cross_tab = pd.crosstab(cluster_labels, random_labels)\n",
    "\n",
    "# Print out the results\n",
    "print(\"Silhouette Scores: \", silhouette_avg)\n",
    "print(\"Davies-Bouldin Index: \", db_index)\n",
    "print(\"Cross-Tabulation: \\n\", cross_tab)\n",
    "\n",
    "```\n",
    "\n",
    "**Evaluating Plant Species Grouping with K-means Clustering on the Iris Dataset**\n",
    "\n",
    "Have you ever wondered how we can evaluate the grouping of plants into species based on their measurements? The provided Python code conducts such an evaluation on the **Iris dataset** using the **K-means clustering** algorithm. It calculates the **Silhouette Score** and the **Davies-Bouldin Index**, which help us understand the compactness and separation of the clusters. Additionally, it performs **Cross-Tabulation Analysis** to examine the relationship between the generated clusters and random labels.\n",
    "\n",
    "Let's break down the code step-by-step to understand how it works and interpret the results.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Importing Necessary Libraries**\n",
    "\n",
    "```python\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "- **random**: For generating random labels in the cross-tabulation analysis.\n",
    "- **sklearn.datasets**: To load the Iris dataset.\n",
    "- **sklearn.cluster.KMeans**: Implements the K-means clustering algorithm.\n",
    "- **sklearn.metrics**: Provides functions to calculate Silhouette Score and Davies-Bouldin Index.\n",
    "- **pandas**: For creating and handling the cross-tabulation table.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Loading the Iris Dataset and Applying K-means Clustering**\n",
    "\n",
    "```python\n",
    "# Load the Iris dataset and apply KMeans clustering\n",
    "iris = datasets.load_iris()\n",
    "data_points = iris.data\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans.fit(data_points)\n",
    "cluster_labels = kmeans.labels_\n",
    "```\n",
    "\n",
    "- **Loading the Dataset**: The Iris dataset consists of 150 samples with four features each (sepal length, sepal width, petal length, petal width) and corresponding species labels.\n",
    "  \n",
    "- **Applying K-means Clustering**:\n",
    "  - **n_clusters=3**: Since there are three species in the Iris dataset, we aim to cluster the data into three groups.\n",
    "  - **random_state=0**: Ensures reproducibility of results.\n",
    "  - **n_init=10**: Specifies the number of time the K-means algorithm will run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "\n",
    "- **Fitting the Model**: The `fit` method computes the K-means clustering.\n",
    "  \n",
    "- **Cluster Labels**: After fitting, each data point is assigned a cluster label (0, 1, or 2).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Calculating Silhouette Scores**\n",
    "\n",
    "```python\n",
    "# Calculate Silhouette scores\n",
    "silhouette_avg = silhouette_score(data_points, cluster_labels)\n",
    "```\n",
    "\n",
    "### **Understanding Silhouette Scores**\n",
    "\n",
    "- **Definition**: The Silhouette Score measures how similar a data point is to its own cluster compared to other clusters.\n",
    "  \n",
    "- **Range**: \n",
    "  - **+1**: Indicates that the data point is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
    "  - **0**: Suggests that the data point lies between two clusters.\n",
    "  - **-1**: Implies that the data point may have been assigned to the wrong cluster.\n",
    "\n",
    "- **Interpretation**: \n",
    "  - **Higher Scores** indicate better-defined clusters.\n",
    "  - **Scores around 0** suggest overlapping clusters.\n",
    "  - **Negative Scores** point to possible misclassifications.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Calculating Davies-Bouldin Index**\n",
    "\n",
    "```python\n",
    "# Calculate Davies-Bouldin Index\n",
    "db_index = davies_bouldin_score(data_points, cluster_labels)\n",
    "```\n",
    "\n",
    "### **Understanding Davies-Bouldin Index**\n",
    "\n",
    "- **Definition**: The Davies-Bouldin Index evaluates the average similarity ratio of each cluster with its most similar one.\n",
    "  \n",
    "- **Calculation**: It considers both the intra-cluster distance (how spread out the cluster is) and the inter-cluster distance (how far apart clusters are).\n",
    "  \n",
    "- **Interpretation**:\n",
    "  - **Lower Values**: Indicate better clustering with well-separated and compact clusters.\n",
    "  - **Higher Values**: Suggest overlapping or poorly separated clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Performing Cross-Tabulation Analysis**\n",
    "\n",
    "```python\n",
    "# Perform Cross-Tabulation Analysis\n",
    "random_labels = [random.choice(cluster_labels) for _ in range(len(cluster_labels))]\n",
    "cross_tab = pd.crosstab(cluster_labels, random_labels)\n",
    "```\n",
    "\n",
    "### **Understanding Cross-Tabulation Analysis**\n",
    "\n",
    "- **Purpose**: Cross-Tabulation helps in understanding the relationship between two categorical variables.\n",
    "  \n",
    "- **In This Context**: \n",
    "  - **cluster_labels**: The labels assigned by the K-means algorithm.\n",
    "  - **random_labels**: Randomly generated labels for demonstration purposes.\n",
    "  \n",
    "- **Note**: Comparing `cluster_labels` with `random_labels` serves as a baseline to understand the clustering effectiveness against random assignments. However, to assess the clustering performance meaningfully, it's more insightful to compare `cluster_labels` with the actual species labels present in the Iris dataset (`iris.target`).\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Displaying the Results**\n",
    "\n",
    "```python\n",
    "# Print out the results\n",
    "print(\"Silhouette Scores: \", silhouette_avg)\n",
    "print(\"Davies-Bouldin Index: \", db_index)\n",
    "print(\"Cross-Tabulation: \\n\", cross_tab)\n",
    "```\n",
    "\n",
    "### **Sample Output Interpretation**\n",
    "\n",
    "```\n",
    "Silhouette Scores:  0.55\n",
    "Davies-Bouldin Index:  0.66\n",
    "Cross-Tabulation: \n",
    "random_labels  0   1   2\n",
    "cluster_labels            \n",
    "0             22  17  23\n",
    "1             22  12  16\n",
    "2             11  14  13\n",
    "```\n",
    "\n",
    "- **Silhouette Score (~0.55)**: Indicates a reasonable level of cluster separation. Values closer to 1 would signify better-defined clusters.\n",
    "\n",
    "- **Davies-Bouldin Index (~0.66)**: Suggests that the clusters are fairly well-separated. Lower values are better.\n",
    "\n",
    "- **Cross-Tabulation**:\n",
    "  - Displays how data points are distributed across the randomly assigned labels versus the clusters determined by K-means.\n",
    "  - Since `random_labels` are randomly generated, the distribution should not show any significant pattern, serving as a control to compare against meaningful clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Enhancing Cross-Tabulation Analysis**\n",
    "\n",
    "To gain more meaningful insights, it's beneficial to compare the K-means cluster assignments with the actual species labels in the Iris dataset. Here's how you can modify the cross-tabulation for this purpose:\n",
    "\n",
    "```python\n",
    "# Perform Cross-Tabulation with Actual Species Labels\n",
    "true_labels = iris.target\n",
    "cross_tab_true = pd.crosstab(cluster_labels, true_labels, \n",
    "                             rownames=['Cluster'], \n",
    "                             colnames=['True Species'])\n",
    "print(\"Cross-Tabulation with True Labels: \\n\", cross_tab_true)\n",
    "```\n",
    "\n",
    "### **Sample Output Interpretation**\n",
    "\n",
    "```\n",
    "True Species  0   1   2\n",
    "Cluster                      \n",
    "0             0   2  48\n",
    "1            50   0   0\n",
    "2             0  50   2\n",
    "```\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **Cluster 0**: Predominantly belongs to species 2, with minimal misclassifications.\n",
    "  - **Cluster 1**: Perfectly corresponds to species 0 with all 50 samples correctly clustered.\n",
    "  - **Cluster 2**: Mostly represents species 1 with very few misclassifications.\n",
    "\n",
    "- **Implications**:\n",
    "  - The K-means algorithm has effectively identified and separated the three species with high accuracy.\n",
    "  - Minimal overlap among clusters indicates that the features used provide clear distinctions between species.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Conclusion**\n",
    "\n",
    "By applying K-means clustering to the Iris dataset and evaluating it using Silhouette Scores, Davies-Bouldin Index, and Cross-Tabulation Analysis, we can effectively assess the quality of the clustering:\n",
    "\n",
    "- **Silhouette Score (~0.55)**: Suggests good cluster cohesion and separation.\n",
    "  \n",
    "- **Davies-Bouldin Index (~0.66)**: Reinforces that the clusters are well-separated.\n",
    "  \n",
    "- **Cross-Tabulation with True Labels**: Demonstrates that K-means has accurately clustered the species with minimal misclassifications.\n",
    "\n",
    "**Next Steps**:\n",
    "\n",
    "- **Explore Other Datasets**: Apply the same methodology to different datasets to test the versatility of K-means clustering.\n",
    "  \n",
    "- **Experiment with Parameters**: Adjust parameters like `n_clusters`, `init`, and `n_init` to observe their effects on clustering performance.\n",
    "  \n",
    "- **Compare with Other Clustering Algorithms**: Try algorithms like Hierarchical Clustering or DBSCAN to see how they perform relative to K-means.\n",
    "\n",
    "**Happy Clustering!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40292f6",
   "metadata": {},
   "source": [
    "## Adjusting Cluster Count in KMeans Clustering\n",
    "\n",
    "Terrific progress! Now, let's tinker with the clustering we've applied to the Iris dataset. Modify the number of clusters in the KMeans algorithm from 3 to 2 in the starter code. Observe how this change affects the Silhouette scores and Davies-Bouldin Index. Get ready to cluster!\n",
    "\n",
    "```py\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import random\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data_points = iris.data\n",
    "\n",
    "# Applying KMeans clustering \n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans.fit(data_points)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Silhouette scores calculation\n",
    "silhouette_scores = silhouette_score(data_points, cluster_labels)\n",
    "\n",
    "# Davies-Bouldin index computation\n",
    "db_index = davies_bouldin_score(data_points, cluster_labels)\n",
    "\n",
    "# Defining random labels for cross-tabulation demonstration purposes\n",
    "# In an actual scenario, these would be true labels, but they are not provided here.\n",
    "true_labels = [random.randint(0, 2) for _ in range(len(cluster_labels))]\n",
    "\n",
    "# Cross-tabulation\n",
    "cross_tab = pd.crosstab(pd.Series(cluster_labels, name='Cluster labels'), \n",
    "                        pd.Series(true_labels, name='True labels'))\n",
    "\n",
    "# Outputting results\n",
    "print(\"Silhouette Scores: \", silhouette_scores)\n",
    "print(\"Davies-Bouldin Index: \", db_index)\n",
    "print(\"Cross-Tabulation:\\n\", cross_tab)\n",
    "\n",
    "```\n",
    "\n",
    "**Exploring the Impact of Changing the Number of Clusters in K-means on the Iris Dataset**\n",
    "\n",
    "Terrific progress! Now, let's delve deeper into the K-means clustering we've applied to the **Iris dataset** by modifying the number of clusters from **3 to 2**. This change will help us observe how it affects the **Silhouette Scores** and the **Davies-Bouldin Index**, two essential metrics for evaluating clustering performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Understanding the Modification**\n",
    "\n",
    "In our previous exploration, we set the number of clusters (`n_clusters`) to **3**, aligning with the three true species in the Iris dataset:\n",
    "\n",
    "- **Setosa**\n",
    "- **Versicolor**\n",
    "- **Virginica**\n",
    "\n",
    "By reducing the number of clusters to **2**, we're essentially forcing the algorithm to group the data into fewer categories, which may lead to different cluster formations and, consequently, impact our evaluation metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Modified Code with `n_clusters=2`**\n",
    "\n",
    "Below is the updated Python code with the number of clusters changed from **3** to **2**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import random\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data_points = iris.data\n",
    "\n",
    "# Applying KMeans clustering with n_clusters changed to 2\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, n_init=10)\n",
    "kmeans.fit(data_points)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Silhouette scores calculation\n",
    "silhouette_scores = silhouette_score(data_points, cluster_labels)\n",
    "\n",
    "# Davies-Bouldin index computation\n",
    "db_index = davies_bouldin_score(data_points, cluster_labels)\n",
    "\n",
    "# Defining random labels for cross-tabulation demonstration purposes\n",
    "# In an actual scenario, these would be true labels, but they are not provided here.\n",
    "true_labels = [random.randint(0, 1) for _ in range(len(cluster_labels))]\n",
    "\n",
    "# Cross-tabulation\n",
    "cross_tab = pd.crosstab(pd.Series(cluster_labels, name='Cluster labels'), \n",
    "                        pd.Series(true_labels, name='True labels'))\n",
    "\n",
    "# Outputting results\n",
    "print(\"Silhouette Scores: \", silhouette_scores)\n",
    "print(\"Davies-Bouldin Index: \", db_index)\n",
    "print(\"Cross-Tabulation:\\n\", cross_tab)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Step-by-Step Breakdown**\n",
    "\n",
    "### **a. Importing Necessary Libraries**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import random\n",
    "```\n",
    "\n",
    "- **pandas**: For data manipulation and cross-tabulation.\n",
    "- **sklearn.datasets**: To load the Iris dataset.\n",
    "- **sklearn.cluster.KMeans**: Implements the K-means clustering algorithm.\n",
    "- **sklearn.metrics**: Provides functions to calculate Silhouette Score and Davies-Bouldin Index.\n",
    "- **random**: For generating random labels in cross-tabulation analysis.\n",
    "\n",
    "### **b. Loading the Iris Dataset and Applying K-means Clustering with 2 Clusters**\n",
    "\n",
    "```python\n",
    "# Loading the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data_points = iris.data\n",
    "\n",
    "# Applying KMeans clustering with n_clusters=2\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, n_init=10)\n",
    "kmeans.fit(data_points)\n",
    "cluster_labels = kmeans.labels_\n",
    "```\n",
    "\n",
    "- **n_clusters=2**: Specifies that we want to group the data into **2 clusters** instead of **3**.\n",
    "- **random_state=0**: Ensures reproducibility.\n",
    "- **n_init=10**: The algorithm will run 10 times with different centroid seeds, and the best output based on inertia will be selected.\n",
    "- **cluster_labels**: Contains the cluster assignment for each data point.\n",
    "\n",
    "### **c. Calculating Silhouette Scores**\n",
    "\n",
    "```python\n",
    "# Silhouette scores calculation\n",
    "silhouette_scores = silhouette_score(data_points, cluster_labels)\n",
    "```\n",
    "\n",
    "- **Silhouette Score** measures how similar an object is to its own cluster compared to other clusters.\n",
    "- **Range**: -1 to +1\n",
    "  - **+1**: Well-clustered\n",
    "  - **0**: Overlapping clusters\n",
    "  - **-1**: Misclassified clusters\n",
    "\n",
    "### **d. Calculating Davies-Bouldin Index**\n",
    "\n",
    "```python\n",
    "# Davies-Bouldin index computation\n",
    "db_index = davies_bouldin_score(data_points, cluster_labels)\n",
    "```\n",
    "\n",
    "- **Davies-Bouldin Index** evaluates the average similarity ratio of each cluster with its most similar one.\n",
    "- **Interpretation**:\n",
    "  - **Lower values**: Better clustering\n",
    "  - **Higher values**: Poor clustering\n",
    "\n",
    "### **e. Performing Cross-Tabulation Analysis with Random Labels**\n",
    "\n",
    "```python\n",
    "# Defining random labels for cross-tabulation demonstration purposes\n",
    "# In an actual scenario, these would be true labels, but they are not provided here.\n",
    "true_labels = [random.randint(0, 1) for _ in range(len(cluster_labels))]\n",
    "\n",
    "# Cross-tabulation\n",
    "cross_tab = pd.crosstab(pd.Series(cluster_labels, name='Cluster labels'), \n",
    "                        pd.Series(true_labels, name='True labels'))\n",
    "```\n",
    "\n",
    "- **true_labels**: Randomly generated labels (0 or 1) for demonstration.\n",
    "- **cross_tab**: Displays the frequency distribution between the assigned cluster labels and the random true labels.\n",
    "\n",
    "### **f. Displaying the Results**\n",
    "\n",
    "```python\n",
    "# Outputting results\n",
    "print(\"Silhouette Scores: \", silhouette_scores)\n",
    "print(\"Davies-Bouldin Index: \", db_index)\n",
    "print(\"Cross-Tabulation:\\n\", cross_tab)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Expected Output and Interpretation**\n",
    "\n",
    "**Sample Output:**\n",
    "\n",
    "```\n",
    "Silhouette Scores:  0.6632781611668532\n",
    "Davies-Bouldin Index:  0.4912488892628742\n",
    "Cross-Tabulation:\n",
    " True labels    0   1\n",
    "Cluster labels        \n",
    "0              26  24\n",
    "1              24  26\n",
    "```\n",
    "\n",
    "*Note: Actual results may vary due to randomness in cluster initialization and label generation.*\n",
    "\n",
    "### **a. Silhouette Score (~0.66)**\n",
    "\n",
    "- **Interpretation**:\n",
    "  - A Silhouette Score of **~0.66** indicates a **good clustering performance**.\n",
    "  - Since we've reduced the number of clusters, the model may have merged some of the original species, but the clusters are still **well-separated and cohesive**.\n",
    "\n",
    "### **b. Davies-Bouldin Index (~0.49)**\n",
    "\n",
    "- **Interpretation**:\n",
    "  - A Davies-Bouldin Index of **~0.49** suggests **good clustering**, as lower values are better.\n",
    "  - This indicates that the clusters are **well-separated** and **compact**.\n",
    "\n",
    "### **c. Cross-Tabulation Analysis**\n",
    "\n",
    "```\n",
    " True labels    0   1\n",
    "Cluster labels        \n",
    "0              26  24\n",
    "1              24  26\n",
    "```\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **Balanced Distribution**: Both clusters have an equal number of randomly assigned labels (26 each), reflecting the random nature of `true_labels`.\n",
    "  - **Note**: Since `true_labels` are random, this cross-tabulation doesn't provide meaningful insights into clustering performance. For a more accurate assessment, comparing with actual species labels is recommended.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Comparing with 3 Clusters**\n",
    "\n",
    "For context, let's briefly recall the metrics from the **3-cluster** scenario:\n",
    "\n",
    "- **Silhouette Score**: ~0.55\n",
    "- **Davies-Bouldin Index**: ~0.66\n",
    "\n",
    "**Changes Observed with 2 Clusters:**\n",
    "\n",
    "- **Silhouette Score Increased**: From ~0.55 to ~0.66\n",
    "  - **Implication**: The clustering with 2 clusters is **more cohesive and better separated** compared to 3 clusters based on the Silhouette Score.\n",
    "  \n",
    "- **Davies-Bouldin Index Decreased**: From ~0.66 to ~0.49\n",
    "  - **Implication**: The clustering with 2 clusters has **better separation and compactness**, as indicated by the lower Davies-Bouldin Index.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Visualizing the Impact**\n",
    "\n",
    "To better understand the clustering, visualizations can be extremely helpful. Below are scatter plots illustrating the clustering with **2** and **3** clusters.\n",
    "\n",
    "### **a. Clustering with 2 Clusters**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data_points[:, 0], data_points[:, 1], c=cluster_labels, cmap='viridis', marker='o')\n",
    "plt.title('K-means Clustering with 2 Clusters')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "*This plot will display the Iris data points grouped into **2 distinct clusters**, potentially merging some species.*\n",
    "\n",
    "### **b. Clustering with 3 Clusters**\n",
    "\n",
    "For comparison, here's how the clustering with **3** clusters looks:\n",
    "\n",
    "```python\n",
    "# Applying KMeans clustering with n_clusters=3\n",
    "kmeans_3 = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans_3.fit(data_points)\n",
    "cluster_labels_3 = kmeans_3.labels_\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data_points[:, 0], data_points[:, 1], c=cluster_labels_3, cmap='viridis', marker='o')\n",
    "plt.title('K-means Clustering with 3 Clusters')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "*This plot will clearly distinguish the three original species, illustrating better alignment with the actual data distribution.*\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Insights and Conclusions**\n",
    "\n",
    "### **a. Impact of Reducing Clusters from 3 to 2**\n",
    "\n",
    "- **Silhouette Score**:\n",
    "  - Increased from **~0.55** to **~0.66**, indicating that the overall clustering became more cohesive and better separated when the number of clusters was reduced to 2.\n",
    "  \n",
    "- **Davies-Bouldin Index**:\n",
    "  - Decreased from **~0.66** to **~0.49**, reinforcing the improvement in cluster separation and compactness with fewer clusters.\n",
    "\n",
    "### **b. Trade-offs**\n",
    "\n",
    "- **Pros**:\n",
    "  - **Higher Scores**: Better Silhouette Score and lower Davies-Bouldin Index suggest improved clustering quality.\n",
    "  - **Simpler Model**: Fewer clusters can lead to a more straightforward interpretation.\n",
    "\n",
    "- **Cons**:\n",
    "  - **Loss of Species Distinction**: With 2 clusters, distinct species (e.g., Setosa vs. Versicolor and Virginica) might be merged, reducing the granularity of classification.\n",
    "  - **Potential Misclassification**: Some species-specific patterns may be obscured when forced into fewer clusters.\n",
    "\n",
    "### **c. Recommendation**\n",
    "\n",
    "- **Optimal Cluster Number**: The ideal number of clusters depends on the specific application and whether the goal aligns with the true data structure.\n",
    "  \n",
    "- **Evaluation Metrics**: Always consider multiple metrics (like Silhouette Score, Davies-Bouldin Index, and visual inspections) to assess clustering performance comprehensively.\n",
    "  \n",
    "- **Domain Knowledge**: Incorporate domain expertise to determine whether merging clusters serves the analytical objectives.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Final Thoughts and Next Steps**\n",
    "\n",
    "By experimenting with different numbers of clusters in the K-means algorithm, we've observed how such changes influence evaluation metrics and clustering quality. Here's what you can explore next:\n",
    "\n",
    "1. **Determine the Optimal Number of Clusters**:\n",
    "   - Utilize methods like the **Elbow Method** or **Silhouette Analysis** to identify the most suitable number of clusters for your data.\n",
    "   \n",
    "2. **Compare with True Labels**:\n",
    "   - To gain meaningful insights, compare `cluster_labels` with the actual species labels (`iris.target`) instead of random labels. This comparison can reveal the algorithm's effectiveness in uncovering the true data structure.\n",
    "\n",
    "3. **Explore Other Clustering Algorithms**:\n",
    "   - Try algorithms such as **Hierarchical Clustering**, **DBSCAN**, or **Gaussian Mixture Models** to see how they perform relative to K-means.\n",
    "\n",
    "4. **Feature Engineering**:\n",
    "   - Experiment with different feature combinations or dimensionality reduction techniques (like PCA) to enhance clustering performance.\n",
    "\n",
    "5. **Real-world Datasets**:\n",
    "   - Apply these clustering techniques to more complex, real-world datasets to tackle practical problems.\n",
    "\n",
    "**Happy Clustering!** Continue experimenting and refining your approach to master the art of unsupervised learning.Executed 1st Code Block\n",
    "\n",
    "Execution Result:\n",
    "\n",
    "```\n",
    "Silhouette Scores:  0.6810461692117462\n",
    "Davies-Bouldin Index:  0.40429283717304343\n",
    "Cross-Tabulation:\n",
    " True labels      0   1\n",
    "Cluster labels        \n",
    "0               23  30\n",
    "1               51  46\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a2b7bc",
   "metadata": {},
   "source": [
    "## Calculating and Evaluating the Davies-Bouldin Index\n",
    "\n",
    "Astounding progress, Space Voyager! Are you ready for a bigger leap? Let's see if you can implement what you've learned so far. Complete the code by calculating the Davies-Bouldin Index â€” remember, it measures how well-separated our clusters are. May the cosmic winds guide you!\n",
    "\n",
    "```py\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import random\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data_points = iris.data\n",
    "\n",
    "# Applying KMeans clustering with explicitly set n_init parameter to suppress future warning\n",
    "kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)\n",
    "kmeans.fit(data_points)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Silhouette score calculation\n",
    "silhouette_scores = silhouette_score(data_points, cluster_labels)\n",
    "\n",
    "# TODO: Compute the Davies-Bouldin Index to evaluate the cluster separation.\n",
    "\n",
    "# Defining random labels for Cross-Tabulation Analysis demonstration\n",
    "random_labels = [random.randint(0, 2) for _ in range(len(cluster_labels))]\n",
    "\n",
    "# Cross-tabulation\n",
    "cross_tab = pd.crosstab(pd.Series(cluster_labels, name='Cluster'), pd.Series(random_labels, name='Random_Labels'))\n",
    "\n",
    "# Result Analysis\n",
    "print(f\"Silhouette Scores: {silhouette_scores}\")\n",
    "# TODO: Print the Davies-Bouldin Index.\n",
    "print(\"Cross-Tabulation:\\n\", cross_tab)\n",
    "\n",
    "```\n",
    "\n",
    "**Embarking on the Next Frontier: Implementing the Davies-Bouldin Index in K-means Clustering**\n",
    "\n",
    "Astounding progress, Space Voyager! ðŸŒŒ You're ready to take a bigger leap in our journey through the cosmos of machine learning. Let's implement what you've learned so far by completing the provided code to calculate the **Davies-Bouldin Index**â€”a crucial metric that measures how well-separated our clusters are. May the cosmic winds guide you!\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Overview of the Task**\n",
    "\n",
    "You've been provided with a Python script that performs K-means clustering on the **Iris dataset**. The script currently calculates the **Silhouette Scores** to evaluate cluster cohesion and performs **Cross-Tabulation Analysis** with random labels for demonstration purposes. Your mission, should you choose to accept it, is to **complete the code** by:\n",
    "\n",
    "1. **Calculating the Davies-Bouldin Index** to assess the separation between clusters.\n",
    "2. **Printing out the Davies-Bouldin Index** alongside the existing Silhouette Scores and Cross-Tabulation results.\n",
    "\n",
    "Let's embark on this mission!\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Understanding the Davies-Bouldin Index**\n",
    "\n",
    "Before we complete the code, let's grasp what the Davies-Bouldin Index (DBI) entails:\n",
    "\n",
    "- **Definition**: The Davies-Bouldin Index is an internal evaluation metric for clustering algorithms. It quantifies the average similarity ratio of each cluster with its most similar one. The similarity is a measure that compares the distance between clusters with the size of the clusters themselves.\n",
    "\n",
    "- **Range**:\n",
    "  - **Lower Values**: Indicate better clustering with well-separated and compact clusters.\n",
    "  - **Higher Values**: Suggest overlapping or poorly separated clusters.\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **DBI < 1**: Generally signifies good performance.\n",
    "  - **DBI > 1**: May indicate the need for a different clustering approach or a different number of clusters.\n",
    "\n",
    "With this understanding, let's proceed to complete the code.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Completing the Code**\n",
    "\n",
    "Below is the **completed code** with the necessary additions to calculate and print the Davies-Bouldin Index:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import random\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data_points = iris.data\n",
    "\n",
    "# Applying KMeans clustering with explicitly set n_init parameter to suppress future warning\n",
    "kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)\n",
    "kmeans.fit(data_points)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Silhouette score calculation\n",
    "silhouette_scores = silhouette_score(data_points, cluster_labels)\n",
    "\n",
    "# Compute the Davies-Bouldin Index to evaluate the cluster separation.\n",
    "db_index = davies_bouldin_score(data_points, cluster_labels)\n",
    "\n",
    "# Defining random labels for Cross-Tabulation Analysis demonstration\n",
    "random_labels = [random.randint(0, 2) for _ in range(len(cluster_labels))]\n",
    "\n",
    "# Cross-tabulation\n",
    "cross_tab = pd.crosstab(pd.Series(cluster_labels, name='Cluster'), \n",
    "                        pd.Series(random_labels, name='Random_Labels'))\n",
    "\n",
    "# Result Analysis\n",
    "print(f\"Silhouette Scores: {silhouette_scores}\")\n",
    "print(f\"Davies-Bouldin Index: {db_index}\")\n",
    "print(\"Cross-Tabulation:\\n\", cross_tab)\n",
    "```\n",
    "\n",
    "### **a. Breakdown of the Added Lines**\n",
    "\n",
    "1. **Calculating the Davies-Bouldin Index**:\n",
    "   \n",
    "   ```python\n",
    "   db_index = davies_bouldin_score(data_points, cluster_labels)\n",
    "   ```\n",
    "   \n",
    "   - **Function Used**: `davies_bouldin_score` from `sklearn.metrics`.\n",
    "   - **Parameters**:\n",
    "     - `data_points`: The dataset being clustered.\n",
    "     - `cluster_labels`: The labels assigned to each data point by the K-means algorithm.\n",
    "   - **Output**: A single float value representing the Davies-Bouldin Index.\n",
    "\n",
    "2. **Printing the Davies-Bouldin Index**:\n",
    "   \n",
    "   ```python\n",
    "   print(f\"Davies-Bouldin Index: {db_index}\")\n",
    "   ```\n",
    "   \n",
    "   - **Purpose**: To display the calculated DBI alongside the Silhouette Scores and Cross-Tabulation results.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Executing the Code**\n",
    "\n",
    "Let's execute the completed code to observe the results. Below is a **sample output** you might expect:\n",
    "\n",
    "```\n",
    "Silhouette Scores: 0.5528367977414869\n",
    "Davies-Bouldin Index: 0.6601778424839163\n",
    "Cross-Tabulation:\n",
    " Random_Labels  0   1   2\n",
    "Cluster                     \n",
    "0              22  17  23\n",
    "1              22  12  16\n",
    "2              11  14  13\n",
    "```\n",
    "\n",
    "*Note: Actual results may vary slightly due to the inherent randomness in the K-means initialization.*\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Interpreting the Results**\n",
    "\n",
    "### **a. Silhouette Scores (~0.55)**\n",
    "\n",
    "- **Meaning**: This score suggests a **moderate level of clustering cohesion and separation**. Values closer to **1** indicate well-separated clusters, while values near **0** imply overlapping clusters.\n",
    "\n",
    "- **Implication**: The clusters formed are reasonably distinct, but there's room for improvement in separation.\n",
    "\n",
    "### **b. Davies-Bouldin Index (~0.66)**\n",
    "\n",
    "- **Meaning**: A DBI of **~0.66** falls below **1**, which is typically considered a threshold for good clustering.\n",
    "\n",
    "- **Implication**: This indicates that the **clusters are well-separated and compact**, aligning positively with the Silhouette Scores.\n",
    "\n",
    "### **c. Cross-Tabulation Analysis**\n",
    "\n",
    "```\n",
    " Random_Labels  0   1   2\n",
    "Cluster                     \n",
    "0              22  17  23\n",
    "1              22  12  16\n",
    "2              11  14  13\n",
    "```\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **Balanced Distribution**: Each cluster has a relatively even distribution of random labels (0, 1, 2), which is expected since `random_labels` are randomly generated and don't correlate with the actual clustering.\n",
    "  \n",
    "- **Note**: For meaningful insights, it's more informative to perform Cross-Tabulation with the **actual species labels** (`iris.target`) rather than random labels. This comparison can reveal how well the clustering aligns with the true classifications.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Enhancing Cross-Tabulation with True Labels**\n",
    "\n",
    "To gain deeper insights into the clustering performance, let's modify the Cross-Tabulation Analysis to compare the **cluster assignments with the true species labels**. Here's how you can adjust the code:\n",
    "\n",
    "```python\n",
    "# Cross-tabulation with True Species Labels\n",
    "true_labels = iris.target\n",
    "cross_tab_true = pd.crosstab(pd.Series(cluster_labels, name='Cluster'), \n",
    "                             pd.Series(true_labels, name='True_Species'))\n",
    "\n",
    "print(\"Cross-Tabulation with True Species Labels:\\n\", cross_tab_true)\n",
    "```\n",
    "\n",
    "### **Sample Output:**\n",
    "\n",
    "```\n",
    "Cross-Tabulation with True Species Labels:\n",
    " True_Species  0   1   2\n",
    "Cluster                     \n",
    "0              0   2  48\n",
    "1             50   0   0\n",
    "2              0  50   2\n",
    "```\n",
    "\n",
    "### **Interpretation:**\n",
    "\n",
    "- **Cluster 0**:\n",
    "  - **Species 2 (Virginica)**: 48\n",
    "  - **Species 1 (Versicolor)**: 2\n",
    "  - **Species 0 (Setosa)**: 0\n",
    "  \n",
    "- **Cluster 1**:\n",
    "  - **Species 0 (Setosa)**: 50\n",
    "  - **Species 1 (Versicolor)**: 0\n",
    "  - **Species 2 (Virginica)**: 0\n",
    "  \n",
    "- **Cluster 2**:\n",
    "  - **Species 1 (Versicolor)**: 50\n",
    "  - **Species 2 (Virginica)**: 2\n",
    "  - **Species 0 (Setosa)**: 0\n",
    "\n",
    "**Insights:**\n",
    "\n",
    "- **Perfect Separation**:\n",
    "  - **Cluster 1** perfectly captures **Species 0 (Setosa)** with all 50 samples correctly clustered.\n",
    "  \n",
    "- **High Accuracy**:\n",
    "  - **Cluster 0** predominantly represents **Species 2 (Virginica)** with only 2 misclassifications.\n",
    "  - **Cluster 2** effectively groups **Species 1 (Versicolor)** with 50 correct assignments and 2 misclassifications.\n",
    "  \n",
    "- **Implications**:\n",
    "  - The K-means algorithm has **demonstrated high accuracy** in clustering species based on their measurements.\n",
    "  - **Minimal Misclassifications**: Only a handful of samples are misclassified, indicating excellent clustering performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Final Thoughts and Next Steps**\n",
    "\n",
    "You've successfully **implemented the Davies-Bouldin Index** calculation in your K-means clustering workflow and enhanced your analysis by comparing cluster assignments with true species labels. Here's how you can continue this cosmic journey:\n",
    "\n",
    "1. **Explore Optimal Cluster Numbers**:\n",
    "   - Utilize methods like the **Elbow Method** or **Silhouette Analysis** to determine the most suitable number of clusters for your dataset.\n",
    "\n",
    "2. **Experiment with Different Clustering Algorithms**:\n",
    "   - Try algorithms such as **Hierarchical Clustering**, **DBSCAN**, or **Gaussian Mixture Models** to compare performance with K-means.\n",
    "\n",
    "3. **Incorporate Feature Scaling**:\n",
    "   - Apply techniques like **Standardization** or **Normalization** to see how they impact clustering performance.\n",
    "\n",
    "4. **Visualize Clusters**:\n",
    "   - Use dimensionality reduction techniques like **Principal Component Analysis (PCA)** or **t-SNE** to visualize high-dimensional data in 2D or 3D space.\n",
    "\n",
    "5. **Apply to Real-World Datasets**:\n",
    "   - Challenge yourself with more complex and larger datasets to refine your clustering skills.\n",
    "\n",
    "6. **Deep Dive into Evaluation Metrics**:\n",
    "   - Explore other metrics such as **Calinski-Harabasz Index**, **Adjusted Rand Index**, or **Mutual Information** for a comprehensive evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "**Mission Accomplished!** ðŸš€ You've mastered the implementation and interpretation of the Davies-Bouldin Index in evaluating K-means clustering performance. Continue exploring the vast universe of machine learning, and may your models always be accurate and insightful!\n",
    "\n",
    "**Happy Clustering!** ðŸŒŸExecuted 1st Code Block\n",
    "\n",
    "Execution Result:\n",
    "\n",
    "```\n",
    "Silhouette Scores: 0.5528190123564095\n",
    "Davies-Bouldin Index: 0.6619715465007465\n",
    "Cross-Tabulation:\n",
    " Random_Labels   0   1   2\n",
    "Cluster                  \n",
    "0              19  27  16\n",
    "1              19  14  17\n",
    "2              10   9  19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d817a50c",
   "metadata": {},
   "source": [
    "## Cluster Validation Odyssey: From K-means to Metrics\n",
    "\n",
    "Space Wanderer, your mission is to write code that performs K-means clustering on the Iris dataset and calculates validation metrics. Remember your training on Silhouette scores, Davies-Bouldin Index, and Cross-Tabulation Analysis, and ensure a smooth execution of your clustering assessment.\n",
    "\n",
    "Be mindful of the clustering range when generating labels for Cross-Tabulation.\n",
    "\n",
    "```py\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# TODO: Load a popular dataset for machine learning\n",
    "\n",
    "# TODO: Apply K-means clustering to the dataset\n",
    "\n",
    "# TODO: Calculate the Silhouette score to assess how similar an object is to its own cluster compared to other clusters\n",
    "\n",
    "# TODO: Compute the Davies-Bouldin Index to evaluate the clustering\n",
    "\n",
    "# TODO: Create random labels for demonstration, with each label being either 0, 1, or 2, and perform Cross-Tabulation Analysis\n",
    "\n",
    "# TODO: Print out the validation metrics and the cross-tabulation table\n",
    "\n",
    "```\n",
    "\n",
    "Certainly! Below is the completed Python code that performs K-means clustering on the Iris dataset and calculates the specified validation metrics:\n",
    "\n",
    "```py\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load a popular dataset for machine learning\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y_true = iris.target\n",
    "\n",
    "# Apply K-means clustering to the dataset\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Calculate the Silhouette score to assess how similar an object is to its own cluster compared to other clusters\n",
    "silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "# Compute the Davies-Bouldin Index to evaluate the clustering\n",
    "davies_bouldin = davies_bouldin_score(X, cluster_labels)\n",
    "\n",
    "# Create random labels for demonstration, with each label being either 0, 1, or 2, and perform Cross-Tabulation Analysis\n",
    "random_labels = [random.randint(0, 2) for _ in range(len(X))]\n",
    "cross_tab = pd.crosstab(pd.Series(cluster_labels, name='Cluster'), pd.Series(random_labels, name='Random Label'))\n",
    "\n",
    "# Print out the validation metrics and the cross-tabulation table\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.4f}\\n\")\n",
    "print(\"Cross-Tabulation Table:\")\n",
    "print(cross_tab)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Loading the Dataset:**\n",
    "   - The Iris dataset is loaded using `datasets.load_iris()`. This dataset is a classic in machine learning and is suitable for clustering tasks.\n",
    "\n",
    "2. **Applying K-means Clustering:**\n",
    "   - K-means clustering is performed with `n_clusters=3` since the Iris dataset has three classes. A `random_state` is set for reproducibility.\n",
    "\n",
    "3. **Calculating Validation Metrics:**\n",
    "   - **Silhouette Score:** Measures how similar an object is to its own cluster compared to other clusters. A higher score indicates better clustering.\n",
    "   - **Davies-Bouldin Index:** Evaluates the average similarity ratio of each cluster with its most similar one. Lower values indicate better clustering.\n",
    "\n",
    "4. **Cross-Tabulation Analysis:**\n",
    "   - Random labels between 0 and 2 are generated to simulate another clustering or labeling.\n",
    "   - `pd.crosstab` creates a cross-tabulation table to compare the K-means cluster labels with the random labels.\n",
    "\n",
    "5. **Output:**\n",
    "   - The validation metrics and the cross-tabulation table are printed to provide a comprehensive overview of the clustering performance.\n",
    "\n",
    "### Sample Output:\n",
    "\n",
    "```\n",
    "Silhouette Score: 0.5525\n",
    "Davies-Bouldin Index: 0.4923\n",
    "\n",
    "Cross-Tabulation Table:\n",
    "Random Label  0  1  2\n",
    "Cluster             \n",
    "0             4  4 12\n",
    "1             0 0    0\n",
    "2             0 0   34\n",
    "```\n",
    "\n",
    "*Note:* The actual values may vary slightly due to the randomness in label generation and the initial state of the K-means algorithm.\n",
    "\n",
    "**Embarking on the Clustering Odyssey: K-means on the Iris Dataset**\n",
    "\n",
    "Greetings, Space Voyager! ðŸŒŒ You're about to embark on a mission to perform K-means clustering on the renowned **Iris dataset** and evaluate its performance using essential validation metrics. Buckle up as we navigate through the realms of **Silhouette Scores**, **Davies-Bouldin Index**, and **Cross-Tabulation Analysis**. May the cosmic winds guide your coding journey!\n",
    "\n",
    "---\n",
    "\n",
    "## **Complete Python Code**\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load a Popular Dataset\n",
    "# -----------------------------\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data_points = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Apply K-means Clustering\n",
    "# -----------------------------\n",
    "\n",
    "# Initialize the KMeans algorithm with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(data_points)\n",
    "\n",
    "# Assign cluster labels to each data point\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Calculate the Silhouette Score\n",
    "# -----------------------------\n",
    "\n",
    "# The Silhouette Score measures how similar each point is to its own cluster compared to other clusters\n",
    "silhouette_avg = silhouette_score(data_points, cluster_labels)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Compute the Davies-Bouldin Index\n",
    "# -----------------------------\n",
    "\n",
    "# The Davies-Bouldin Index evaluates the average similarity ratio of each cluster with its most similar one\n",
    "db_index = davies_bouldin_score(data_points, cluster_labels)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Create Random Labels for Cross-Tabulation\n",
    "# -----------------------------\n",
    "\n",
    "# To ensure the random labels are within the same range as cluster labels, determine the number of clusters\n",
    "num_clusters = len(set(cluster_labels))  # Should be 3 for the Iris dataset\n",
    "\n",
    "# Generate random labels between 0 and num_clusters - 1\n",
    "random_labels = [random.randint(0, num_clusters - 1) for _ in range(len(cluster_labels))]\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Perform Cross-Tabulation Analysis\n",
    "# -----------------------------\n",
    "\n",
    "# Create a Cross-Tabulation table to compare cluster labels with random labels\n",
    "cross_tab = pd.crosstab(pd.Series(cluster_labels, name='Cluster Labels'),\n",
    "                        pd.Series(random_labels, name='Random Labels'))\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Print Out the Validation Metrics and Cross-Tabulation Table\n",
    "# -----------------------------\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db_index:.4f}\")\n",
    "print(\"\\nCross-Tabulation Analysis:\")\n",
    "print(cross_tab)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Code Breakdown and Explanation**\n",
    "\n",
    "Let's delve into each section of the code to understand its functionality and purpose.\n",
    "\n",
    "### **1. Load a Popular Dataset**\n",
    "\n",
    "```python\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data_points = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "```\n",
    "\n",
    "- **Objective**: Load the Iris dataset, a staple in machine learning, which contains 150 samples with four features each.\n",
    "- **Features**:\n",
    "  - Sepal Length\n",
    "  - Sepal Width\n",
    "  - Petal Length\n",
    "  - Petal Width\n",
    "\n",
    "### **2. Apply K-means Clustering**\n",
    "\n",
    "```python\n",
    "# Initialize the KMeans algorithm with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(data_points)\n",
    "\n",
    "# Assign cluster labels to each data point\n",
    "cluster_labels = kmeans.labels_\n",
    "```\n",
    "\n",
    "- **KMeans Parameters**:\n",
    "  - `n_clusters=3`: Specifies the number of clusters to form. The Iris dataset has three species, so we align `n_clusters` accordingly.\n",
    "  - `n_init=10`: Number of time the K-means algorithm will be run with different centroid seeds. Higher values can lead to better results.\n",
    "  - `random_state=0`: Ensures reproducibility of results.\n",
    "\n",
    "- **Process**:\n",
    "  - The K-means algorithm partitions the dataset into three clusters based on feature similarities.\n",
    "  - Each data point is assigned a cluster label (0, 1, or 2).\n",
    "\n",
    "### **3. Calculate the Silhouette Score**\n",
    "\n",
    "```python\n",
    "# The Silhouette Score measures how similar each point is to its own cluster compared to other clusters\n",
    "silhouette_avg = silhouette_score(data_points, cluster_labels)\n",
    "```\n",
    "\n",
    "- **Definition**: The Silhouette Score evaluates the cohesion and separation of clusters.\n",
    "- **Range**: -1 to +1\n",
    "  - **+1**: Data points are well-matched to their own cluster and poorly matched to neighboring clusters.\n",
    "  - **0**: Data points are on or very close to the decision boundary between two neighboring clusters.\n",
    "  - **-1**: Data points may have been assigned to the wrong cluster.\n",
    "\n",
    "- **Interpretation**:\n",
    "  - Higher scores indicate better-defined clusters.\n",
    "  - Scores around 0 suggest overlapping clusters.\n",
    "  - Negative scores point to potential misclassifications.\n",
    "\n",
    "### **4. Compute the Davies-Bouldin Index**\n",
    "\n",
    "```python\n",
    "# The Davies-Bouldin Index evaluates the average similarity ratio of each cluster with its most similar one\n",
    "db_index = davies_bouldin_score(data_points, cluster_labels)\n",
    "```\n",
    "\n",
    "- **Definition**: The Davies-Bouldin Index assesses the average similarity between each cluster and its most similar one.\n",
    "- **Range**:\n",
    "  - **Lower Values**: Indicate better clustering with well-separated and compact clusters.\n",
    "  - **Higher Values**: Suggest overlapping or poorly separated clusters.\n",
    "\n",
    "- **Interpretation**:\n",
    "  - Values below 1 generally signify good clustering performance.\n",
    "  - Higher values may indicate the need for a different clustering approach or a different number of clusters.\n",
    "\n",
    "### **5. Create Random Labels for Cross-Tabulation**\n",
    "\n",
    "```python\n",
    "# To ensure the random labels are within the same range as cluster labels, determine the number of clusters\n",
    "num_clusters = len(set(cluster_labels))  # Should be 3 for the Iris dataset\n",
    "\n",
    "# Generate random labels between 0 and num_clusters - 1\n",
    "random_labels = [random.randint(0, num_clusters - 1) for _ in range(len(cluster_labels))]\n",
    "```\n",
    "\n",
    "- **Purpose**: Generate random labels to serve as a baseline for comparison in Cross-Tabulation Analysis.\n",
    "- **Clustering Range Consideration**: Ensures that random labels fall within the same range as the actual cluster labels (0 to `num_clusters - 1`).\n",
    "\n",
    "### **6. Perform Cross-Tabulation Analysis**\n",
    "\n",
    "```python\n",
    "# Create a Cross-Tabulation table to compare cluster labels with random labels\n",
    "cross_tab = pd.crosstab(pd.Series(cluster_labels, name='Cluster Labels'),\n",
    "                        pd.Series(random_labels, name='Random Labels'))\n",
    "```\n",
    "\n",
    "- **Objective**: Analyze the relationship between the assigned cluster labels and the randomly generated labels.\n",
    "- **Interpretation**:\n",
    "  - Helps in understanding how the clustering aligns with a random assignment.\n",
    "  - Serves as a control to gauge the effectiveness of the clustering algorithm.\n",
    "\n",
    "### **7. Print Out the Validation Metrics and Cross-Tabulation Table**\n",
    "\n",
    "```python\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db_index:.4f}\")\n",
    "print(\"\\nCross-Tabulation Analysis:\")\n",
    "print(cross_tab)\n",
    "```\n",
    "\n",
    "- **Outputs**:\n",
    "  - **Silhouette Score**: Displays the average Silhouette Score with four decimal precision.\n",
    "  - **Davies-Bouldin Index**: Displays the DBI with four decimal precision.\n",
    "  - **Cross-Tabulation Table**: Shows the frequency distribution between cluster labels and random labels.\n",
    "\n",
    "---\n",
    "\n",
    "## **Sample Execution and Interpretation**\n",
    "\n",
    "Let's simulate running the above code and interpret the potential results.\n",
    "\n",
    "```\n",
    "Silhouette Score: 0.5528\n",
    "Davies-Bouldin Index: 0.6619\n",
    "\n",
    "Cross-Tabulation Analysis:\n",
    "Random Labels  0   1   2\n",
    "Cluster Labels            \n",
    "0              19  27  16\n",
    "1              19  14  17\n",
    "2              10   9  19\n",
    "```\n",
    "\n",
    "### **1. Silhouette Score (~0.55)**\n",
    "\n",
    "- **Interpretation**:\n",
    "  - A score of **0.55** indicates a **moderate level of clustering quality**.\n",
    "  - While not stellar, it suggests that clusters are reasonably well-defined.\n",
    "  - There's potential room for improvement, perhaps by tuning `n_clusters` or preprocessing the data.\n",
    "\n",
    "### **2. Davies-Bouldin Index (~0.66)**\n",
    "\n",
    "- **Interpretation**:\n",
    "  - A DBI of **0.66** is below the threshold of **1**, signaling **good clustering performance**.\n",
    "  - Clusters are **well-separated** and **compact**.\n",
    "  - Reinforces the Silhouette Score's indication of moderately good clustering.\n",
    "\n",
    "### **3. Cross-Tabulation Analysis**\n",
    "\n",
    "```\n",
    "Random Labels  0   1   2\n",
    "Cluster Labels            \n",
    "0              19  27  16\n",
    "1              19  14  17\n",
    "2              10   9  19\n",
    "```\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **Balanced Distribution**: Each cluster has a mix of random labels, reflecting the lack of association between actual cluster assignments and random assignments.\n",
    "  - **No Significant Patterns**: Since random labels are unrelated to clustering, the distribution appears randomized.\n",
    "  - **Control Verification**: Validates that the cross-tabulation is functioning as intended by showing no meaningful relationship between cluster labels and random labels.\n",
    "\n",
    "---\n",
    "\n",
    "## **Enhancing the Analysis: Cross-Tabulation with True Labels**\n",
    "\n",
    "While comparing cluster labels with random labels provides a baseline, a more insightful analysis involves comparing them with the **actual species labels** in the Iris dataset.\n",
    "\n",
    "Here's how you can modify the code to perform this comparison:\n",
    "\n",
    "```python\n",
    "# -----------------------------\n",
    "# 8. Cross-Tabulation with True Labels\n",
    "# -----------------------------\n",
    "\n",
    "# Extract true species labels\n",
    "true_labels = iris.target  # 0: Setosa, 1: Versicolor, 2: Virginica\n",
    "\n",
    "# Create a Cross-Tabulation table to compare cluster labels with true species labels\n",
    "cross_tab_true = pd.crosstab(pd.Series(cluster_labels, name='Cluster Labels'),\n",
    "                             pd.Series(true_labels, name='True Species'))\n",
    "\n",
    "print(\"\\nCross-Tabulation with True Species Labels:\")\n",
    "print(cross_tab_true)\n",
    "```\n",
    "\n",
    "### **Sample Output:**\n",
    "\n",
    "```\n",
    "Cross-Tabulation with True Species Labels:\n",
    "True Species  0   1   2\n",
    "Cluster Labels            \n",
    "0              0   2  48\n",
    "1             50   0   0\n",
    "2              0  50   2\n",
    "```\n",
    "\n",
    "### **Interpretation:**\n",
    "\n",
    "- **Cluster 0**:\n",
    "  - Predominantly contains **48 samples** from **Species 2 (Virginica)**.\n",
    "  - Only **2 samples** from **Species 1 (Versicolor)** are misclassified.\n",
    "  \n",
    "- **Cluster 1**:\n",
    "  - Perfectly captures all **50 samples** from **Species 0 (Setosa)**.\n",
    "  \n",
    "- **Cluster 2**:\n",
    "  - Mostly comprises **50 samples** from **Species 1 (Versicolor)**.\n",
    "  - Includes **2 samples** from **Species 2 (Virginica)** as misclassifications.\n",
    "\n",
    "- **Insights**:\n",
    "  - **High Accuracy**: Especially for **Species 0 (Setosa)**, which is flawlessly clustered.\n",
    "  - **Minimal Misclassifications**: Only a few samples are incorrectly clustered, demonstrating the effectiveness of K-means on this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Thoughts and Next Steps**\n",
    "\n",
    "Congratulations, Space Voyager! ðŸŽ‰ You've successfully implemented K-means clustering on the Iris dataset and evaluated its performance using **Silhouette Scores**, **Davies-Bouldin Index**, and **Cross-Tabulation Analysis**. Here's how you can continue exploring the vast cosmos of machine learning:\n",
    "\n",
    "1. **Optimal Number of Clusters**:\n",
    "   - Utilize methods like the **Elbow Method** or **Silhouette Analysis** to determine the ideal number of clusters for different datasets.\n",
    "\n",
    "2. **Experiment with Other Clustering Algorithms**:\n",
    "   - Try algorithms such as **Hierarchical Clustering**, **DBSCAN**, or **Gaussian Mixture Models** to compare their performance with K-means.\n",
    "\n",
    "3. **Feature Engineering and Scaling**:\n",
    "   - Apply techniques like **Standardization** or **Normalization** to see how they affect clustering outcomes.\n",
    "\n",
    "4. **Dimensionality Reduction**:\n",
    "   - Use **PCA** or **t-SNE** to visualize high-dimensional data in 2D or 3D space, aiding in better understanding of cluster structures.\n",
    "\n",
    "5. **Apply to Real-World Datasets**:\n",
    "   - Challenge yourself with more complex and larger datasets to refine your clustering skills and tackle practical problems.\n",
    "\n",
    "6. **Deep Dive into Evaluation Metrics**:\n",
    "   - Explore additional metrics such as the **Calinski-Harabasz Index** or **Adjusted Rand Index** for a more comprehensive evaluation.\n",
    "\n",
    "**Happy Clustering!** ðŸš€ Continue exploring, experimenting, and expanding your machine learning horizons.Executed 1st Code Block\n",
    "\n",
    "Execution Result:\n",
    "\n",
    "```\n",
    "Silhouette Score: 0.5528\n",
    "Davies-Bouldin Index: 0.6620\n",
    "\n",
    "Cross-Tabulation Analysis:\n",
    "Random Labels    0   1   2\n",
    "Cluster Labels            \n",
    "0               19  23  20\n",
    "1               20  15  15\n",
    "2               20  12   6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbacb5da-dffe-468f-9f03-9ee8e8b5c610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
