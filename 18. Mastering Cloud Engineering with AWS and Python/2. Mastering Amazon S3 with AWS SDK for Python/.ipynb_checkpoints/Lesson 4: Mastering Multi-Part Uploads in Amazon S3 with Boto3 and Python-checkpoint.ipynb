{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6781a33",
   "metadata": {},
   "source": [
    "# Lesson 4: Mastering Multi-Part Uploads in Amazon S3 with Boto3 and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ca2d3",
   "metadata": {},
   "source": [
    "## Multipart Upload of Different Sized Chunks to Amazon S3\n",
    "\n",
    "In this task, we'll perform a multipart upload using different chunk sizes. First, we'll upload a chunk of size 5MB, then a chunk of size 6MB, and finally, we'll finish uploading by sending the remaining part of the file. Moreover, we'll not use any loop in this task to highlight how we can handle multipart uploads manually. The file we will upload is \"cosmo-hadoop-course-data-set.zip\" and the destination bucket is called \"cosmo-archive-2023\".\n",
    "\n",
    "Your task has the following steps:\n",
    "\n",
    "Initialize the multipart upload.\n",
    "Upload the first chunk of size 5MB.\n",
    "Upload the second chunk of size 6MB.\n",
    "Upload the final chunk, which should contain the rest of the file.\n",
    "Complete the multipart upload.\n",
    "Important Note: Running scripts can alter the filesystem's state or modify the resources in our AWS simulator. To revert to the initial state, you can use the reset button located in the top right corner. However, keep in mind that resetting will erase any code changes. To preserve your code during a reset, consider copying it to the clipboard.\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Create the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create a new bucket\n",
    "bucket_name = 'cosmo-archive-2023'\n",
    "s3_client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# Path to your dataset\n",
    "file_path = '/usercode/FILESYSTEM/assets/cosmo-hadoop-course-data-set.zip'\n",
    "key = 'cosmos-hadoop-course-data-set.zip'\n",
    "\n",
    "# Initiate multipart upload\n",
    "multipart_upload = s3_client.create_multipart_upload(Bucket=bucket_name, Key=key)\n",
    "upload_id = multipart_upload['UploadId']\n",
    "\n",
    "uploaded_parts = []\n",
    "\n",
    "# Open the file for reading data\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Upload the first chunk\n",
    "    data = f.read(1024 * 1024 * 5)  # 5MB\n",
    "    response = s3_client.upload_part(Bucket=bucket_name, Key=key, PartNumber=1, UploadId=upload_id, Body=data)\n",
    "    uploaded_parts.append({'PartNumber': 1, 'ETag': response['ETag']})\n",
    "\n",
    "    # Upload the second chunk\n",
    "    data = f.read(1024 * 1024 * 6)  # 6MB\n",
    "    response = s3_client.upload_part(Bucket=bucket_name, Key=key, PartNumber=2, UploadId=upload_id, Body=data)\n",
    "    uploaded_parts.append({'PartNumber': 2, 'ETag': response['ETag']})\n",
    "\n",
    "    # Upload the final chunk (which is the rest of the file)\n",
    "    data = f.read()\n",
    "    response = s3_client.upload_part(Bucket=bucket_name, Key=key, PartNumber=3, UploadId=upload_id, Body=data)\n",
    "    uploaded_parts.append({'PartNumber': 3, 'ETag': response['ETag']})\n",
    "\n",
    "# Complete the multipart upload\n",
    "s3_client.complete_multipart_upload(Bucket=bucket_name, Key=key, UploadId=upload_id, MultipartUpload={'Parts': uploaded_parts})\n",
    "print(\"Dataset uploaded successfully in chunks of varying sizes. Multipart upload completed.\")\n",
    "\n",
    "```\n",
    "\n",
    "This script demonstrates a **manual multipart upload** to Amazon S3 using **different chunk sizes** without looping. Here's a breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“Œ Steps in the Script**\n",
    "1. **Initialize the multipart upload** â†’ Creates an upload session.\n",
    "2. **Upload the first chunk (5MB)** â†’ Reads and uploads the first part.\n",
    "3. **Upload the second chunk (6MB)** â†’ Reads and uploads the second part.\n",
    "4. **Upload the final chunk** â†’ Uploads the remaining data.\n",
    "5. **Complete the upload** â†’ Informs S3 that all parts are uploaded.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ’¡ Key Takeaways**\n",
    "- **Manual Multipart Upload:** Unlike automated loops, this script explicitly uploads each chunk separately.\n",
    "- **Chunk Sizes Can Vary:** Different parts (5MB, 6MB, and remaining file) showcase flexibility in uploading.\n",
    "- **AWS Constraints:** Minimum part size is **5MB**, except for the last chunk, which can be smaller.\n",
    "- **No Loop Used:** A simple, step-by-step manual upload.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸš€ Usage Scenario**\n",
    "Ideal for cases where you need **precise control** over multipart uploads, such as:\n",
    "- **Optimizing upload performance** for unstable networks.\n",
    "- **Handling custom chunk sizes** based on system constraints.\n",
    "- **Debugging and testing multipart upload workflows.**\n",
    "\n",
    "This approach ensures better efficiency and control over large file transfers to **Amazon S3**. ðŸš€\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f086a0",
   "metadata": {},
   "source": [
    "## Multipart Upload Mastery - Segmenting the Cosmos\n",
    "\n",
    "Embark on a mission to enhance your cloud storage skills by completing a multipart upload to Amazon S3. Your challenge is to upload a 43 MB dataset in three parts to the cosmo-archive-2023 bucket. The dataset is segmented into chunks of 15 MB, 15 MB, and 13 MB. Although the code for uploading the first chunk is provided, it has not been executed yet. Your objective is to finalize the script by seamlessly uploading all three chunks, showcasing your adeptness in managing significant datasets in S3 with efficiency and accuracy. This task is your opportunity to demonstrate proficiency in ensuring data integrity and accessibility in the cloud.\n",
    "\n",
    "Important Note: Running scripts can alter the filesystem's state or modify the resources in our AWS simulator. To revert to the initial state, you can use the reset button located in the top right corner. However, keep in mind that resetting will erase any code changes. To preserve your code during a reset, consider copying it to the clipboard.\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "# Configure the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create a new bucket and enable versioning\n",
    "bucket_name = 'cosmo-archive-2023'\n",
    "s3_client.create_bucket(Bucket=bucket_name)\n",
    "s3_client.put_bucket_versioning(Bucket=bucket_name, VersioningConfiguration={'Status': 'Enabled'})\n",
    "\n",
    "# Path to your large dataset\n",
    "file_path = '/usercode/FILESYSTEM/assets/cosmo-hadoop-course-data-set.zip'\n",
    "key = 'cosmos-hadoop-course-data-set.zip'\n",
    "\n",
    "# Initiate multipart upload\n",
    "multipart_upload = s3_client.create_multipart_upload(Bucket=bucket_name, Key=key)\n",
    "upload_id = multipart_upload['UploadId']\n",
    "\n",
    "# Upload the first chunk (15 MB) as an example\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = f.read(1024 * 1024 * 15)  # Read the first 15 MB for the first chunk\n",
    "    s3_client.upload_part(Bucket=bucket_name, Key=key, PartNumber=1, UploadId=upload_id, Body=data)\n",
    "\n",
    "    # TODO: Upload the second chunk of 15 MB\n",
    "    # TODO: Upload the final chunk of 13 MB\n",
    "\n",
    "# TODO: Complete the multipart upload by combining all the uploaded parts\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Hereâ€™s the complete script that uploads the 43 MB dataset in three parts to Amazon S3:  \n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "# Configure the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create a new bucket and enable versioning\n",
    "bucket_name = 'cosmo-archive-2023'\n",
    "s3_client.create_bucket(Bucket=bucket_name)\n",
    "s3_client.put_bucket_versioning(Bucket=bucket_name, VersioningConfiguration={'Status': 'Enabled'})\n",
    "\n",
    "# Path to your large dataset\n",
    "file_path = '/usercode/FILESYSTEM/assets/cosmo-hadoop-course-data-set.zip'\n",
    "key = 'cosmos-hadoop-course-data-set.zip'\n",
    "\n",
    "# Initiate multipart upload\n",
    "multipart_upload = s3_client.create_multipart_upload(Bucket=bucket_name, Key=key)\n",
    "upload_id = multipart_upload['UploadId']\n",
    "\n",
    "uploaded_parts = []\n",
    "\n",
    "# Open the file for reading data\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Upload the first chunk (15 MB)\n",
    "    data = f.read(1024 * 1024 * 15)\n",
    "    response = s3_client.upload_part(Bucket=bucket_name, Key=key, PartNumber=1, UploadId=upload_id, Body=data)\n",
    "    uploaded_parts.append({'PartNumber': 1, 'ETag': response['ETag']})\n",
    "\n",
    "    # Upload the second chunk (15 MB)\n",
    "    data = f.read(1024 * 1024 * 15)\n",
    "    response = s3_client.upload_part(Bucket=bucket_name, Key=key, PartNumber=2, UploadId=upload_id, Body=data)\n",
    "    uploaded_parts.append({'PartNumber': 2, 'ETag': response['ETag']})\n",
    "\n",
    "    # Upload the final chunk (13 MB)\n",
    "    data = f.read()  # Read the remaining file data\n",
    "    response = s3_client.upload_part(Bucket=bucket_name, Key=key, PartNumber=3, UploadId=upload_id, Body=data)\n",
    "    uploaded_parts.append({'PartNumber': 3, 'ETag': response['ETag']})\n",
    "\n",
    "# Complete the multipart upload\n",
    "s3_client.complete_multipart_upload(Bucket=bucket_name, Key=key, UploadId=upload_id, MultipartUpload={'Parts': uploaded_parts})\n",
    "\n",
    "print(\"Dataset successfully uploaded in three parts (15MB, 15MB, 13MB). Multipart upload completed.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“Œ Whatâ€™s Completed?**\n",
    "âœ” **Uploaded First Chunk** â€“ 15MB  \n",
    "âœ” **Uploaded Second Chunk** â€“ 15MB  \n",
    "âœ” **Uploaded Final Chunk** â€“ 13MB  \n",
    "âœ” **Completed Multipart Upload**  \n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ’¡ Why Use Multipart Upload?**\n",
    "- ðŸš€ **Faster Uploads** â€“ Uploading in parallel is possible.  \n",
    "- ðŸ”„ **Resilience** â€“ If a part fails, retry only that part.  \n",
    "- ðŸ”§ **Handles Large Files** â€“ Efficiently manages multi-GB uploads.  \n",
    "\n",
    "Now, you have a complete, **efficient**, and **error-free** multipart upload process! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905174be",
   "metadata": {},
   "source": [
    "## Multipart Upload of a Large Dataset to Amazon S3\n",
    "\n",
    "Prepare to further advance your cloud storage skills with a new challenge: uploading a dataset in smaller increments to Amazon S3. Your mission involves dividing a 43 MB dataset into 5 MB chunks and uploading each to the cosmo-archive-2023 bucket. Utilizing a loop for this multipart upload process, you will demonstrate your ability to efficiently manage large datasets in S3. This task is an excellent opportunity to showcase your adeptness in optimizing data transfers and ensuring the dataset's accessibility in the cloud.\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create a new bucket for your uploads\n",
    "bucket_name = 'cosmo-archive-2023'\n",
    "s3_client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# Path to your dataset\n",
    "file_path = '/usercode/FILESYSTEM/assets/cosmo-hadoop-course-data-set.zip'\n",
    "key = 'cosmos-hadoop-course-data-set.zip'\n",
    "\n",
    "# TODO: Initiate a multipart upload session\n",
    "\n",
    "# TODO: Upload the dataset in 5 MB chunks using a loop\n",
    "\n",
    "# TODO: Complete the multipart upload by combining all the uploaded parts\n",
    "```\n",
    "\n",
    "Here's the complete script that efficiently uploads the 43 MB dataset in **5 MB chunks** to Amazon S3 using a loop:  \n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create a new bucket for your uploads\n",
    "bucket_name = 'cosmo-archive-2023'\n",
    "s3_client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# Path to your dataset\n",
    "file_path = '/usercode/FILESYSTEM/assets/cosmo-hadoop-course-data-set.zip'\n",
    "key = 'cosmos-hadoop-course-data-set.zip'\n",
    "\n",
    "# Initiate a multipart upload session\n",
    "multipart_upload = s3_client.create_multipart_upload(Bucket=bucket_name, Key=key)\n",
    "upload_id = multipart_upload['UploadId']\n",
    "\n",
    "uploaded_parts = []\n",
    "part_size = 1024 * 1024 * 5  # 5MB per chunk\n",
    "file_size = os.path.getsize(file_path)\n",
    "part_count = (file_size + part_size - 1) // part_size  # Calculate number of parts\n",
    "\n",
    "# Upload the dataset in 5 MB chunks using a loop\n",
    "with open(file_path, 'rb') as f:\n",
    "    for part_no in range(1, part_count + 1):\n",
    "        data = f.read(part_size)  # Read 5MB chunk\n",
    "        response = s3_client.upload_part(\n",
    "            Bucket=bucket_name,\n",
    "            Key=key,\n",
    "            PartNumber=part_no,\n",
    "            UploadId=upload_id,\n",
    "            Body=data\n",
    "        )\n",
    "        uploaded_parts.append({'PartNumber': part_no, 'ETag': response['ETag']})\n",
    "\n",
    "# Complete the multipart upload by combining all the uploaded parts\n",
    "s3_client.complete_multipart_upload(\n",
    "    Bucket=bucket_name,\n",
    "    Key=key,\n",
    "    UploadId=upload_id,\n",
    "    MultipartUpload={'Parts': uploaded_parts}\n",
    ")\n",
    "\n",
    "print(f\"Dataset successfully uploaded in {part_count} chunks of 5MB each. Multipart upload completed.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“Œ Whatâ€™s Implemented?**\n",
    "âœ” **Multipart Upload Initialized**  \n",
    "âœ” **File Split into 5MB Chunks**  \n",
    "âœ” **Upload Handled in a Loop**  \n",
    "âœ” **Upload Finalized & Combined**  \n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ’¡ Why Use This Approach?**\n",
    "- **ðŸ“Š Scalability:** Handles any file size efficiently.  \n",
    "- **ðŸ”„ Reliability:** Only failed parts need re-uploading.  \n",
    "- **ðŸš€ Performance:** Parallel uploads possible for faster processing.  \n",
    "\n",
    "Now you have a **robust, scalable**, and **automated** multipart upload process! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
