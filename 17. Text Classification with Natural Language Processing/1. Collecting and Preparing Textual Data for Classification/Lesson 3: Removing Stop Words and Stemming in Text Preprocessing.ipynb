{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf992502",
   "metadata": {},
   "source": [
    "# Lesson 3: Removing Stop Words and Stemming in Text Preprocessing\n",
    "\n",
    "Hello and welcome to this lesson on **Removing Stop Words and Stemming**! In this lesson, we will dive deep into two essential steps for preparing text data for machine learning models: removing stop words and stemming. These techniques will help us improve the efficiency and accuracy of our models. Let's get started!\n",
    "\n",
    "## Understanding Stop Words\n",
    "\n",
    "In Natural Language Processing (NLP), **stop words** refer to the most common words in a language that do not provide significant meaning. Examples include \"and\", \"the\", \"is\", and others. These words are often removed to speed up processing without losing crucial information. Python's Natural Language Toolkit (NLTK) provides a pre-defined list of stop words. Let's take a look:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Defining the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print 5 stop words\n",
    "examples_of_stopwords = list(stop_words)[:5]\n",
    "print(f\"Examples of stop words: {examples_of_stopwords}\")\n",
    "```\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```\n",
    "Examples of stop words: ['or', 'some', 'couldn', 'hasn', 'after']\n",
    "```\n",
    "\n",
    "Here, the `stopwords.words('english')` function returns a list of English stop words. You might sometimes need to add domain-specific stop words to this list based on the nature of your text data.\n",
    "\n",
    "## Introduction to Stemming\n",
    "\n",
    "**Stemming** is a technique that reduces a word to its root form. Although the stemmed word may not always be a real or grammatically correct word in English, it helps consolidate different forms of the same word into a common base form, reducing the complexity of text data. This simplification leads to quicker computation and potentially better performance when implementing NLP algorithms, as there are fewer unique words to consider.\n",
    "\n",
    "For example, the words \"run\", \"runs\", and \"running\" might all be stemmed to the common root \"run\". This helps our algorithm understand that these words are related and carry a similar semantic meaning.\n",
    "\n",
    "Let's illustrate this with the **Porter Stemmer**, a well-known stemming algorithm from the NLTK library:\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Stemming with NLTK Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_word = stemmer.stem('running')\n",
    "print(f\"Stemmed word: {stemmed_word}\")\n",
    "```\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```\n",
    "Stemmed word: run\n",
    "```\n",
    "\n",
    "The `PorterStemmer` class comes with the `stem` method that takes in a word and returns its root form. In this case, \"running\" is correctly stemmed to its root word \"run\". This form of preprocessing, although it may lead to words that are not recognizable, is a standard practice in text preprocessing for NLP tasks.\n",
    "\n",
    "## Stop Words Removal and Stemming in Action\n",
    "\n",
    "Having understood stop words and stemming, let's develop a function that removes stop words and applies stemming to a given text. We will tokenize the text (split it into individual words) and apply these transformations word by word.\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "example_text = \"This is an example text to demonstrate the removal of stop words and stemming.\"\n",
    "\n",
    "print(f\"Original Text: {example_text}\")\n",
    "print(f\"Processed Text: {remove_stopwords_and_stem(example_text)}\")\n",
    "```\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```\n",
    "Original Text: This is an example text to demonstrate the removal of stop words and stemming.\n",
    "Processed Text: thi exampl text demonstr remov stop word stem .\n",
    "```\n",
    "\n",
    "The `remove_stopwords_and_stem` function performs the required processing and provides the cleaned-up text.\n",
    "\n",
    "## Stop Words Removal and Stemming on a Dataset\n",
    "\n",
    "Let's implement the above concepts on a real-world text dataset â€“ the **20 Newsgroups Dataset**.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Fetching 20 newsgroups dataset\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Limit to first 100 data points for efficient code execution\n",
    "newsgroups_data = newsgroups_data['data'][:100]\n",
    "\n",
    "processed_newsgroups_data = [remove_stopwords_and_stem(text) for text in newsgroups_data]\n",
    "\n",
    "# Print first 100 characters of the first document\n",
    "print(\"First 100 characters of the first processed document:\")\n",
    "print(processed_newsgroups_data[0][:100])\n",
    "```\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```\n",
    "First 100 characters of the first processed document:\n",
    "from : mamatha devineni ratnam < mr47+ @ andrew.cmu.edu > subject : pen fan reaction organ : post of\n",
    "```\n",
    "\n",
    "This process can take a while for large datasets, but the output will be much cleaner and easier for a machine learning model to work with.\n",
    "\n",
    "## Summary and Conclusion\n",
    "\n",
    "And that's a wrap! In today's lesson, we've learned about stop words and stemming as crucial steps in text preprocessing for machine learning models. We used Python's NLTK library to work with stop words and perform stemming. We processed some example sentences and a real-world dataset to practice these concepts.\n",
    "\n",
    "As we proceed to more advanced NLP tasks, preprocessing techniques like removing stop words and stemming will serve as a solid foundation. In the upcoming lessons, we will delve deeper into handling missing text data and learn about reshaping textual data for analysis. Let's keep going!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d56c1",
   "metadata": {},
   "source": [
    "## Switch from LancasterStemmer to PorterStemmer\n",
    "\n",
    "Excellent work so far, Stellar Navigator! Now it's time to tweak the implemented text preprocessing method. Replace the use of LancasterStemmer with PorterStemmer. Remember to import PorterStemmer from nltk.stem. After adjusting your code, run it and observe the differences in your processed text.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Defining the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Creating an instance of LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "example_text = \"This is a example text to demonstrate the removal of stop words and stemming.\"\n",
    "processed_text = remove_stopwords_and_stem(example_text)\n",
    "print(f\"Original Text: {example_text}\")\n",
    "print(f\"Processed Text: {processed_text}\")\n",
    "```\n",
    "\n",
    "To modify the provided code by replacing the `LancasterStemmer` with `PorterStemmer`, you will need to import `PorterStemmer` from `nltk.stem` and create an instance of it. Below is the adjusted code:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  # Importing PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Defining the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Creating an instance of PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "example_text = \"This is an example text to demonstrate the removal of stop words and stemming.\"\n",
    "processed_text = remove_stopwords_and_stem(example_text)\n",
    "print(f\"Original Text: {example_text}\")\n",
    "print(f\"Processed Text: {processed_text}\")\n",
    "```\n",
    "\n",
    "### Explanation of Changes:\n",
    "1. **Import Statement**: Changed the import from `LancasterStemmer` to `PorterStemmer`.\n",
    "2. **Stemmer Instance**: Created an instance of `PorterStemmer` instead of `LancasterStemmer`.\n",
    "\n",
    "### Expected Output:\n",
    "When you run the modified code, you should see the processed text reflecting the stemming done by the Porter Stemmer. The output will look something like this:\n",
    "\n",
    "```\n",
    "Original Text: This is an example text to demonstrate the removal of stop words and stemming.\n",
    "Processed Text: exampl text demonstr remov stop word stem .\n",
    "```\n",
    "\n",
    "### Differences in Processed Text:\n",
    "The main difference you might observe between the outputs of `LancasterStemmer` and `PorterStemmer` is in how they handle certain words. The Porter Stemmer is generally more conservative and may produce different stemmed forms compared to the Lancaster Stemmer, which is more aggressive in its stemming approach. \n",
    "\n",
    "If you run the code, you can compare the outputs to see how the two stemming algorithms differ in their results.Executed 1st Code Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b97711",
   "metadata": {},
   "source": [
    "## Removing Stop Words and Punctuation from Text\n",
    "\n",
    "Well done on completing the first practice, stellar navigator! Now it's time to put your skills further to the test.\n",
    "\n",
    "The skeleton code for this task is already written for you. You just need to fill in the blank spots (____) to make the code run without errors.\n",
    "\n",
    "First, you'll need to create a set of English stop words.\n",
    "Then, tokenize the sample text using an appropriate function from the NLTK tokenization module.\n",
    "Take on this challenge and let's see how you do\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(____.____('english'))\n",
    "# Adding string punctuation to stop words to filter them out\n",
    "stop_words.update(string.punctuation)\n",
    "\n",
    "ex_text = \"Here's a sample sentence to remove stop words from. It has generic and specific words.\"\n",
    "\n",
    "# Tokenize the sample sentence with casual_tokenize\n",
    "ex_words = _____(ex_text)\n",
    "\n",
    "# Remove stop words and punctuation from the list of tokenized words\n",
    "filtered_words = [word for word in ex_words  if not word.lower() in stop_words]\n",
    "\n",
    "print(\"Filtered words:\", filtered_words)\n",
    "\n",
    "```\n",
    "\n",
    "To complete the provided code, you need to fill in the blanks with the appropriate functions and methods from the NLTK library. Hereâ€™s the completed code with the necessary modifications:\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Create a set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Adding string punctuation to stop words to filter them out\n",
    "stop_words.update(string.punctuation)\n",
    "\n",
    "ex_text = \"Here's a sample sentence to remove stop words from. It has generic and specific words.\"\n",
    "\n",
    "# Tokenize the sample sentence with casual_tokenize\n",
    "ex_words = casual_tokenize(ex_text)\n",
    "\n",
    "# Remove stop words and punctuation from the list of tokenized words\n",
    "filtered_words = [word for word in ex_words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered words:\", filtered_words)\n",
    "```\n",
    "\n",
    "### Explanation of the Filled Blanks:\n",
    "1. **Creating the Set of Stop Words**:\n",
    "   - `stop_words = set(stopwords.words('english'))`: This line creates a set of English stop words using the `stopwords` module from NLTK.\n",
    "\n",
    "2. **Tokenizing the Sample Text**:\n",
    "   - `ex_words = casual_tokenize(ex_text)`: This line tokenizes the sample text using the `casual_tokenize` function from the NLTK tokenization module.\n",
    "\n",
    "### Expected Output:\n",
    "When you run the completed code, you should see the filtered words printed out, which will exclude any stop words and punctuation from the original text. The output will look something like this:\n",
    "\n",
    "```\n",
    "Filtered words: ['sample', 'sentence', 'remove', 'stop', 'words', 'generic', 'specific', 'words']\n",
    "```\n",
    "\n",
    "### Note:\n",
    "Make sure you have the necessary NLTK resources downloaded, especially the stop words. If you encounter any errors related to missing resources, you can download them using:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "This will ensure that the `stopwords` corpus is available for use in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76290bd4",
   "metadata": {},
   "source": [
    "## Stemming Words with PorterStemmer\n",
    "\n",
    "Stellar work so far, Space Voyager! Now it's time to hone your skills in stemming. Fill in the blank spots (____) to make the code functional. Stem the provided words, and print the stemmed versions. Let's get processing!\n",
    "\n",
    "```python\n",
    "from nltk.stem import ____\n",
    "\n",
    "# List of sample words\n",
    "example_words = [\"connection\", \"connections\", \"connective\", \"connected\", \"connecting\", \"connection\"]\n",
    "\n",
    "# Create object of the Porter Stemmer Class\n",
    "stemmer = ____\n",
    "\n",
    "# Stem each word in the list of words\n",
    "stemmed_words = [stemmer.____(word) for word in example_words]\n",
    "\n",
    "print(\"Stemmed words: \", stemmed_words)\n",
    "\n",
    "```\n",
    "\n",
    "To complete the provided code for stemming words using the `PorterStemmer`, you need to fill in the blanks with the appropriate class and method from the NLTK library. Hereâ€™s the completed code:\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer  # Fill in the first blank\n",
    "\n",
    "# List of sample words\n",
    "example_words = [\"connection\", \"connections\", \"connective\", \"connected\", \"connecting\", \"connection\"]\n",
    "\n",
    "# Create object of the Porter Stemmer Class\n",
    "stemmer = PorterStemmer()  # Fill in the second blank\n",
    "\n",
    "# Stem each word in the list of words\n",
    "stemmed_words = [stemmer.stem(word) for word in example_words]  # Fill in the third blank\n",
    "\n",
    "print(\"Stemmed words: \", stemmed_words)\n",
    "```\n",
    "\n",
    "### Explanation of the Filled Blanks:\n",
    "1. **Importing the PorterStemmer**:\n",
    "   - `from nltk.stem import PorterStemmer`: This line imports the `PorterStemmer` class from the NLTK library.\n",
    "\n",
    "2. **Creating an Instance of PorterStemmer**:\n",
    "   - `stemmer = PorterStemmer()`: This line creates an instance of the `PorterStemmer` class.\n",
    "\n",
    "3. **Stemming Each Word**:\n",
    "   - `stemmed_words = [stemmer.stem(word) for word in example_words]`: This line applies the `stem` method of the `PorterStemmer` instance to each word in the `example_words` list.\n",
    "\n",
    "### Expected Output:\n",
    "When you run the completed code, you should see the stemmed versions of the provided words printed out. The output will look something like this:\n",
    "\n",
    "```\n",
    "Stemmed words:  ['connect', 'connect', 'connect', 'connect', 'connect', 'connect']\n",
    "```\n",
    "\n",
    "This output shows that all variations of the word \"connect\" have been reduced to their root form \"connect\" by the Porter Stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f9d1f",
   "metadata": {},
   "source": [
    "## Implementing Stopword Removal and Stemming Function\n",
    "\n",
    "Celestial Traveler, letâ€™s put our text preprocessing skills to the test. Can you add the missing line inside the function remove_stopwords_and_stem to tokenize the given text, remove stop words, stem the remaining words, and return the processed text? Give it a shot!\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Defining the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Creating an instance of PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    # TODO: Add the code to tokenize the text, remove stopwords and apply stemming\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "example_text = \"This is a example text to demonstrate the removal of stop words and stemming.\"\n",
    "processed_text = remove_stopwords_and_stem(example_text)\n",
    "print(f\"Original Text: {example_text}\")\n",
    "print(f\"Processed Text: {processed_text}\")\n",
    "\n",
    "```\n",
    "\n",
    "To complete the `remove_stopwords_and_stem` function, you need to add the code that tokenizes the input text, removes the stop words, stems the remaining words, and then returns the processed text. Hereâ€™s the completed code:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Defining the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Creating an instance of PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    # Tokenize the text\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and apply stemming\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if word.lower() not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "example_text = \"This is an example text to demonstrate the removal of stop words and stemming.\"\n",
    "processed_text = remove_stopwords_and_stem(example_text)\n",
    "print(f\"Original Text: {example_text}\")\n",
    "print(f\"Processed Text: {processed_text}\")\n",
    "```\n",
    "\n",
    "### Explanation of the Added Code:\n",
    "1. **Tokenization**:\n",
    "   - `tokenized_text = word_tokenize(text)`: This line tokenizes the input text into individual words.\n",
    "\n",
    "2. **Removing Stop Words and Stemming**:\n",
    "   - `filtered_text = [stemmer.stem(word) for word in tokenized_text if word.lower() not in stop_words]`: This line iterates over each tokenized word, checks if it is not in the set of stop words (case insensitive), and applies stemming using the `stem` method of the `PorterStemmer` instance.\n",
    "\n",
    "3. **Returning Processed Text**:\n",
    "   - `return \" \".join(filtered_text)`: This line joins the filtered and stemmed words into a single string and returns it.\n",
    "\n",
    "### Expected Output:\n",
    "When you run the completed code, you should see the original text and the processed text printed out. The output will look something like this:\n",
    "\n",
    "```\n",
    "Original Text: This is an example text to demonstrate the removal of stop words and stemming.\n",
    "Processed Text: exampl text demonstr remov stop word stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f9500",
   "metadata": {},
   "source": [
    "## Cleaning and Processing the First Newsgroup Article\n",
    "\n",
    "Good job, Stellar Navigator! Could you now define the stop words and instantiate the PorterStemmer using NLTK's predefined English stop words? Additionally, could you implement the tokenization of the text, remove stop words, apply stemming, and return the processed text in the function remove_stopwords_and_stem? After doing so, you will preprocess the first newsgroup article and display it. The stars are in your favor!\n",
    "\n",
    "```python\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# TODO: Define the stop words and instantiate the PorterStemmer\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = remove_stopwords_and_stem(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    # TODO: Implement the process to tokenize the text, remove stopwords and apply stemming\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Fetching 20 newsgroups dataset\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')['data'][:1]\n",
    "\n",
    "# Process and display the first newsgroup article\n",
    "cleaned_data = clean_text(newsgroups_data[0])\n",
    "print(\"First newsgroup article after cleaning and textual preprocessing:\")\n",
    "print(cleaned_data)\n",
    "```\n",
    "\n",
    "Abacus AI ChatLLM:\n",
    "Sure! Let's complete the code by defining the stop words and instantiating the `PorterStemmer`. We will also implement the `remove_stopwords_and_stem` function to tokenize the text, remove stop words, apply stemming, and return the processed text. Hereâ€™s the complete code:\n",
    "\n",
    "```python\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Define the stop words and instantiate the PorterStemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = remove_stopwords_and_stem(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    # Tokenize the text\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and apply stemming\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if word.lower() not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Fetching 20 newsgroups dataset\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')['data'][:1]\n",
    "\n",
    "# Process and display the first newsgroup article\n",
    "cleaned_data = clean_text(newsgroups_data[0])\n",
    "print(\"First newsgroup article after cleaning and textual preprocessing:\")\n",
    "print(cleaned_data)\n",
    "```\n",
    "\n",
    "### Explanation of the Code:\n",
    "1. **Stop Words and Stemmer Initialization**:\n",
    "   - `stop_words = set(stopwords.words('english'))`: This line defines a set of English stop words using NLTK's predefined list.\n",
    "   - `stemmer = PorterStemmer()`: This line creates an instance of the `PorterStemmer`.\n",
    "\n",
    "2. **Text Cleaning Function**:\n",
    "   - The `clean_text` function processes the input text by converting it to lowercase, removing email addresses, URLs, punctuation, digits, and extra spaces. Finally, it calls `remove_stopwords_and_stem` to further process the text.\n",
    "\n",
    "3. **Stop Words Removal and Stemming Function**:\n",
    "   - The `remove_stopwords_and_stem` function tokenizes the cleaned text, removes stop words, applies stemming to the remaining words, and returns the processed text as a single string.\n",
    "\n",
    "4. **Fetching and Processing Newsgroup Data**:\n",
    "   - The code fetches the first article from the 20 Newsgroups dataset and processes it using the `clean_text` function. The cleaned article is then printed.\n",
    "\n",
    "### Expected Output:\n",
    "When you run the completed code, you should see the first newsgroup article printed after cleaning and preprocessing. The output will show the text with stop words removed and words stemmed, making it more suitable for further analysis or modeling. \n",
    "\n",
    "Make sure you have the necessary NLTK resources downloaded, especially the stop words and punkt tokenizer. If you encounter any errors related to missing resources, you can download them using:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc6fea-1756-4f17-9bd4-d14c50d3bcee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
