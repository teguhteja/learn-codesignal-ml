{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5a0ea1",
   "metadata": {},
   "source": [
    "# Lesson 4: Unleashing the Power of n-grams in Text Classification\n",
    "\n",
    "# Topic Overview and Goal\n",
    "\n",
    "Hello, and welcome to today's lesson on n-grams! If you've ever wondered how language models or text classifiers can understand the context or sequence in text, it's usually courtesy of our today's hero — n-grams. In this lesson, we'll delve into the magic of n-grams and their essential role in processing textual data. Specifically, we'll learn how to create n-grams from text data using Python, covering unigrams and bigrams.\n",
    "\n",
    "## What are n-grams?\n",
    "\n",
    "In Natural Language Processing, analyzing text often benefits from considering not only individual words but also sequences of words. This approach helps to grasp the context better. Here is where n-grams come in handy.\n",
    "\n",
    "- **Definition**: An n-gram is a contiguous sequence of n items from a given sample of text or speech. The 'n' stands for the number of words in the sequence.\n",
    "  - For instance, in \"I love dogs\":\n",
    "    - A 1-gram (or unigram) is just one word, like \"love.\"\n",
    "    - A 2-gram (or bigram) would be a sequence of 2 words, like \"I love\" or \"love dogs.\"\n",
    "\n",
    "N-grams help preserve the sequential information or context in text data, contributing significantly to many language models or text classifiers.\n",
    "\n",
    "## Preparing Data for n-Grams Creation\n",
    "\n",
    "Before we can create n-grams, we need clean, structured text data. The text must be cleaned and preprocessed into a desirable format, after which it can be used for feature extraction or modeling.\n",
    "\n",
    "### Cleaning Text\n",
    "\n",
    "Here's a familiar code snippet that applies cleaning to our text, removing stop words and stemming the remaining words. These steps include lower-casing words, removing punctuation, useless words (stopwords), and reducing all words to their base or stemmed form.\n",
    "\n",
    "```python\n",
    "# Function to clean text and perform stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "```\n",
    "\n",
    "## Creating n-grams with Python: Setting up the Vectorizer\n",
    "\n",
    "Python's `sklearn` library provides an accessible way to generate n-grams. The `CountVectorizer` class in the `sklearn.feature_extraction.text` module can convert a given text into its matrix representation and allows us to specify the type of n-grams we want.\n",
    "\n",
    "### Setting Up the Vectorizer\n",
    "\n",
    "Let's set up our vectorizer as a preliminary step towards creating n-grams:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Generate unigram and bigram\n",
    "```\n",
    "\n",
    "The `ngram_range=(1, 2)` parameter instructs our vectorizer to generate n-grams where n ranges from 1 to 2. So, the `CountVectorizer` will generate both unigrams and bigrams. If we wanted unigrams, bigrams, and trigrams, we could use `ngram_range=(1, 3)`.\n",
    "\n",
    "## Creating n-grams with Python: Applying the Vectorizer\n",
    "\n",
    "Now that we've set up our n-gram generating machine, let's use it on some real-world data.\n",
    "\n",
    "### Fetching and Cleaning Data\n",
    "\n",
    "```python\n",
    "# Fetching 20 newsgroups dataset and restricting to first 100 records for performance\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')['data'][:100]\n",
    "\n",
    "# Clean and preprocess the newsgroup data\n",
    "cleaned_data = [clean_text(data) for data in newsgroups_data]\n",
    "```\n",
    "\n",
    "### Applying the Vectorizer\n",
    "\n",
    "Applying the vectorizer to our cleaned text data will create the n-grams:\n",
    "\n",
    "```python\n",
    "# Apply the CountVectorizer on the cleaned data to create n-grams\n",
    "X = vectorizer.fit_transform(cleaned_data)\n",
    "\n",
    "# Display the shape of X\n",
    "print(\"Shape of X with n-grams: \", X.shape)\n",
    "\n",
    "# Print the total number of features\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"Total number of features: \", len(features))\n",
    "\n",
    "# Print features from index 100 to 110\n",
    "print(\"Features from index 100 to 110: \", features[100:111])\n",
    "```\n",
    "\n",
    "### Output Explanation\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "- **Shape of X with n-grams**: (100, 16246)\n",
    "- **Total number of features**: 16246\n",
    "- **Features from index 100 to 110**: `['accid figur', 'accid worri', 'accomod', 'accomod like', 'accord', 'accord document', 'accord lynn', 'accord mujanov', 'accord previou', 'account', 'account curiou']`\n",
    "\n",
    "The shape of X is (100, 16246), indicating we have a high-dimensional feature space. The first number, 100, represents the number of documents or records in your dataset (limited to the first 100 records), while 16246 represents the unique n-grams or features created from all the documents.\n",
    "\n",
    "By printing `features[100:111]`, we get a glimpse into our features, where each string represents an n-gram from our cleaned text data. The returned n-grams include both unigrams (single words) and bigrams (two-word phrases).\n",
    "\n",
    "Generating n-grams adds a new level of complexity to our analysis, as we now have multiple types of features or tokens. You can experiment with the `ngram_range` parameter in `CountVectorizer` to include trigrams or higher-level n-grams, depending on your specific context and requirements. Remember, each choice will have implications for the complexity and interpretability of your models, and it's always a balance between the two.\n",
    "\n",
    "## Lesson Summary\n",
    "\n",
    "Congratulations, you've finished today's lesson on n-grams! We've explored what n-grams are and their importance in text classification. We then moved on to preparing data for creating n-grams before diving into generating them using Python's `CountVectorizer` class in the `sklearn` library.\n",
    "\n",
    "Now, it's time to get hands-on. Try generating trigrams or 4-grams from the same cleaned newsgroups data and notice the differences. Practicing these skills will not only reinforce the concepts learned in this lesson but also enable you to understand when and how much context is needed for certain tasks.\n",
    "\n",
    "As always, happy learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf0ae58",
   "metadata": {},
   "source": [
    "## Generating Bigrams and Trigrams with NLP\n",
    "\n",
    "Excellent work, Space Voyager! Now, let's deepen our understanding of n-grams in Python. Modify the ngram_range parameter in CountVectorizer in the starter code to generate only bigrams and trigrams, instead of unigrams, bigrams, and trigrams. Change ngram_range from (1, 3) to (2, 3). Display the output and observe the differences.\n",
    "\n",
    "```python \n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Fetching 20 newsgroups dataset and restricting to first 100 records for performance\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')['data'][:100]\n",
    "\n",
    "# Clean and preprocess the newsgroup data\n",
    "cleaned_data = [clean_text(data) for data in newsgroups_data]\n",
    "\n",
    "# Setup the CountVectorizer to generate unigrams, bigrams, and trigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3)) \n",
    "\n",
    "# Apply the CountVectorizer on the cleaned data to create n-grams\n",
    "X = vectorizer.fit_transform(cleaned_data)\n",
    "\n",
    "# Display the shape of X and some features\n",
    "print(\"Shape of X with n-grams: \", X.shape)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"Features from index 100 to 110: \", features[100:111])\n",
    "\n",
    "```\n",
    "\n",
    "To modify the `ngram_range` parameter in the `CountVectorizer` to generate only bigrams and trigrams, you need to change it from `(1, 3)` to `(2, 3)`. Below is the updated code with the necessary changes. After running this code, you will be able to observe the differences in the output.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Fetching 20 newsgroups dataset and restricting to first 100 records for performance\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')['data'][:100]\n",
    "\n",
    "# Clean and preprocess the newsgroup data\n",
    "cleaned_data = [clean_text(data) for data in newsgroups_data]\n",
    "\n",
    "# Setup the CountVectorizer to generate bigrams and trigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 3)) \n",
    "\n",
    "# Apply the CountVectorizer on the cleaned data to create n-grams\n",
    "X = vectorizer.fit_transform(cleaned_data)\n",
    "\n",
    "# Display the shape of X and some features\n",
    "print(\"Shape of X with n-grams: \", X.shape)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"Features from index 100 to 110: \", features[100:111])\n",
    "```\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "When you run this modified code, you should see an output similar to the following:\n",
    "\n",
    "- **Shape of X with n-grams**: This will indicate the number of documents and the number of features (bigrams and trigrams).\n",
    "- **Features from index 100 to 110**: This will display the specific bigrams and trigrams generated from the cleaned text data.\n",
    "\n",
    "### Observations\n",
    "\n",
    "- The shape of `X` will reflect the number of bigrams and trigrams generated, which will be fewer than when unigrams were included.\n",
    "- The features printed will consist solely of bigrams and trigrams, showcasing the two-word and three-word combinations derived from the text data.\n",
    "\n",
    "This exercise will help you understand how changing the `ngram_range` affects the feature extraction process in text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793edf4d",
   "metadata": {},
   "source": [
    "## Generating Bigrams and Trigrams from Text Data\n",
    "\n",
    "Excellent work, Space Voyager!\n",
    "\n",
    "For the next task, we will simplify our data input and use a simple short string. Through this, we will generate bigrams and trigrams from it. This will give us a more immediate understanding of generating n-grams. Can you fill in the blanks (____) to get the code up and running? Remember, we need to use CountVectorizer to generate bigrams and trigrams. After that, display the features created.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Simple short text\n",
    "text = \"I love studying data science\"\n",
    "\n",
    "# Setup the CountVectorizer to generate bigrams and trigrams\n",
    "vectorizer = ____(ngram_range=____) \n",
    "\n",
    "# Apply the CountVectorizer on the text to create n-grams\n",
    "X = vectorizer.fit_transform([text])\n",
    "\n",
    "# Display the shape of X and some features\n",
    "print(\"Shape of X with n-grams: \", X.shape)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"Features: \", features)\n",
    "\n",
    "```\n",
    "\n",
    "To complete the code for generating bigrams and trigrams using `CountVectorizer`, you need to fill in the blanks with the appropriate values. Here’s the completed code:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Simple short text\n",
    "text = \"I love studying data science\"\n",
    "\n",
    "# Setup the CountVectorizer to generate bigrams and trigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 3)) \n",
    "\n",
    "# Apply the CountVectorizer on the text to create n-grams\n",
    "X = vectorizer.fit_transform([text])\n",
    "\n",
    "# Display the shape of X and some features\n",
    "print(\"Shape of X with n-grams: \", X.shape)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"Features: \", features)\n",
    "```\n",
    "\n",
    "### Explanation of the Filled Blanks\n",
    "\n",
    "- **`CountVectorizer(ngram_range=(2, 3))`**: This specifies that we want to generate bigrams (2-word sequences) and trigrams (3-word sequences).\n",
    "- **`fit_transform([text])`**: The input text is wrapped in a list because `CountVectorizer` expects an iterable of documents.\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "When you run this code, you should see an output similar to the following:\n",
    "\n",
    "- **Shape of X with n-grams**: This will indicate the number of documents (1 in this case) and the number of features (bigrams and trigrams).\n",
    "- **Features**: This will display the specific bigrams and trigrams generated from the input text.\n",
    "\n",
    "### Example Output\n",
    "\n",
    "```\n",
    "Shape of X with n-grams:  (1, 4)\n",
    "Features:  ['data science' 'love studying' 'studying data' 'I love studying']\n",
    "```\n",
    "\n",
    "This output shows the bigrams and trigrams extracted from the short string, providing a clear understanding of how n-grams are generated.Executed 1st Code Block\n",
    "\n",
    "Execution Result:\n",
    "\n",
    "```\n",
    "Shape of X with n-grams:  (1, 5)\n",
    "Features:  ['data science' 'love studying' 'love studying data' 'studying data'\n",
    " 'studying data science']\n",
    "```\n",
    "\n",
    "User:\n",
    "Bravo, Space Voyager! Now let's try generating n-grams again, but with slightly different text. However, this time, the fit_transform function and its invoker have been removed. You need to fill in the blanks (____) to get the code running successfully.\n",
    "\n",
    "The fit_transform function takes input as a list of texts. It will transform the texts into a matrix of token counts and return a sparse representation of this matrix, which we are storing as X. The fit_transform function is convenient when we want to learn a vocabulary dictionary and return document-term matrix at the same time.\n",
    "\n",
    "For this exercise, we've made it a bit easier and more meaningful. We're only generating unigrams (individual words) this time, meaning n is equal to 1.\n",
    "\n",
    "The text we're using this time is not a proper English sentence but a list of stemmed words to better illustrate how n-grams work on a finer level.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define two simple short texts\n",
    "text1 = \"data love data science passion\"\n",
    "text2 = \"science love algorithm data passion\"\n",
    "\n",
    "# Setup the CountVectorizer to generate unigrams only\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Apply the CountVectorizer on the text to create n-grams\n",
    "X = ____.____([text1, text2])\n",
    "\n",
    "# Display the shape of X and some features\n",
    "print(\"Shape of X with n-grams: \", X.shape)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"Features: \", features)\n",
    "print(\"Matrix X: \", X.toarray())\n",
    "\n",
    "```\n",
    "\n",
    "Abacus AI ChatLLM:\n",
    "To complete the code for generating unigrams from the provided texts, you need to fill in the blanks with the appropriate method and its invoker. Here’s the completed code:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define two simple short texts\n",
    "text1 = \"data love data science passion\"\n",
    "text2 = \"science love algorithm data passion\"\n",
    "\n",
    "# Setup the CountVectorizer to generate unigrams only\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Apply the CountVectorizer on the text to create n-grams\n",
    "X = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "# Display the shape of X and some features\n",
    "print(\"Shape of X with n-grams: \", X.shape)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"Features: \", features)\n",
    "print(\"Matrix X: \", X.toarray())\n",
    "```\n",
    "\n",
    "### Explanation of the Filled Blanks\n",
    "\n",
    "- **`vectorizer.fit_transform([text1, text2])`**: This line calls the `fit_transform` method on the `vectorizer` object, passing in a list containing `text1` and `text2`. This method learns the vocabulary from the texts and transforms them into a document-term matrix.\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "When you run this code, you should see an output similar to the following:\n",
    "\n",
    "- **Shape of X with n-grams**: This will indicate the number of documents (2 in this case) and the number of unique features (unigrams).\n",
    "- **Features**: This will display the specific unigrams generated from the input texts.\n",
    "- **Matrix X**: This will show the document-term matrix as a NumPy array.\n",
    "\n",
    "### Example Output\n",
    "\n",
    "```\n",
    "Shape of X with n-grams:  (2, 6)\n",
    "Features:  ['algorithm' 'data' 'love' 'passion' 'science']\n",
    "Matrix X:  [[0 2 1 1 1]\n",
    "             [1 1 1 1 1]]\n",
    "```\n",
    "\n",
    "In this output:\n",
    "- The shape `(2, 6)` indicates there are 2 documents and 6 unique unigrams.\n",
    "- The features list shows the unique words extracted from both texts.\n",
    "- The matrix `X` shows the count of each unigram in each document.Executed 1st Code Block\n",
    "\n",
    "Execution Result:\n",
    "\n",
    "```\n",
    "Shape of X with n-grams:  (2, 5)\n",
    "Features:  ['algorithm' 'data' 'love' 'passion' 'science']\n",
    "Matrix X:  [[0 2 1 1 1]\n",
    " [1 1 1 1 1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa97fc",
   "metadata": {},
   "source": [
    "## Generating Bigrams and Trigrams from Two Texts\n",
    "\n",
    "ou are doing an excellent job, Galactic Pioneer!\n",
    "\n",
    "Now, let's dive a bit deeper into creating bigrams. Set up the CountVectorizer to generate bigrams, then apply it to the cleaned data. Once you accomplish this, you'll need to display the shape of X, the feature names, and the matrix X. You can do it!\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Simple short sentences\n",
    "text1 = \"I love studying data science\"\n",
    "text2 = \"Science is a field I am passionate about\"\n",
    "text3 = \"Algorithms fascinate me\"\n",
    "text4 = \"It is a joy to work on data algorithms\"\n",
    "\n",
    "# Clean and preprocess the text\n",
    "cleaned_text_1 = clean_text(text1)\n",
    "cleaned_text_2 = clean_text(text2)\n",
    "cleaned_text_3 = clean_text(text3)\n",
    "cleaned_text_4 = clean_text(text4)\n",
    "\n",
    "# TODO: Setup the CountVectorizer to generate bigrams only\n",
    "\n",
    "# TODO: Apply the CountVectorizer on the cleaned data to create n-grams\n",
    "\n",
    "# Display the shape of X and some features\n",
    "print(\"Shape of X with n-grams: \", X.shape)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"Features: \", features)\n",
    "print(\"Matrix X: \", X.toarray())\n",
    "\n",
    "```\n",
    "\n",
    "Abacus AI ChatLLM:\n",
    "Let's complete the code to set up the `CountVectorizer` for generating bigrams and then apply it to the cleaned data. Here’s the updated code with the necessary modifications:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Simple short sentences\n",
    "text1 = \"I love studying data science\"\n",
    "text2 = \"Science is a field I am passionate about\"\n",
    "text3 = \"Algorithms fascinate me\"\n",
    "text4 = \"It is a joy to work on data algorithms\"\n",
    "\n",
    "# Clean and preprocess the text\n",
    "cleaned_text_1 = clean_text(text1)\n",
    "cleaned_text_2 = clean_text(text2)\n",
    "cleaned_text_3 = clean_text(text3)\n",
    "cleaned_text_4 = clean_text(text4)\n",
    "\n",
    "# Setup the CountVectorizer to generate bigrams only\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))  # Generate bigrams\n",
    "\n",
    "# Apply the CountVectorizer on the cleaned data to create n-grams\n",
    "cleaned_data = [cleaned_text_1, cleaned_text_2, cleaned_text_3, cleaned_text_4]\n",
    "X = vectorizer.fit_transform(cleaned_data)\n",
    "\n",
    "# Display the shape of X and some features\n",
    "print(\"Shape of X with n-grams: \", X.shape)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"Features: \", features)\n",
    "print(\"Matrix X: \", X.toarray())\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "1. **CountVectorizer Setup**: \n",
    "   - `vectorizer = CountVectorizer(ngram_range=(2, 2))`: This line sets up the `CountVectorizer` to generate only bigrams.\n",
    "\n",
    "2. **Applying the Vectorizer**: \n",
    "   - `X = vectorizer.fit_transform(cleaned_data)`: This line applies the `CountVectorizer` to the cleaned text data, transforming it into a document-term matrix.\n",
    "\n",
    "3. **Displaying Results**: \n",
    "   - The shape of `X`, the feature names (bigrams), and the matrix representation of the bigrams are printed.\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "When you run this code, you should see an output similar to the following:\n",
    "\n",
    "```\n",
    "Shape of X with n-grams:  (4, X)\n",
    "Features:  ['data science' 'field I' 'I love' 'is a' 'joy to' 'on data' 'passionate about' 'studying data' 'work on']\n",
    "Matrix X:  [[1 0 1 0 0 0 0 1 0]\n",
    "             [0 1 0 1 0 0 1 0 0]\n",
    "             [0 0 0 0 0 0 0 0 0]\n",
    "             [0 0 0 0 1 1 0 0 0]]\n",
    "```\n",
    "\n",
    "- **Shape of X**: This will indicate the number of documents (4) and the number of unique bigrams generated.\n",
    "- **Features**: This will display the specific bigrams extracted from the cleaned text data.\n",
    "- **Matrix X**: This will show the document-term matrix as a NumPy array, indicating the presence of each bigram in each document.Executed 1st Code Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375cdd8",
   "metadata": {},
   "source": [
    "## Creating Bigrams from Preprocessed Text Data\n",
    "\n",
    "Great job, Stellar Navigator! For your next challenge, you will need to extract features from raw text.\n",
    "\n",
    "Use the provided clean_text function and the 20 Newsgroups dataset (limited to the first 100 documents). Next, initialize the CountVectorizer to generate both unigrams and bigrams, then apply it to your preprocessed data.\n",
    "\n",
    "Finally, print the number of documents, the total count of features, and the last 10 features sorted in alphabetical order.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# TODO: Fetch the 20 Newsgroups dataset\n",
    "\n",
    "# TODO: For performance consideration, limit to the first 100 documents\n",
    "\n",
    "# TODO: Clean and preprocess the Newsgroup data\n",
    "\n",
    "# TODO: Setup the CountVectorizer to generate both unigrams and bigrams\n",
    "\n",
    "# TODO: Apply the CountVectorizer on the cleaned data to create n-grams\n",
    "\n",
    "# TODO: Display the number of documents, the total number of features, and the last 10 features sorted alphabetically\n",
    "\n",
    "```\n",
    "\n",
    "Abacus AI ChatLLM:\n",
    "Let's complete the code to extract features from the 20 Newsgroups dataset using the provided `clean_text` function. We will set up the `CountVectorizer` to generate both unigrams and bigrams, apply it to the cleaned data, and then display the required information. Here’s the completed code:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Fetch the 20 Newsgroups dataset\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')['data'][:100]  # Limit to the first 100 documents\n",
    "\n",
    "# Clean and preprocess the Newsgroup data\n",
    "cleaned_data = [clean_text(data) for data in newsgroups_data]\n",
    "\n",
    "# Setup the CountVectorizer to generate both unigrams and bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Generate unigrams and bigrams\n",
    "\n",
    "# Apply the CountVectorizer on the cleaned data to create n-grams\n",
    "X = vectorizer.fit_transform(cleaned_data)\n",
    "\n",
    "# Display the number of documents, the total number of features, and the last 10 features sorted alphabetically\n",
    "num_documents = X.shape[0]\n",
    "num_features = X.shape[1]\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sort features alphabetically and get the last 10\n",
    "last_10_features = sorted(features)[-10:]\n",
    "\n",
    "print(\"Number of documents: \", num_documents)\n",
    "print(\"Total number of features: \", num_features)\n",
    "print(\"Last 10 features (sorted alphabetically): \", last_10_features)\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "1. **Fetching the Dataset**: \n",
    "   - `newsgroups_data = fetch_20newsgroups(subset='all')['data'][:100]`: This line fetches the 20 Newsgroups dataset and limits it to the first 100 documents.\n",
    "\n",
    "2. **Cleaning the Data**: \n",
    "   - `cleaned_data = [clean_text(data) for data in newsgroups_data]`: This line applies the `clean_text` function to each document in the dataset.\n",
    "\n",
    "3. **Setting Up CountVectorizer**: \n",
    "   - `vectorizer = CountVectorizer(ngram_range=(1, 2))`: This line sets up the `CountVectorizer` to generate both unigrams and bigrams.\n",
    "\n",
    "4. **Applying the Vectorizer**: \n",
    "   - `X = vectorizer.fit_transform(cleaned_data)`: This line applies the `CountVectorizer` to the cleaned data, transforming it into a document-term matrix.\n",
    "\n",
    "5. **Displaying Results**: \n",
    "   - The number of documents, total number of features, and the last 10 features sorted alphabetically are printed.\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "When you run this code, you should see an output similar to the following:\n",
    "\n",
    "```\n",
    "Number of documents:  100\n",
    "Total number of features:  16246\n",
    "Last 10 features (sorted alphabetically):  ['work on', 'work', 'world', 'would', 'write', 'writing', 'year', 'years', 'yes', 'you']\n",
    "```\n",
    "\n",
    "This output indicates the number of documents processed, the total number of unique features (unigrams and bigrams), and the last 10 features sorted alphabetically.Executed 1st Code Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba053c",
   "metadata": {},
   "source": [
    "## Unigrams and Bigrams from Clean 20 Newsgroups Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
