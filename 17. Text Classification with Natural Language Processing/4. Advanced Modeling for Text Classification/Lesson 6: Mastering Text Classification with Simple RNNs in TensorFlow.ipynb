{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e369cc8e",
   "metadata": {},
   "source": [
    "# Lesson 6: Mastering Text Classification with Simple RNNs in TensorFlow\n",
    "\n",
    "\n",
    "Hello again! In today's lesson, we'll delve into the fascinating world of Recurrent Neural Networks (RNNs) and explore their application in text classification. Whether you are new to this concept or have some familiarity with it from your Natural Language Processing (NLP) journey, you'll appreciate the unique capabilities of RNNs in handling sequential data, such as text or time series.\n",
    "\n",
    "RNNs are distinctive because they have a form of memory. They retain the output of a layer and feed it back into the input to assist in predicting the layer's outcome. To understand this better, think of how we read a novel: we don't start from scratch on each new page but build our comprehension based on all the previous pages. Similarly, RNNs remember everything they've processed up to a given point, using this information to generate current output.\n",
    "\n",
    "Due to their ability to capture temporal dependencies in sequences, RNNs excel in NLP tasks. They leverage past information to understand context more effectively, making them ideal for language modeling, translation, sentiment analysis, and our focus for today â€” text classification.\n",
    "\n",
    "## Pre-processing for Text Classification\n",
    "\n",
    "Before we proceed, it's crucial to recall the pre-processing steps performed on our data:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)\n",
    "\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "```\n",
    "\n",
    "This pre-processing step ensures our RNN model receives inputs in a compatible and meaningful format, allowing it to learn effectively from the textual information presented.\n",
    "\n",
    "## Building and Training a Simple RNN Model Using TensorFlow\n",
    "\n",
    "Armed with an understanding of RNNs, it's time to build and train a simple RNN model with TensorFlow.\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=100, output_dim=8),\n",
    "    tf.keras.layers.SimpleRNN(16),\n",
    "    tf.keras.layers.Dense(len(categories), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=64)\n",
    "```\n",
    "\n",
    "The training process indicates a gradual improvement in accuracy and a decrease in loss, demonstrating our model's learning journey:\n",
    "\n",
    "```\n",
    " 1/27 - accuracy: 0.0469 - loss: 0.8066\n",
    "...\n",
    "27/27 - accuracy: 0.6404 - loss: 0.6420 - val_accuracy: 0.9657 - val_loss: 0.2967\n",
    "```\n",
    "\n",
    "## Exploring the Model's Details\n",
    "\n",
    "After training, let's examine our model's architecture and parameters with `model.summary()`:\n",
    "\n",
    "```python\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "This reveals the structure and parameters of our RNN model:\n",
    "\n",
    "```\n",
    "Model: \"sequential\"\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
    "â”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ\n",
    "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
    "â”‚ embedding (Embedding)           â”‚ (None, 50, 8)          â”‚           800 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ simple_rnn (SimpleRNN)          â”‚ (None, 16)             â”‚           400 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ dense (Dense)                   â”‚ (None, 2)              â”‚            34 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "Total params: 3,704 (14.47 KB)\n",
    "Trainable params: 1,234 (4.82 KB)\n",
    "Non-trainable params: 0 (0.00 B)\n",
    "Optimizer params: 2,470 (9.65 KB)\n",
    "```\n",
    "\n",
    "## Evaluating the Model\n",
    "\n",
    "After understanding our model's architecture, we evaluate its performance on unseen data (`X_test`, `y_test`) to gauge its effectiveness:\n",
    "\n",
    "```python\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "The output will be:\n",
    "\n",
    "```\n",
    "Loss: 0.3834\n",
    "Accuracy: 0.9700\n",
    "```\n",
    "\n",
    "## Conclusion and Next Steps\n",
    "\n",
    "By walking through the construction, training, and evaluation of a Simple RNN for text classification, you've gained a practical insight into harnessing the power of RNNs within TensorFlow for NLP tasks. Understanding how to leverage past information in sequential data opens up numerous avenues for effective text analysis.\n",
    "\n",
    "To solidify your comprehension, proceed to the practice exercises in the next section. These exercises are tailored to challenge and expand your understanding further. Happy learning!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8bc1bd",
   "metadata": {},
   "source": [
    "## Changing Activation Function in Dense Layer\n",
    "\n",
    "Great going, Stellar Navigator! Now, let's tread on a different path. Instead of changing the number of neurons in SimpleRNN, let's modify something else. Change the activation function of the Dense layer in your model. Currently, it's set to 'softmax'. Replace 'softmax' with 'sigmoid', as per Keras standards, and observe any subsequent changes in the model's output.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=100, output_dim=8),\n",
    "    tf.keras.layers.SimpleRNN(32), \n",
    "    tf.keras.layers.Dense(len(categories), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=100, output_dim=8),\n",
    "    tf.keras.layers.SimpleRNN(32), \n",
    "    tf.keras.layers.Dense(len(categories), activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "I've changed the activation function of the Dense layer from 'softmax' to 'sigmoid'. Let me know if you need further modifications or observations on the model's behavior! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020871e",
   "metadata": {},
   "source": [
    "## Configuring SimpleRNN and Dense Layers in Model\n",
    "\n",
    "Stellar work, Space Voyager! Now, let's continue. In the blanks within the code, please fill in the missing details for our SimpleRNN layer and Dense layer. Do you recall the number of neurons for the SimpleRNN and the number of classes for the Dense layer? Let's verify it!\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=100, output_dim=8),  \n",
    "    tf.keras.layers.SimpleRNN(____), \n",
    "    tf.keras.layers.Dense(len(____), ____='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "```\n",
    "\n",
    "Here's the corrected version of your code with the missing details filled in:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)\n",
    "\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=100, output_dim=8),  \n",
    "    tf.keras.layers.SimpleRNN(16),  # 16 neurons for the SimpleRNN layer\n",
    "    tf.keras.layers.Dense(len(categories), activation='softmax')  # Number of classes = len(categories)\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **`tf.keras.layers.SimpleRNN(16)`** â†’ The RNN layer has **16 neurons**, as specified in the original lesson.\n",
    "2. **`len(categories)`** â†’ The Dense layer's output corresponds to the **number of classes**, which is determined by `len(categories)`.\n",
    "3. **`activation='softmax'`** â†’ Used for multi-class classification.\n",
    "\n",
    "This should work perfectly for your text classification model. ğŸš€ Let me know if you need further clarifications!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b945d95",
   "metadata": {},
   "source": [
    "## Fill in the blanks: Building a Simple RNN with TensorFlow\n",
    "\n",
    "Stellar work, Voyager! We're diving into TensorFlow to construct a simple Recurrent Neural Network (RNN), and for that, I have removed some important components from the provided code. Your mission, should you choose to accept it, is to fill in the blanks (represented by ____) with the correct values or variable names to make the code run successfully. Remember the previous lesson when doing so! Good luck!\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(____, padding='post', maxlen=50)\n",
    "y = ____().fit_transform(categories_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.____(input_dim=100, output_dim=8),\n",
    "    tf.keras.layers.____(____), \n",
    "    tf.keras.layers.Dense(len(categories), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=____, optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "Here's your completed code with all the blanks filled in correctly:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)  # Filling in sequences\n",
    "\n",
    "y = LabelEncoder().fit_transform(categories_data)  # Filling in LabelEncoder\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=100, output_dim=8),  # Filling in Embedding layer\n",
    "    tf.keras.layers.SimpleRNN(16),  # Filling in SimpleRNN with 16 neurons\n",
    "    tf.keras.layers.Dense(len(categories), activation='softmax')  # Filling in Dense layer\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Filling in loss function\n",
    "\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **`pad_sequences(sequences, padding='post', maxlen=50)`** â†’ Ensures sequences have the same length.\n",
    "2. **`LabelEncoder().fit_transform(categories_data)`** â†’ Converts categorical labels into numerical form.\n",
    "3. **`tf.keras.layers.Embedding(input_dim=100, output_dim=8)`** â†’ Converts word indices to dense vectors.\n",
    "4. **`tf.keras.layers.SimpleRNN(16)`** â†’ The recurrent layer with **16 neurons**.\n",
    "5. **`tf.keras.layers.Dense(len(categories), activation='softmax')`** â†’ The output layer with **softmax activation** for classification.\n",
    "6. **`loss='sparse_categorical_crossentropy'`** â†’ Since labels are encoded as integers, we use sparse categorical cross-entropy.\n",
    "\n",
    "You're now ready to train a simple RNN text classifier! ğŸš€ Let me know if you need more adjustments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c339a53",
   "metadata": {},
   "source": [
    "## Adding Layers to the RNN Model\n",
    "\n",
    "Great job, Space Voyager! It's time to stretch your wings a bit. I've left out the lines for adding the SimpleRNN and Dense layers to our model. Your mission is to complete these TODO lines. Remember, the SimpleRNN layer uses the number of neurons as an argument, and the Dense layer requires an activation function.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=100, output_dim=8),  \n",
    "    # TODO: Add the SimpleRNN layer with 16 neurons\n",
    "    # TODO: Add the Dense layer with 'softmax' activation function\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "```\n",
    "\n",
    "Below is the complete code with the SimpleRNN and Dense layers added. We use a SimpleRNN layer with 16 neurons and a Dense layer with 2 neurons (one for each category) with a softmax activation function. This will build our RNN model with the appropriate layers.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Ensure that the Reuters corpus is downloaded (uncomment if needed)\n",
    "# nltk.download('reuters')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Select a couple of categories to simplify our example.\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=100, output_dim=8),\n",
    "    # Adding the SimpleRNN layer with 16 neurons\n",
    "    tf.keras.layers.SimpleRNN(16),\n",
    "    # Adding the Dense layer with 2 neurons and 'softmax' activation function\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "Explanation\n",
    "The Embedding layer converts input words into dense vectors of fixed size.\n",
    "The SimpleRNN layer with 16 neurons processes the sequence data from the embeddings.\n",
    "The Dense layer with 2 neurons and softmax activation outputs a probability distribution over our two classes.\n",
    "We compile the model with a sparse categorical cross-entropy loss function, which is suitable when classes are integer encoded.\n",
    "Finally, we fit the model for one epoch, evaluate it on the test set, and print out the loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390077d-4efd-4d34-92b8-5d33765a8a08",
   "metadata": {},
   "source": [
    "Excellent work, Space Voyager! Now, for the final challenge in this lesson: using TensorFlow, create a SimpleRNN model from scratch for text classification with two categories. Compile and fit the model to our text data, then evaluate its performance on the test data. Keep on exploring!\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# TODO: Create a Sequential model\n",
    "# TODO: Add an Embedding layer to the model\n",
    "# TODO: Add a SimpleRNN layer to the model\n",
    "# TODO: Add a Dense layer to the model\n",
    "# TODO: Compile the model\n",
    "# TODO: Fit the model to the training data\n",
    "# TODO: Evaluate the model's performance on the test data\n",
    "\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "```\n",
    "\n",
    "Here's your completed TensorFlow SimpleRNN model for text classification with two categories:  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "# Load and preprocess data\n",
    "categories = reuters.categories()[:2]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Create a Sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=100, output_dim=8),  # Embedding layer\n",
    "    tf.keras.layers.SimpleRNN(16),  # SimpleRNN layer with 16 neurons\n",
    "    tf.keras.layers.Dense(2, activation='softmax')  # Dense layer with 2 output classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "### Breakdown:\n",
    "1. **Tokenization & Padding**:  \n",
    "   - Converts text into numerical sequences.  \n",
    "   - Pads sequences to ensure uniform input length.  \n",
    "2. **Model Architecture**:  \n",
    "   - **Embedding Layer**: Converts word indices into dense vectors.  \n",
    "   - **SimpleRNN Layer**: 16 recurrent neurons to process sequence data.  \n",
    "   - **Dense Layer**: 2 neurons (one per category) with softmax activation.  \n",
    "3. **Compilation & Training**:  \n",
    "   - **Loss Function**: Sparse categorical cross-entropy (suitable for integer labels).  \n",
    "   - **Optimizer**: Adam (adaptive learning rate).  \n",
    "   - **Metrics**: Accuracy.  \n",
    "\n",
    "ğŸš€ Now, your model is ready to classify text into two categories! Keep exploring! ğŸ’¡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
