{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea7de09",
   "metadata": {},
   "source": [
    "# Lesson 1: Ensemble Methods in NLP: Mastering Bagging for Text Classification\n",
    "\n",
    "# Introduction to Ensemble Methods and BAGGING\n",
    "\n",
    "Hello there! In this lesson, we'll dive into the fascinating world of machine learning ensemble methods. Ensemble methods are based on a simple but powerful concept: a team of learners, or algorithms, can achieve better results working together than any individual learner on its own.\n",
    "\n",
    "## What is Bagging?\n",
    "\n",
    "Bagging, which stands for Bootstrap Aggregating, is a prime example of an ensemble method. In the context of this course, where we are working with the Reuters-21578 Text Categorization Collection, our goal is to train a model that can accurately predict the category of a document based on its text.\n",
    "\n",
    "Bagging helps us achieve this by:\n",
    "- Building multiple base learners (e.g., Decision Trees) on random subsets (bootstrapped samples) of the original dataset.\n",
    "- Aggregating their predictions to yield a final verdict.\n",
    "- For classification tasks, taking the mode of the predictions from each model.\n",
    "\n",
    "Bagging enhances model robustness by diminishing overfitting risks, effectively reducing variance without significantly increasing bias.\n",
    "\n",
    "## Loading and Inspecting the Reuters-21578 Data\n",
    "\n",
    "We'll use the Reuters-21578 Text Categorization Collection, a widely-used dataset for document classification, available via the NLTK library.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "categories = reuters.categories()[:5]  # limiting to 5 categories\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "print(len(categories))  \n",
    "print(len(documents))  \n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "5\n",
    "2648\n",
    "```\n",
    "\n",
    "## Understanding the Reuters-21578 Dataset\n",
    "\n",
    "The dataset consists of news documents categorized by Reuters in the late 1980s. Let's explore it further:\n",
    "\n",
    "```python\n",
    "# Printing the categories\n",
    "print(\"Selected Categories:\", categories)\n",
    "\n",
    "# Printing the content of one document\n",
    "doc_id = documents[0]  \n",
    "print(\"\\nDocument ID:\", doc_id)\n",
    "print(\"Category:\", reuters.categories(doc_id))\n",
    "print(\"Content excerpt:\\n\", \" \".join(reuters.words(doc_id)[:50]))\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Selected Categories: ['acq', 'alum', 'barley', 'bop', 'carcass']\n",
    "\n",
    "Document ID: test/14843\n",
    "Category: ['acq']\n",
    "Content excerpt:\n",
    " SUMITOMO BANK AIMS AT QUICK RECOVERY FROM MERGER...\n",
    "```\n",
    "\n",
    "## Feature Extraction Using Count Vectorizer\n",
    "\n",
    "Before applying machine learning, we need to convert text into numerical format using `CountVectorizer` from scikit-learn.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Preparing the dataset\n",
    "text_data = [\" \".join(reuters.words(fileid)) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Using count vectorizer for feature extraction\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Encoding the category data\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(categories_data)\n",
    "\n",
    "print(\"Categories:\\n\", categories_data[:5])\n",
    "print(\"Encoded Categories:\\n\", y[:5])\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Categories:\n",
    " ['acq', 'acq', 'carcass', 'bop', 'acq']\n",
    "Encoded Categories:\n",
    " [0 0 4 3 0]\n",
    "```\n",
    "\n",
    "## Applying Bagging for Text Classification\n",
    "\n",
    "We will now apply the `BaggingClassifier` using Decision Trees as base learners.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Initiating the BaggingClassifier\n",
    "bag_classifier = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state=1)\n",
    "bag_classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Generate predictions on the test data\n",
    "y_pred = bag_classifier.predict(X_test.toarray())\n",
    "\n",
    "# Displaying the predicted category for the first document\n",
    "print(\"Predicted Category: \", label_encoder.inverse_transform([y_pred[0]])[0])\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Predicted Category:  acq\n",
    "```\n",
    "\n",
    "## Performance Evaluation Using Classification Report\n",
    "\n",
    "To evaluate our model's performance, we'll use a classification report.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Checking the performance of the model on test data\n",
    "y_pred = bag_classifier.predict(X_test.toarray())\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99       601\n",
    "           1       0.82      0.93      0.87        15\n",
    "           2       1.00      1.00      1.00        12\n",
    "           3       0.91      0.95      0.93        22\n",
    "           4       0.90      0.75      0.82        12\n",
    "\n",
    "    accuracy                           0.99       662\n",
    "   macro avg       0.93      0.93      0.92       662\n",
    "weighted avg       0.99      0.99      0.99       662\n",
    "```\n",
    "\n",
    "## Lesson Summary\n",
    "\n",
    "In this lesson, you learned:\n",
    "- How ensemble methods, particularly Bagging, enhance classification performance.\n",
    "- The importance of feature extraction using `CountVectorizer`.\n",
    "- The application of `BaggingClassifier` with Decision Trees.\n",
    "- Model evaluation using a classification report.\n",
    "\n",
    "In the upcoming exercises, you'll apply and reinforce these concepts. Happy coding! ðŸš€\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10969c35",
   "metadata": {},
   "source": [
    "## Exploring the Last Documents and Categories\n",
    "\n",
    "Great stuff, Stellar Navigator! You've now seen a broader overview of the dataset, but there's more to uncover. Change the existing code to examine the second document that is associated with more than two categories instead of the first, and print out both its categories and raw contents. Also, print the words of this document. This will further improve your understanding of the data formatting.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "print(\"Total Categories:\", len(categories))\n",
    "print(\"Total Documents:\", len(documents))\n",
    "\n",
    "multi_cat_docs = [doc for doc in documents if len(reuters.categories(doc)) > 2]\n",
    "\n",
    "doc_id = multi_cat_docs[0]\n",
    "\n",
    "print(f\"Document with more than 2 Categories: {doc_id}\")\n",
    "print(\"Categories:\", reuters.categories(doc_id))\n",
    "print(\"Raw Document:\\n\", reuters.raw(doc_id))\n",
    "print(\"Words:\\n\", \" \".join(reuters.words(doc_id)))\n",
    "```\n",
    "\n",
    "## Introduction to Ensemble Methods and BAGGING\n",
    "\n",
    "Hello there! In this lesson, we'll dive into the fascinating world of machine learning ensemble methods. Ensemble methods are based on a simple but powerful concept: a team of learners, or algorithms, can achieve better results working together than any individual learner on its own.\n",
    "\n",
    "Bagging, which stands for Bootstrap Aggregating, is a prime example of an ensemble method. In the context of this course, where we are working with the Reuters-21578 Text Categorization Collection, our goal is to train a model that can accurately predict the category of a document based on its text. Bagging helps us achieve this by building multiple base learners (for instance, Decision Trees) on random subsets (bootstrapped samples) of the original dataset. Then, it aggregates their predictions to yield a final verdict. For classification tasksâ€”like the text classification scenario we're addressing hereâ€”the aggregation occurs by taking the mode of the predictions from each model. This means we look for the most frequently predicted category across all models for any given observation. The beauty of Bagging lies in its ability to enhance model robustness by diminishing overfitting risks, effectively reducing variance without significantly increasing bias.\n",
    "\n",
    "In text classification tasks, using Bagging can lead to marked improvements in model performance. By applying Bagging to our text data, we increase the predictive generalization capabilities of our model. Let's embark on this journey and put Bagging into action with text data, focusing on its mechanism and benefits in the sections to come.\n",
    "\n",
    "## Loading and Inspecting the Reuters-21578 Data\n",
    "\n",
    "Let's start by loading our dataset. We'll be using the Reuters-21578 Text Categorization Collection, a widely-used text dataset for document categorization and classification tasks. It is available via the NLTK (Natural Language Toolkit) library, which is the go-to library for natural language processing in Python.\n",
    "\n",
    "Let's load the data and print the number of categories and documents:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "categories = reuters.categories()\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "print(\"Total Categories:\", len(categories))\n",
    "print(\"Total Documents:\", len(documents))\n",
    "\n",
    "multi_cat_docs = [doc for doc in documents if len(reuters.categories(doc)) > 2]\n",
    "\n",
    "doc_id = multi_cat_docs[1]  # Selecting the second document with more than two categories\n",
    "\n",
    "print(f\"Document with more than 2 Categories: {doc_id}\")\n",
    "print(\"Categories:\", reuters.categories(doc_id))\n",
    "print(\"Raw Document:\\n\", reuters.raw(doc_id))\n",
    "print(\"Words:\\n\", \" \".join(reuters.words(doc_id)))\n",
    "```\n",
    "\n",
    "This output allows us to examine a document associated with more than two categories, providing deeper insights into the dataset's structure and formatting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1617f0",
   "metadata": {},
   "source": [
    "## Finding Documents with Specific Category Count\n",
    "\n",
    "Good job, Space Voyager! Let's put your skills to the test. Fill in the blank spots (____) to iterate over the documents from the Reuters dataset, find the third-to-last document with exactly four categories, print its categories, and, finally, display the words contained in it. You're doing amazing! Keep it up!\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.____()\n",
    "documents = reuters.fileids(____)\n",
    "\n",
    "print(f\"Total Categories: {len(categories)}\")\n",
    "print(f\"Total Documents: {len(documents)}\")\n",
    "\n",
    "multi_cat_docs = [doc for doc in documents if len(reuters.categories(doc)) == 4]\n",
    "\n",
    "doc_id = multi_cat_docs[____]\n",
    "\n",
    "print(f\"Document with exactly 4 Categories: {doc_id}\")\n",
    "print(\"Categories:\", reuters.categories(doc_id))\n",
    "print(\"Words:\\n\", \" \".join(reuters.words(doc_id)))\n",
    "\n",
    "```\n",
    "\n",
    "Here's the corrected version of your code with the missing parts filled in:  \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()\n",
    "documents = reuters.fileids()\n",
    "\n",
    "print(f\"Total Categories: {len(categories)}\")\n",
    "print(f\"Total Documents: {len(documents)}\")\n",
    "\n",
    "multi_cat_docs = [doc for doc in documents if len(reuters.categories(doc)) == 4]\n",
    "\n",
    "doc_id = multi_cat_docs[-3]  # Third-to-last document with exactly four categories\n",
    "\n",
    "print(f\"Document with exactly 4 Categories: {doc_id}\")\n",
    "print(\"Categories:\", reuters.categories(doc_id))\n",
    "print(\"Words:\\n\", \" \".join(reuters.words(doc_id)))\n",
    "```\n",
    "\n",
    "This will correctly find the third-to-last document that belongs to exactly four categories and print its categories and words. ðŸš€ Keep up the great work, Space Voyager! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b04970",
   "metadata": {},
   "source": [
    "## Implement Bagging Classifier and Evaluate Model Performance\n",
    "\n",
    "Stellar Navigator, fill the gaps (____) in the script given below. You need to import the necessary modules, prepare the text and category data, perform the feature extraction using CountVectorizer, use the Bagging Classifier, and finally, evaluate the model using a Classification Report. Follow your insights from the lesson and good luck!\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import ____\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "categories = ____.categories()[:5]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Using count vectorizer for feature extraction\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Split the data for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Using BaggingClassifier with DecisionTrees\n",
    "bag_classifier = ____(____(), n_estimators=100, random_state=1)\n",
    "bag_classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Checking the performance of the model on test data\n",
    "y_pred = bag_classifier.predict(X_test.toarray())\n",
    "\n",
    "# Printing the first 10 values of y_test and y_pred before the classification report\n",
    "print(f\"First 10 y_test values: {y_test[:10]}\")\n",
    "print(f\"First 10 y_pred values: {y_pred[:10]}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "```\n",
    "\n",
    "Here's the completed script with the missing parts filled in:  \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import reuters  # Import the Reuters dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download Reuters dataset if not already downloaded\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "# Selecting first 5 categories\n",
    "categories = reuters.categories()[:5]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Preparing text and category data\n",
    "text_data = [\" \".join(reuters.words(fileid)) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]  # Selecting the first category for each document\n",
    "\n",
    "# Using CountVectorizer for feature extraction\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Encoding category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(categories_data)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Using BaggingClassifier with DecisionTrees\n",
    "bag_classifier = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state=1)\n",
    "bag_classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Checking the performance of the model on test data\n",
    "y_pred = bag_classifier.predict(X_test.toarray())\n",
    "\n",
    "# Printing the first 10 values of y_test and y_pred before the classification report\n",
    "print(f\"First 10 y_test values: {y_test[:10]}\")\n",
    "print(f\"First 10 y_pred values: {y_pred[:10]}\")\n",
    "\n",
    "# Evaluating the model\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "```\n",
    "\n",
    "This script successfully imports the necessary modules, processes the dataset, extracts features using `CountVectorizer`, trains a `BaggingClassifier` with `DecisionTreeClassifier`, and evaluates its performance using a classification report. ðŸš€ Stellar job, Navigator! Keep up the great work! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e6005",
   "metadata": {},
   "source": [
    "## Bagging Classifier with Different Parameters Evaluation\n",
    "\n",
    "Load the dataset and prepare it for training. Then, add the initialization code for the Bagging Classifier with specific parameters. A Decision Tree Classifier should be used as the base estimator. Feed the training data to the fit method of the Classifier. Evaluate the model on the test data. The status from your performance observation will be stellar!\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# TODO: Load the Reuters dataset but only consider the first 5 categories.\n",
    "\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Using count vectorizer for feature extraction\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Split the data for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# TODO: Initialize the Bagging Classifier with a Decision Tree Classifier with a specific number of estimators and a given seed.\n",
    "\n",
    "bag_classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Check the performance of the model on test data\n",
    "y_pred = bag_classifier.predict(X_test.toarray())\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\n",
    "```\n",
    "\n",
    "Hereâ€™s the completed script with the missing parts filled in:  \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download Reuters dataset if not already downloaded\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "# Load the Reuters dataset but only consider the first 5 categories\n",
    "categories = reuters.categories()[:5]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Prepare text and category data\n",
    "text_data = [\" \".join(reuters.words(fileid)) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]  # Select first category for each document\n",
    "\n",
    "# Feature extraction using CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(categories_data)\n",
    "\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Initialize the Bagging Classifier with a Decision Tree Classifier\n",
    "bag_classifier = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=10),  # Decision Tree with max depth 10\n",
    "    n_estimators=100,  # Number of estimators\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "bag_classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = bag_classifier.predict(X_test.toarray())\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "```\n",
    "\n",
    "### Changes and Enhancements:\n",
    "âœ… **Loaded only the first 5 categories**  \n",
    "âœ… **Used `DecisionTreeClassifier(max_depth=10)` as the base estimator**  \n",
    "âœ… **Set `n_estimators=100` for BaggingClassifier**  \n",
    "âœ… **Ensured proper dataset preparation**  \n",
    "âœ… **Trained and evaluated the model using `classification_report`**  \n",
    "\n",
    "ðŸš€ You're now ready to analyze the model's performance! Keep exploring, Space Voyager! ðŸŒŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133edaa",
   "metadata": {},
   "source": [
    "## Text Classification Using Bagging Classifier\n",
    "\n",
    "Stellar Navigator, it's your turn to demonstrate your mastery of ensemble methods and bagging! Based on the Reuters-21578 dataset, you should write a pipeline script that enables you to load the dataset, prepare the feature matrix and target vector, build the Bagging Classifier model, and print the classification report. Good luck!\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# TODO: Load the first 5 categories from Reuters dataset\n",
    "\n",
    "# TODO: Prepare the text and categories data\n",
    "\n",
    "# Using count vectorizer for feature extraction\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Split the data for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# TODO: Initialize and fit the BaggingClassifier with Decision Tree as base estimator\n",
    "\n",
    "# TODO: Predict on the test data\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "```\n",
    "\n",
    "Hereâ€™s the completed script for **Text Classification Using Bagging Classifier** on the Reuters dataset:  \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download dataset if needed\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "# Load the first 5 categories from Reuters dataset\n",
    "categories = reuters.categories()[:5]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Prepare the text and categories data\n",
    "text_data = [\" \".join(reuters.words(fileid)) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]  # Use the first category per document\n",
    "\n",
    "# Feature extraction using CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(categories_data)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the BaggingClassifier with Decision Tree as base estimator\n",
    "bag_classifier = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=10),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "bag_classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = bag_classifier.predict(X_test.toarray())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "```\n",
    "\n",
    "### Enhancements:\n",
    "âœ… **Selected first 5 categories from Reuters dataset**  \n",
    "âœ… **Applied `CountVectorizer(max_features=1000)` for feature extraction**  \n",
    "âœ… **Encoded categorical labels using `LabelEncoder()`**  \n",
    "âœ… **Split dataset into 80% training and 20% testing**  \n",
    "âœ… **Used `BaggingClassifier` with `DecisionTreeClassifier(max_depth=10)`**  \n",
    "âœ… **Trained and evaluated model, printing a classification report**  \n",
    "\n",
    "ðŸš€ Your **Bagging Classifier** is now ready for text classification! Keep up the stellar work, Navigator! ðŸŒŸ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
