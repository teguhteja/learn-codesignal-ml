{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa60040",
   "metadata": {},
   "source": [
    "# Lesson 3: Boosting Text Classification Power with Gradient Boosting Classifier\n",
    "\n",
    "Greetings learners! Prepare to immerse yourself in advanced text classification techniques as we explore an advanced ensemble method: the **Gradient Boosting Classifier**. By the end of this lesson, you will have a sound understanding of this ensemble method and also gain practical experience in applying it using **Python** and **Scikit-learn**.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Recap on Dataset Preparation\n",
    "\n",
    "First, let's review a few steps that should already be familiar: loading required libraries and preparing the dataset, which is the **Reuters-21578 Text Categorization Collection**.\n",
    "\n",
    "### Python Code:\n",
    "\n",
    "```python\n",
    "# Import required libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "# Limiting the data for quick execution\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Preparing the dataset\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Using CountVectorizer for feature extraction\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Split the data for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "```\n",
    "\n",
    "This code prepares the dataset, using **CountVectorizer** for feature extraction, **LabelEncoder** for changing categories into numeric format, and splitting our data into training and test sets.\n",
    "\n",
    "---\n",
    "\n",
    "## Inside the Gradient Boosting Classifier\n",
    "\n",
    "**Gradient Boosting Classifier** is an ensemble learning technique that fine-tunes its accuracy iteratively by addressing the inaccuracies of prior models, predominantly employing **decision trees** as its weak learners. The process unfolds through several critical stages:\n",
    "\n",
    "1. **Initial Prediction**:  \n",
    "   - It starts with a simple model, often predicting a constant value (like the mean of the target variable), setting the stage for improvement.\n",
    "\n",
    "2. **Iterative Correction**:  \n",
    "   - The essence of **Gradient Boosting** is its ability to learn from the mistakes of previous iterations.  \n",
    "   - It focuses on the **residuals**—the differences between the predicted and actual values.  \n",
    "   - Each new tree in the ensemble attempts to correct these residuals, aiming to **minimize a loss function** reflective of these errors.\n",
    "\n",
    "3. **Learning Rate**:  \n",
    "   - This parameter moderates the contribution of each new tree.  \n",
    "   - A **smaller learning rate** demands more trees to achieve high accuracy but fosters a model that's less prone to overfitting.  \n",
    "   - Conversely, a **larger learning rate** can hasten learning but increase the risk of overfitting by overly adjusting to the training data.\n",
    "\n",
    "4. **Controlling Complexity**:  \n",
    "   - To prevent overfitting, **Gradient Boosting** limits each tree's complexity, primarily using the **max_depth** parameter.  \n",
    "   - This control ensures that individual trees do not grow too complex and start modeling the **noise** within the training data.\n",
    "\n",
    "5. **Optimal Number of Trees**:  \n",
    "   - The algorithm iteratively adds trees until it reaches the specified number (**n_estimators**) or until adding new trees does not significantly reduce the error.  \n",
    "   - This balance is crucial as **too few trees** might not capture all the data patterns, while **too many** could lead to overfitting.\n",
    "\n",
    "📌 **In summary**, Gradient Boosting sequentially builds upon previous trees to correct errors, with careful adjustments of parameters like the **learning rate** and **max depth** to ensure a robust model. Its adaptive nature makes it exceptionally powerful for tasks including text classification, albeit requiring thoughtful parameter tuning to balance complexity with generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing Gradient Boosting Classifier for Text Classification\n",
    "\n",
    "The main attraction is the **Gradient Boosting Classifier**. Let's set up and implement it now.\n",
    "\n",
    "### Python Code:\n",
    "\n",
    "```python\n",
    "# Instantiate the GradientBoostingClassifier with tuned parameters\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "# Train the classifier\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "Here, we create an instance of the **GradientBoostingClassifier** with the following parameters:\n",
    "\n",
    "- **n_estimators** (boosting stages) = `100`\n",
    "- **learning_rate** (model learning speed) = `0.1`\n",
    "- **max_depth** (tree depth) = `3`\n",
    "\n",
    "After this setup, the model is trained using **fit()**, and predictions are made on the test data.\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Evaluation\n",
    "\n",
    "With our model trained and having made some predictions, let's assess its performance.\n",
    "\n",
    "### Python Code:\n",
    "\n",
    "```python\n",
    "# Evaluate the performance of the classifier\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "🔹 **Output:**\n",
    "```\n",
    "Accuracy:  0.9852150537634409\n",
    "```\n",
    "\n",
    "📌 The `accuracy_score` function compares predicted values (`y_pred`) to actual test categories (`y_test`).  \n",
    "📊 **Result**: Our Gradient Boosting model predicts **98.5%** of the instances correctly!\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Today, you learned about the **Gradient Boosting Classifier**, its workings, and how to implement and evaluate it. Advanced ensemble methods like this give you a significant edge in **NLP tasks**.\n",
    "\n",
    "🚀 **Key Takeaways:**\n",
    "- Gradient Boosting **iteratively improves predictions** by minimizing residual errors.\n",
    "- **Learning rate** and **tree depth** must be carefully tuned to balance accuracy and overfitting.\n",
    "- Our **Gradient Boosting Classifier achieved 98.5% accuracy** on the Reuters dataset.\n",
    "\n",
    "🔹 **Next Steps:**  \n",
    "\"**Theory without practice is empty.**\" Sharpen your skills by experimenting with different **parameters**.  \n",
    "For instance, try setting `learning_rate=0.2` and `n_estimators=150` to see the impact!  \n",
    "Keep exploring, and **onwards to mastery!** 🚀\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82080e",
   "metadata": {},
   "source": [
    "## Tuning Learning Rate for Gradient Boosting Classifier\n",
    "\n",
    "Galactic Analyst, your next mission takes you into the depths of NLP to master the dynamic art of parameter tuning on a Gradient Boosting Classifier for classifying Reuters news articles. Achieve equilibrium by setting the learning_rate to 0.2 and increasing n_estimators to 150. This balance is vital; it's a dance between learning speed and model complexity. Post adjustments, retrain and analyze your model's enhanced ability to discern the nuances of textual data.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "text_data = [\" \".join(reuters.words(fileid)) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "It looks like you're looking to tune the **learning rate** and **n_estimators** of the **Gradient Boosting Classifier** for better performance. Below is the updated Python code implementing the suggested changes:\n",
    "\n",
    "### 🔧 **Tuned Gradient Boosting Classifier**  \n",
    "- **Increased `n_estimators` to 150** (more boosting iterations for refined learning).  \n",
    "- **Increased `learning_rate` to 0.2** (faster learning, but requires monitoring for overfitting).  \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "# Load categories and documents\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Prepare text and labels\n",
    "text_data = [\" \".join(reuters.words(fileid)) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Feature extraction with CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Tune and train the Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=150, learning_rate=0.2, max_depth=3)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate accuracy\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "print(\"Tuned Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### 📊 **Expected Outcome:**\n",
    "- Faster learning with `learning_rate=0.2`, but ensure it doesn’t overfit.  \n",
    "- More iterations (`n_estimators=150`) refine performance.  \n",
    "- Accuracy should improve or remain stable—monitor for overfitting.  \n",
    "\n",
    "🚀 **Next Steps**: Try tuning **max_depth** and **subsample** to further optimize your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e442ab3",
   "metadata": {},
   "source": [
    "## Implementing and Training a Gradient Boosting Classifier\n",
    "\n",
    "Stellar Navigator, are you ready for the next data exploration mission? Let's fill in the blank spots (____) in the code. You need to instantiate the GradientBoostingClassifier and fit the model to the training data. Good journey!\n",
    "\n",
    "```python\n",
    "# import required libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "\n",
    "# Limiting the data for quick execution\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Preparing the dataset\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Using CountVectorizer for feature extraction\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Split the data for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Instantiate the GradientBoostingClassifier\n",
    "gb_clf = ____(____=100, learning_rate=0.1, max_depth=3) \n",
    "\n",
    "# Train the classifier\n",
    "gb_clf.____(____, ____)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "Here’s your **completed Python code** with the missing parts filled in:\n",
    "\n",
    "```python\n",
    "# import required libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "\n",
    "# Limiting the data for quick execution\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Preparing the dataset\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Using CountVectorizer for feature extraction\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Split the data for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Instantiate the GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3) \n",
    "\n",
    "# Train the classifier\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### 🔥 **Fixes & Explanation**:\n",
    "1. **`GradientBoostingClassifier` instantiation:**\n",
    "   ```python\n",
    "   gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "   ```\n",
    "   - **`n_estimators=100`** → Number of boosting stages.\n",
    "   - **`learning_rate=0.1`** → Controls weight of new trees.\n",
    "   - **`max_depth=3`** → Depth of each decision tree.\n",
    "\n",
    "2. **Fitting the model:**\n",
    "   ```python\n",
    "   gb_clf.fit(X_train, y_train)\n",
    "   ```\n",
    "   - `.fit()` trains the model using training data.\n",
    "\n",
    "### 🚀 **Mission Accomplished!**\n",
    "Now, your **Gradient Boosting Classifier** is fully functional! 🔥 Run it and analyze the accuracy. You can experiment with tuning `learning_rate` or `n_estimators` to optimize results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9baa0a",
   "metadata": {},
   "source": [
    "## Setting Learning Rate and Making Predictions with GradientBoostingClassifier\n",
    "\n",
    "Greetings, Space Explorer! Understanding how to control the learning speed while building a gradient-boosting model is vital. How do you achieve this? By tuning the learning rate, of course! Let's continue practicing. Insert code to set the learning_rate as 0.1, and also predict results using the trained model. Your journey awaits!\n",
    "\n",
    "```python\n",
    "# import required libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Instantiate the GradientBoostingClassifier and set max_depth\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=____, max_depth=5)\n",
    "\n",
    "# Train the classifier\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ____.____(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy: \", ____(y_test, y_pred))\n",
    "\n",
    "```\n",
    "\n",
    "Here’s your **completed Python code** with the missing parts filled in:\n",
    "\n",
    "```python\n",
    "# import required libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Instantiate the GradientBoostingClassifier and set max_depth\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "\n",
    "# Train the classifier\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### ✅ **Fixes & Explanation**:\n",
    "1. **Setting the learning rate** → `learning_rate=0.1`\n",
    "   ```python\n",
    "   gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "   ```\n",
    "   - **`learning_rate=0.1`** → Controls the contribution of each tree in boosting.\n",
    "\n",
    "2. **Making predictions**:\n",
    "   ```python\n",
    "   y_pred = gb_clf.predict(X_test)\n",
    "   ```\n",
    "   - Uses the trained model to predict on test data.\n",
    "\n",
    "3. **Printing accuracy**:\n",
    "   ```python\n",
    "   print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "   ```\n",
    "   - `accuracy_score(y_test, y_pred)` → Compares predicted vs. actual labels.\n",
    "\n",
    "### 🚀 **Mission Accomplished!**\n",
    "You’ve successfully tuned the **learning rate** and **predicted results** using **Gradient Boosting Classifier**! 🎯 Try adjusting `learning_rate` (e.g., `0.05` or `0.2`) and observe how accuracy changes. 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91252c7",
   "metadata": {},
   "source": [
    "## Building a Gradient Boosting Classifier Model\n",
    "\n",
    "You're doing great, Celestial Traveler! Can you now construct the Gradient Boosting Classifier by setting n_estimators to 100, learning_rate to 0.1, and max_depth to 3? Implement it in the code! Don't forget to test your agent's performance with the prepared data.\n",
    "\n",
    "```python\n",
    "# import required libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# TODO: Instantiate the GradientBoostingClassifier and set max_depth, learning rate and number of estimators\n",
    "\n",
    "# Train the classifier\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "Here’s your **completed code** with the necessary parameters set for the **Gradient Boosting Classifier**:\n",
    "\n",
    "```python\n",
    "# import required libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Instantiate the GradientBoostingClassifier with specified parameters\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "# Train the classifier\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### 🔍 **Breakdown of Updates:**\n",
    "1. **Instantiating Gradient Boosting Classifier**:\n",
    "   ```python\n",
    "   gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "   ```\n",
    "   - **`n_estimators=100`** → Number of boosting stages (trees).\n",
    "   - **`learning_rate=0.1`** → Controls contribution of each tree.\n",
    "   - **`max_depth=3`** → Limits complexity of individual trees.\n",
    "\n",
    "2. **Training the Classifier**:\n",
    "   ```python\n",
    "   gb_clf.fit(X_train, y_train)\n",
    "   ```\n",
    "   - Trains the model using the **Reuters news dataset**.\n",
    "\n",
    "3. **Making Predictions**:\n",
    "   ```python\n",
    "   y_pred = gb_clf.predict(X_test)\n",
    "   ```\n",
    "   - Uses trained model to classify test data.\n",
    "\n",
    "4. **Testing Performance**:\n",
    "   ```python\n",
    "   print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "   ```\n",
    "   - Computes **accuracy** to measure model performance.\n",
    "\n",
    "### 🎯 **Mission Accomplished!** 🚀\n",
    "Your **Gradient Boosting Classifier** is now set with `n_estimators=100`, `learning_rate=0.1`, and `max_depth=3`. 🎉 Try tweaking these values (e.g., increasing `n_estimators` or decreasing `learning_rate`) to observe how performance changes! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25c6c7",
   "metadata": {},
   "source": [
    "## Implementation of Gradient Boosting Classifier\n",
    "\n",
    "Greetings, Cosmos Conqueror! We've tuned the parameters, trained, made predictions, and evaluated our Gradient Boosting Classifier. Now, let's consolidate all we've learned in this crucial task. Your mission, should you choose to accept it, is to craft a Gradient Boosting Classifier, train it, predict labels, and finally measure its accuracy on Reuters' news categories. All the best!\n",
    "\n",
    "```python\n",
    "# import required libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(text_data)\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# TODO: Split your data into training and testing sets\n",
    "\n",
    "# TODO: Instantiate the GradientBoostingClassifier with the parameters - n_estimators=150, learning_rate=0.15, max_depth=3 - and fit your model to the training set\n",
    "\n",
    "# TODO: Make predictions\n",
    "\n",
    "# Evaluating the model's performance\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
