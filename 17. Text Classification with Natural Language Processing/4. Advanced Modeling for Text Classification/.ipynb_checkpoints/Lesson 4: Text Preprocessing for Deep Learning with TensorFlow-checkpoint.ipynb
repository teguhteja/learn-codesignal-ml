{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57268bc0",
   "metadata": {},
   "source": [
    "# Lesson 4: Text Preprocessing for Deep Learning with TensorFlow\n",
    "\n",
    "\n",
    "Welcome, data enthusiasts! In this lesson, we will continue our journey into the world of Natural Language Processing (NLP), with an introduction to deep learning for text classification. To harness the power of deep learning, it's important to start with proper data preparation. That's why we will focus today on text preprocessing, shifting from Scikit-learn, which we used previously in this course, to the powerful TensorFlow library.\n",
    "\n",
    "The goal of this lesson is to leverage TensorFlow for textual data preparation and understand how it differs from the methods we used earlier. We will implement tokenization, convert tokens into sequences, learn how to pad these sequences to a consistent length, and transform categorical labels into integer labels to input into our deep learning model. Let's dive in!\n",
    "\n",
    "## Understanding TensorFlow and its Role in Text Preprocessing\n",
    "\n",
    "TensorFlow is an open-source library developed by Google, encompassing a comprehensive ecosystem of tools, libraries, and resources that facilitate machine learning and deep learning tasks, including NLP. As with any machine learning task, preprocessing of your data is a key step in NLP as well.\n",
    "\n",
    "A significant difference between text preprocessing with TensorFlow and using libraries like Scikit-learn lies in the approach to tokenization and sequence generation. TensorFlow incorporates a highly efficient tokenization process, handling both tokenization and sequence generation within the same library. Let's understand how this process works.\n",
    "\n",
    "## Tokenizing Text Data\n",
    "\n",
    "Tokenization is a foundational step in NLP, where sentences or texts are segmented into individual words or tokens. This process facilitates the comprehension of the language structure and produces meaningful units of text that serve as input for numerous machine learning algorithms.\n",
    "\n",
    "In TensorFlow, we utilize the `Tokenizer` class for tokenization. A unique feature of TensorFlow's tokenizer is its robust handling of 'out-of-vocabulary' (OOV) words, or words not present in the tokenizer's word index. By specifying the `oov_token` parameter, we can assign a special token, `<OOV>`, to represent these OOV words.\n",
    "\n",
    "### Example of Tokenization:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentence = \"Love is a powerful entity.\"\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts([sentence])\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "\n",
    "```plaintext\n",
    "{'<OOV>': 1, 'love': 2, 'is': 3, 'a': 4, 'powerful': 5, 'entity': 6}\n",
    "```\n",
    "\n",
    "Through this mechanism, TensorFlow's `Tokenizer` effectively prepares text data for subsequent machine learning tasks by mapping words to consistent integer values while gracefully handling words not encountered during the initial vocabulary construction.\n",
    "\n",
    "## Converting Text to Sequences\n",
    "\n",
    "After tokenization, the next step is to represent text as sequences of integers. Sequences are lists of integers where each integer corresponds to a token in the dictionary created during tokenization.\n",
    "\n",
    "### Example of Converting Text to Sequences:\n",
    "\n",
    "```python\n",
    "sentences = [sentence, \"very powerful\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "\n",
    "```plaintext\n",
    "[[2, 3, 4, 5, 6], [1, 5]]\n",
    "```\n",
    "\n",
    "The word ‚Äúvery‚Äù is not found in the tokenizer's word index, thus it is labeled as token `1`, which we designated as the `<OOV>` token. The word ‚Äúpowerful‚Äù, being recognized in the vocabulary, retains its assigned index `5`.\n",
    "\n",
    "## Padding Sequences for Consistent Input Shape\n",
    "\n",
    "Deep learning models require input data of a consistent shape. Padding ensures this by adding zeros to shorter sequences to match the length of the longest sequence.\n",
    "\n",
    "### Example of Padding Sequences:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "print(padded_sequences)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "\n",
    "```plaintext\n",
    "[[2 3 4 5 6]\n",
    " [1 5 0 0 0]]\n",
    "```\n",
    "\n",
    "The padding ensures all sequences are unified in length, catering to the requirements of deep learning models for consistent input shape.\n",
    "\n",
    "## Implementing Text Preprocessing with TensorFlow\n",
    "\n",
    "Finally, let's implement the entire preprocessing workflow with a limited set of data from the Reuters-21578 text categorization dataset.\n",
    "\n",
    "### Full Implementation:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Download the reuters dataset from nltk\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "# Limiting the data for quick execution\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Preparing the dataset\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Tokenize the text data, using TensorFlow's Tokenizer class\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Padding sequences for uniform input shape\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Translating categories into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(categories_data)\n",
    "\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of Y: \", y.shape)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "\n",
    "```plaintext\n",
    "Shape of X:  (2477, 2380)\n",
    "Shape of Y:  (2477,)\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Great work! You've successfully ventured into TensorFlow for text preprocessing, an essential step in leveraging the true potential of deep learning for text classification. You've seen how tokenization, sequence creation, and padding can be swiftly handled in TensorFlow, a key difference from methods we used in Scikit-learn. These foundations will serve you well as we move forward in our NLP journey. Up next, we're diving deeper into building Neural Network Models for Text Classification!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc8ced",
   "metadata": {},
   "source": [
    "## Adjusting Tokenizer Parameters\n",
    "\n",
    "Great work so far, Stellar Navigator. Now, let's adjust the parameters of the tokenizer. Initially, we have set the num_words parameter in the Tokenizer class to 10. Change the num_words parameter to 5. Run the given sentence through the tokenizer once more and observe the differences in the word_index and how the tokens outside the top limited number are labeled as in the sequence.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Original sentence\n",
    "sentence = \"Love is a powerful entity that can change the world.\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts([sentence])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Print word index\n",
    "print(\"Updated word index: \", word_index)\n",
    "\n",
    "# Translating text to sequences\n",
    "sequence = tokenizer.texts_to_sequences([sentence])\n",
    "print(\"Updated sequence: \", sequence)\n",
    "```\n",
    "\n",
    "## Introduction to Deep Learning for Text Classification\n",
    "\n",
    "Welcome, data enthusiasts! In this lesson, we will continue our journey into the world of Natural Language Processing (NLP), with an introduction to deep learning for text classification. To harness the power of deep learning, it's important to start with proper data preparation. That's why we will focus today on text preprocessing, shifting from Scikit-learn, which we used previously in this course, to the powerful TensorFlow library.\n",
    "\n",
    "The goal of this lesson is to leverage TensorFlow for textual data preparation and understand how it differs from the methods we used earlier. We will implement tokenization, convert tokens into sequences, learn how to pad these sequences to a consistent length, and transform categorical labels into integer labels to input into our deep learning model. Let's dive in!\n",
    "\n",
    "## Understanding TensorFlow and its Role in Text Preprocessing\n",
    "\n",
    "TensorFlow is an open-source library developed by Google, encompassing a comprehensive ecosystem of tools, libraries, and resources that facilitate machine learning and deep learning tasks, including NLP. As with any machine learning task, preprocessing of your data is a key step in NLP as well.\n",
    "\n",
    "A significant difference between text preprocessing with TensorFlow and using libraries like Scikit-learn, lies in the approach to tokenization and sequence generation. TensorFlow incorporates a highly efficient tokenization process, handling both tokenization and sequence generation within the same library. Let's understand how this process works.\n",
    "\n",
    "## Tokenizing Text Data\n",
    "\n",
    "Tokenization is a foundational step in NLP, where sentences or texts are segmented into individual words or tokens. This process facilitates the comprehension of the language structure and produces meaningful units of text that serve as input for numerous machine learning algorithms.\n",
    "\n",
    "In TensorFlow, we utilize the Tokenizer class for tokenization. A unique feature of TensorFlow's tokenizer is its robust handling of 'out-of-vocabulary' (OOV) words, or words not present in the tokenizer's word index. By specifying the oov_token parameter, we can assign a special token, <OOV>, to represent these OOV words.\n",
    "\n",
    "Let's look at a practical example of tokenization:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Original sentence\n",
    "sentence = \"Love is a powerful entity that can change the world.\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts([sentence])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Print word index\n",
    "print(\"Updated word index: \", word_index)\n",
    "\n",
    "# Translating text to sequences\n",
    "sequence = tokenizer.texts_to_sequences([sentence])\n",
    "print(\"Updated sequence: \", sequence)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```plaintext\n",
    "Updated word index: {'<OOV>': 1, 'love': 2, 'is': 3, 'a': 4, 'powerful': 5}\n",
    "Updated sequence: [[2, 3, 4, 5, 1, 1, 1, 1, 1, 1]]\n",
    "```\n",
    "\n",
    "In this example, tokenizer.fit_on_texts([...]) examines the text it receives and constructs a vocabulary from the unique words found within, but now with a restriction of num_words=5. Any word beyond the first five most frequent ones is replaced with the <OOV> token. This adjustment affects how unknown words are represented in the generated sequences.\n",
    "\n",
    "Through this mechanism, TensorFlow's Tokenizer effectively prepares text data for subsequent machine learning tasks by mapping words to consistent integer values while gracefully handling words not encountered during the initial vocabulary construction.\n",
    "\n",
    "## Converting Text to Sequences\n",
    "\n",
    "After tokenization, the next step is to represent text as sequences of integers. Sequences are lists of integers where each integer corresponds to a token in the dictionary created during tokenization. This conversion process translates natural language text into structured data that can be input into a machine learning model.\n",
    "\n",
    "## Padding Sequences for Consistent Input Shape\n",
    "\n",
    "Deep learning models require input data of a consistent shape. In the context of NLP, it means all text must be represented by the same number of tokens. Padding is a process to ensure this by adding zeros to shorter sequences to match the length of the longest sequence.\n",
    "\n",
    "Here's how we pad sequences in TensorFlow:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "print(padded_sequences)\n",
    "```\n",
    "\n",
    "## Implementing Text Preprocessing with TensorFlow\n",
    "\n",
    "Finally, let's implement the entire preprocessing workflow with a limited set of data from the Reuters-21578 text categorization dataset.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Download the reuters dataset from nltk\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "# Limiting the data for quick execution\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Preparing the dataset\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Tokenize the text data, using TensorFlow's Tokenizer class\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Padding sequences for uniform input shape\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Translating categories into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(categories_data)\n",
    "\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of Y: \", y.shape)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```plaintext\n",
    "Shape of X:  (2477, 2380)\n",
    "Shape of Y:  (2477,)\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Great work! You've successfully ventured into TensorFlow for text preprocessing, an essential step in leveraging the true potential of deep learning for text classification. You've seen how tokenization, sequence creation, and padding can be swiftly handled in TensorFlow, a key difference from methods we used in Scikit-learn. These foundations will serve you well as we move forward in our NLP journey. Up next, we're diving deeper into building Neural Network Models for Text Classification!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cc8603",
   "metadata": {},
   "source": [
    "## Tokenizer Text Processing Practice\n",
    "\n",
    "Alright, Stellar Navigator! Your challenge is to modify our Tokenizer to tokenize a different text. Fill in the missing parts (____) to process the new text sample. Use 10 for the number of words and \"<OOV>\" for out-of-vocabulary words. Remember to fit the tokenizer to the sample and generate a sequence from it. Additionally, print the sequence and the word_index to see the results of your processing.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define text sample\n",
    "text_sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = Tokenizer(num_words=____, oov_token=____)\n",
    "\n",
    "# Fit the tokenizer on the text sample\n",
    "tokenizer.____([text_sample])\n",
    "\n",
    "# Retrieve the word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert the text to a sequence\n",
    "sequence = tokenizer.____([text_sample])\n",
    "\n",
    "# Print the sequence and word index\n",
    "print(f\"Sequence: {sequence}\")\n",
    "print(f\"Word Index: {word_index}\")\n",
    "\n",
    "```\n",
    "\n",
    "Here's your updated code with the missing parts filled in:  \n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define text sample\n",
    "text_sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = Tokenizer(num_words=10, oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit the tokenizer on the text sample\n",
    "tokenizer.fit_on_texts([text_sample])\n",
    "\n",
    "# Retrieve the word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert the text to a sequence\n",
    "sequence = tokenizer.texts_to_sequences([text_sample])\n",
    "\n",
    "# Print the sequence and word index\n",
    "print(f\"Sequence: {sequence}\")\n",
    "print(f\"Word Index: {word_index}\")\n",
    "```\n",
    "\n",
    "This will tokenize the sentence, limit the vocabulary size to 10 words, and replace any out-of-vocabulary words with `<OOV>`. Let me know if you need further modifications! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a633ae",
   "metadata": {},
   "source": [
    "## Filling the Gaps in Text Preprocessing Code\n",
    "\n",
    "Fantastic! Now, Stellar Navigator, let's fill in the code at the marked spots (____)! We are focusing on some key parts of our text preprocessing pipeline. In particular, you need to correctly categorize the data, specify the number of words for tokenization, and ensure the text data is properly tokenized. Happy coding!\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Limiting the data for quick execution\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Preparing the dataset\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.____(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Tokenize the text data, using TensorFlow's Tokenizer class\n",
    "tokenizer = ____(num_words=____, oov_token=\"<OOV>\")\n",
    "tokenizer.____(text_data)\n",
    "sequences = tokenizer.____(text_data)\n",
    "\n",
    "# Padding sequences for uniform input shape\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Translating categories into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(categories_data)\n",
    "\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of Y: \", y.shape)\n",
    "\n",
    "```\n",
    "\n",
    "Here‚Äôs your completed code with the missing parts filled in:  \n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Limiting the data for quick execution\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "\n",
    "# Preparing the dataset\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Tokenize the text data, using TensorFlow's Tokenizer class\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Padding sequences for uniform input shape\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Translating categories into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(categories_data)\n",
    "\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of Y: \", y.shape)\n",
    "```\n",
    "\n",
    "### üîç Breakdown of Fixes:\n",
    "1. `categories_data = [reuters.categories(fileid)[0] for fileid in documents]`  \n",
    "   - Extracts the primary category for each document.\n",
    "2. `tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")`  \n",
    "   - Initializes the Tokenizer with a vocabulary limit of 500 words.\n",
    "3. `tokenizer.fit_on_texts(text_data)`  \n",
    "   - Trains the tokenizer on the dataset.\n",
    "4. `sequences = tokenizer.texts_to_sequences(text_data)`  \n",
    "   - Converts text into sequences of tokenized integers.\n",
    "\n",
    "Now, your text preprocessing pipeline is complete! üöÄ Let me know if you need further modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b1d02e",
   "metadata": {},
   "source": [
    "## Initiating the Tokenizer Process\n",
    "\n",
    "Alright, Stellar Navigator! Your next task is to initialize the tokenizer. The text sample and the fit function are already provided. You merely need to fill in the line for the tokenizer. Use 10 for the number of words and \"<OOV>\" for out-of-vocabulary words. After fitting the tokenizer, print the word index and sequence to see the results of your tokenizer.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define text sample\n",
    "text_sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# TODO: Initialize the tokenizer\n",
    "\n",
    "# Fit the tokenizer on the text sample\n",
    "tokenizer.fit_on_texts([text_sample])\n",
    "\n",
    "# Get word index and sequence\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Word Index: \", word_index)\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences([text_sample])\n",
    "print(\"Sequence: \", sequence)\n",
    "\n",
    "```\n",
    "\n",
    "Here‚Äôs the completed code with the tokenizer properly initialized:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define text sample\n",
    "text_sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# TODO: Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=10, oov_token=\"<OOV>\")  # ‚úÖ Correctly initializes tokenizer\n",
    "\n",
    "# Fit the tokenizer on the text sample\n",
    "tokenizer.fit_on_texts([text_sample])\n",
    "\n",
    "# Get word index and sequence\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Word Index: \", word_index)\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences([text_sample])\n",
    "print(\"Sequence: \", sequence)\n",
    "```\n",
    "\n",
    "### ‚úÖ What's Implemented?\n",
    "1. **Tokenizer Initialization:**\n",
    "   - `num_words=10`: Limits vocabulary to the top 10 words.\n",
    "   - `oov_token=\"<OOV>\"`: Handles out-of-vocabulary words.\n",
    "\n",
    "2. **Tokenization Process:**\n",
    "   - `fit_on_texts([text_sample])`: Learns word frequencies.\n",
    "   - `texts_to_sequences([text_sample])`: Converts text to numerical tokens.\n",
    "\n",
    "### üîç Expected Output:\n",
    "- The word index will show the most frequent words assigned unique numbers.\n",
    "- The sequence will show the numerical representation of the input text.\n",
    "\n",
    "Give it a run and see how it tokenizes the sentence! üöÄüî•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8b2bf",
   "metadata": {},
   "source": [
    "## Tokenizing Text Data with TensorFlow\n",
    "\n",
    "Impressive progress, Stellar Navigator! Now, harness the power of TensorFlow and tokenize unique text data of your choice from scratch. After implementing the process of tokenization, print the word index and sequences to verify your implementation!\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "text_data = \"The sky is blue\"\n",
    "\n",
    "# TODO: Initialize tokenizer with num_words and oov_token parameters\n",
    "\n",
    "# TODO: Fit tokenizer on your text data\n",
    "\n",
    "# TODO: Generate word index and print it\n",
    "\n",
    "# TODO: Convert text data into a sequence and print it\n",
    "```\n",
    "\n",
    "Here‚Äôs the complete implementation with tokenization from scratch:  \n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define unique text data\n",
    "text_data = \"The sky is blue\"\n",
    "\n",
    "# TODO: Initialize tokenizer with num_words and oov_token parameters\n",
    "tokenizer = Tokenizer(num_words=10, oov_token=\"<OOV>\")  # Limits vocabulary to top 10 words and handles OOV words\n",
    "\n",
    "# TODO: Fit tokenizer on your text data\n",
    "tokenizer.fit_on_texts([text_data])\n",
    "\n",
    "# TODO: Generate word index and print it\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Word Index:\", word_index)  # Shows how words are indexed\n",
    "\n",
    "# TODO: Convert text data into a sequence and print it\n",
    "sequence = tokenizer.texts_to_sequences([text_data])\n",
    "print(\"Sequence:\", sequence)  # Outputs tokenized numerical representation\n",
    "```\n",
    "\n",
    "### ‚úÖ What This Does:\n",
    "1. **Tokenizer Initialization:**\n",
    "   - `num_words=10`: Restricts vocabulary size.\n",
    "   - `oov_token=\"<OOV>\"`: Assigns unknown words a default token.\n",
    "\n",
    "2. **Tokenization Process:**\n",
    "   - `fit_on_texts([text_data])`: Learns word frequencies.\n",
    "   - `texts_to_sequences([text_data])`: Converts words to numerical tokens.\n",
    "\n",
    "### üîç Expected Output:\n",
    "- **Word Index:** A dictionary mapping each word to a unique number.\n",
    "- **Sequence:** A list of numbers representing words in the sentence.\n",
    "\n",
    "Try running it and see how it tokenizes your text! üöÄüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c544d2-2d74-4d84-a83d-f996612f0fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
