{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e0dc0c",
   "metadata": {},
   "source": [
    "# Lesson 5: Understanding and Building Neural Networks for Text Classification\n",
    "\n",
    "Hello and welcome to today's lesson! We are now moving towards an exciting journey into the field of Neural Networks, significant players in the Natural Language Processing (NLP) arena. Neural Networks implicitly capture the structure of the data, a phenomenon that's of high value in text data, given its sequential nature. Remember how our ensemble models did a good job on the Reuters-21578 Text Categorization Collection? Now, imagine how we can unlock even higher performance by using these powerful models.\n",
    "\n",
    "## Getting Familiar with the Task\n",
    "\n",
    "Before discussing Neural Networks in detail, let's recall the code we have already executed:\n",
    "\n",
    "```python\n",
    "# Importing libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('reuters', quiet=True)\n",
    "\n",
    "# Loading and preparing the Reuters-21578 Text Categorization Collection dataset\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Tokenizing and padding sequences\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Label Encoding\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "```\n",
    "\n",
    "So far, we have preprocessed our text data and transformed it into a format suitable for input into models. We have our train and test datasets ready, which means we are all set to dive into creating our Neural Network model for text classification.\n",
    "\n",
    "## Building the Neural Network Model\n",
    "\n",
    "When dealing with text data, our neural network usually starts with an **Embedding layer**. This layer is tasked with converting the tokenized textual data into a dense vector representation which the neural network can understand. The embedding matrix created by this process captures the general understanding of words and their contextual meanings.\n",
    "\n",
    "Here's our simple, initial neural network model with the embedding layer:\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=500, output_dim=16),\n",
    "])\n",
    "```\n",
    "\n",
    "Notice the parameters we passed to the embedding layer - the `input_dim` and `output_dim`. The `input_dim` is set to 500, the same as the number of words we encoded with our tokenizer. The `output_dim` sets how many dimensions we want to have in the dense vector representing each word - we set it to 16.\n",
    "\n",
    "Still, the model is not yet complete. Let's add the next layer.\n",
    "\n",
    "## Introduction to GlobalAveragePooling1D\n",
    "\n",
    "Next, we will use a pooling layer - **GlobalAveragePooling1D**. This layer reduces the dimensionality of the model's representation by taking the average of each word vector. This process effectively creates an overall context vector per text sequence, a necessary process before predicting the text category.\n",
    "\n",
    "Our model with the `GlobalAveragePooling1D` layer now looks like this:\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=500, output_dim=16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "])\n",
    "```\n",
    "\n",
    "## Compile, Train, and Evaluate the Model\n",
    "\n",
    "Our last layer is a **Dense layer**, the output layer, with three units and `'softmax'` activation function. The number three here represents the number of our output categories. The `'softmax'` activation will ensure the output probabilities of all categories sum up to 1.\n",
    "\n",
    "Lastly, we compile our model with the loss function `'sparse_categorical_crossentropy'`, `'adam'` optimizer, and `'accuracy'` as the metric. We train our model for **10 epochs** using our training set and then evaluate it using the test set:\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=500, output_dim=16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```plaintext\n",
    "Test Loss: 0.22081851959228516\n",
    "Test Accuracy: 0.9556451439857483\n",
    "```\n",
    "\n",
    "This summary indicates successful training of a Neural Network model for text classification on the Reuters dataset, achieving significant accuracy with minimal loss. The use of an **embedding layer** coupled with `GlobalAveragePooling1D` and `Dense` layers allows for effective understanding and categorization of text sequences.\n",
    "\n",
    "We use **sparse_categorical_crossentropy** as our loss function because our labels are integers. In multi-class classification tasks where labels are not one-hot encoded (which would require `categorical_crossentropy`), `sparse_categorical_crossentropy` allows for a more efficient and straightforward handling of the labels. It expects integers as labels and calculates the loss between the true labels and predicted labels, guiding the model's optimization.\n",
    "\n",
    "## Lesson Summary and Practice\n",
    "\n",
    "Congratulations on taking a big step in your NLP journey! You've learned how to prepare and use **Neural Networks for text classification** efficiently. You've gone a long way, and it's the perfect time to apply these concepts. In the next set of exercises, you will get to apply these concepts and consolidate your learning.\n",
    "\n",
    "Practice is crucial - it helps us understand the concepts better and gives us the confidence to handle real-world datasets and tasks. Remember, you're just one lesson away from unlocking the power of **Simple RNN**, which we will cover in our next class. Let's get practicing!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1c8f5",
   "metadata": {},
   "source": [
    "## Improve Neural Network Performance with Additional Layer\n",
    "\n",
    "Great job, Stellar Navigator! For the next challenge, enhance the model's architecture to improve performance. In the provided model below, find the spots marked with ____ and fill them in to add an additional fully connected layer. This layer should use 16 as the dimension and 'relu' as the activation function. After making this modification, compile, train, and evaluate the model to see how this change affects the model's performance.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Building the modified model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=500, output_dim=16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(____, activation=____),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Building the modified model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=500, output_dim=16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "I've added a fully connected layer with 16 units and 'relu' activation to enhance the model's architecture. Let me know if you need further modifications or performance evaluations! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0433e",
   "metadata": {},
   "source": [
    "## Inserting the Missing Model Layer\n",
    "\n",
    "Outstanding work, Stellar Navigator!\n",
    "\n",
    "I have a new challenge for you. There is a missing layer in our model; without that layer, our model will not work! Fill in the blank spot (____) with the appropriate layer that reduces dimensionality by averaging word vectors. Let's get this model working!\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Building the modified model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=500, output_dim=16),\n",
    "    tf.keras.layers.____(),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.____(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.____(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Building the modified model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=500, output_dim=16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "I've added the missing `GlobalAveragePooling1D()` layer and completed the placeholders for model compilation and training. Your model should now work correctly! Let me know if you need further improvements. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367dbe0",
   "metadata": {},
   "source": [
    "## Preparing the Tokenizer, Data, and Model\n",
    "\n",
    "Amazing work, Stellar Navigator! Now, let‚Äôs fill in some blanks. Your task is to correctly prepare the tokenizer, data, and model. Remember to specify the correct number of words for our tokenizer and to use the correct layers and activation functions in our model. Good luck!\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=____, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "y = ____().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Building the model\n",
    "model = tf.keras.____([\n",
    "    tf.keras.layers.____(input_dim=500, output_dim=16),\n",
    "    tf.keras.layers.____(),\n",
    "    tf.keras.layers.____(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "Here is the corrected version of your code with all missing parts filled in:  \n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Define tokenizer with a vocabulary size of 500\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Label Encoding\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Building the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=500, output_dim=16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "### Changes and Fixes:\n",
    "1. **Tokenizer's `num_words`** ‚Üí Set to `500` to limit the vocabulary size.\n",
    "2. **Label Encoding** ‚Üí Used `LabelEncoder()` to convert category labels into numerical values.\n",
    "3. **Model Definition**:\n",
    "   - Used `tf.keras.Sequential()` to define a sequential model.\n",
    "   - Added an `Embedding` layer (`input_dim=500`, `output_dim=16`).\n",
    "   - Inserted `GlobalAveragePooling1D()` to reduce dimensionality.\n",
    "   - Added a `Dense` output layer with 3 neurons (for 3 categories) and `softmax` activation.\n",
    "\n",
    "Now, the model is fully prepared for text classification! üöÄ Let me know if you need further modifications. üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fb17d",
   "metadata": {},
   "source": [
    "## Extend the Neural Network Model\n",
    "\n",
    "Fantastic Stellar Navigator, keep soaring! Let's put your skills to the test. Build the model from scratch using the appropriate Neural Network layers that suit the current problem. Also, ensure you use the 'relu' activation function for the hidden layer and the 'softmax' activation function for the output layer. Proceed to compile, train, and test the model. Let's see how you do!\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# TODO: Build a neural network model with an embedding layer, a GlobalAveragePooling1D layer, a Dense layer with 'relu' activation, and an output Dense layer with 'softmax' activation\n",
    "\n",
    "# Compile, train, and evaluate the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "Here's your complete neural network model built from scratch with the appropriate layers and activation functions:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Load and preprocess data\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Label Encoding\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=500, output_dim=16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),  # Hidden layer with 'relu' activation\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # Output layer with 'softmax' activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "### Key Enhancements:\n",
    "‚úÖ **Added a Hidden Layer**: A `Dense` layer with 16 neurons and `'relu'` activation.  \n",
    "‚úÖ **Retained GlobalAveragePooling1D Layer**: Helps in reducing dimensionality.  \n",
    "‚úÖ **Maintained Output Layer**: 3 neurons with `'softmax'` activation for classification.  \n",
    "‚úÖ **Ensured Proper Compilation & Training**: Using `sparse_categorical_crossentropy` as the loss function.\n",
    "\n",
    "Your model is now complete and should classify text into the three categories effectively. üöÄ Let me know if you need any further tweaks! üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfa7d8",
   "metadata": {},
   "source": [
    "## Creating and Training a Neural Network Model\n",
    "\n",
    "Stellar Navigator, your final task has arrived! Create a Neural Network model, compile it, train it, and predict the class of the first test data point in the Reuters-21578 Text Categorization Collection dataset. Follow everything we have learned so far, and show me that our Neural Network training has been a success!\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# TODO: Build a Neural Network model\n",
    "\n",
    "# TODO: Compile the model\n",
    "\n",
    "# TODO: Train the model\n",
    "\n",
    "# Predict on the first test text\n",
    "prediction = np.argmax(model.predict(X_test[0].reshape(1, -1)))\n",
    "\n",
    "print(f\"Predicted class: {prediction}, Actual class: {y_test[0]}\")\n",
    "\n",
    "```\n",
    "\n",
    "Here's your fully built Neural Network model, trained and tested on the Reuters-21578 dataset! üöÄ  \n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Load and preprocess data\n",
    "categories = reuters.categories()[:3]\n",
    "documents = reuters.fileids(categories)\n",
    "text_data = [\" \".join([word for word in reuters.words(fileid)]) for fileid in documents]\n",
    "categories_data = [reuters.categories(fileid)[0] for fileid in documents]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=500, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Label Encoding\n",
    "y = LabelEncoder().fit_transform(categories_data)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the Neural Network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=500, output_dim=16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),  # Hidden layer with ReLU activation\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # Output layer with Softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predict on the first test text\n",
    "prediction = np.argmax(model.predict(X_test[0].reshape(1, -1)))\n",
    "\n",
    "print(f\"Predicted class: {prediction}, Actual class: {y_test[0]}\")\n",
    "```\n",
    "\n",
    "### Breakdown of the Model:\n",
    "‚úÖ **Embedding Layer**: Converts words into dense vector representations.  \n",
    "‚úÖ **GlobalAveragePooling1D**: Reduces dimensionality and improves generalization.  \n",
    "‚úÖ **Dense Layer (16 neurons, ReLU)**: Learns useful patterns from data.  \n",
    "‚úÖ **Output Layer (Softmax, 3 neurons)**: Classifies into 3 categories.  \n",
    "‚úÖ **Adam Optimizer & Sparse Categorical Crossentropy**: Optimizes performance.  \n",
    "\n",
    "üîç **Final Task Completed!** Your model is now trained, tested, and predicting! üöÄ  \n",
    "Let me know if you want enhancements! üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac758e-c1d0-4900-abaa-63979138326d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
