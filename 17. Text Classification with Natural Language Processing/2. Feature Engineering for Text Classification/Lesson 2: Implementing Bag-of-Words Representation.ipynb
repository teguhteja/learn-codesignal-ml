{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55773f94",
   "metadata": {},
   "source": [
    "# Lesson 2: Implementing Bag-of-Words Representation\n",
    "\n",
    "**Introducing the Bag-of-Words Representation**\n",
    "\n",
    "In the realm of text analysis, transforming raw data into a format that is both computer-friendly and preserves essential information for further processing is crucial. One of the simplest yet most versatile methods for achieving this is the Bag-of-Words (BoW) representation.\n",
    "\n",
    "The BoW method is essentially a way to extract features from text. Imagine having a large bag filled with words sourced from various texts like a book, a website, or, in our example, movie reviews from the IMDB dataset. For each document or sentence, the BoW representation will tally how many times each word appears. Crucially, in this \"bag,\" the order of words is disregarded; only their frequency matters.\n",
    "\n",
    "**Example with Three Sentences:**\n",
    "\n",
    "Consider the following sentences:\n",
    "1. The cat sat on the mat.\n",
    "2. The cat sat near the mat.\n",
    "3. The cat played with a ball.\n",
    "\n",
    "Using a BoW representation, our table would look like this:\n",
    "\n",
    "|   | the | cat | sat | on | mat | near | played | with | a | ball |\n",
    "|---|-----|-----|-----|----|-----|------|--------|------|---|------|\n",
    "| 1 | 2   | 1   | 1   | 1  | 1   | 0    | 0      | 0    | 0 | 0    |\n",
    "| 2 | 2   | 1   | 1   | 0  | 1   | 1    | 0      | 0    | 0 | 0    |\n",
    "| 3 | 1   | 1   | 0   | 0  | 0   | 0    | 1      | 1    | 1 | 1    |\n",
    "\n",
    "Each row corresponds to a sentence (document), and each unique word forms a column. The cell values represent the word count in the corresponding sentence.\n",
    "\n",
    "**Illustrating Bag-of-Words with a Simple Example**\n",
    "\n",
    "We can start practicing the Bag-of-Words model by using Scikit-learn's CountVectorizer on the same three sentences:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Simple example sentences\n",
    "sentences = ['The cat sat on the mat.',\n",
    "             'The cat sat near the mat.',\n",
    "             'The cat played with a ball.']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print('Feature names:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "print(X.toarray())\n",
    "```\n",
    "\n",
    "The output will be:\n",
    "\n",
    "```\n",
    "Feature names:\n",
    "['ball', 'cat', 'mat', 'near', 'on', 'played', 'sat', 'the', 'with']\n",
    "Bag of Words Representation:\n",
    "[[0, 1, 1, 0, 1, 0, 1, 2, 0],\n",
    " [0, 1, 1, 1, 0, 0, 1, 2, 0],\n",
    " [1, 1, 0, 0, 0, 1, 0, 1, 1]]\n",
    "```\n",
    "\n",
    "From the output, you can see that Scikit-learn's CountVectorizer has replicated our previous manual process, creating a Bag-of-Words representation where each row corresponds to a sentence and each column to a unique word.\n",
    "\n",
    "**Applying Bag-of-Words to Our Dataset**\n",
    "\n",
    "Now that we understand what Bag-of-Words is and how it functions, let's apply it to our dataset:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('movie_reviews')  \n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(reviews)\n",
    "\n",
    "print(f\"The shape of our Bag-of-Words is: {bag_of_words.shape}\")\n",
    "```\n",
    "\n",
    "The output will show:\n",
    "\n",
    "```\n",
    "The shape of our Bag-of-Words is: (2000, 39659)\n",
    "```\n",
    "\n",
    "This indicates that the result is a matrix where each row corresponds to a movie review and each column to a unique word, with entries representing word counts.\n",
    "\n",
    "**Understanding the Bag-of-Words Matrix and Most Used Word**\n",
    "\n",
    "Let's examine what's inside the bag_of_words matrix:\n",
    "\n",
    "```python\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "first_review_word_counts = bag_of_words[0].toarray()[0]\n",
    "\n",
    "max_count_index = first_review_word_counts.argmax()\n",
    "most_used_word = feature_names[max_count_index]\n",
    "\n",
    "print(f\"The most used word is '{most_used_word}' with a count of {first_review_word_counts[max_count_index]}\")\n",
    "```\n",
    "\n",
    "Running the code will output:\n",
    "\n",
    "```\n",
    "The most used word is 'the' with a count of 38\n",
    "```\n",
    "\n",
    "This output reveals the most used word in the first review and its count. The script finds the index of the word with the highest count in the first review, then uses this index to identify the corresponding word in the feature_names. This demonstrates how we can identify the most used word in a specific review using the Bag-of-Words model.\n",
    "\n",
    "**Lesson Summary**\n",
    "\n",
    "Congratulations! You've successfully navigated through this lesson on the Bag-of-Words method, a fundamental concept in text classification. You've not only grasped the theoretical aspect but also applied it on a real-world dataset using Python. You even used it to extract insights about word frequency, a crucial aspect of many text classification problems.\n",
    "\n",
    "As we move forward in upcoming lessons, we'll build on what you've learned today and continue our journey to understand and apply more advanced text classification techniques. Remember, practice makes perfect, so try to apply what you've learned today on different text data on your own. Happy coding, and see you in the next lesson!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745af6c",
   "metadata": {},
   "source": [
    "## Customizing Bag-of-Words Representation\n",
    "\n",
    "Great job! Now, let's play around with the text. Change the provided sentences to any three sentences of your choice. Then, observe how this affects the Bag-of-Words representation. Remember, the output should be a matrix, with each row representing one of your sentences and each column representing a unique word.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Custom sentences\n",
    "sentences = ['The cat sat on the mat.',\n",
    "             'The cat sat near the mat.',\n",
    "             'The cat played with a ball.']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print('Feature names:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "print(X.toarray())\n",
    "```\n",
    "\n",
    "Sure! Let's change the sentences to something different and observe how the Bag-of-Words representation changes accordingly. Here are three new sentences:\n",
    "\n",
    "1. Dogs love playing in the park.\n",
    "2. Children enjoy playing games.\n",
    "3. The park is full of trees.\n",
    "\n",
    "Now, let's plug these sentences into the provided Python script and see how the Bag-of-Words matrix looks:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Custom sentences\n",
    "sentences = ['Dogs love playing in the park.',\n",
    "             'Children enjoy playing games.',\n",
    "             'The park is full of trees.']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print('Feature names:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "print(X.toarray())\n",
    "```\n",
    "\n",
    "### Expected Output Explanation:\n",
    "\n",
    "- **Feature names:** This will output a list of all unique words found in the three sentences, sorted in alphabetical order.\n",
    "- **Bag of Words Representation:** This will be a matrix where each row corresponds to one of the sentences, and each column corresponds to one of the unique words. The values in the matrix represent the count of each word in each sentence.\n",
    "\n",
    "For example, if the word \"playing\" appears in the first and second sentences but not in the third, its column will have values like `[1, 1, 0]` (assuming \"playing\" appears once in each of the first two sentences). The exact output will depend on the unique words extracted by the `CountVectorizer` and their frequency in each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb8bb9",
   "metadata": {},
   "source": [
    "## Applying CountVectorizer on Sentences\n",
    "\n",
    "Great work, Space Voyager!\n",
    "\n",
    "Now, do you remember how to use the CountVectorizer on the sentences to obtain the Bag-of-Words Representation? In the placeholders, marked with ____, insert the missing parts of the code. Keep on exploring!\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import _____\n",
    "\n",
    "# Simple example sentences\n",
    "sentences = ['Machine learning is fascinating.',\n",
    "             'Deep learning is a subset of machine learning.',\n",
    "             'We use Python for machine learning.']\n",
    "\n",
    "vectorizer = _____()\n",
    "X = vectorizer._____(sentences)\n",
    "\n",
    "print('Feature names:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "print(X.toarray())\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Certainly! To fill in the blanks in the provided Python script using `CountVectorizer` from the `sklearn.feature_extraction.text` module, follow these steps:\n",
    "\n",
    "1. Import `CountVectorizer` from the `sklearn.feature_extraction.text` module.\n",
    "2. Create an instance of `CountVectorizer`.\n",
    "3. Fit and transform the sentences using the `fit_transform` method of `CountVectorizer`.\n",
    "\n",
    "Here's the complete and corrected version of your script:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Simple example sentences\n",
    "sentences = ['Machine learning is fascinating.',\n",
    "             'Deep learning is a subset of machine learning.',\n",
    "             'We use Python for machine learning.']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print('Feature names:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "print(X.toarray())\n",
    "```\n",
    "\n",
    "This script will output the feature names, which are the unique words from the sentences, and the Bag-of-Words representation, which is a matrix indicating the frequency of each word in each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c761996",
   "metadata": {},
   "source": [
    "## Bag-of-Words Transformation on IMDB Reviews Dataset\n",
    "\n",
    "Amazing progress, Space Voyager!\n",
    "\n",
    "Now, for the next adventure: Here's some code with a few blank spots (____) for you to fill in. This code analyzes the first 100 reviews of the IMDB movie dataset and creates a Bag-of-Words representation. Finally, it prints the last ten feature names. Fill in the missing parts to complete the analysis. Happy coding!\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# We need to download the dataset before we can use it\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Take only the first 100 reviews for simplicity\n",
    "reviews = [movie_reviews.____(fileid) for fileid in movie_reviews.____()[:100]]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Print the last ten feature names\n",
    "feature_names = vectorizer.____()\n",
    "print(\"Last ten feature names: \", feature_names[-10:])\n",
    "\n",
    "```\n",
    "\n",
    "To complete the provided Python script for analyzing the first 100 reviews of the IMDB movie dataset using a Bag-of-Words representation, let's fill in the blanks:\n",
    "\n",
    "1. **First blank**: Use `raw()` method to get the text of each review.\n",
    "2. **Second blank**: Use `fileids()` to retrieve all the file identifiers for the reviews.\n",
    "3. **Third blank**: Use `get_feature_names_out()` to obtain the list of feature names from the vectorizer.\n",
    "\n",
    "Here is the completed script:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# We need to download the dataset before we can use it\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Take only the first 100 reviews for simplicity\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()[:100]]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Print the last ten feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Last ten feature names: \", feature_names[-10:])\n",
    "```\n",
    "\n",
    "This script will successfully download the necessary movie reviews dataset from NLTK, extract the first 100 reviews, create a Bag-of-Words model using `CountVectorizer`, and finally, print the last ten feature names from the vectorizer's feature names list. These names represent some of the vocabulary used in the reviews, particularly those that appear at the end of an alphabetically sorted list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e821e94",
   "metadata": {},
   "source": [
    "## Creating Bag-of-Words Representation Yourself\n",
    "\n",
    "Fantastic job, Space Voyager!\n",
    "\n",
    "Now, let's see if you can create a CountVectorizer yourself. Fill in the corresponding TODO lines to successfully vectorize the sentences and observe the resulting Bag-of-Words representation and feature names. Exploration time!\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Simple sentences\n",
    "sentences = [\"I love to sing\", \n",
    "             \"Singing in the rain is my favorite\", \n",
    "             \"She sang the whole night at the concert\"]\n",
    "\n",
    "# TODO: Initialize a CountVectorizer\n",
    "\n",
    "# TODO: Fit transform the sentences\n",
    "\n",
    "print('Feature names:')\n",
    "print(_____.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "print(_____.toarray())\n",
    "\n",
    "```\n",
    "\n",
    "To complete the script and successfully vectorize the sentences using `CountVectorizer`, follow these steps:\n",
    "\n",
    "1. **Initialize a CountVectorizer**: Create an instance of `CountVectorizer`.\n",
    "2. **Fit transform the sentences**: Use the `fit_transform` method to transform the sentences into a Bag-of-Words model.\n",
    "3. **Print the feature names and Bag-of-Words representation**: Replace the placeholders in the print statements with the variable that holds the transformed data.\n",
    "\n",
    "Here's how you can fill in the TODO lines and the placeholders in the print statements:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Simple sentences\n",
    "sentences = [\"I love to sing\", \n",
    "             \"Singing in the rain is my favorite\", \n",
    "             \"She sang the whole night at the concert\"]\n",
    "\n",
    "# TODO: Initialize a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# TODO: Fit transform the sentences\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print('Feature names:')\n",
    "# Replace the placeholder to print feature names\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "# Replace the placeholder to print the Bag of Words representation\n",
    "print(X.toarray())\n",
    "```\n",
    "\n",
    "This script initializes the `CountVectorizer`, fits it to the provided sentences, and then transforms these sentences into a Bag-of-Words representation. The output will include a list of all unique words (feature names) extracted from the sentences and a matrix showing the frequency of each word in each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1426ee",
   "metadata": {},
   "source": [
    "## Turn Rich Text into Bag-of-Words Representation\n",
    "\n",
    "Bravo, Space Voyager! Now, let's tackle a real-world challenge. Given some pre-processing code and three rich example sentences, your mission is to clean these sentences, then use the Bag-of-Words model to vectorize them, and finally, print out the feature names and their Bag-of-Words representation. Ready for a stellar run?\n",
    "\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "original_sentences = ['It is a lovely day, isn\\'t it?', \n",
    "                      'The sun is shining brightly!', \n",
    "                      'I love the taste of lemonade on a sunny day.']\n",
    "\n",
    "# Preprocess the sentences\n",
    "preprocessed_sentences = [clean_text(sentence) for sentence in original_sentences]\n",
    "\n",
    "# TODO: Initialize a CountVectorizer\n",
    "\n",
    "# TODO: Fit transform the preprocessed sentences\n",
    "\n",
    "# TODO: Print the feature names \n",
    "\n",
    "# TODO: Print the Bag of Words Representation\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "To complete the task of cleaning the sentences, vectorizing them using the Bag-of-Words model, and printing out the feature names and their Bag-of-Words representation, you'll need to follow these steps:\n",
    "\n",
    "1. **Initialize a CountVectorizer**: Create an instance of `CountVectorizer`.\n",
    "2. **Fit transform the preprocessed sentences**: Use the `fit_transform` method to transform the cleaned sentences into a Bag-of-Words model.\n",
    "3. **Print the feature names and Bag-of-Words representation**: Utilize the methods provided by `CountVectorizer` to display the desired outputs.\n",
    "\n",
    "Here's how to complete the script:\n",
    "\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "original_sentences = ['It is a lovely day, isn\\'t it?', \n",
    "                      'The sun is shining brightly!', \n",
    "                      'I love the taste of lemonade on a sunny day.']\n",
    "\n",
    "# Preprocess the sentences\n",
    "preprocessed_sentences = [clean_text(sentence) for sentence in original_sentences]\n",
    "\n",
    "# Initialize a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit transform the preprocessed sentences\n",
    "X = vectorizer.fit_transform(preprocessed_sentences)\n",
    "\n",
    "# Print the feature names \n",
    "print('Feature names:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the Bag of Words Representation\n",
    "print('Bag of Words Representation:')\n",
    "print(X.toarray())\n",
    "```\n",
    "\n",
    "This script takes the original sentences, processes them through the `clean_text` function to remove stopwords, punctuation, URLs, and applies stemming. Then, it uses `CountVectorizer` to create and display a Bag-of-Words representation of these cleaned sentences. The output will show the unique features (words) extracted from the text and their respective counts in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d606acf-5d48-4f18-ab3a-11b1d1cf87eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
