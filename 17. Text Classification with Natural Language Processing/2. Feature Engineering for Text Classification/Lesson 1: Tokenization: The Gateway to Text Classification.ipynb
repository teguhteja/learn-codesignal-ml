{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b385654",
   "metadata": {},
   "source": [
    "# Lesson 1: Tokenization: The Gateway to Text Classification\n",
    "\n",
    "\n",
    "Hello and welcome! Today's lesson will introduce a crucial component of text feature engineering: tokenization. Used in text classification, tokenization is a pre-processing step that transforms raw text into units of meaning known as tokens. By breaking down text into these consumable pieces, we can provide feeding material for machine learning models to understand the text better. Our goal in this lesson is to apply tokenization on a raw text dataset (IMDB movie review dataset) and understand how it can be beneficial in the process of text classification.\n",
    "\n",
    "## Understanding the Concept and Importance of Text Tokenization\n",
    "\n",
    "Text tokenization is a type of pre-processing step where a text string is split up into individual units (tokens). In most cases, these tokens are words, digits, or punctuation marks. For instance, consider this text: \"I love Python.\" After tokenization, this sentence is split into `['I', 'love', 'Python', '.']`, with each word and punctuation mark becoming a separate token.\n",
    "\n",
    "Text tokenization plays a foundational role in text classification and many Natural Language Processing (NLP) tasks. Consider the fact that most machine learning algorithms prefer numerical input. But when dealing with text data, we can't feed raw text directly into these algorithms. This is where tokenization steps in. It breaks down the text into individual tokens, which can then be transformed into some numerical form (via techniques like Bag-of-Words, TF-IDF, etc.). This transformed form can then be processed by the machine learning algorithms.\n",
    "\n",
    "## Applying Tokenization on a Text Example Using NLTK\n",
    "\n",
    "Before we tackle our dataset, let's understand how tokenization works with a simple example. Python and the NLTK (Natural Language Toolkit) library, a comprehensive library built specifically for NLP tasks, make tokenization simple and efficient. For our example, suppose we have a sentence: \"The cat is on the mat.\" Let's tokenize it:\n",
    "\n",
    "```python\n",
    "from nltk import word_tokenize\n",
    "text = \"The cat is on the mat.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```\n",
    "['The', 'cat', 'is', 'on', 'the', 'mat', '.']\n",
    "```\n",
    "\n",
    "## Text Classification Dataset Overview\n",
    "\n",
    "For the purpose of this lesson, we'll use the IMDB movie reviews dataset (provided in the NLTK corpus). This dataset contains movie reviews along with their associated binary sentiment polarity labels. The core dataset has 50,000 reviews split evenly into 25k for training and 25k for testing. Each set has 12.5k positive and 12.5k negative reviews. However, for the purpose of these lessons, we will focus on using the first 100 reviews.\n",
    "\n",
    "It's important to note that the IMDB dataset provided in the NLTK corpus has been preprocessed. The text is already lowercased, and common punctuation is typically separated from the words. This pre-cleaning makes the dataset well-suited for the tokenization process we'll be exploring.\n",
    "\n",
    "Let's get these reviews and print a few of them:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "movie_reviews_ids = movie_reviews.fileids()[:100]\n",
    "review_texts = [movie_reviews.raw(fileid) for fileid in movie_reviews_ids]\n",
    "print(\"First movie review:\\n\", review_texts[0][:260])\n",
    "```\n",
    "\n",
    "Note that we're only printing the first 260 characters of the first review to prevent lengthy output.\n",
    "\n",
    "## Applying Tokenization on the Dataset\n",
    "\n",
    "Now it's time to transform our data. For this, we will apply tokenization on all our 100 movie reviews.\n",
    "\n",
    "```python\n",
    "from nltk import word_tokenize\n",
    "tokenized_reviews = [word_tokenize(review) for review in review_texts]\n",
    "```\n",
    "\n",
    "So, what changes did tokenization bring to our data? Each review, which was initially a long string of text, is now a list of individual tokens (words, punctuation, etc), which collectively represent the review. In other words, our dataset evolved from being a list of strings to being a list of lists.\n",
    "\n",
    "```python\n",
    "for i, review in enumerate(tokenized_reviews[:3]):\n",
    "  print(f\"\\n Review {i+1} first 10 tokens:\\n\", review[:10])\n",
    "```\n",
    "\n",
    "The output of the above code will be:\n",
    "\n",
    "```\n",
    "\n",
    " Review 1 first 10 tokens:\n",
    " ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party']\n",
    "\n",
    " Review 2 first 10 tokens:\n",
    " ['the', 'happy', 'bastard', \"'s\", 'quick', 'movie', 'review', 'damn', 'that', 'y2k']\n",
    "\n",
    " Review 3 first 10 tokens:\n",
    " ['it', 'is', 'movies', 'like', 'these', 'that', 'make', 'a', 'jaded', 'movie']\n",
    "```\n",
    "\n",
    "## Lesson Summary and Next Steps\n",
    "\n",
    "Well done! Today, you learned about the fundamental concept of text tokenization and its importance in text classification. You also applied tokenization to the IMDB movie reviews dataset using Python and NLTK. Your text data is now effectively transformed into a form that machine learning models can digest better.\n",
    "\n",
    "As you advance in the course, you will refine this dataset further for your text classification objectives. We are laying the foundation one brick at a time, and tokenization was a sturdy one! Upcoming lessons will build upon this understanding. You'll harness this tokenized data to generate Bag-of-Words representations, implement TF-IDF representations, handle sparse features, and apply dimensionality reduction.\n",
    "\n",
    "Remember, practice consolidates learning. Make sure to reinforce your knowledge by practicing the code samples and applying these concepts contextually. Don't forget to use your creativity to manipulate codes and see the outcomes. Happy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd50842",
   "metadata": {},
   "source": [
    "## Filter Punctuation from Tokenized Review\n",
    "\n",
    "Good job, Space Voyager! The existing code is intended to tokenize a movie review and filter out punctuation, leaving only word tokens. However, there's a bug causing it to act incorrectly.\n",
    "\n",
    "Can you locate the error and adjust the code to correctly execute the task? After your revision, the code should tokenize the first movie review from the dataset, omit any punctuation, and print the first 10 word tokens.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Load the first movie review\n",
    "review_text = movie_reviews.raw(movie_reviews.fileids()[0])\n",
    "\n",
    "# Apply tokenization on the review\n",
    "tokenized_review = word_tokenize(review_text)\n",
    "\n",
    "# Should keep only word tokens\n",
    "current_tokens = [token for token in tokenized_review if not token.isalpha()]\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(\"\\nThe first 10 tokens are:\\n\", current_tokens[:10])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "The issue in the provided code lies in the list comprehension used to filter out non-word tokens. The condition used (`if not token.isalpha()`) is incorrect because it keeps tokens that are not alphabetic, which are typically punctuation. To correct this, you should change the condition to keep only alphabetic tokens by using `if token.isalpha()`.\n",
    "\n",
    "Here is the corrected version of the code:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Load the first movie review\n",
    "review_text = movie_reviews.raw(movie_reviews.fileids()[0])\n",
    "\n",
    "# Apply tokenization on the review\n",
    "tokenized_review = word_tokenize(review_text)\n",
    "\n",
    "# Should keep only word tokens\n",
    "current_tokens = [token for token in tokenized_review if token.isalpha()]\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(\"\\nThe first 10 tokens are:\\n\", current_tokens[:10])\n",
    "```\n",
    "\n",
    "This adjustment ensures that the list `current_tokens` will only include tokens that consist of alphabetic characters, effectively filtering out punctuation and other non-word elements. The print statement will then display the first 10 word tokens of the first movie review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded50fe7",
   "metadata": {},
   "source": [
    "## Filtering Word Tokens from a Sentence\n",
    "\n",
    "Hey Space Voyager, you're doing great!\n",
    "\n",
    "In the provided starter code, fill in the blanks (____) to convert a given sentence into individual tokens using the nltk library. Then, filter out the word tokens, leaving out the punctuation marks.\n",
    "\n",
    "```python\n",
    "from nltk import ____\n",
    "\n",
    "# Given text\n",
    "text = \"Who doesn't enjoy studying Feature Engineering for Text Classification?\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = ____(text)\n",
    "\n",
    "# Filter out the words\n",
    "words = [token for token in tokens if token.____()]\n",
    "\n",
    "print(\"\\n Word Tokens:\\n\", words)\n",
    "\n",
    "```\n",
    "\n",
    "To complete the task of tokenizing the given text and filtering out only the word tokens while leaving out punctuation marks, you'll need to use the `word_tokenize` function from the `nltk.tokenize` module. Then, you can filter out words by checking if each token is alphabetic using the `isalpha()` method. Here's how to fill in the blanks in your code:\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize  # Import the word_tokenize function\n",
    "\n",
    "# Given text\n",
    "text = \"Who doesn't enjoy studying Feature Engineering for Text Classification?\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)  # Use word_tokenize to tokenize the text\n",
    "\n",
    "# Filter out the words\n",
    "words = [token for token in tokens if token.isalpha()]  # Use isalpha() to check if the token consists only of letters\n",
    "\n",
    "print(\"\\n Word Tokens:\\n\", words)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **`from nltk.tokenize import word_tokenize`**: This line imports the `word_tokenize` function from the `nltk.tokenize` module, which is used to split the string into individual tokens.\n",
    "- **`tokens = word_tokenize(text)`**: This line tokenizes the input text into individual elements, including words and punctuation.\n",
    "- **`words = [token for token in tokens if token.isalpha()]`**: This list comprehension filters out only those tokens that are purely alphabetic, excluding punctuation and other non-word tokens like contractions (e.g., \"doesn't\").\n",
    "\n",
    "When you run this code, it will print out only the alphabetic word tokens from the given sentence, omitting punctuation and non-alphabetic characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faed51b",
   "metadata": {},
   "source": [
    "## Completing Code for Data Loading and Tokenizing\n",
    "\n",
    "Great job so far, Space Voyager!\n",
    "\n",
    "Your task now is to fill in the blanks with the correct code to load the IMDB movie reviews dataset. After loading the data, tokenize the first movie review, and print the first 10 tokens of the tokenized review.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from ____ import ____\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.____('movie_reviews', quiet=True)\n",
    "\n",
    "# Load the first movie review\n",
    "review_text = ____.raw(____.fileids()[0])\n",
    "\n",
    "# Apply tokenization on the review\n",
    "tokenized_review = word_tokenize(review_text)\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(\"\\nThe first 10 tokens are:\\n\", tokenized_review[:10])\n",
    "\n",
    "```\n",
    "\n",
    "To complete the provided code snippet for loading the IMDB movie reviews dataset, tokenizing the first movie review, and printing the first 10 tokens, you need to fill in the blanks appropriately with the correct imports and function calls. Hereâ€™s the filled-out version of the code:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews  # Import the movie_reviews dataset\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('movie_reviews', quiet=True)  # Ensure the movie_reviews dataset is downloaded\n",
    "\n",
    "# Load the first movie review\n",
    "review_text = movie_reviews.raw(movie_reviews.fileids()[0])  # Access the raw text of the first review\n",
    "\n",
    "# Apply tokenization on the review\n",
    "tokenized_review = word_tokenize(review_text)  # Tokenize the text of the first review\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(\"\\nThe first 10 tokens are:\\n\", tokenized_review[:10])\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **`from nltk.corpus import movie_reviews`**: This line imports the `movie_reviews` dataset from the `nltk.corpus` module. `movie_reviews` is a built-in dataset within NLTK that contains a collection of movie reviews.\n",
    "- **`nltk.download('movie_reviews', quiet=True)`**: This function call ensures that the `movie_reviews` dataset is downloaded and available for use. The `quiet=True` parameter suppresses the output to keep the console clean unless an error occurs.\n",
    "- **`review_text = movie_reviews.raw(movie_reviews.fileids()[0])`**: Here, `movie_reviews.fileids()[0]` gets the file ID of the first movie review. The `raw()` method retrieves the full raw text of that review.\n",
    "- **`tokenized_review = word_tokenize(review_text)`**: This function call tokenizes the retrieved text of the first movie review into individual tokens, which include words and punctuation.\n",
    "- **`print(\"\\nThe first 10 tokens are:\\n\", tokenized_review[:10])`**: Finally, this line prints the first 10 tokens of the tokenized review to the console.\n",
    "\n",
    "When you run this code, it will display the first 10 tokens of the first movie review in the IMDB dataset, providing a glimpse into how the text data is structured at the token level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c31953",
   "metadata": {},
   "source": [
    "## Tokenizing and Filtering a Movie Review\n",
    "\n",
    "Hail, Stellar Navigator! In this exercise, you'll need to tokenize the first movie review from our dataset, filter out any punctuation, and print the first 10 word tokens. Oh and don't forget to include the sentiment of the review. Let's add the missing pieces to our script and complete this task!\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews \n",
    "from nltk import word_tokenize \n",
    "\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Fetch the first movie review and its sentiment label\n",
    "first_review_id = movie_reviews.fileids()[0]\n",
    "review_text = movie_reviews.raw(first_review_id) \n",
    "review_sentiment = first_review_id.split('/')[0] \n",
    "\n",
    "# TODO: Apply tokenization on the review \n",
    "\n",
    "# TODO: Filter out the punctuation, keeping only word tokens \n",
    "\n",
    "# Print the sentiment label and the first 10 word tokens \n",
    "print(f\"\\nSentiment of the first review: {review_sentiment}\")\n",
    "print(f\"\\nFirst 10 word tokens of the first review:\\n\", ____[:10]) \n",
    "\n",
    "```\n",
    "\n",
    "To complete this script, you'll need to tokenize the movie review text, filter out any punctuation from the tokens, and then print the first 10 word tokens along with the sentiment of the review. Here's how you can fill in the missing parts of the script:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Fetch the first movie review and its sentiment label\n",
    "first_review_id = movie_reviews.fileids()[0]\n",
    "review_text = movie_reviews.raw(first_review_id)\n",
    "review_sentiment = first_review_id.split('/')[0]  # Extract sentiment from file ID\n",
    "\n",
    "# Apply tokenization on the review\n",
    "tokens = word_tokenize(review_text)\n",
    "\n",
    "# Filter out the punctuation, keeping only word tokens\n",
    "word_tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "# Print the sentiment label and the first 10 word tokens\n",
    "print(f\"\\nSentiment of the first review: {review_sentiment}\")\n",
    "print(f\"\\nFirst 10 word tokens of the first review:\\n\", word_tokens[:10])\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Tokenization**: The `word_tokenize(review_text)` function tokenizes the entire content of the review, splitting it into individual elements like words and punctuation.\n",
    "- **Filtering Punctuation**: The list comprehension `[token for token in tokens if token.isalpha()]` filters out any tokens that contain non-alphabetic characters (i.e., punctuation and numbers), keeping only word tokens.\n",
    "- **Sentiment Extraction**: The sentiment label (`pos` for positive or `neg` for negative) is extracted from the file ID (`first_review_id`). The ID format typically includes the sentiment as the first part of the path (e.g., \"pos/xxxxx.txt\" or \"neg/xxxxx.txt\"), so splitting by '/' and taking the first item gives the sentiment.\n",
    "- **Printing Results**: The script prints the sentiment of the review followed by the first 10 word tokens of the review.\n",
    "\n",
    "This script effectively processes the first movie review in the dataset, providing insights into its sentiment and content through tokenization and filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd07eba",
   "metadata": {},
   "source": [
    "## Tokenizing First Review and Printing Tokens\n",
    "\n",
    "Keep it up, Space Voyager! Now, write a script to load the movie_reviews dataset, find the index of the first movie review, get that review, then tokenize it and lastly print out the first 10 tokens.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# TODO: Get the index of the first movie review\n",
    "\n",
    "# TODO: Load the first movie review using its index\n",
    "\n",
    "# TODO: Apply tokenization on the review\n",
    "\n",
    "# TODO: Print the first 10 tokens\n",
    "\n",
    "```\n",
    "\n",
    "To complete the script that loads the `movie_reviews` dataset, retrieves the first movie review, tokenizes it, and prints the first 10 tokens, follow these steps:\n",
    "\n",
    "1. **Get the index of the first movie review**: The movie reviews in the `movie_reviews` corpus are accessed via file IDs. You can retrieve the first file ID directly.\n",
    "2. **Load the review using its index**: Use the file ID to get the raw text of the review.\n",
    "3. **Tokenize the review**: Use the `word_tokenize` function to break the text into tokens.\n",
    "4. **Print the first 10 tokens**: Display the initial tokens to check the tokenization.\n",
    "\n",
    "Here's how you can implement this:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Get the index of the first movie review\n",
    "first_review_id = movie_reviews.fileids()[0]  # File ID of the first review\n",
    "\n",
    "# Load the first movie review using its index\n",
    "review_text = movie_reviews.raw(first_review_id)  # Raw text of the first review\n",
    "\n",
    "# Apply tokenization on the review\n",
    "tokens = word_tokenize(review_text)  # Tokenize the text\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(\"\\nThe first 10 tokens are:\\n\", tokens[:10])\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **`first_review_id = movie_reviews.fileids()[0]`**: This retrieves the ID of the first review. The `fileids()` method lists all the file IDs in the dataset, and `[0]` accesses the first one.\n",
    "- **`review_text = movie_reviews.raw(first_review_id)`**: This loads the raw text of the review identified by `first_review_id`.\n",
    "- **`tokens = word_tokenize(review_text)`**: This line tokenizes the review text into words and punctuation.\n",
    "- **`print(\"\\nThe first 10 tokens are:\\n\", tokens[:10])`**: This prints the first 10 tokens of the tokenized review, giving you an initial look at the content.\n",
    "\n",
    "This script effectively demonstrates how to access and process text data from the `movie_reviews` dataset, making it a useful example for learning text processing with Python and NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab9cdf-0a4f-4caa-a7d6-95292b312cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
