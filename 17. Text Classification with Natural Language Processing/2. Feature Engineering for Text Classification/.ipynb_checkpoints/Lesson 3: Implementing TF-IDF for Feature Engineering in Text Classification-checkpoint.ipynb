{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827e97d3",
   "metadata": {},
   "source": [
    "# Lesson 3: Implementing TF-IDF for Feature Engineering in Text Classification\n",
    "\n",
    "Welcome! Today, we're going to take a deep dive into the concept of TF-IDF and its crucial role in Text Classification. TF-IDF stands for Term Frequency-Inverse Document Frequency. It's a numerical statistic that reflects how important a word is in a document within a corpus of documents. The TF-IDF value increases proportionally to the number of times a word appears in the document but is counterbalanced by the frequency of the word in the corpus, helping to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "TF-IDF is used in information retrieval and text mining, to assist in identifying key words that contribute the most to the document's relevancy. In simple terms, terms that are more frequent in a specific document and less frequent in other documents from the corpus are significant and have high TF-IDF scores.\n",
    "\n",
    "Now, let's understand it in practice.\n",
    "\n",
    "## Introduction to TfidfVectorizer\n",
    "\n",
    "In the Python ecosystem, scikit-learn is a widely used library offering various machine learning methods, along with utilities for pre-processing data, cross-validation, and other related tasks. One of the utilities it provides for text processing is TfidfVectorizer.\n",
    "\n",
    "Let's walk through each line of the code:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "```\n",
    "\n",
    "We first import the necessary libraries. Next, we set up a small list of text documents:\n",
    "\n",
    "```python\n",
    "sentences = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "```\n",
    "\n",
    "We then create an instance of the TfidfVectorizer class and fit the vectorizer to our set of documents:\n",
    "\n",
    "```python\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(sentences)\n",
    "```\n",
    "\n",
    "\"The fitting process\" involves tokenization and learning the vocabulary. The text documents are tokenized into a set of tokens, and the vocabulary, which is a set of all tokens, is learned. At this point, we have effectively transformed our sentences into a numerical format that our machine can understand!\n",
    "\n",
    "## Understanding Vocabulary and IDF from TfidfVectorizer\n",
    "\n",
    "We can now print out the vocabulary and the Inverse Document Frequency (IDF) for each word in the vocabulary:\n",
    "\n",
    "```python\n",
    "print(f'Vocabulary: {vectorizer.vocabulary_}\\n')\n",
    "print(f'IDF: {vectorizer.idf_}\\n')\n",
    "```\n",
    "\n",
    "The output looks something like this:\n",
    "\n",
    "```\n",
    "Vocabulary: {'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
    "\n",
    "IDF: [1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
    " 1.         1.91629073 1.        ]\n",
    "```\n",
    "\n",
    "The 'Vocabulary' shows the numerical encoding of our sentences; each distinct word is assigned a unique numerical value. The 'IDF' values are the computed Inverse Document Frequencies for each word. These values define how important a word is to the document within the overall corpus. From these outputs, we get an important inference: terms that are very common in all documents (such as 'is' and 'the') have lower IDF scores, showing less importance. On the other hand, terms that are less common have higher IDF scores, indicating they may be more important or distinctive in our text data.\n",
    "\n",
    "## Transforming Sentences to TF-IDF Vectors and Understanding the Output\n",
    "\n",
    "Next, let's transform one of the text documents to a sparse vector of TF-IDF values:\n",
    "\n",
    "```python\n",
    "vector = vectorizer.transform([sentences[0]])\n",
    "```\n",
    "\n",
    "This step encodes our sentences using TF-IDF scores. Simply put, each word in the sentence is translated into a numerical value. This numerical value - generated by the TF-IDF algorithm - represents the word's relevance or significance within the document.\n",
    "\n",
    "Finally, let's print out the resulting vector and its shape:\n",
    "\n",
    "```python\n",
    "print('Shape:', vector.shape)\n",
    "print('Array:', vector.toarray())\n",
    "```\n",
    "\n",
    "The output reveals the shape of our encoded array and the TF-IDF score associated with each word in our sentence:\n",
    "\n",
    "```\n",
    "Shape: (1, 9)\n",
    "Array: [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
    " 0.38408524 0.         0.38408524]]\n",
    "```\n",
    "\n",
    "In the array, the order of the TF-IDF scores matches the order of the words in the 'vocabulary'. So, for instance, the first word 'and' (in the 'vocabulary') has a score of 0.0 as it does not occur in the sentence, while the word 'this' has a score of 0.38408524, which gives us the relevance of the word 'this' in our sentence.\n",
    "\n",
    "This way, we have transformed human language into a numerical representation that our machine can understand and learn from!\n",
    "\n",
    "## Working with the IMDB Movie Reviews Dataset\n",
    "\n",
    "Expanding on the simple sentences, let's apply the same process to the IMDB movie reviews dataset available in the NLTK library. This gives us a real-world scenario where TfidfVectorizer is utilized for text classification tasks - in this case, movie review classification.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "print('Shape:', X.shape)\n",
    "\n",
    "```\n",
    "\n",
    "After applying the TfidfVectorizer to the movie reviews dataset, our output will show a shape that signifies the matrix dimension with the number of reviews and the unique words across all reviews:\n",
    "\n",
    "```sh\n",
    "Shape: (2000, 39659)\n",
    "```\n",
    "\n",
    "## Introduction to Sparse Matrices\n",
    "In cases of large text datasets like ours, the matrix will have many zero entries because many words won't appear in a given review. Storing all these zeros can be highly memory-intensive and inefficient. Instead, we use a sparse matrix where we only store the non-zero elements, optimizing our memory usage. This is the storage method used for X, which holds all the TF-IDF vectors.\n",
    "\n",
    "Let's look into the structure of this sparse matrix a bit more:\n",
    "\n",
    "```Python\n",
    "print(\"Total non-zero elements in the matrix X: \", len(X.data))\n",
    "print(\"Length of the column indices array in X: \", len(X.indices))\n",
    "print(\"Length of the row pointer array in X: \", len(X.indptr))\n",
    "```\n",
    "The outputs will look like this:\n",
    "\n",
    "```sh\n",
    "Total non-zero elements in the matrix X:  666842\n",
    "Length of the column indices array in X:  666842\n",
    "Length of the row pointer array in X:  2001\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "X.data: This array holds all the non-zero elements in our matrix, hence its length signifies the total number of non-zero elements.\n",
    "\n",
    "X.indices: This array holds the column (word) indice for each non-zero element—it is as long as the X.data, telling us which word each data point corresponds to.\n",
    "\n",
    "X.indptr: This is the \"row pointer\" array. It has as many elements as the number of rows in the matrix plus one. Each value signifies where the corresponding row starts in the X.data and X.indices arrays. It helps us locate which data points belong to which review.\n",
    "\n",
    "## Lesson Summary and Practice\n",
    "Congratulations! You've just learned about the concept of TF-IDF, how to apply the TF-IDF Vectorizer to text data in Python using the Scikit-Learn library, and how to understand the subsequent output. Additionally, you've been introduced to sparse matrices—a helpful concept when handling large text datasets—and understood how such matrices are represented.\n",
    "\n",
    "In the coming practice exercises, you will independently apply these concepts, helping you solidify your understanding and deepen your comprehension of how TF-IDF fits into text classification tasks. Keep up the excellent work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fb0a8",
   "metadata": {},
   "source": [
    "## Change TF-IDF Vector for Different Sentence\n",
    "\n",
    "Excellent progress, Space Explorer! Now, let's take it a step further. Your task is to modify the provided code so that it transforms the second sentence in the list instead of the first sentence.\n",
    "\n",
    "Give it a try and observe the difference in the output of the TF-IDF Vector. Keep in mind that this change will further enhance your understanding of how the TF-IDF Vector is influenced by the frequency of words in the chosen document.\n",
    "\n",
    "Get ready to code!\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this document is the third one.',\n",
    "    'Is this the first document here?'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(sentences)\n",
    "vector = vectorizer.transform([sentences[0]])\n",
    "print('Shape:', vector.shape)\n",
    "print('Features:', vectorizer.get_feature_names_out())\n",
    "print('Array:', vector.toarray())\n",
    "\n",
    "```\n",
    "\n",
    "To modify the provided code to transform the second sentence in the list instead of the first, you simply need to change the index from 0 to 1 in the `transform` method call. This will allow you to see how the TF-IDF scores vary for the second sentence, which has a different word composition and frequency compared to the first.\n",
    "\n",
    "Here's the updated code:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# List of sentences\n",
    "sentences = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this document is the third one.',\n",
    "    'Is this the first document here?'\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the sentences\n",
    "vectorizer.fit(sentences)\n",
    "\n",
    "# Transform the second sentence\n",
    "vector = vectorizer.transform([sentences[1]])\n",
    "\n",
    "# Output the results\n",
    "print('Shape:', vector.shape)\n",
    "print('Features:', vectorizer.get_feature_names_out())\n",
    "print('Array:', vector.toarray())\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "- **Shape:** This will show the dimensions of the TF-IDF vector (1 row for the single sentence and columns equal to the number of unique words across all documents).\n",
    "- **Features:** This will list all the words that the vectorizer has learned from the entire set of documents.\n",
    "- **Array:** This will display the actual TF-IDF scores for each word as they appear in the second sentence. Since the word \"document\" appears twice in the second sentence, its TF-IDF score might be influenced differently compared to its appearance in other sentences.\n",
    "\n",
    "This modification will help you observe how the TF-IDF values change when a word (like \"document\") appears more frequently within a single document relative to its frequency in the entire corpus. This is a key aspect of understanding the weighting mechanism of TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbbcf7",
   "metadata": {},
   "source": [
    "## Implementing TF-IDF Vectorizer on Provided Text\n",
    "\n",
    "Right on, Space Voyager! Let's continue our exploration of TF-IDF. Fill in the blanks to fit the vectorizer to the sentences and transform the third sentence into a TF-IDF vector. Finally, print the shape of the array and the array itself. Happy coding!\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# sentences collection\n",
    "sentences = ['The quick brown fox.', 'Foxes are quick and agile.', 'The fox jumped.']\n",
    "\n",
    "# create an instance of TfidfVectorizer\n",
    "vectorizer = ____()\n",
    "\n",
    "# fit the vectorizer to the sentences\n",
    "vectorizer.____(____)\n",
    "\n",
    "# transform the third sentence\n",
    "vector = vectorizer.____([sentences[2]])\n",
    "\n",
    "# print the shape and array\n",
    "print('Shape:', vector.shape)\n",
    "print('Array:', vector.toarray())\n",
    "\n",
    "```\n",
    "\n",
    "To complete the code snippet and correctly use the TF-IDF vectorizer to transform the third sentence from the list of sentences, you need to fill in the blanks with appropriate method calls and parameters. Here's the completed code:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# sentences collection\n",
    "sentences = ['The quick brown fox.', 'Foxes are quick and agile.', 'The fox jumped.']\n",
    "\n",
    "# create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit the vectorizer to the sentences\n",
    "vectorizer.fit(sentences)\n",
    "\n",
    "# transform the third sentence\n",
    "vector = vectorizer.transform([sentences[2]])\n",
    "\n",
    "# print the shape and array\n",
    "print('Shape:', vector.shape)\n",
    "print('Array:', vector.toarray())\n",
    "```\n",
    "\n",
    "### Explanation of the Filled Blanks:\n",
    "- **`TfidfVectorizer()`**: This initializes a new instance of the TfidfVectorizer class.\n",
    "- **`fit(sentences)`**: This method is used to learn the vocabulary of all the sentences in the list.\n",
    "- **`transform([sentences[2]])`**: This method transforms the third sentence (indexed at 2) into its TF-IDF vector representation based on the vocabulary learned during the fitting process.\n",
    "\n",
    "This code will output the shape of the TF-IDF vector for the third sentence and the actual TF-IDF values for each term in the sentence. These values help in understanding the importance of each word within the context of the provided documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ed98e",
   "metadata": {},
   "source": [
    "## Understanding Sparse Matrix Components\n",
    "\n",
    "Impressive navigation so far, Space Explorer!\n",
    "\n",
    "Let's test your understanding of the sparse matrix components. Your mission is to fill in the blanks (____) to complete the code. The objective here is to learn and understand the structure of a sparse matrix by printing its key components: \"data\", \"indices\", and \"indptr\".\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = ['Expedition to Mars', 'NASA launched an expedition to Mars', 'Mars expedition was successful']\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "  \n",
    "# fit_transform to convert text to vector  \n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# print the shape and array\n",
    "print('Shape:', X.shape)\n",
    "print('Array:', X.toarray())\n",
    "\n",
    "# Print the data\n",
    "print(\"Data of Sparse Matrix \", X.____)\n",
    "\n",
    "# Get the indices of the non-zero elements \n",
    "print(\"Indices of non-zero elements of Sparse Matrix \", X.____)\n",
    "\n",
    "# Get the array that points to where the start of each row is in the data and indices array\n",
    "print(\"Pointer to start of each row in indices and data \", X.____)\n",
    "```\n",
    "\n",
    "To complete the code snippet and correctly print the structure of the sparse matrix created by the `TfidfVectorizer`, you need to access the internal components of the sparse matrix representation. Here's the completed code with the correct attributes filled in:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = ['Expedition to Mars', 'NASA launched an expedition to Mars', 'Mars expedition was successful']\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "  \n",
    "# fit_transform to convert text to vector  \n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# print the shape and array\n",
    "print('Shape:', X.shape)\n",
    "print('Array:', X.toarray())\n",
    "\n",
    "# Print the data\n",
    "print(\"Data of Sparse Matrix \", X.data)\n",
    "\n",
    "# Get the indices of the non-zero elements \n",
    "print(\"Indices of non-zero elements of Sparse Matrix \", X.indices)\n",
    "\n",
    "# Get the array that points to where the start of each row is in the data and indices array\n",
    "print(\"Pointer to start of each row in indices and data \", X.indptr)\n",
    "```\n",
    "\n",
    "### Explanation of the Filled Blanks:\n",
    "- **`X.data`**: This attribute holds all the non-zero elements of the sparse matrix. These are the TF-IDF values for each non-zero term in the documents.\n",
    "- **`X.indices`**: This attribute contains the column indices of the non-zero elements in the sparse matrix. It tells which word (column in the TF-IDF matrix) each value in `X.data` corresponds to.\n",
    "- **`X.indptr`**: This attribute is an array that helps in identifying the start and end of each row in the `X.data` and `X.indices` arrays. It essentially points to where each document's (row's) data starts in the `X.data` array.\n",
    "\n",
    "This code effectively demonstrates how to access and understand the internal structure of a sparse matrix, which is crucial for handling and optimizing large datasets in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2dada0",
   "metadata": {},
   "source": [
    "## Applying TF-IDF Vectorizer On Reviews Dataset\n",
    "\n",
    "Nice navigation so far, Space Voyager! Now, let's spice things up a bit. Recall how we transformed our text into a matrix of TF-IDF features? Let's do that again. In the provided code, add the code to import TfidfVectorizer and then create an instance of TfidfVectorizer. Apply fit_transform to our reviews data. Now, go ahead and amplify your learning!\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "\n",
    "# TODO: Create an instance of TfidfVectorizer\n",
    "\n",
    "# TODO: Apply fit_transform on reviews\n",
    "\n",
    "print('Feature Names:', vectorizer.get_feature_names_out()[-5:])\n",
    "\n",
    "```\n",
    "\n",
    "To complete the provided code snippet, you need to create an instance of the `TfidfVectorizer` and use it to apply `fit_transform` to the `reviews` dataset. This will convert the text reviews into a matrix of TF-IDF features. Here's how you can do it:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Apply fit_transform on reviews\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "print('Feature Names:', vectorizer.get_feature_names_out()[-5:])\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **`TfidfVectorizer()`**: This initializes an instance of `TfidfVectorizer`, which will be used to transform the text data into TF-IDF features.\n",
    "- **`fit_transform(reviews)`**: This method fits the vectorizer to the text data and transforms the text into a TF-IDF encoded sparse matrix. `fit_transform` combines the fitting and transformation steps into one, which is more efficient than running them separately when you're processing the data for the first time.\n",
    "- **`vectorizer.get_feature_names_out()`**: After transforming the data, this method retrieves the feature names (i.e., the vocabulary words), and `[-5:]` is used to print the last five feature names to give a glimpse of the vocabulary.\n",
    "\n",
    "This code will output the shape of the TF-IDF matrix and the last five features in the vocabulary, demonstrating how to work with text data for feature extraction in natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a626d",
   "metadata": {},
   "source": [
    "## Implementing TF-IDF Vectorizer from Scratch\n",
    "\n",
    "Your journey into the cosmos of TF-IDF is nearing its zenith, Stellar Navigator! Your mission is to write the complete code to apply TfidfVectorizer to a set of sentences. Fit the vectorizer to the sentences, transform the first sentence into a TF-IDF vector, and print the TF-IDF features, dimension, and array. Are you ready to complete this fascinating voyage in the space of Text Feature Engineering? Let's code!\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# List of example sentences \n",
    "sentences = ['Exploration is really the essence of the human spirit.', \n",
    "             'We are driven by a desire to explore and discover the unknown.',\n",
    "             'The will to explore is a significant part of being human.']\n",
    "\n",
    "# TODO: Create an instance of TfidfVectorizer\n",
    "\n",
    "# TODO: Fit the vectorizer to the sentences\n",
    "\n",
    "# TODO: Transform the first sentence into a TF-IDF vector\n",
    "\n",
    "# TODO: Print the shape of the TF-IDF vector\n",
    "\n",
    "# TODO: Print the TF-IDF features\n",
    "\n",
    "# TODO: Print the TF-IDF vector\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "To fulfill your mission of applying the `TfidfVectorizer` to a set of sentences, transforming one of them into a TF-IDF vector, and then printing relevant details, follow this complete code guide:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# List of example sentences \n",
    "sentences = ['Exploration is really the essence of the human spirit.', \n",
    "             'We are driven by a desire to explore and discover the unknown.',\n",
    "             'The will to explore is a significant part of being human.']\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the sentences\n",
    "vectorizer.fit(sentences)\n",
    "\n",
    "# Transform the first sentence into a TF-IDF vector\n",
    "tfidf_vector = vectorizer.transform([sentences[0]])\n",
    "\n",
    "# Print the shape of the TF-IDF vector\n",
    "print('Shape:', tfidf_vector.shape)\n",
    "\n",
    "# Print the TF-IDF features\n",
    "print('TF-IDF Features:', vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the TF-IDF vector\n",
    "print('TF-IDF Vector:', tfidf_vector.toarray())\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **`TfidfVectorizer()`**: This line initializes the `TfidfVectorizer`. This object will convert text documents into a matrix of TF-IDF features.\n",
    "- **`fit(sentences)`**: This method learns the vocabulary of the entire set of sentences. It's necessary to fit the model to the data before transforming it.\n",
    "- **`transform([sentences[0]])`**: This method transforms the first sentence of the list into a sparse matrix of TF-IDF features. The brackets around `sentences[0]` ensure that the input is treated as a list.\n",
    "- **`tfidf_vector.shape`**: This prints the shape of the TF-IDF vector, showing the number of documents (in this case, 1) and the number of features in the vocabulary.\n",
    "- **`vectorizer.get_feature_names_out()`**: This retrieves all the feature names (i.e., words in the learned vocabulary) from the vectorizer.\n",
    "- **`tfidf_vector.toarray()`**: Converts the sparse matrix to a dense array and prints it, showing the TF-IDF weights for each word in the first sentence.\n",
    "\n",
    "This code snippet effectively demonstrates how to use `TfidfVectorizer` to process text data, making it ready for further analysis or machine learning modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8829d5-3439-4c5c-b7be-7026b0a88601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
