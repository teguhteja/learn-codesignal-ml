{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981b3427",
   "metadata": {},
   "source": [
    "# Lesson 1: Preprocessing Text Data: Train-Test Split and Stratified Cross-Validation\n",
    "\n",
    "Welcome to this lesson where we'll explore a critical aspect of text data analysis: Dimensionality Reduction. As you've learned in previous lessons, raw text is transformed into a feature matrix using techniques like Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF). However, these matrices often become high-dimensional, which can complicate the model, lengthen training times, and potentially degrade performance due to the \"curse of dimensionality.\" To combat these issues, we employ Dimensionality Reduction techniques to decrease the size of the feature space. By the end of this lesson, you'll understand the basics of Dimensionality Reduction and how to implement the TruncatedSVD method in Python on the IMDB movie reviews dataset.\n",
    "\n",
    "## Understanding Dimensionality Reduction\n",
    "\n",
    "In many machine learning problems, data is represented using a large number of features or dimensions. When the dimensionality, or number of features, is too large, data can become sparse and scattered, potentially causing the learning algorithm to perform poorly, struggle to find patterns, or even overfit to noise in the training set.\n",
    "\n",
    "This well-known issue is referred to as the \"curse of dimensionality,\" and to address it, we use Dimensionality Reduction techniques. The goal of these techniques is to reduce the number of features in your data while retaining the essential information and structure. This transformation results in a new representation of the original data in a reduced feature space, generally leading to lower computational requirements, reduced storage space, and, importantly, improved model performance by mitigating overfitting.\n",
    "\n",
    "One popular method for dimensionality reduction is Singular Value Decomposition (SVD). In NLP, we often use a variant called TruncatedSVD, which reduces the feature space to a user-specified smaller dimension while preserving maximum data variance.\n",
    "\n",
    "## Implementing TruncatedSVD in Python\n",
    "\n",
    "The dimensionality reduction technique we'll be using in this lesson is TruncatedSVD, available in the `sklearn.decomposition` module of the scikit-learn library. We instantiate a TruncatedSVD object and specify the number of desired output features with the `n_components` parameter.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "```\n",
    "\n",
    "In the code above, `n_components=50` specifies that we want to reduce our feature space to 50 dimensions. The `fit_transform` method is then used to apply this transformation to a given matrix.\n",
    "\n",
    "## Applying Dimensionality Reduction on a TF-IDF Matrix\n",
    "\n",
    "We can apply dimensionality reduction directly on the TF-IDF matrix. The TruncatedSVD transformer is fit on the TF-IDF matrix and then used to transform the matrix to its reduced form.\n",
    "\n",
    "```python\n",
    "features = svd.fit_transform(tfidf_matrix)\n",
    "```\n",
    "\n",
    "In the code above, the `fit_transform` method applies the TruncatedSVD transformation to the `tfidf_matrix` and stores the reduced feature matrix in `features`.\n",
    "\n",
    "## Implementing Dimensionality Reduction on the IMDB Dataset\n",
    "\n",
    "Now, let's combine all the steps and apply this to our IMDB movie reviews dataset.\n",
    "\n",
    "```python\n",
    "# Import necessary Libraries\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load IMDB Movie Reviews Dataset\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# We will be working with the first 100 reviews\n",
    "first_100_reviewids = movie_reviews.fileids()[:100]\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in first_100_reviewids]\n",
    "\n",
    "# Transform raw data into a TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(reviews)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {tfidf_matrix.shape}\\n\")\n",
    "\n",
    "# Now we will apply TruncatedSVD for Dimensionality Reduction\n",
    "# We've set n_components=50, which specifies we want to reduce our feature space to 50 dimensions. \n",
    "svd = TruncatedSVD(n_components=50)\n",
    "features = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")\n",
    "```\n",
    "\n",
    "The output of the above code will show:\n",
    "\n",
    "```\n",
    "Shape of the features matrix before dimensionality reduction: (100, 8865)\n",
    "\n",
    "Shape of the features matrix after dimensionality reduction: (100, 50)\n",
    "```\n",
    "\n",
    "This output highlights the effectiveness of TruncatedSVD in reducing the dimensionality of the TF-IDF matrix from 8865 to just 50. This significant reduction in the number of features can simplify models and potentially improve their performance on predictive tasks.\n",
    "\n",
    "## Selecting the Number of Components\n",
    "\n",
    "In the previous code, you saw how TruncatedSVD was implemented on the TF-IDF matrix of the IMDB movie reviews. The primary parameter we set was `n_components=50`, meaning we wanted to reduce our original feature space down to 50 dimensions. But how does this reduction actually happen?\n",
    "\n",
    "TruncatedSVD uses a mathematical technique called Singular Value Decomposition. This method breaks down the original feature matrix into three components - two orthogonal matrices, and a diagonal matrix containing singular values. In TruncatedSVD's case, the technique only keeps the top 'n' largest singular values, effectively reducing the dimension of the matrix.\n",
    "\n",
    "Selecting the number of components or 'n' value is a critical decision. A smaller value could speed up the training process and potentially prevent overfitting but might lose essential information. On the contrary, a larger value would preserve more information but might not address the 'curse of dimensionality' effectively.\n",
    "\n",
    "A common strategy is to fit the SVD transformer and plot the explained variance ratio as a function of the number of components. The ideal number of components is typically chosen at a point where the explained variance ratio plot starts to show diminishing returns.\n",
    "\n",
    "## Lesson Summary and Upcoming Practice\n",
    "\n",
    "That's a wrap on this lesson about dimensionality reduction in text classification! You've learned about the importance of dimensionality reduction, the theory behind TruncatedSVD—a popular dimensionality reduction technique, and implemented it in Python on a real-world text dataset.\n",
    "\n",
    "As always, understanding the principles is just the first step. The effectiveness of learning lies in the practice. We've curated some fun and challenging exercises for you to solidify this new knowledge, which you'll find in the subsequent practice segment. Remember the principle - learning happens by doing. So go ahead and apply your newly acquired skills!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5049f01",
   "metadata": {},
   "source": [
    "## Implement Stratified Cross-Validation in Train-Test Split\n",
    "\n",
    "Great job so far, Space Voyager! Your next task is to tweak the TruncatedSVD dimensionality reduction by changing the n_components from 100 to 50. After adjusting your code, run it and compare the shapes of the features matrices before and after applying TruncatedSVD.\n",
    "\n",
    "```python\n",
    "# Import necessary Libraries\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load IMDB Movie Reviews Dataset\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Working with first 100 reviews\n",
    "first_100_reviewids = movie_reviews.fileids()[:100]\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in first_100_reviewids]\n",
    "\n",
    "# Transform raw data into TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(reviews)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {tfidf_matrix.shape}\\n\")\n",
    "\n",
    "# Apply TruncatedSVD for Dimensionality Reduction\n",
    "svd = TruncatedSVD(n_components=100)  \n",
    "features = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")\n",
    "\n",
    "```\n",
    "\n",
    "To address your task, we'll adjust the `n_components` parameter in the `TruncatedSVD` instantiation from 100 to 50, as requested. This change will show how the shape of the features matrix is affected when reducing the dimensions further. Here's the updated code snippet with the necessary adjustment:\n",
    "\n",
    "```python\n",
    "# Import necessary Libraries\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load IMDB Movie Reviews Dataset\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Working with the first 100 reviews\n",
    "first_100_reviewids = movie_reviews.fileids()[:100]\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in first_100_reviewids]\n",
    "\n",
    "# Transform raw data into TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(reviews)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {tfidf_matrix.shape}\\n\")\n",
    "\n",
    "# Apply TruncatedSVD for Dimensionality Reduction\n",
    "# Adjusting n_components from 100 to 50\n",
    "svd = TruncatedSVD(n_components=50)  \n",
    "features = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")\n",
    "```\n",
    "\n",
    "### Explanation of Changes and Expected Output:\n",
    "- **Change in `n_components`:** The `n_components` parameter of the `TruncatedSVD` object is set to 50, down from 100. This parameter determines the number of dimensions to which the data will be reduced.\n",
    "- **Expected Output:** The output will show the shape of the TF-IDF matrix before and after the application of `TruncatedSVD`. Initially, the shape reflects the number of documents (100 reviews) and the number of features (words/terms) extracted by the TF-IDF vectorizer. After applying `TruncatedSVD`, the number of features will be reduced to 50.\n",
    "\n",
    "When you run this script, you should see an output similar to this:\n",
    "\n",
    "```\n",
    "Shape of the features matrix before dimensionality reduction: (100, 8865)\n",
    "\n",
    "Shape of the features matrix after dimensionality reduction: (100, 50)\n",
    "```\n",
    "\n",
    "This output demonstrates how the dimensionality of the feature space has been significantly reduced from potentially thousands of dimensions (terms in the TF-IDF matrix) to just 50, simplifying the dataset while attempting to preserve the most informative aspects of the data for further analysis or machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18dc27",
   "metadata": {},
   "source": [
    "## Analyzing Spam and Ham Distribution in Train-Test Split\n",
    "\n",
    "Great work, Stellar Navigator! Let's expand our knowledge further. Fill in the blanks (____) in the code to practice importing and applying the TruncatedSVD dimensionality reduction technique. Remember to set the n_components to 2, reducing our feature space to two dimensions. Run your code to observe the changes in the shape of the feature matrix and examine how the reduced matrix appears!\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import ____\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"The dog barked at the mail carrier\", \n",
    "             \"The mail carrier was afraid of the small dog\", \n",
    "             \"The small dog was not ferocious but it did like to bark\"]\n",
    "\n",
    "# Transform sentences into TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {tfidf_matrix.shape}\\n\")\n",
    "\n",
    "# Initialize TruncatedSVD with desired number of components\n",
    "svd = __(n_components=2)\n",
    "\n",
    "# Apply TruncatedSVD on the TF-IDF matrix\n",
    "features = svd.____(tfidf_matrix)\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")\n",
    "print(f\"Reduced features matrix: \\n{features}\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "To complete your Python script and successfully apply the TruncatedSVD dimensionality reduction technique, you'll need to fill in the blanks with the appropriate code. Here's the corrected and complete version of the script:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"The dog barked at the mail carrier\", \n",
    "             \"The mail carrier was afraid of the small dog\", \n",
    "             \"The small dog was not ferocious but it did like to bark\"]\n",
    "\n",
    "# Transform sentences into TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {tfidf_matrix.shape}\\n\")\n",
    "\n",
    "# Initialize TruncatedSVD with desired number of components\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "\n",
    "# Apply TruncatedSVD on the TF-IDF matrix\n",
    "features = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")\n",
    "print(f\"Reduced features matrix: \\n{features}\")\n",
    "```\n",
    "\n",
    "### Explanation of Changes:\n",
    "\n",
    "1. **Import TruncatedSVD**: The import statement for `TruncatedSVD` is completed by specifying `from sklearn.decomposition import TruncatedSVD`. This imports the `TruncatedSVD` class from the `sklearn.decomposition` module.\n",
    "\n",
    "2. **Initialize TruncatedSVD**: The `TruncatedSVD` object is initialized with `n_components=2`, which sets up the dimensionality reduction to reduce the feature space to two dimensions.\n",
    "\n",
    "3. **Apply TruncatedSVD**: The `fit_transform` method of the `svd` object is used to apply the TruncatedSVD transformation to the `tfidf_matrix`. This method both fits the model to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c79d9",
   "metadata": {},
   "source": [
    "## Exploring the Spam Dataset\n",
    "\n",
    "Good progress, Space Voyager! Now, let's apply TruncatedSVD to a Bag-of-Words matrix instead of a TF-IDF. Fill the blanks (____) to transform the sentences into a Bag-of-Words matrix, initialize TruncatedSVD with 2 as the number of desired components, apply the TruncatedSVD, and print all information as indicated.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.____ import ____\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"The cat meowed at the room window\", \n",
    "             \"The small kitten looked at the window and meowed\", \n",
    "             \"The ginger kitten didn't meow but it looked at the window\"]\n",
    "\n",
    "# Transform sentences into Bag-of-Words matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {bow_matrix.shape}\\n\")\n",
    "\n",
    "# Initialize TruncatedSVD with desired number of components\n",
    "svd = ____(___=2)\n",
    "\n",
    "# Apply TruncatedSVD on the Bag-of-Words matrix\n",
    "features = svd.fit_transform(bow_matrix)\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")\n",
    "print(f\"Reduced features matrix: \\n{features}\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "To complete your Python script for applying TruncatedSVD to a Bag-of-Words (BoW) matrix, you need to fill in the missing parts correctly. Here’s how you can do it:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"The cat meowed at the room window\", \n",
    "             \"The small kitten looked at the window and meowed\", \n",
    "             \"The ginger kitten didn't meow but it looked at the window\"]\n",
    "\n",
    "# Transform sentences into Bag-of-Words matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {bow_matrix.shape}\\n\")\n",
    "\n",
    "# Initialize TruncatedSVD with desired number of components\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "\n",
    "# Apply TruncatedSVD on the Bag-of-Words matrix\n",
    "features = svd.fit_transform(bow_matrix)\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")\n",
    "print(f\"Reduced features matrix: \\n{features}\")\n",
    "```\n",
    "\n",
    "### Explanation of Changes:\n",
    "\n",
    "1. **Import TruncatedSVD**: The import statement is completed with `from sklearn.decomposition import TruncatedSVD`, which correctly imports the `TruncatedSVD` class from the `sklearn.decomposition` module.\n",
    "\n",
    "2. **Initialize TruncatedSVD**: The `TruncatedSVD` object is correctly initialized with `n_components=2`, setting up the dimensionality reduction to reduce the feature space to two dimensions.\n",
    "\n",
    "3. **Apply TruncatedSVD**: The `fit_transform` method is used to apply the TruncatedSVD to the BoW matrix (`bow_matrix`). This method both fits the model to the data and transforms the data into a lower-dimensional space.\n",
    "\n",
    "By running this script, you will see the effects of applying TruncatedSVD to a BoW matrix, which reduces the high-dimensional sparse matrix into a 2-dimensional representation. This is useful for visualization, clustering, or as a preprocessing step for more complex machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d25fa",
   "metadata": {},
   "source": [
    "## Stratified Train-Test Split for Text Data\n",
    "\n",
    "Stellar Navigator, it's time to apply TruncatedSVD to our Bag-of-Words matrix. To do this, initialize TruncatedSVD with 2 components, and then fit and transform our matrix X. Don't forget to print both the shapes and the reduced matrix.\n",
    "\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "original_sentences = ['It is a lovely day, isn\\'t it?', \n",
    "                      'The sun is shining brightly!', \n",
    "                      'I love the taste of lemonade on a sunny day.']\n",
    "# Preprocess the sentences\n",
    "preprocessed_sentences = [clean_text(sentence) for sentence in original_sentences]\n",
    "\n",
    "# Initialize a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit transform the preprocessed sentences\n",
    "X = vectorizer.fit_transform(preprocessed_sentences)\n",
    "\n",
    "# TODO: Initialize TruncatedSVD with 2 components\n",
    "\n",
    "# TODO: Fit and transform X with TruncatedSVD\n",
    "\n",
    "print(f\"Original BoW shape: {X.shape}\")\n",
    "print(f\"Shape after TruncatedSVD: {X_reduced.shape}\")\n",
    "print(f\"Reduced features matrix: \\n{X_reduced}\")\n",
    "\n",
    "```\n",
    "\n",
    "To complete your task of applying TruncatedSVD to the Bag-of-Words (BoW) matrix, you need to initialize TruncatedSVD with 2 components, fit and transform the matrix \\( X \\), and then print the shapes and the reduced matrix. Here's the completed script with the necessary additions:\n",
    "\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "original_sentences = ['It is a lovely day, isn\\'t it?', \n",
    "                      'The sun is shining brightly!', \n",
    "                      'I love the taste of lemonade on a sunny day.']\n",
    "# Preprocess the sentences\n",
    "preprocessed_sentences = [clean_text(sentence) for sentence in original_sentences]\n",
    "\n",
    "# Initialize a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit transform the preprocessed sentences\n",
    "X = vectorizer.fit_transform(preprocessed_sentences)\n",
    "\n",
    "# Initialize TruncatedSVD with 2 components\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "\n",
    "# Fit and transform X with TruncatedSVD\n",
    "X_reduced = svd.fit_transform(X)\n",
    "\n",
    "print(f\"Original BoW shape: {X.shape}\")\n",
    "print(f\"Shape after TruncatedSVD: {X_reduced.shape}\")\n",
    "print(f\"Reduced features matrix: \\n{X_reduced}\")\n",
    "```\n",
    "\n",
    "### Explanation of Code Additions:\n",
    "1. **Initialize TruncatedSVD**: A `TruncatedSVD` object is created with `n_components=2`. This sets up the dimensionality reduction to reduce the feature space to two dimensions, which is useful for visualization or further analysis.\n",
    "\n",
    "2. **Fit and Transform with TruncatedSVD**: The `fit_transform` method is used on the BoW matrix \\( X \\). This step fits the TruncatedSVD model to the data and then transforms the data into a lower-dimensional space.\n",
    "\n",
    "3. **Print Outputs**: The script prints the shape of the original BoW matrix and the shape of the matrix after applying TruncatedSVD. It also prints the reduced features matrix, which shows the new two-dimensional representation of the original text data.\n",
    "\n",
    "This approach demonstrates how to effectively reduce the dimensionality of textual data, making it more manageable for various machine learning tasks while attempting to preserve the most informative aspects of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1fdc5f",
   "metadata": {},
   "source": [
    "## Stratified Train-Test Split and Class Distribution Analysis\n",
    "\n",
    "Time to put your learning to the test, Space Voyager! Implement from scratch the TruncatedSVD on the first 150 reviews of the IMDB Movie Reviews dataset. Remember to create the TF-IDF matrix, initialize and apply TruncatedSVD(n_components=100), and then print the shapes of the resulting feature matrices.\n",
    "\n",
    "```python\n",
    "# TODO: Import the necessary libraries\n",
    "\n",
    "# TODO: Load the IMDB Movie Reviews dataset\n",
    "\n",
    "# TODO: Load the first 150 reviews from the dataset\n",
    "\n",
    "# TODO: Transform the movie reviews into a TF-IDF matrix\n",
    "\n",
    "# TODO: Initialize and apply TruncatedSVD to the TF-IDF matrix\n",
    "\n",
    "# TODO: Print the shape of the features matrix before and after applying TruncatedSVD\n",
    "\n",
    "```\n",
    "\n",
    "To complete your task, you'll need to import the necessary libraries, load the IMDB movie reviews dataset, preprocess the data to create a TF-IDF matrix, apply TruncatedSVD, and finally print the shapes of the matrices before and after dimensionality reduction. Here's how you can do it step-by-step:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Download and load the IMDB Movie Reviews dataset\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Load the first 150 reviews from the dataset\n",
    "first_150_reviewids = movie_reviews.fileids()[:150]\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in first_150_reviewids]\n",
    "\n",
    "# Transform the movie reviews into a TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(reviews)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Initialize and apply TruncatedSVD to the TF-IDF matrix\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "features_reduced = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Print the shape of the features matrix before and after applying TruncatedSVD\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features_reduced.shape}\")\n",
    "```\n",
    "\n",
    "### Explanation of the Code:\n",
    "\n",
    "1. **Import Libraries**: This code imports `nltk` for accessing the movie reviews dataset, `TfidfVectorizer` for creating a TF-IDF matrix from text, and `TruncatedSVD` for dimensionality reduction.\n",
    "\n",
    "2. **Load Dataset**: The `nltk` library's `movie_reviews` dataset is used, which is a standard dataset for text analysis in natural language processing.\n",
    "\n",
    "3. **Preprocess Data**: The first 150 movie reviews are loaded and transformed into a TF-IDF matrix. The TF-IDF vectorizer is configured to ignore English stop words.\n",
    "\n",
    "4. **Dimensionality Reduction**: `TruncatedSVD` is initialized with 100 components, meaning the high-dimensional TF-IDF matrix is reduced to 100 dimensions. This helps in capturing the most significant features while reducing the size of the data.\n",
    "\n",
    "5. **Print Shapes**: The shapes of the original and reduced feature matrices are printed to show how the dimensionality has been decreased.\n",
    "\n",
    "When you run this script, you will see the shape of the TF-IDF matrix before dimensionality reduction and the new shape after applying TruncatedSVD, demonstrating the effectiveness of this technique in reducing the complexity of high-dimensional text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706bd0d-3f0a-41d2-a032-1d439ae826fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
