{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Chunking and Storing Text for Efficient LLM Processing\n",
                "\n",
                "# Welcome to the first lesson of our course on Chunking and Storing Text for Efficient LLM Processing\n",
                "\n",
                "In this lesson, we will explore the concept of **chunking**, which is crucial for efficient LLM processing. By the end of this lesson, you will understand how to break down large texts into manageable pieces for processing. This foundational skill will be essential as you progress through the course and tackle more complex data processing tasks.\n",
                "\n",
                "## Understanding Text Chunking\n",
                "\n",
                "**Text chunking** is the process of dividing a large text into smaller, more manageable pieces, or \"chunks.\" This is particularly important for LLMs, which have limitations on the amount of text they can process at once. By chunking text, we ensure that each piece is small enough to be processed efficiently by the model while retaining coherence and meaning.\n",
                "\n",
                "## Why Chunking is Essential for LLMs\n",
                "\n",
                "LLMs have token limitations that dictate how much text they can process at once. If we exceed these limits, the model may truncate the text, leading to loss of important information. Chunking helps avoid this issue by breaking text into meaningful sections that can be processed independently or recombined when necessary.\n",
                "\n",
                "  - **GPT-4:** Can process up to **8,192 tokens** in standard versions, with some variations going up to **32,000 tokens**. Text must be split into chunks that fit within these limits.\n",
                "  - **BERT:** Has a strict **512-token limit**, making chunking necessary when processing longer documents.\n",
                "  - **T5:** Supports different token limits depending on the version (e.g., **512 tokens for T5-Base**). Chunking ensures input remains within this limit.\n",
                "  - **Claude:** Depending on the version, it can process anywhere from **100,000 to 1,000,000 tokens**, allowing for much larger text inputs but still benefiting from structured chunking.\n",
                "\n",
                "By understanding these limits, we can implement chunking strategies that align with the capabilities of different models.\n",
                "\n",
                "## Tokenization and Chunking\n",
                "\n",
                "Tokenization is the process of converting text into tokens, which are the smallest units of meaning that a model can process. Tokenization and chunking work together to ensure that text is divided into manageable pieces that respect the model's token limits. When chunking text, it's important to consider how the text will be tokenized, as this affects the number of tokens in each chunk. By aligning chunking strategies with tokenization, we can optimize the text for efficient processing by LLMs.\n",
                "\n",
                "## Common Chunking Strategies\n",
                "\n",
                "Different strategies can be used to split text into chunks, depending on the use case:\n",
                "\n",
                "  - **Fixed-Length Chunking:** Dividing text into equal-sized segments based on character count or token count.\n",
                "  - **Sentence-Based Chunking:** Splitting text at sentence boundaries to maintain readability.\n",
                "  - **Paragraph-Based Chunking:** Keeping paragraphs intact while breaking long texts into smaller sections.\n",
                "\n",
                "Let's implement these strategies in Python.\n",
                "\n",
                "## Implementing Text Chunking in Python\n",
                "\n",
                "We will delve into practical implementations of various text chunking strategies using Python. By applying these methods, you will gain hands-on experience in breaking down large texts into manageable chunks suitable for LLM processing. In this lesson, we will implement several methods, including Fixed-Length Chunking, Sentence-Based Chunking, and Paragraph-Based Chunking.\n",
                "\n",
                "### Fixed-Length Chunking\n",
                "\n",
                "Fixed-length chunking divides text into equally sized chunks based on character count or token count. This method is simple and effective for processing large amounts of text efficiently. However, it does not consider the meaning of sentences or paragraphs, which may result in chunks being cut off at arbitrary points, potentially disrupting the context.\n",
                "\n",
                "```python\n",
                "import textwrap\n",
                "\n",
                "def fixed_length_chunking(text, chunk_size=500):\n",
                "    return textwrap.wrap(text, width=chunk_size)\n",
                "```\n",
                "\n",
                "This method is useful when working with models that require a strict limit on input size but does not prioritize preserving sentence structure.\n",
                "\n",
                "**Pros:** Simple and fast.\n",
                "**Cons:** May break words or sentences mid-way, losing coherence.\n",
                "\n",
                "### Sentence-Based Chunking\n",
                "\n",
                "Sentence-based chunking ensures that each chunk consists of whole sentences. This method is particularly useful for models that require better contextual integrity. Instead of splitting text based on character count alone, it groups complete sentences together until the chunk reaches the predefined limit.\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "nltk.download('punkt')\n",
                "\n",
                "def sentence_chunking(text, chunk_size=100):\n",
                "    sentences = sent_tokenize(text)\n",
                "    chunks = []\n",
                "    current_chunk = \"\"\n",
                "    for sentence in sentences:\n",
                "        if len(current_chunk) + len(sentence) < chunk_size:\n",
                "            current_chunk += \" \" + sentence\n",
                "        else:\n",
                "            chunks.append(current_chunk.strip())\n",
                "            current_chunk = sentence\n",
                "    if current_chunk:\n",
                "        chunks.append(current_chunk.strip())\n",
                "    return chunks\n",
                "```\n",
                "\n",
                "This method ensures that sentences remain intact within each chunk, making it better suited for tasks that require natural language processing with full context.\n",
                "\n",
                "**Pros:** Maintains sentence structure, preserving context.\n",
                "**Cons:** Chunks may vary in size, leading to uneven distribution.\n",
                "\n",
                "### Paragraph-Based Chunking\n",
                "\n",
                "Paragraph-based chunking keeps entire paragraphs intact, making it ideal for maintaining the original document's formatting and logical flow. This approach is beneficial when working with structured texts such as articles, reports, or books.\n",
                "\n",
                "```python\n",
                "def paragraph_chunking(text):\n",
                "    return text.split(\"\\n\\n\")\n",
                "```\n",
                "\n",
                "Unlike fixed-length chunking, this method avoids breaking paragraphs, ensuring that information stays grouped together in meaningful sections.\n",
                "\n",
                "**Pros:** Preserves paragraph integrity, making it easier for the model to understand the text.\n",
                "**Cons:** Some paragraphs may still be too long for LLMs with strict token limits.\n",
                "\n",
                "## Summary\n",
                "\n",
                "This lesson introduces the concept of text chunking, which is essential for efficient processing by large language models (LLMs) due to their token limitations. It explains the importance of chunking to prevent information loss and outlines the token limits of various models like GPT-4, BERT, T5, and Claude. The lesson covers common chunking strategies, including fixed-length, sentence-based, and paragraph-based chunking, and provides Python implementations for each method. Fixed-length chunking is simple but may disrupt context, sentence-based chunking maintains sentence integrity, and paragraph-based chunking preserves paragraph structure. Each method has its pros and cons, depending on the use case and model requirements. Additionally, the lesson highlights how tokenization and chunking work together to optimize text for LLM processing."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementing Fixed Length Text Chunking\n",
                "\n",
                "Now that you understand the concept of text chunking, let's put theory into practice! In this exercise, you'll implement your first chunking strategy: fixed-length chunking using Python's textwrap module.\n",
                "\n",
                "After learning about the different chunking methods, it's time to see how fixed-length chunking actually works with real text. You'll complete a function that breaks text into chunks of specified character lengths and observe how different chunk sizes affect the results.\n",
                "\n",
                "Your tasks are as follows:\n",
                "\n",
                "Use the sample text provided in the text.txt file.\n",
                "Complete the fixed_length_chunking function using textwrap.wrap().\n",
                "Run the function with three different chunk sizes (30, 50, and 100 characters).\n",
                "Print the first three chunks for each chunk size.\n",
                "This hands-on experience will help you understand when fixed-length chunking is appropriate and what trade-offs you make when choosing this method over sentence- or paragraph-based approaches.\n",
                "\n",
                "```python\n",
                "import textwrap\n",
                "\n",
                "# Load sample text from a file\n",
                "with open('text.txt', 'r') as file:\n",
                "    sample_text = file.read()\n",
                "\n",
                "\n",
                "# Fixed-Length Chunking function\n",
                "def fixed_length_chunking(text, chunk_size=50):\n",
                "    # TODO: Use textwrap.wrap() to split the text into chunks of the specified size\n",
                "    pass\n",
                "\n",
                "\n",
                "# Test with different chunk sizes (30, 50, 100)\n",
                "# TODO: Test with multiple chunk sizes to observe how chunk size affects the output\n",
                "# TODO: Print the first three chunks for each size\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "import textwrap\n",
                "\n",
                "# Load sample text from a file\n",
                "with open('text.txt', 'r') as file:\n",
                "    sample_text = file.read()\n",
                "\n",
                "\n",
                "# Fixed-Length Chunking function\n",
                "def fixed_length_chunking(text, chunk_size=50):\n",
                "    # TODO: Use textwrap.wrap() to split the text into chunks of the specified size\n",
                "    return textwrap.wrap(text, width=chunk_size)\n",
                "\n",
                "\n",
                "# Test with different chunk sizes (30, 50, 100)\n",
                "# TODO: Test with multiple chunk sizes to observe how chunk size affects the output\n",
                "chunk_sizes = [30, 50, 100]\n",
                "\n",
                "for size in chunk_sizes:\n",
                "    print(f\"--- Chunk Size: {size} ---\")\n",
                "    chunks = fixed_length_chunking(sample_text, chunk_size=size)\n",
                "    # TODO: Print the first three chunks for each size\n",
                "    for i, chunk in enumerate(chunks[:3]):\n",
                "        print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
                "\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sentence Boundaries for Smarter Chunking\n",
                "\n",
                "Excellent work with fixed-length chunking! Now let's move on to a more sophisticated approach: sentence-based chunking. While fixed-length chunking is simple, it often cuts sentences in awkward places, which can confuse language models.\n",
                "\n",
                "In this exercise, you'll implement a sentence-based chunking function that respects natural language boundaries while still controlling chunk size. This approach helps maintain the meaning and context of your text.\n",
                "\n",
                "Your tasks:\n",
                "\n",
                "Complete the sentence_chunking function using NLTK's sent_tokenize to split text into complete sentences.\n",
                "Group these sentences into chunks that stay under specified maximum sizes.\n",
                "Test your function with two different maximum chunk sizes (50 and 400 characters).\n",
                "Print the first three chunks for each size setting.\n",
                "By comparing this method with the fixed-length chunking you implemented earlier, you'll gain insight into which approach works best for different scenarios. This skill will be invaluable when preparing text for more complex LLM applications.\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "# Download the punkt tokenizer if not already downloaded\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "# Load sample text from a file\n",
                "with open('text.txt', 'r') as file:\n",
                "    sample_text = file.read()\n",
                "\n",
                "# Sentence-Based Chunking function\n",
                "def sentence_chunking(text, max_chunk_size=100):\n",
                "    # TODO: Split the text into sentences using sent_tokenize\n",
                "    \n",
                "    chunks = []\n",
                "    current_chunk = \"\"\n",
                "    \n",
                "    # TODO: Loop through each sentence and add it to the current chunk\n",
                "    # TODO: If adding the sentence would exceed max_chunk_size, start a new chunk\n",
                "    # TODO: Make sure to add the last chunk if it's not empty\n",
                "    \n",
                "    return chunks\n",
                "\n",
                "# Test with different chunk sizes\n",
                "# TODO: Test the function with max_chunk_size of 50 and 400\n",
                "# TODO: Print the first three chunks for each size setting\n",
                "```\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "# Download the punkt tokenizer if not already downloaded.\n",
                "# The `punkt_tab` resource does not exist; the correct name is 'punkt'.\n",
                "# We can simplify the download logic to avoid the `LookupError` and `AttributeError`.\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "# Load sample text from a file\n",
                "with open('text.txt', 'r') as file:\n",
                "    sample_text = file.read()\n",
                "\n",
                "# Sentence-Based Chunking function\n",
                "def sentence_chunking(text, max_chunk_size=100):\n",
                "    # Split the text into sentences using sent_tokenize\n",
                "    sentences = sent_tokenize(text)\n",
                "    \n",
                "    chunks = []\n",
                "    current_chunk = \"\"\n",
                "    \n",
                "    # Loop through each sentence and add it to the current chunk\n",
                "    # If adding the sentence would exceed max_chunk_size, start a new chunk\n",
                "    for sentence in sentences:\n",
                "        # Check if adding the next sentence would exceed the max size\n",
                "        # Add a space for separation, if it's not the first sentence in the chunk.\n",
                "        if len(current_chunk) + len(sentence) + (1 if current_chunk else 0) > max_chunk_size:\n",
                "            if current_chunk: # Only append if the chunk is not empty\n",
                "                chunks.append(current_chunk.strip())\n",
                "            current_chunk = sentence\n",
                "        else:\n",
                "            if current_chunk:\n",
                "                current_chunk += \" \" + sentence\n",
                "            else:\n",
                "                current_chunk = sentence\n",
                "    \n",
                "    # Make sure to add the last chunk if it's not empty\n",
                "    if current_chunk:\n",
                "        chunks.append(current_chunk.strip())\n",
                "    \n",
                "    return chunks\n",
                "\n",
                "# Test with different chunk sizes\n",
                "chunk_sizes = [50, 400]\n",
                "\n",
                "for size in chunk_sizes:\n",
                "    print(f\"--- Chunking with max_chunk_size: {size} ---\")\n",
                "    chunks = sentence_chunking(sample_text, max_chunk_size=size)\n",
                "    # Print the first three chunks for each size setting\n",
                "    for i, chunk in enumerate(chunks[:3]):\n",
                "        print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Chunking Methods Head to Head\n",
                "\n",
                "Now that you've implemented both fixed-length and sentence-based chunking methods, let's see how they compare in action! This exercise will help you visualize the real differences between these two approaches when processing the same text.\n",
                "\n",
                "You'll use the chunking functions you've already built to create a side-by-side comparison that clearly shows why sentence boundaries matter for LLM processing.\n",
                "\n",
                "Your tasks:\n",
                "\n",
                "Use both chunking methods on the sample text with a chunk size of 50 characters.\n",
                "For each method, print out the first 3 chunks side by side.\n",
                "When comparing the chunks, look for differences in how the text is split: specifically, check if the fixed-length chunks break sentences in the middle, while the sentence-based chunks keep sentences whole.\n",
                "Analyze the results and identify exactly where fixed-length chunking cuts sentences awkwardly, and how sentence-based chunking preserves sentence boundaries.\n",
                "This visual comparison will provide you with concrete evidence of when to choose each chunking strategy. Understanding these differences is crucial when designing systems that need to process text efficiently while preserving meaning.\n",
                "\n",
                "```python\n",
                "import textwrap\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "# Download the punkt tokenizer if not already downloaded\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "# Load sample text from a file\n",
                "with open('text.txt', 'r') as file:\n",
                "    sample_text = file.read()\n",
                "\n",
                "# Fixed-Length Chunking function\n",
                "def fixed_length_chunking(text, chunk_size=50):\n",
                "    return textwrap.wrap(text, width=chunk_size)\n",
                "\n",
                "# Sentence-Based Chunking function\n",
                "def sentence_chunking(text, max_chunk_size=50):\n",
                "    # Split the text into sentences\n",
                "    sentences = sent_tokenize(text)\n",
                "    chunks = []\n",
                "    current_chunk = \"\"\n",
                "    \n",
                "    # Group sentences into chunks without exceeding max_chunk_size\n",
                "    for sentence in sentences:\n",
                "        # If adding this sentence doesn't exceed the limit, add it to the current chunk\n",
                "        if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
                "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
                "        # Otherwise, save the current chunk and start a new one with this sentence\n",
                "        else:\n",
                "            if current_chunk:  # Only append non-empty chunks\n",
                "                chunks.append(current_chunk.strip())\n",
                "            current_chunk = sentence  # Start a new chunk\n",
                "    \n",
                "    # Add the last chunk if it's not empty\n",
                "    if current_chunk:\n",
                "        chunks.append(current_chunk.strip())\n",
                "    \n",
                "    return chunks\n",
                "\n",
                "# TODO: Get chunks using both fixed_length_chunking and sentence_chunking functions\n",
                "\n",
                "# TODO: Compare the first 3 chunks from each method\n",
                "\n",
                "```\n",
                "\n",
                "import textwrap\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "# Download the punkt tokenizer if not already downloaded\n",
                "# Note: 'punkt_tab' is a common typo. The correct resource name is 'punkt'.\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "# Load sample text from a file\n",
                "with open('text.txt', 'r') as file:\n",
                "    sample_text = file.read()\n",
                "\n",
                "# Fixed-Length Chunking function\n",
                "def fixed_length_chunking(text, chunk_size=50):\n",
                "    return textwrap.wrap(text, width=chunk_size)\n",
                "\n",
                "# Sentence-Based Chunking function\n",
                "def sentence_chunking(text, max_chunk_size=50):\n",
                "    # Split the text into sentences\n",
                "    sentences = sent_tokenize(text)\n",
                "    chunks = []\n",
                "    current_chunk = \"\"\n",
                "    \n",
                "    # Group sentences into chunks without exceeding max_chunk_size\n",
                "    for sentence in sentences:\n",
                "        # If adding this sentence plus a space fits, add it\n",
                "        if len(current_chunk) + len(sentence) + (1 if current_chunk else 0) <= max_chunk_size:\n",
                "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
                "        # Otherwise, save the current chunk and start a new one with this sentence\n",
                "        else:\n",
                "            if current_chunk:\n",
                "                chunks.append(current_chunk.strip())\n",
                "            current_chunk = sentence\n",
                "    \n",
                "    # Add the last chunk if it's not empty\n",
                "    if current_chunk:\n",
                "        chunks.append(current_chunk.strip())\n",
                "    \n",
                "    return chunks\n",
                "\n",
                "# Get chunks using both fixed_length_chunking and sentence_chunking functions\n",
                "chunk_size = 50\n",
                "fixed_chunks = fixed_length_chunking(sample_text, chunk_size)\n",
                "sentence_chunks = sentence_chunking(sample_text, max_chunk_size=chunk_size)\n",
                "\n",
                "# Compare the first 3 chunks from each method\n",
                "print(f\"--- Comparison of Chunking Methods (Chunk Size: {chunk_size}) ---\\n\")\n",
                "\n",
                "print(\"Fixed-Length Chunking Chunks:\")\n",
                "for i, chunk in enumerate(fixed_chunks[:3]):\n",
                "    print(f\"Chunk {i+1}: '{chunk}'\")\n",
                "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
                "\n",
                "print(\"Sentence-Based Chunking Chunks:\")\n",
                "for i, chunk in enumerate(sentence_chunks[:3]):\n",
                "    print(f\"Chunk {i+1}: '{chunk}'\")\n",
                "\n",
                "# Analyze the results\n",
                "print(\"\\n--- Analysis ---\")\n",
                "print(\"As you can see, the fixed-length chunking method breaks the text at exactly 50 characters, often cutting off words and sentences mid-way.\")\n",
                "print(\"For example, the first chunk ends with 'to text' but the sentence clearly continues.\")\n",
                "print(\"\\nIn contrast, the sentence-based chunking method preserves complete sentences, even if it means the chunk size is slightly smaller than the maximum.\")\n",
                "print(\"This maintains the natural flow and meaning of the text, which is crucial for LLMs that rely on context.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preserving Document Structure with Paragraph Chunking\n",
                "\n",
                "After mastering sentence-based chunking, let's complete our chunking toolkit with the third key strategy: paragraph-based chunking. This method is perfect for preserving the logical structure of documents where paragraph breaks represent meaningful divisions.\n",
                "\n",
                "In this exercise, you'll implement a paragraph_chunking function that respects the natural organization of text while ensuring chunks remain manageable for LLM processing.\n",
                "\n",
                "Your tasks:\n",
                "\n",
                "Complete the paragraph_chunking function to split text at paragraph boundaries (double newlines).\n",
                "Test your function with the sample text by comparing the first three chunks (paragraphs) it produces: for each, print the paragraph itself and its character length. This will help you verify that your function is correctly splitting the text at paragraph boundaries and preserving the structure of each chunk.\n",
                "By adding this technique to your toolkit, you'll have a complete set of chunking strategies to handle any text processing scenario. You'll be able to choose the right approach based on your specific needs â€” whether prioritizing consistent chunk sizes, sentence integrity, or document structure.\n",
                "\n",
                "```python\n",
                "# Load sample text from a file\n",
                "with open('text.txt', 'r') as file:\n",
                "    sample_text = file.read()\n",
                "\n",
                "# Paragraph-Based Chunking function\n",
                "def paragraph_chunking(text):\n",
                "    # TODO: Split text at paragraph boundaries (double newlines)\n",
                "    return []  # Replace this with your implementation\n",
                "\n",
                "# TODO: Test the function with the sample text and print the first three paragraphs with their lengths\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "# Load sample text from a file\n",
                "with open('text.txt', 'r') as file:\n",
                "    sample_text = file.read()\n",
                "\n",
                "# Paragraph-Based Chunking function\n",
                "def paragraph_chunking(text):\n",
                "    # TODO: Split text at paragraph boundaries (double newlines)\n",
                "    return text.split('\\n\\n')\n",
                "\n",
                "# TODO: Test the function with the sample text and print the first three paragraphs with their lengths\n",
                "paragraphs = paragraph_chunking(sample_text)\n",
                "\n",
                "# Print the first three paragraphs and their character lengths\n",
                "for i, paragraph in enumerate(paragraphs[:3]):\n",
                "    print(f\"Paragraph {i+1}:\\n{paragraph}\")\n",
                "    print(f\"Character Length: {len(paragraph)}\\n{'-'*20}\\n\")\n",
                "```\n",
                "\n",
                "### Explanation of the `paragraph_chunking` Function\n",
                "\n",
                "The `paragraph_chunking` function is designed to split a document into logical, paragraph-sized chunks. The key to this is recognizing that most digital text formats, including plain text files, use a double newline character (`\\n\\n`) to denote a new paragraph.\n",
                "\n",
                "My implementation uses Python's built-in `str.split('\\n\\n')` method. This method efficiently breaks the input text string into a list of smaller strings, with each element in the list representing a single paragraph. This approach respects the natural divisions in the original document, ensuring that each chunk retains its contextual integrity. The resulting chunks are then easy to process individually, which is a common requirement in natural language processing tasks."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
