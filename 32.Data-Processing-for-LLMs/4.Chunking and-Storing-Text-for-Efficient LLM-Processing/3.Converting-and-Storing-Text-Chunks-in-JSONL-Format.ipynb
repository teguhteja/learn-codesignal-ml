{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Converting and Storing Text Chunks in JSONL Format\n",
                "\n",
                "Of course. Here is the text converted to English and formatted in Markdown.\n",
                "\n",
                "-----\n",
                "\n",
                "# Welcome to the Final Lesson\n",
                "\n",
                "Welcome to the final lesson of our course on data processing for **Large Language Models (LLMs)**. In this lesson, we will focus on converting text chunks into **JSONL** format and storing them for efficient retrieval and processing.\n",
                "\n",
                "This skill is crucial for managing text data in LLM applications, allowing for streamlined data handling and processing. By the end of this lesson, you will be able to convert text chunks into JSONL format and store them for later use.\n",
                "\n",
                "-----\n",
                "\n",
                "### **Recall: Text Chunking Basics**\n",
                "\n",
                "Before we dive into JSONL, let's briefly recall the concept of **text chunking**. In previous lessons, we discussed how breaking down large text into smaller, manageable chunks is essential for efficient processing in LLMs. This process helps maintain context and ensures that the model can handle the data effectively. Remember, chunking can be done by sentences, characters, or tokens, depending on the specific requirements of your task.\n",
                "\n",
                "-----\n",
                "\n",
                "### **Understanding JSONL Format**\n",
                "\n",
                "**JSONL**, or **JSON Lines**, is a format that stores JSON objects in a line-by-line manner. Each line in a JSONL file is a valid JSON object, making it easy to process large datasets one line at a time. This format is particularly useful for streaming data and handling large files efficiently.\n",
                "\n",
                "-----\n",
                "\n",
                "### **Why JSONL?**\n",
                "\n",
                "  * **Efficiency**: JSONL allows for line-by-line processing, which is memory-efficient.\n",
                "  * **Simplicity**: Each line is a complete JSON object, making it easy to parse and manipulate.\n",
                "  * **Scalability**: Ideal for large datasets, as it supports incremental processing.\n",
                "\n",
                "-----\n",
                "\n",
                "### **Converting Text Chunks to JSONL**\n",
                "\n",
                "Let's start by converting text chunks into JSONL format using Python. We'll use the `json` module, which is part of Python's standard library, to handle JSON data.\n",
                "\n",
                "#### **Step 1: Chunk Your Text**\n",
                "\n",
                "Before converting text into JSONL format, we need to chunk the text. Let's assume we have a large text that we want to break into smaller chunks. We'll use sentence-based chunking for this example.\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "nltk.download('punkt')\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "large_text = \"This is the first sentence. Here is the second sentence. And this is the third sentence.\"\n",
                "chunks = sent_tokenize(large_text)\n",
                "```\n",
                "\n",
                "In this code, we use the `sent_tokenize` function from the `nltk` library to split the `large_text` into individual sentences, which will serve as our text chunks.\n",
                "\n",
                "#### **Step 2: Create JSON Objects**\n",
                "\n",
                "Next, we'll create a list of JSON objects, where each object contains an `id` and the corresponding `text` chunk.\n",
                "\n",
                "```python\n",
                "import json\n",
                "\n",
                "chunk_data = [{\"id\": i, \"text\": chunk} for i, chunk in enumerate(chunks)]\n",
                "```\n",
                "\n",
                "In this code, we use a list comprehension to create a list of dictionaries. Each dictionary has an `id` (the index of the chunk) and the `text` (the chunk itself).\n",
                "\n",
                "#### **Step 3: Convert to JSONL Format**\n",
                "\n",
                "Now, let's convert these JSON objects into JSONL format and write them to a file using the `jsonlines` library.\n",
                "\n",
                "Install the `jsonlines` library if you haven't already:\n",
                "\n",
                "```bash\n",
                "pip install jsonlines\n",
                "```\n",
                "\n",
                "Use the `jsonlines` library to write the JSON objects to a JSONL file:\n",
                "\n",
                "```python\n",
                "import jsonlines\n",
                "\n",
                "with jsonlines.open(\"chunked_data.jsonl\", mode='w') as writer:\n",
                "    writer.write_all(chunk_data)\n",
                "```\n",
                "\n",
                "In this code, `jsonlines.open()` is used to open the file in write mode, and `writer.write_all()` writes all the JSON objects from `chunk_data` to the file in JSONL format. This approach eliminates the need for a manual loop to write each line.\n",
                "\n",
                "-----\n",
                "\n",
                "### **Storing and Retrieving JSONL Data**\n",
                "\n",
                "Once we have stored our data in JSONL format, we need to know how to retrieve it for further processing.\n",
                "\n",
                "#### **Step 4: Read JSONL Data**\n",
                "\n",
                "To read the stored JSONL data, we open the file in read mode and load each line as a JSON object.\n",
                "\n",
                "```python\n",
                "with jsonlines.open(\"chunked_data.jsonl\", mode='r') as reader:\n",
                "    stored_chunks = [obj for obj in reader]\n",
                "```\n",
                "\n",
                "In this snippet, we use a list comprehension to read each line from the file and convert it back into a JSON object using the `jsonlines` library.\n",
                "\n",
                "#### **Step 5: Verify the Output**\n",
                "\n",
                "Finally, let's print the first two chunks to verify that our data has been correctly stored and retrieved.\n",
                "\n",
                "```python\n",
                "print(\"Stored Chunks:\", stored_chunks[:2])\n",
                "```\n",
                "\n",
                "The output will be:\n",
                "\n",
                "```\n",
                "Stored Chunks: [{'id': 0, 'text': 'This is the first sentence.'}, {'id': 1, 'text': 'Here is the second sentence.'}]\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### **Summary and Next Steps**\n",
                "\n",
                "In this lesson, you learned how to convert text chunks into JSONL format and store them for efficient retrieval. We covered the benefits of JSONL, how to use Python's `json` module and `jsonlines` library to handle JSON data, and how to store and read JSONL files.\n",
                "\n",
                "Congratulations on reaching the end of this course\\! You've gained valuable skills in text processing for LLMs, and I encourage you to apply these skills in the practice exercises that follow. Well done on your progress and dedication\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Convert Text Chunks to JSONL\n",
                "\n",
                "You've done well learning about the JSONL format! Now, let's put that knowledge into practice. Your task is to complete a function that takes a list of text chunks and converts them into JSON objects with an ID and text content.\n",
                "\n",
                "Use the jsonlines library to write these objects to a JSONL file.\n",
                "This exercise will help you solidify your understanding of converting text data to the JSONL format. Dive in and see how efficiently you can manage text data!\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "import jsonlines\n",
                "\n",
                "# Step 1: Chunk Your Text\n",
                "nltk.download('punkt_tab', quiet = True)\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "large_text = \"This is the first sentence. Here is the second sentence. And this is the third sentence.\"\n",
                "chunks = sent_tokenize(large_text)\n",
                "\n",
                "# TODO: Define the function convert_chunks_to_jsonl(chunks, filename)\n",
                "def convert_chunks_to_jsonl(chunks, filename):\n",
                "    # TODO: Create a list of dictionaries with 'id' and 'text' for each chunk\n",
                "    chunk_data = []\n",
                "    \n",
                "    with jsonlines.open(filename, mode='w') as writer:\n",
                "        # TODO: Use jsonlines to write chunk_data to a JSONL file\n",
                "\n",
                "# Use the function to write to a file\n",
                "convert_chunks_to_jsonl(chunks, \"chunked_data.jsonl\")\n",
                "\n",
                "# Step 4: Read JSONL Data\n",
                "with jsonlines.open(\"chunked_data.jsonl\", mode='r') as reader:\n",
                "    stored_chunks = [obj for obj in reader]\n",
                "\n",
                "# Step 5: Verify the Output\n",
                "print(\"Stored Chunks:\", stored_chunks)\n",
                "```\n",
                "\n",
                "I can definitely help with that. Here is the completed Python code that includes the function to convert text chunks into a JSONL file, just as you requested.\n",
                "\n",
                "The `convert_chunks_to_jsonl` function is now fully implemented. It first creates a list of dictionaries, where each dictionary contains an `\"id\"` (the index) and the `\"text\"` for each chunk. Then, it uses the `jsonlines` library to write this data to the specified file.\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "import jsonlines\n",
                "\n",
                "# Step 1: Chunk Your Text\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "large_text = \"This is the first sentence. Here is the second sentence. And this is the third sentence.\"\n",
                "chunks = sent_tokenize(large_text)\n",
                "\n",
                "# Define the function convert_chunks_to_jsonl(chunks, filename)\n",
                "def convert_chunks_to_jsonl(chunks, filename):\n",
                "    # Create a list of dictionaries with 'id' and 'text' for each chunk\n",
                "    chunk_data = [{\"id\": i, \"text\": chunk} for i, chunk in enumerate(chunks)]\n",
                "    \n",
                "    with jsonlines.open(filename, mode='w') as writer:\n",
                "        # Use jsonlines to write chunk_data to a JSONL file\n",
                "        writer.write_all(chunk_data)\n",
                "\n",
                "# Use the function to write to a file\n",
                "convert_chunks_to_jsonl(chunks, \"chunked_data.jsonl\")\n",
                "\n",
                "# Step 4: Read JSONL Data\n",
                "with jsonlines.open(\"chunked_data.jsonl\", mode='r') as reader:\n",
                "    stored_chunks = [obj for obj in reader]\n",
                "\n",
                "# Step 5: Verify the Output\n",
                "print(\"Stored Chunks:\", stored_chunks)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Filter Text Chunks with JSONL\n",
                "\n",
                "Nice job on learning how to store text chunks in JSONL format! Now, let's focus on retrieving and filtering this data. Your task is to implement a function that reads from a JSONL file containing text chunks and filters them based on specific criteria.\n",
                "\n",
                "Use the jsonlines library to read the file.\n",
                "Filter chunks by minimum length or by checking for specific keywords.\n",
                "Return the filtered results.\n",
                "This exercise will reinforce your skills in handling JSONL data. Dive in and see how effectively you can process and filter text data!\n",
                "\n",
                "```python\n",
                "import jsonlines\n",
                "\n",
                "def filter_chunks(file_path, min_length=0, keyword=None):\n",
                "    filtered_chunks = []\n",
                "    # TODO: Open the JSONL file in read mode using jsonlines\n",
                "        # TODO: Iterate over each JSON object in the file\n",
                "        for ____ in _____:\n",
                "            # TODO: Extract the 'text' field from the JSON object\n",
                "            text = obj[_____]\n",
                "            # TODO: Check if the text meets the minimum length requirement\n",
                "            # TODO: Check if the keyword is present in the text (if a keyword is provided)\n",
                "            if len(text) >= _________ and (___________ is None or ________ in text):\n",
                "                # TODO: If both conditions are met, add the object to the filtered list\n",
                "                filtered_chunks.append(_____)\n",
                "    # TODO: Return the list of filtered chunks\n",
                "    return ___________\n",
                "\n",
                "# Example usage\n",
                "filtered = filter_chunks(\"chunked_data.jsonl\", min_length=20, keyword=\"second\")\n",
                "print(\"Filtered Chunks:\", filtered)\n",
                "\n",
                "```\n",
                "\n",
                "No problem. I've completed the Python code for the `filter_chunks` function.\n",
                "\n",
                "This function efficiently reads each line from a JSONL file and applies two filters: a **minimum length** and an optional **keyword**. Only the chunks that satisfy both conditions are added to the `filtered_chunks` list, which is then returned.\n",
                "\n",
                "You can see how the logic is implemented in the completed code below.\n",
                "\n",
                "```python\n",
                "import jsonlines\n",
                "\n",
                "def filter_chunks(file_path, min_length=0, keyword=None):\n",
                "    filtered_chunks = []\n",
                "    # Open the JSONL file in read mode using jsonlines\n",
                "    with jsonlines.open(file_path, mode='r') as reader:\n",
                "        # Iterate over each JSON object in the file\n",
                "        for obj in reader:\n",
                "            # Extract the 'text' field from the JSON object\n",
                "            text = obj['text']\n",
                "            # Check if the text meets the minimum length requirement\n",
                "            # and if the keyword is present (if a keyword is provided)\n",
                "            if len(text) >= min_length and (keyword is None or keyword in text):\n",
                "                # If both conditions are met, add the object to the filtered list\n",
                "                filtered_chunks.append(obj)\n",
                "    # Return the list of filtered chunks\n",
                "    return filtered_chunks\n",
                "\n",
                "# Example usage\n",
                "filtered = filter_chunks(\"chunked_data.jsonl\", min_length=20, keyword=\"second\")\n",
                "print(\"Filtered Chunks:\", filtered)\n",
                "```\n",
                "\n",
                "Now you have a flexible function that can filter your text data based on different criteria. This is a very useful skill for managing and processing large datasets for LLMs."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Text Processing Pipeline with JSONL\n",
                "\n",
                "You've done a fantastic job learning about JSONL! Now, let's build a complete text processing pipeline. Your task is to:\n",
                "\n",
                "Chunk a large text using sentence tokenization from text.txt.\n",
                "Enrich each chunk with metadata:\n",
                "char_count (length of the text)\n",
                "source (set this to \"example_text\" or the filename, e.g., \"text.txt\")\n",
                "(Optional but recommended): Add an id field to each chunk to uniquely identify it (e.g., use the index of the chunk).\n",
                "Store these enriched chunks in a JSONL file.\n",
                "Implement a search_chunks(query) function that takes a string and returns all chunk objects whose \"text\" field contains the query string. For example, search_chunks(\"Artificial Intelligence\") should return all chunks where that phrase appears in the text.\n",
                "This exercise will help you integrate all the skills you've learned. Dive in and see how effectively you can manage and search text data!\n",
                "\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "import jsonlines\n",
                "\n",
                "# Step 1: Chunk Your Text\n",
                "nltk.download('punkt_tab', quiet = True)\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "# Load the sample text from text.txt\n",
                "# TODO: Read the text from 'text.txt' and store it in 'large_text'\n",
                "\n",
                "# TODO: Tokenize the text into chunks using sentence tokenization\n",
                "\n",
                "# TODO: Create JSON Objects with Metadata\n",
                "# Add metadata such as 'char_count' and 'source' to each chunk\n",
                "\n",
                "# Step 3: Convert to JSONL Format\n",
                "# TODO: Write the JSON objects to a JSONL file\n",
                "\n",
                "# Step 4: Read JSONL Data\n",
                "# TODO: Read the JSONL file and load the data into a list\n",
                "\n",
                "# TODO: Implement a `search_chunks` function to find chunks containing specific terms\n",
                "\n",
                "# Verify the Output\n",
                "# TODO: Print the first two stored chunks and search results for verification\n",
                "# TODO: Find and print chunks where \"Artificial Intelligence\" exists\n",
                "\n",
                "```\n",
                "\n",
                "It's a great next step to combine all the skills you've learned into a complete pipeline. Here is the completed code that chunks text, enriches it with metadata, stores it in a JSONL file, and includes a function to search for specific terms.\n",
                "\n",
                "To get the pipeline working, you'll need to create a `text.txt` file in the same directory as your Python script. The example text below includes the phrase \"Artificial Intelligence\" so you can test the `search_chunks` function.\n",
                "\n",
                "**text.txt**\n",
                "\n",
                "```\n",
                "Artificial Intelligence is a field of computer science that focuses on creating intelligent machines. It has many applications, from self-driving cars to natural language processing. The goal is to build systems that can think and learn like humans.\n",
                "```\n",
                "\n",
                "Here is the completed Python code.\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "import jsonlines\n",
                "\n",
                "# Step 1: Chunk Your Text\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "# Load the sample text from text.txt\n",
                "with open('text.txt', 'r') as file:\n",
                "    large_text = file.read()\n",
                "\n",
                "# Tokenize the text into chunks using sentence tokenization\n",
                "chunks = sent_tokenize(large_text)\n",
                "\n",
                "# Create JSON Objects with Metadata\n",
                "# Add metadata such as 'char_count' and 'source' to each chunk\n",
                "enriched_chunks = []\n",
                "for i, chunk in enumerate(chunks):\n",
                "    enriched_chunk = {\n",
                "        'id': i,\n",
                "        'text': chunk,\n",
                "        'char_count': len(chunk),\n",
                "        'source': 'text.txt'\n",
                "    }\n",
                "    enriched_chunks.append(enriched_chunk)\n",
                "\n",
                "# Step 3: Convert to JSONL Format\n",
                "# Write the JSON objects to a JSONL file\n",
                "with jsonlines.open('enriched_chunks.jsonl', mode='w') as writer:\n",
                "    writer.write_all(enriched_chunks)\n",
                "\n",
                "# Step 4: Read JSONL Data\n",
                "# Read the JSONL file and load the data into a list\n",
                "def load_chunks(file_path):\n",
                "    with jsonlines.open(file_path, mode='r') as reader:\n",
                "        return [obj for obj in reader]\n",
                "\n",
                "# Implement a `search_chunks` function to find chunks containing specific terms\n",
                "def search_chunks(query, file_path='enriched_chunks.jsonl'):\n",
                "    chunks_data = load_chunks(file_path)\n",
                "    results = [chunk for chunk in chunks_data if query.lower() in chunk['text'].lower()]\n",
                "    return results\n",
                "\n",
                "# Verify the Output\n",
                "print(\"First two stored chunks:\", enriched_chunks[:2])\n",
                "print(\"\\n---\")\n",
                "print(\"Search results for 'Artificial Intelligence':\")\n",
                "search_results = search_chunks(\"Artificial Intelligence\")\n",
                "for result in search_results:\n",
                "    print(result)\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "You can watch a YouTube Shorts video on [writing to a text file in Python](https://www.youtube.com/shorts/Dw85RIvQlc8). This video is relevant because it provides a quick, visual guide on a core Python skill needed for this exercise: writing data to a file.\n",
                "http://googleusercontent.com/youtube_content/5"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
