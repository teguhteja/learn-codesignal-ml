{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 4"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Chunking and Storing Text for Efficient LLM Processing with Chroma DB\n",
                "\n",
                "# Chunking and Storing Text for Efficient LLM Processing with Chroma DB\n",
                "\n",
                "-----\n",
                "\n",
                "## Introduction and Context Setting\n",
                "\n",
                "Welcome to the final lesson of our course on data processing for Large Language Models (LLMs). In previous lessons, you've learned about chunking text, advanced chunking techniques, and storing text chunks in JSONL format. Now, we'll focus on using the **Chroma DB** library to efficiently store and retrieve text chunks. This lesson will integrate these concepts, allowing you to handle text data effectively in LLM applications.\n",
                "\n",
                "LLMs have a limitation when processing long text due to their fixed token window. **Chunking** text into smaller parts helps maintain context and ensures that important information isn't lost. In this lesson, we'll explore how to use **vector embeddings** and the Chroma DB library to store and retrieve these chunks efficiently.\n",
                "\n",
                "-----\n",
                "\n",
                "## Recall: Basics of Text Embeddings\n",
                "\n",
                "Before we dive into the new material, let's briefly recall what **text embeddings** are. Text embeddings are numerical representations of text that capture semantic meaning. They allow us to perform operations like similarity searches, which are crucial for retrieving relevant information from large datasets.\n",
                "\n",
                "In previous lessons, you learned about tokenization and basic vector operations. These concepts are foundational for understanding how embeddings work. Remember, embeddings transform text into a format that LLMs can process more efficiently.\n",
                "\n",
                "-----\n",
                "\n",
                "## Loading and Using an Embedding Model\n",
                "\n",
                "To work with text embeddings, we need an embedding model. We'll use the **SentenceTransformer** library, which provides pre-trained models for generating embeddings.\n",
                "\n",
                "First, let's load the \"all-MiniLM-L6-v2\" model:\n",
                "\n",
                "```python\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# Load embedding model\n",
                "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
                "```\n",
                "\n",
                "In this code snippet, we import the `SentenceTransformer` class and load the \"all-MiniLM-L6-v2\" model. This model is pre-trained to generate embeddings for sentences, making it ideal for our task.\n",
                "\n",
                "-----\n",
                "\n",
                "## Chunking Text for LLMs\n",
                "\n",
                "Chunking is the process of splitting long documents into smaller, manageable parts. This is crucial for maintaining context when processing text with LLMs.\n",
                "\n",
                "Consider the following text:\n",
                "\n",
                "```python\n",
                "documents = [\n",
                "    \"Large language models process text within a limited token window.\",\n",
                "    \"If the input text is too long, models may truncate information.\",\n",
                "    \"Splitting long documents into smaller chunks helps maintain relevant context.\",\n",
                "]\n",
                "```\n",
                "\n",
                "Here, we have a list of sentences, each representing a chunk of text. By breaking down the text into these smaller parts, we ensure that each chunk can be processed without losing important context.\n",
                "\n",
                "-----\n",
                "\n",
                "## Converting Text to Vector Embeddings\n",
                "\n",
                "Now that we have our text chunks, let's convert them into vector embeddings using the loaded model:\n",
                "\n",
                "```python\n",
                "# Convert text to vector embeddings\n",
                "embeddings = embedding_model.encode(documents)\n",
                "```\n",
                "\n",
                "In this step, we use the `encode` method of our embedding model to transform the list of text chunks into a list of vector embeddings. These embeddings are numerical representations that capture the semantic meaning of each chunk.\n",
                "\n",
                "-----\n",
                "\n",
                "## Storing and Retrieving Chunks with Chroma DB\n",
                "\n",
                "With our embeddings ready, we can now store and retrieve them using the Chroma DB library. Chroma DB is a library for efficient similarity search and clustering of dense vectors.\n",
                "\n",
                "### Step 1: Initialize Chroma DB\n",
                "\n",
                "First, initialize the Chroma DB client and create or load a collection:\n",
                "\n",
                "```python\n",
                "import chromadb\n",
                "from chromadb.config import Settings\n",
                "\n",
                "# Initialize ChromaDB client\n",
                "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
                "\n",
                "# Create or load collection with a custom name\n",
                "collection = client.get_or_create_collection(name=\"my_custom_collection\")\n",
                "```\n",
                "\n",
                "**Explanation:** We import the necessary modules from Chroma DB and initialize a persistent client, specifying the path where the database will be stored. We then create or load a collection with a custom name, which will hold our vector embeddings.\n",
                "\n",
                "### Step 2: Store Embeddings\n",
                "\n",
                "Next, store the embeddings in Chroma DB:\n",
                "\n",
                "```python\n",
                "from chromadb.utils import embedding_functions\n",
                "from chromadb import Client\n",
                "\n",
                "# Load embedding model\n",
                "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                "\n",
                "# Create collection with embedding function\n",
                "collection = client.get_or_create_collection(name=\"vector_collection\", embedding_function=embed_func)\n",
                "\n",
                "# Sample documents\n",
                "documents = [\n",
                "    {\"id\": \"doc1\", \"content\": \"ChromaDB is an open-source vector database.\"},\n",
                "    {\"id\": \"doc2\", \"content\": \"Vector databases help in efficient similarity search.\"},\n",
                "    {\"id\": \"doc3\", \"content\": \"Embedding models convert text into numerical representations.\"}\n",
                "]\n",
                "\n",
                "# Insert documents into ChromaDB\n",
                "collection.add(\n",
                "    documents=[doc[\"content\"] for doc in documents],\n",
                "    ids=[doc[\"id\"] for doc in documents]\n",
                ")\n",
                "```\n",
                "\n",
                "**Explanation:** We initialize a Chroma DB client and load an embedding model using the `SentenceTransformerEmbeddingFunction`. We create a collection with this embedding function and insert sample documents into the collection. Each document is stored with a unique ID and its content.\n",
                "\n",
                "### Step 3: Retrieve Relevant Chunks\n",
                "\n",
                "To retrieve relevant chunks for a query, perform a retrieval operation:\n",
                "\n",
                "```python\n",
                "# Perform a retrieval operation to verify everything works\n",
                "query_text = \"What is an AI\"\n",
                "results = collection.query(\n",
                "    query_texts=[query_text],\n",
                "    n_results=3\n",
                ")\n",
                "\n",
                "# Display results\n",
                "for i, doc in enumerate(results[\"documents\"][0]):\n",
                "    print(f\"Result {i+1}: {doc} (Score: {results['distances'][0][i]})\")\n",
                "```\n",
                "\n",
                "**Explanation:** We perform a retrieval operation by querying the collection with a text query. The `query` method returns the most similar text chunks along with their similarity scores. The output displays the top 3 results, showing the retrieved documents and their respective scores.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, you've learned how to integrate chunking, embedding, and retrieval techniques using the Chroma DB library. By storing text chunks as vector embeddings, you can efficiently retrieve relevant information, making your LLM applications more effective.\n",
                "\n",
                "Congratulations on reaching the end of the course\\! You've gained valuable skills in processing text for LLMs, from chunking and storing text to using advanced retrieval techniques. As you move forward, I encourage you to apply these skills in real-world applications and continue exploring the exciting field of natural language processing."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Converting Text Chunks to Vector Embeddings\n",
                "\n",
                "You've done well in understanding how to load and use an embedding model. Now, let's take it a step further.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Load the SentenceTransformer embedding model.\n",
                "Convert a list of text chunks into vector embeddings.\n",
                "This exercise will solidify your understanding of integrating these steps. Dive in and see how efficiently you can transform text into embeddings!\n",
                "\n",
                "```python\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# TODO: Load embedding model\n",
                "\n",
                "# Sample chunked text\n",
                "documents = [\n",
                "    \"Large language models process text within a limited token window.\",\n",
                "    \"If the input text is too long, models may truncate information.\",\n",
                "    \"Splitting long documents into smaller chunks helps maintain relevant context.\",\n",
                "]\n",
                "\n",
                "# TODO: Convert text to vector embeddings\n",
                "\n",
                "print(\"Vector Embeddings:\", embeddings)\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# Load embedding model\n",
                "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
                "\n",
                "# Sample chunked text\n",
                "documents = [\n",
                "    \"Large language models process text within a limited token window.\",\n",
                "    \"If the input text is too long, models may truncate information.\",\n",
                "    \"Splitting long documents into smaller chunks helps maintain relevant context.\",\n",
                "]\n",
                "\n",
                "# Convert text to vector embeddings\n",
                "embeddings = embedding_model.encode(documents)\n",
                "\n",
                "print(\"Vector Embeddings:\", embeddings)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initializing ChromaDB for Vector Storage\n",
                "\n",
                "Nice job on converting text chunks to vector embeddings! Now, let's move forward and initialize a ChromaDB vector database.\n",
                "\n",
                "Here's what you'll do:\n",
                "\n",
                "Import the necessary modules from chromadb.\n",
                "Initialize a PersistentClient for ChromaDB.\n",
                "Create or load a collection with a custom name.\n",
                "This task will help you set up a vector database to store embeddings. Let's see how efficiently you can set this up!\n",
                "\n",
                "```python\n",
                "# TODO: Import the necessary modules from chromadb\n",
                "\n",
                "# TODO: Initialize a PersistentClient for ChromaDB\n",
                "\n",
                "# TODO: Create or load a collection with a custom name\n",
                "\n",
                "# TODO: Print the name of the collection\n",
                "\n",
                "# Hint: You can access the collection's name using collection.name\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "import chromadb\n",
                "from chromadb.config import Settings\n",
                "\n",
                "# Initialize a PersistentClient for ChromaDB\n",
                "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
                "\n",
                "# Create or load a collection with a custom name\n",
                "collection = client.get_or_create_collection(name=\"my_custom_collection\")\n",
                "\n",
                "# Print the name of the collection\n",
                "print(\"Collection Name:\", collection.name)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Storing Embeddings in ChromaDB\n",
                "\n",
                "Excellent work setting up your ChromaDB vector database! Now it's time to put it to use by implementing the functionality to store embeddings — a crucial part of our text processing workflow.\n",
                "\n",
                "In this exercise, you'll work with a pre-populated ChromaDB collection and learn how to:\n",
                "\n",
                "Set up an embedding function for ChromaDB to process text chunks\n",
                "Create a collection that uses this embedding function\n",
                "Store text chunks as embeddings in the ChromaDB collection\n",
                "This skill is essential for the text processing cycle for LLMs: chunking, embedding, and storing. Efficiently storing text as embeddings allows for powerful semantic search capabilities in modern LLM applications.\n",
                "\n",
                "```python\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import chromadb\n",
                "from chromadb.utils import embedding_functions\n",
                "\n",
                "# Load embedding model\n",
                "model_name = \"all-MiniLM-L6-v2\"\n",
                "embedding_model = SentenceTransformer(model_name)\n",
                "\n",
                "# Initialize ChromaDB client\n",
                "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
                "\n",
                "# TODO: Set up embedding function for ChromaDB\n",
                "# Hint: Use SentenceTransformerEmbeddingFunction with the model name\n",
                "\n",
                "# TODO: Create or get collection with embedding function\n",
                "# Hint: Use client.get_or_create_collection and pass the embedding function\n",
                "\n",
                "# Sample documents\n",
                "documents = [\n",
                "    {\"id\": \"doc1\", \"content\": \"ChromaDB is an open-source vector database.\"},\n",
                "    {\"id\": \"doc2\", \"content\": \"Vector databases help in efficient similarity search.\"},\n",
                "    {\"id\": \"doc3\", \"content\": \"Embedding models convert text into numerical representations.\"}\n",
                "]\n",
                "\n",
                "# TODO: Add documents to the collection\n",
                "# Hint: Use collection.add with document contents and their IDs\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import chromadb\n",
                "from chromadb.utils import embedding_functions\n",
                "\n",
                "# Load embedding model\n",
                "model_name = \"all-MiniLM-L6-v2\"\n",
                "embedding_model = SentenceTransformer(model_name)\n",
                "\n",
                "# Initialize ChromaDB client\n",
                "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
                "\n",
                "# Set up embedding function for ChromaDB\n",
                "embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                "\n",
                "# Create or get collection with embedding function\n",
                "collection = client.get_or_create_collection(name=\"vector_collection\", embedding_function=embed_func)\n",
                "\n",
                "# Sample documents\n",
                "documents = [\n",
                "    {\"id\": \"doc1\", \"content\": \"ChromaDB is an open-source vector database.\"},\n",
                "    {\"id\": \"doc2\", \"content\": \"Vector databases help in efficient similarity search.\"},\n",
                "    {\"id\": \"doc3\", \"content\": \"Embedding models convert text into numerical representations.\"}\n",
                "]\n",
                "\n",
                "# Add documents to the collection\n",
                "collection.add(\n",
                "    documents=[doc[\"content\"] for doc in documents],\n",
                "    ids=[doc[\"id\"] for doc in documents]\n",
                ")\n",
                "\n",
                "print(\"Documents successfully added to the collection.\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Persisting Vector Databases for Production\n",
                "\n",
                "Fantastic job setting up the environment for ChromaDB! Now, let's tackle a critical real-world requirement: performing a retrieval operation on your vector database.\n",
                "\n",
                "In production applications, you don't want to rebuild your database from scratch every time. Instead, you'll want to persist it and reload it efficiently. We've set up the database for you, and your task is to perform a retrieval operation to verify everything works.\n",
                "\n",
                "In this exercise, you'll:\n",
                "\n",
                "Perform a retrieval operation to verify everything works\n",
                "Specifically, perform a retrieval operation by querying:\n",
                "\n",
                "```sh\n",
                "What is a vector database?\n",
                "```\n",
                "\n",
                "Retrieve the top 3 most relevant documents and print the results.\n",
                "\n",
                "This skill completes your toolkit for working with vector databases in LLM applications, allowing you to build systems that can efficiently store and retrieve information across multiple sessions.\n",
                "\n",
                "```python\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import chromadb\n",
                "from chromadb.utils import embedding_functions\n",
                "\n",
                "# Define the persistence directory\n",
                "persist_dir = \"./chroma_db_persist\"\n",
                "\n",
                "# Load embedding model\n",
                "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
                "\n",
                "# Initialize ChromaDB client with persistence directory\n",
                "client = chromadb.PersistentClient(path=persist_dir)\n",
                "\n",
                "# Load embedding model for ChromaDB\n",
                "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                "\n",
                "# Create collection with embedding function\n",
                "collection = client.get_or_create_collection(name=\"vector_collection\", embedding_function=embed_func)\n",
                "\n",
                "# Sample documents\n",
                "documents = [\n",
                "    {\"id\": \"doc1\", \"content\": \"ChromaDB is an open-source vector database.\"},\n",
                "    {\"id\": \"doc2\", \"content\": \"Vector databases help in efficient similarity search.\"},\n",
                "    {\"id\": \"doc3\", \"content\": \"Embedding models convert text into numerical representations.\"}\n",
                "]\n",
                "\n",
                "# Insert documents into ChromaDB\n",
                "collection.add(\n",
                "    documents=[doc[\"content\"] for doc in documents],\n",
                "    ids=[doc[\"id\"] for doc in documents]\n",
                ")\n",
                "\n",
                "print(f\"\\nDatabase loaded from {persist_dir}\")\n",
                "print(\"Performing a retrieval operation...\\n\")\n",
                "\n",
                "# TODO: Perform a retrieval operation by querying:\n",
                "# \"What is a vector database?\"\n",
                "# Retrieve the top 3 most relevant documents and print the results.\n",
                "```\n",
                "\n",
                "```python\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import chromadb\n",
                "from chromadb.utils import embedding_functions\n",
                "\n",
                "# Define the persistence directory\n",
                "persist_dir = \"./chroma_db_persist\"\n",
                "\n",
                "# Load embedding model\n",
                "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
                "\n",
                "# Initialize ChromaDB client with persistence directory\n",
                "client = chromadb.PersistentClient(path=persist_dir)\n",
                "\n",
                "# Load embedding model for ChromaDB\n",
                "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
                "\n",
                "# Create collection with embedding function\n",
                "collection = client.get_or_create_collection(name=\"vector_collection\", embedding_function=embed_func)\n",
                "\n",
                "# Sample documents\n",
                "documents = [\n",
                "    {\"id\": \"doc1\", \"content\": \"ChromaDB is an open-source vector database.\"},\n",
                "    {\"id\": \"doc2\", \"content\": \"Vector databases help in efficient similarity search.\"},\n",
                "    {\"id\": \"doc3\", \"content\": \"Embedding models convert text into numerical representations.\"}\n",
                "]\n",
                "\n",
                "# Insert documents into ChromaDB\n",
                "collection.add(\n",
                "    documents=[doc[\"content\"] for doc in documents],\n",
                "    ids=[doc[\"id\"] for doc in documents]\n",
                ")\n",
                "\n",
                "print(f\"\\nDatabase loaded from {persist_dir}\")\n",
                "print(\"Performing a retrieval operation...\\n\")\n",
                "\n",
                "# Perform a retrieval operation by querying:\n",
                "# \"What is a vector database?\"\n",
                "# Retrieve the top 3 most relevant documents and print the results.\n",
                "query_text = \"What is a vector database?\"\n",
                "results = collection.query(\n",
                "    query_texts=[query_text],\n",
                "    n_results=3\n",
                ")\n",
                "\n",
                "# Display results\n",
                "for i, doc in enumerate(results[\"documents\"][0]):\n",
                "    print(f\"Result {i+1}: {doc}\")\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
