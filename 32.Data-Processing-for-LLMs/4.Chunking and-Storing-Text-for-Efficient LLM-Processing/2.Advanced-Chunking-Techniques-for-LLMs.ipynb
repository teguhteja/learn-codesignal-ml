{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advanced Chunking Techniques for LLMs\n",
                "\n",
                "# Advanced Chunking Techniques\n",
                "\n",
                "Welcome back\\! In our previous lesson, we explored the basics of chunking and storing text for efficient processing with Large Language Models (LLMs). We learned that breaking down large text is crucial for effective data handling. Today, we'll dive deeper into advanced chunking techniques, focusing on **recursive character-based** and **token-based** methods. These techniques will help you optimize text processing for AI models, making your applications more efficient.\n",
                "\n",
                "## Introduction to Overlapping Chunks\n",
                "\n",
                "**Overlapping chunks** ensure that consecutive chunks share some common content. This shared content is crucial for maintaining continuity and context, which helps AI models understand and process text effectively.\n",
                "\n",
                "Imagine a chatbot answering questions about an article. If chunks don't overlap, the model might lose track of key details, leading to incomplete or incorrect responses. Overlapping chunks solve this by ensuring key phrases appear in consecutive chunks. This is especially important for applications like summarization, search indexing, and document processing, where information needs to flow smoothly.\n",
                "\n",
                "## Understanding Recursive Character-Based Chunking\n",
                "\n",
                "To achieve effective overlapping, we can use **Recursive Character-Based Chunking**. This technique breaks down text into smaller pieces based on characters while preserving context. It respects natural boundaries like sentences and paragraphs, ensuring that chunks are readable and maintain their logical structure. We'll use the `RecursiveCharacterTextSplitter` from the `langchain` library to implement this.\n",
                "\n",
                "### How it Works\n",
                "\n",
                "1.  **Define a Maximum Chunk Size**: Set a limit for each chunk (e.g., 100 characters).\n",
                "2.  **Choose Separators**: These define where text should be split, such as:\n",
                "      * Paragraphs (`\\n\\n`)\n",
                "      * Sentences (`.`)\n",
                "      * Spaces (`     `) as a last resort\n",
                "3.  **Recursive Splitting**:\n",
                "      * The text is first split using the **largest separator** (paragraphs).\n",
                "      * If any chunk is still **too long**, it's further split using the **next separator** (sentences).\n",
                "      * This continues down to spaces if necessary, to ensure all chunks fit within the limit.\n",
                "4.  **Apply Overlap**: A defined number of characters from the end of one chunk is included at the beginning of the next to maintain context.\n",
                "\n",
                "### Step 1: Importing Necessary Libraries\n",
                "\n",
                "First, import the required library.\n",
                "\n",
                "```python\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "```\n",
                "\n",
                "### Step 2: Loading the Sample Text\n",
                "\n",
                "Next, load the sample text from a file.\n",
                "\n",
                "```python\n",
                "with open('text.txt', 'r') as file:\n",
                "    text = file.read()\n",
                "```\n",
                "\n",
                "### Step 3: Setting Up the Recursive Character-Based Splitter\n",
                "\n",
                "Now, configure the `RecursiveCharacterTextSplitter` with your desired parameters.\n",
                "\n",
                "```python\n",
                "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, separators=[\"\\n\\n\", \".\", \" \"])\n",
                "```\n",
                "\n",
                "  * `chunk_size=100`: The maximum size for each chunk.\n",
                "  * `chunk_overlap=20`: The number of characters that overlap between chunks.\n",
                "  * `separators=[\"\\n\\n\", \".\", \" \"]`: The hierarchical list of characters used to split the text.\n",
                "\n",
                "### Step 4: Splitting the Text\n",
                "\n",
                "Finally, use the splitter to break the text into chunks and print the results.\n",
                "\n",
                "```python\n",
                "recursive_chunks = recursive_splitter.split_text(text)\n",
                "print(\"Recursive Character-Based Chunking:\")\n",
                "for i, chunk in enumerate(recursive_chunks):\n",
                "    print(f\"Chunk {i+1}: {chunk}\\n\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Exploring Token-Based Chunking\n",
                "\n",
                "**Token-based chunking** breaks text into tokens, the smallest units of meaning, using a tokenizer. This method is effective for LLMs that process tokenized input. The process involves:\n",
                "\n",
                "  * **Tokenization**: The text is converted into tokens, which are meaningful units like words or subwords.\n",
                "  * **Chunk Size and Overlap**: You define the maximum number of tokens per chunk (`chunk_size`) and the overlap between chunks (`chunk_overlap`) to maintain context.\n",
                "  * **Splitting**: The tokenized text is split into chunks based on the defined size and overlap.\n",
                "  * **Encoding**: An encoding, such as OpenAI's `cl100k_base`, is specified to guide the tokenization process.\n",
                "\n",
                "### Step 1: Importing Necessary Libraries\n",
                "\n",
                "Import the libraries needed for token-based chunking.\n",
                "\n",
                "```python\n",
                "import tiktoken\n",
                "from langchain.text_splitter import TokenTextSplitter\n",
                "```\n",
                "\n",
                "### Step 2: Understanding Tokens vs. Characters\n",
                "\n",
                "Let's see how tokens, characters, and words differ.\n",
                "\n",
                "```python\n",
                "text_example = \"AI-powered models process data in an efficient way.\"\n",
                "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
                "print(f\"Character count: {len(text_example)}\")\n",
                "print(f\"Word count: {len(text_example.split())}\")\n",
                "print(f\"Token count: {len(enc.encode(text_example))}\")\n",
                "```\n",
                "\n",
                "**Output**:\n",
                "\n",
                "```\n",
                "Character count: 51\n",
                "Word count: 8\n",
                "Token count: 10\n",
                "```\n",
                "\n",
                "As you can see, tokens don't always match character or word counts because tokenizers may split words into smaller subword units. This is why token-based chunking is more effective for LLMs that rely on tokenized input.\n",
                "\n",
                "### Step 3: Setting Up the Token-Based Splitter\n",
                "\n",
                "Set up the `TokenTextSplitter` using OpenAI's tokenizer.\n",
                "\n",
                "```python\n",
                "token_splitter = TokenTextSplitter(encoding_name=\"cl100k_base\", chunk_size=40, chunk_overlap=10)\n",
                "```\n",
                "\n",
                "  * `encoding_name=\"cl100k_base\"`: Specifies the encoding for tokenization.\n",
                "  * `chunk_size=40`: Sets the maximum number of tokens per chunk.\n",
                "  * `chunk_overlap=10`: Sets the overlap between chunks to 10 tokens.\n",
                "\n",
                "### Step 4: Splitting the Text\n",
                "\n",
                "Finally, split the text into token-based chunks and print the results.\n",
                "\n",
                "```python\n",
                "token_chunks = token_splitter.split_text(text)\n",
                "print(\"Token-Based Chunking:\")\n",
                "for i, chunk in enumerate(token_chunks):\n",
                "    print(f\"Chunk {i+1}: {chunk}\\n\")\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "## Comparing Chunking Methods\n",
                "\n",
                "Both recursive character-based and token-based chunking have their own strengths and weaknesses. Recursive character-based chunking is useful for preserving context in a flexible manner, while token-based chunking offers precision by focusing on meaningful units of text. The right choice depends on your specific NLP task.\n",
                "\n",
                "### Side-by-Side Comparison\n",
                "\n",
                "| Method | Strengths | Weaknesses |\n",
                "| :--- | :--- | :--- |\n",
                "| **Recursive Character-Based** | Respects sentence structure, flexible | Not always aligned with token limits |\n",
                "| **Token-Based** | Ensures compatibility with LLM token windows | May split at unnatural sentence points |\n",
                "\n",
                "Understanding these trade-offs helps you choose the right method for your specific AI application.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, we explored advanced chunking techniques, focusing on **recursive character-based** and **token-based** methods. Overlapping chunks help maintain context, enhancing a model's understanding. Recursive character-based chunking respects natural text boundaries, while token-based chunking aligns with token limits.\n",
                "\n",
                "As you prepare for practice, familiarize yourself with the `langchain` and `tiktoken` libraries, as you'll apply these concepts in the upcoming exercises."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exploring Separator Configurations\n",
                "\n",
                "Nice work on understanding recursive character-based chunking! Now, let's put that knowledge into practice. Your task is to experiment with the separators parameter in RecursiveCharacterTextSplitter. For this task, imagine you have loaded a sample long document from text.txt.\n",
                "\n",
                "Modify the code to try three different separator configurations:\n",
                "The default [\"\\n\\n\", \"\\n\", \" \", \"\"]: Prioritizes paragraph and line breaks.\n",
                "[\".\", \",\", \" \", \"\"]: Focuses on sentence and clause boundaries.\n",
                "[\" \"]: Splits only on spaces, treating each word as a boundary.\n",
                "Analyze how these affect the chunking results. This exercise will help you see how different separator hierarchies impact the logical structure and readability of the chunks. Print only the first 3 chunks for each configuration to explore the effects!\n",
                "\n",
                "```python\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
                "\n",
                "# Load the sample text from text.txt\n",
                "with open('text.txt', 'r') as file:\n",
                "    text = file.read()\n",
                "\n",
                "# Recursive Character-Based Chunking with different separators\n",
                "separator_configs = [\n",
                "    # TODO: Add the first separator configuration\n",
                "    # TODO: Add the second separator configuration\n",
                "    # TODO: Add the third separator configuration\n",
                "]\n",
                "\n",
                "for config in separator_configs:\n",
                "    # Use the current config for separators\n",
                "    recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, separators=config)\n",
                "    recursive_chunks = recursive_splitter.split_text(text)\n",
                "    \n",
                "    print(f\"Recursive Character-Based Chunking with separators {config}:\")\n",
                "    for i, chunk in enumerate(recursive_chunks[:3]):\n",
                "        print(f\"Chunk {i+1}: {chunk}\\n\")\n",
                "```\n",
                "\n",
                "```python\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
                "\n",
                "# Load the sample text from text.txt\n",
                "with open('text.txt', 'r') as file:\n",
                "    text = file.read()\n",
                "\n",
                "# Recursive Character-Based Chunking with different separators\n",
                "separator_configs = [\n",
                "    [\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
                "    [\".\", \",\", \" \", \"\"],\n",
                "    [\" \"],\n",
                "]\n",
                "\n",
                "for config in separator_configs:\n",
                "    # Use the current config for separators\n",
                "    recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, separators=config)\n",
                "    recursive_chunks = recursive_splitter.split_text(text)\n",
                "    \n",
                "    print(f\"Recursive Character-Based Chunking with separators {config}:\")\n",
                "    for i, chunk in enumerate(recursive_chunks[:3]):\n",
                "        print(f\"Chunk {i+1}: {chunk}\\n\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exploring Overlap in Text Chunking\n",
                "\n",
                "You've done well in understanding recursive character-based chunking! Now, let's explore how different chunk_overlap values affect context preservation.\n",
                "\n",
                "Use the text from text.txt.\n",
                "Adjust the chunk_overlap parameter to 0, 30, and 50 characters.\n",
                "Print only the first 3 chunks for each overlap setting.\n",
                "Analyze how each setting impacts the continuity between chunks.\n",
                "Identify specific examples (e.g., repeated sentences or phrases) where increased overlap helps maintain the flow of information or prevents important details from being lost between chunks.\n",
                "This exercise will help you find the optimal overlap value that balances efficiency and context. Dive in and see how these adjustments can enhance your text processing!\n",
                "\n",
                "```python\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "# Load text from text.txt\n",
                "with open('text.txt', 'r') as file:\n",
                "    text = file.read()\n",
                "\n",
                "# Function to test different chunk_overlap values\n",
                "def test_chunk_overlap(overlap):\n",
                "    print(f\"Testing with chunk_overlap={overlap}\")\n",
                "    recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=overlap, separators=[\"\\n\\n\", \".\", \" \"])\n",
                "    recursive_chunks = recursive_splitter.split_text(text)\n",
                "    \n",
                "    # TODO: Print only the first 3 chunks\n",
                "\n",
                "# TODO: Test with different overlap values (0, 30, 50)\n",
                "```\n",
                "\n",
                "```python\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "# Load text from text.txt\n",
                "with open('text.txt', 'r') as file:\n",
                "    text = file.read()\n",
                "\n",
                "# Function to test different chunk_overlap values\n",
                "def test_chunk_overlap(overlap):\n",
                "    print(f\"Testing with chunk_overlap={overlap}\\n{'-'*30}\")\n",
                "    recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=overlap, separators=[\"\\n\\n\", \".\", \" \"])\n",
                "    recursive_chunks = recursive_splitter.split_text(text)\n",
                "    \n",
                "    # Print only the first 3 chunks\n",
                "    for i, chunk in enumerate(recursive_chunks[:3]):\n",
                "        print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
                "\n",
                "# Test with different overlap values (0, 30, 50)\n",
                "test_chunk_overlap(0)\n",
                "test_chunk_overlap(30)\n",
                "test_chunk_overlap(50)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Token-Based Chunking Implementation\n",
                "\n",
                "Well done on mastering recursive character-based chunking! Now, let's shift our focus to token-based chunking. Your task is to implement token-based chunking using TokenTextSplitter.\n",
                "\n",
                "Set up the TokenTextSplitter with the correct parameters.\n",
                "Use it to split a sample text into token-based chunks.\n",
                "Print the first 4 token-based chunks to see how the text is divided.\n",
                "This exercise will help you understand how to optimize text processing for AI models. Dive in and see how token-based chunking can enhance your applications!\n",
                "\n",
                "```python\n",
                "from langchain.text_splitter import TokenTextSplitter\n",
                "import tiktoken  # OpenAI's tokenizer library for precise token-based chunking\n",
                "\n",
                "# Load the sample text from text.txt\n",
                "with open('text.txt', 'r') as file:\n",
                "    text = file.read()\n",
                "\n",
                "# TODO: Implement Token-Based Chunking using OpenAI's tokenizer\n",
                "# Set up the TokenTextSplitter with the following parameters:\n",
                "# - encoding_name=\"cl100k_base\" (the encoding name used for tokenization, specific to OpenAI's tokenizer)\n",
                "# - chunk_size=100 (the maximum number of tokens per chunk)\n",
                "# - chunk_overlap=10 (the number of tokens that overlap between consecutive chunks)\n",
                "# TODO: Use the TokenTextSplitter to split the text into token-based chunks\n",
                "\n",
                "# TODO: Print first 4 token-based chunks\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from langchain.text_splitter import TokenTextSplitter\n",
                "import tiktoken  # OpenAI's tokenizer library for precise token-based chunking\n",
                "\n",
                "# Load the sample text from text.txt\n",
                "with open('text.txt', 'r') as file:\n",
                "    text = file.read()\n",
                "\n",
                "# Implement Token-Based Chunking using OpenAI's tokenizer\n",
                "# Set up the TokenTextSplitter with the specified parameters\n",
                "token_splitter = TokenTextSplitter(\n",
                "    encoding_name=\"cl100k_base\", \n",
                "    chunk_size=100, \n",
                "    chunk_overlap=10\n",
                ")\n",
                "\n",
                "# Use the TokenTextSplitter to split the text into token-based chunks\n",
                "token_chunks = token_splitter.split_text(text)\n",
                "\n",
                "# Print first 4 token-based chunks\n",
                "print(\"Token-Based Chunking:\")\n",
                "for i, chunk in enumerate(token_chunks[:4]):\n",
                "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
