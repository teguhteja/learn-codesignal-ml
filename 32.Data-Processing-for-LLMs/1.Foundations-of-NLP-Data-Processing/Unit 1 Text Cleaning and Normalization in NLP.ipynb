{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 1 \n",
                                    "\n",
                                    "# Introduction to NLP Data Processing\n",
                                    "\n",
                                    "Welcome to the first lesson of the \"Foundations of NLP Data Processing\" course. In this lesson, we will explore the essential techniques for **cleaning and normalizing text data**, which are crucial steps in preparing data for **Natural Language Processing (NLP)** models. Text preprocessing helps in removing noise and ensuring that the data is in a consistent format, making it easier for NLP models to understand and analyze. By the end of this lesson, you will be able to create a text-cleaning pipeline that effectively prepares text data for further processing.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Setting Up the Environment\n",
                                    "\n",
                                    "Before we dive into text cleaning, let's set up our environment. We will use several Python libraries: **nltk**, **autocorrect**, and **re**. These libraries are pre-installed in CodeSignal environments, so you don't need to worry about installation here. However, on your own device, you can install them using `pip`:\n",
                                    "\n",
                                    "```bash\n",
                                    "pip install nltk autocorrect\n",
                                    "```\n",
                                    "\n",
                                    "The `nltk` library is a powerful toolkit for working with human language data, and it provides tools for text processing, including stopwords removal, stemming, and lemmatization. Note that even after installing `nltk` via `pip`, you still need to download its specific packages within your Python code. For example, to use the WordNet lemmatizer, you need to download the WordNet data:\n",
                                    "\n",
                                    "```python\n",
                                    "import nltk\n",
                                    "nltk.download('wordnet')\n",
                                    "```\n",
                                    "\n",
                                    "Similarly, for stopwords removal, you need to download the stopwords data:\n",
                                    "\n",
                                    "```python\n",
                                    "nltk.download('stopwords')\n",
                                    "```\n",
                                    "\n",
                                    "The `autocorrect` library helps in correcting misspelled words, and `re` is a built-in Python library for working with regular expressions, which we will use to remove unwanted text elements.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Removing Unwanted Text Elements\n",
                                    "\n",
                                    "In text data, you often encounter unwanted elements such as URLs, email addresses, special characters, numbers, and punctuation. These elements can introduce noise and affect the performance of NLP models. We will use **regular expressions (re)** to remove these unwanted elements. By crafting specific patterns, we can efficiently identify and eliminate these elements from the text, leaving behind only the relevant words.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Text Normalization Techniques\n",
                                    "\n",
                                    "Text normalization is the process of converting text into a standard format, which is essential for consistent and accurate text processing. This involves several key steps:\n",
                                    "\n",
                                    "  * **Unicode Normalization**: Text data can come from various sources and may contain characters from different languages and scripts. Unicode normalization ensures that these characters are represented consistently across the text. We will use the `unicodedata` library to perform this normalization. This step is crucial for accurate text processing, especially when dealing with multilingual data or text from diverse sources.\n",
                                    "      * **Example**: Consider the character \"é\" which can be represented in two ways: as a single character \"é\" or as a combination of \"e\" and an acute accent \"é\". These two representations look the same but are different in terms of Unicode. By normalizing them, we can ensure they are treated as the same character.\n",
                                    "\n",
                                    "<!-- end list -->\n",
                                    "\n",
                                    "```python\n",
                                    "import unicodedata\n",
                                    "text1 = \"é\"  # Single character\n",
                                    "text2 = \"é\"  # 'e' + combining acute accent\n",
                                    "print(text1 == text2)  # False (they look the same but are different)\n",
                                    "\n",
                                    "# Normalize both to NFC (composed form)\n",
                                    "normalized_text1 = unicodedata.normalize(\"NFC\", text1)\n",
                                    "normalized_text2 = unicodedata.normalize(\"NFC\", text2)\n",
                                    "print(normalized_text1 == normalized_text2)  # True (now they are the same)\n",
                                    "```\n",
                                    "\n",
                                    "  * **Lowercasing**: Converting text to lowercase is a simple yet effective normalization technique. It helps in maintaining uniformity across the text data by ensuring that words are treated the same regardless of their case. For example, \"Natural Language Processing\" and \"natural language processing\" would be considered the same phrase after lowercasing, which is important for tasks like text classification and sentiment analysis.\n",
                                    "      * **Example**: \"Natural Language Processing\" → \"natural language processing\".\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Stemming and Lemmatization\n",
                                    "\n",
                                    "Stemming and lemmatization are also techniques of text normalization, used to reduce words to their base or root form, which helps in normalizing text data.\n",
                                    "\n",
                                    "  * **Stemming**: This process involves removing suffixes from words to obtain their root form. It is a rule-based approach and may not always produce a valid word. For example, \"running\" becomes \"run\" and \"better\" becomes \"better\" (no change).\n",
                                    "  * **Lemmatization**: This process involves reducing words to their base or dictionary form, known as the lemma. It considers the context and part of speech, resulting in more accurate normalization. The `pos` argument in the `lemmatize` method specifies the part of speech for the word, which helps the lemmatizer understand the context. For example, \"running\" becomes \"run\" and \"better\" becomes \"good\" when treated as an adjective. The `pos` argument can be set to:\n",
                                    "      * `'v'`: Verb\n",
                                    "      * `'n'`: Noun\n",
                                    "      * `'a'`: Adjective\n",
                                    "      * `'r'`: Adverb\n",
                                    "      * By specifying the correct part of speech, the lemmatizer can more accurately reduce words to their base forms.\n",
                                    "\n",
                                    "We will use the `nltk` library for both stemming and lemmatization.\n",
                                    "\n",
                                    "### Example Code\n",
                                    "\n",
                                    "```python\n",
                                    "from nltk.stem import PorterStemmer\n",
                                    "from nltk.stem import WordNetLemmatizer\n",
                                    "import nltk\n",
                                    "\n",
                                    "# Download WordNet data for lemmatization\n",
                                    "nltk.download('wordnet')\n",
                                    "\n",
                                    "stemmer = PorterStemmer()\n",
                                    "lemmatizer = WordNetLemmatizer()\n",
                                    "words = [\"running\", \"better\", \"flies\"]\n",
                                    "stemmed_words = [stemmer.stem(word) for word in words]\n",
                                    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]  # 'v' for verb\n",
                                    "print(\"Stemmed Words:\", stemmed_words)\n",
                                    "print(\"Lemmatized Words:\", lemmatized_words)\n",
                                    "```\n",
                                    "\n",
                                    "**Output**:\n",
                                    "\n",
                                    "```text\n",
                                    "Stemmed Words: ['run', 'better', 'fli']\n",
                                    "Lemmatized Words: ['run', 'good', 'fly']\n",
                                    "```\n",
                                    "\n",
                                    "By incorporating stemming and lemmatization, we can further enhance the normalization process, ensuring that words are consistently represented in their base forms.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Stopwords Removal and Spell Checking\n",
                                    "\n",
                                    "**Stopwords** are common words like \"and,\" \"the,\" and \"is\" that usually do not contribute much to the meaning of a sentence. Removing stopwords can help in reducing the size of the text data and focusing on the more meaningful words. We will use the `nltk` library to remove stopwords from our text. Additionally, we will use the `autocorrect` library to correct any misspelled words, ensuring that the text data is clean and accurate.\n",
                                    "\n",
                                    "### Example: Cleaning a Text Sample\n",
                                    "\n",
                                    "Let's break down the text-cleaning pipeline into smaller steps to better understand each aspect of the process. Consider the following example text:\n",
                                    "\n",
                                    "```text\n",
                                    "\"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "```\n",
                                    "\n",
                                    "This text contains various elements that we need to clean and normalize, such as email addresses, URLs, special characters, and misspellings.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "### Step 1: Remove URLs, Emails, and Unwanted Characters\n",
                                    "\n",
                                    "In this step, we will use regular expressions to remove URLs, email addresses, special characters, numbers, and punctuation from the text. The `re.sub` function from the `re` library is a powerful tool for this task. It allows us to search for specific patterns in the text and replace them with a desired string, which in this case is an empty string to remove the unwanted elements.\n",
                                    "\n",
                                    "Here's a deeper explanation of how `re.sub` works:\n",
                                    "\n",
                                    "  * **Pattern**: The first argument to `re.sub` is the pattern we want to search for. This pattern is defined using regular expressions, which are sequences of characters that form a search pattern. For example, `r'http\\S+'` matches any substring that starts with \"http\" followed by any non-whitespace characters, effectively capturing URLs.\n",
                                    "  * **Replacement**: The second argument is the replacement string. In our case, we use an empty string `''` to remove the matched patterns from the text.\n",
                                    "  * **String**: The third argument is the input string where the search and replace operation will be performed.\n",
                                    "\n",
                                    "Let's apply this to our example text:\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "\n",
                                    "# Example text\n",
                                    "text_sample = \"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "\n",
                                    "# Remove URLs and emails\n",
                                    "text_sample = re.sub(r'http\\S+|www\\S+|[\\w.-]+@[\\w.-]+', '', text_sample)\n",
                                    "\n",
                                    "# Remove special characters, numbers, and punctuation\n",
                                    "text_sample = re.sub(r'[^a-zA-Z\\s]', '', text_sample)\n",
                                    "\n",
                                    "# Output after Step 1\n",
                                    "print(\"After Removing URLs, Emails, and Unwanted Characters:\", text_sample)\n",
                                    "```\n",
                                    "\n",
                                    "**Explanation of Patterns**:\n",
                                    "\n",
                                    "  * `r'http\\S+|www\\S+|[\\w.-]+@[\\w.-]+'`: This pattern matches URLs and email addresses.\n",
                                    "      * `http\\S+` matches any URL starting with \"http\" followed by non-whitespace characters.\n",
                                    "      * `www\\S+` matches URLs starting with \"www\".\n",
                                    "      * `[\\w.-]+@[\\w.-]+` matches email addresses by looking for a sequence of word characters, dots, or hyphens followed by an \"@\" symbol and another sequence of word characters, dots, or hyphens.\n",
                                    "  * `r'[^a-zA-Z\\s]'`: This pattern matches any character that is not a letter (both uppercase and lowercase) or a whitespace. The `^` inside the square brackets negates the character class, so it matches anything that is not a letter or space, effectively removing special characters, numbers, and punctuation.\n",
                                    "\n",
                                    "**Output**:\n",
                                    "\n",
                                    "```text\n",
                                    "After Removing URLs, Emails, and Unwanted Characters: Hello Email me at  Visit  Natural language processing is amzing\n",
                                    "```\n",
                                    "\n",
                                    "By using `re.sub`, we efficiently clean the text by removing unwanted elements, leaving behind only the relevant words for further processing.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "### Step 2: Normalize Unicode and Convert to Lowercase\n",
                                    "\n",
                                    "Next, we normalize the Unicode text to ensure consistent character representation and convert the text to lowercase to maintain uniformity:\n",
                                    "\n",
                                    "```python\n",
                                    "import unicodedata\n",
                                    "\n",
                                    "# Normalize Unicode\n",
                                    "text_sample = unicodedata.normalize(\"NFKC\", text_sample)\n",
                                    "\n",
                                    "# Convert to lowercase\n",
                                    "text_sample = text_sample.lower()\n",
                                    "\n",
                                    "# Output after Step 2\n",
                                    "print(\"After Unicode Normalization and Lowercasing:\", text_sample)\n",
                                    "```\n",
                                    "\n",
                                    "**Output**:\n",
                                    "\n",
                                    "```text\n",
                                    "After Unicode Normalization and Lowercasing: hello email me at  visit  natural language processing is amzing\n",
                                    "```\n",
                                    "\n",
                                    "Unicode normalization helps in handling characters from different languages and scripts consistently. Lowercasing ensures that words are treated the same regardless of their case.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "### Step 3: Remove Stopwords, Correct Misspellings, and Apply Stemming/Lemmatization\n",
                                    "\n",
                                    "Finally, we remove stopwords, correct any misspellings, and apply stemming or lemmatization to clean the text further:\n",
                                    "\n",
                                    "```python\n",
                                    "import nltk\n",
                                    "from nltk.corpus import stopwords\n",
                                    "from autocorrect import Speller\n",
                                    "from nltk.stem import PorterStemmer\n",
                                    "from nltk.stem import WordNetLemmatizer\n",
                                    "\n",
                                    "# Download stopwords and WordNet data\n",
                                    "nltk.download('stopwords')\n",
                                    "nltk.download('wordnet')\n",
                                    "\n",
                                    "# Initialize spell checker, stopwords, stemmer, and lemmatizer\n",
                                    "spell = Speller(lang='en')\n",
                                    "stop_words = set(stopwords.words('english'))\n",
                                    "stemmer = PorterStemmer()\n",
                                    "lemmatizer = WordNetLemmatizer()\n",
                                    "\n",
                                    "# Remove stopwords, spell-check, and apply stemming/lemmatization\n",
                                    "words = text_sample.split()\n",
                                    "words = [spell(word) for word in words if word not in stop_words]\n",
                                    "stemmed_words = [stemmer.stem(word) for word in words]\n",
                                    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
                                    "\n",
                                    "# Output after Step 3\n",
                                    "print(\"Cleaned Text with Stemming:\", \" \".join(stemmed_words))\n",
                                    "print(\"Cleaned Text with Lemmatization:\", \" \".join(lemmatized_words))\n",
                                    "```\n",
                                    "\n",
                                    "**Output**:\n",
                                    "\n",
                                    "```text\n",
                                    "Cleaned Text with Stemming: hello email visit natur languag process amaz\n",
                                    "Cleaned Text with Lemmatization: hello email visit natural language process amaze\n",
                                    "```\n",
                                    "\n",
                                    "Note that the misspelling \"amzing\" has been autocorrected to \"amazing\" during this step. The spell checker helps in ensuring that the text data is clean and accurate by fixing such errors. Additionally, stemming reduces \"amazing\" to \"amaz\" while lemmatization changes it to \"amaze,\" further normalizing the text.\n",
                                    "\n",
                                    "By breaking down the process into these steps, you can focus on each aspect of text cleaning and normalization, making it easier to understand and apply these techniques in practice.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Summary and Next Steps\n",
                                    "\n",
                                    "In this lesson, we covered the foundational techniques for cleaning and normalizing text data. We explored how to set up the environment, remove unwanted elements, normalize text, handle stopwords and misspellings, and apply stemming and lemmatization. These preprocessing steps are crucial for preparing text data for NLP models, as they help in reducing noise and ensuring consistency. As you move on to the practice exercises, apply these techniques to clean and prepare text data effectively. This will set a strong foundation for more advanced NLP tasks in the subsequent lessons."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Text Cleaning with Regular Expressions\n",
                                    "\n",
                                    "You've just learned about removing unwanted text elements using regular expressions. Now, let's put that knowledge into practice!\n",
                                    "\n",
                                    "Your task is to write a function that cleans a given text by removing:\n",
                                    "\n",
                                    "URLs\n",
                                    "Email addresses\n",
                                    "Special characters\n",
                                    "Punctuation\n",
                                    "Digits\n",
                                    "This will help you understand how to handle these specific types of noise early in the text-cleaning pipeline. Dive in and see how clean you can make the text!\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "\n",
                                    "def clean_text(text):\n",
                                    "    # TODO: Remove URLs and emails\n",
                                    "    # TODO: Remove special characters, numbers, and punctuation\n",
                                    "    return text\n",
                                    "\n",
                                    "# Example usage\n",
                                    "text_sample = \"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "\n",
                                    "cleaned_text = clean_text(text_sample)\n",
                                    "print(\"Cleaned Text:\", cleaned_text)\n",
                                    "```\n",
                                    "\n",
                                    "### Step 1: Remove URLs and Emails\n",
                                    "\n",
                                    "In the `clean_text` function, your first task is to remove URLs and email addresses. You can achieve this using the `re.sub()` function with the same regular expression patterns we discussed in the lesson.\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "\n",
                                    "def clean_text(text):\n",
                                    "    # TODO: Remove URLs and emails\n",
                                    "    text = re.sub(r'http\\S+|www\\S+|[\\w.-]+@[\\w.-]+', '', text)\n",
                                    "    # TODO: Remove special characters, numbers, and punctuation\n",
                                    "    return text\n",
                                    "\n",
                                    "# Example usage\n",
                                    "text_sample = \"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "\n",
                                    "cleaned_text = clean_text(text_sample)\n",
                                    "print(\"After removing URLs and emails:\", cleaned_text)\n",
                                    "```\n",
                                    "\n",
                                    "The output of this step will be: `After removing URLs and emails: Hello! Email me at . Visit . Natural language processing is amzing! 😊`\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "### Step 2: Remove Special Characters, Numbers, and Punctuation\n",
                                    "\n",
                                    "Now, add the second part of the solution to the `clean_text` function to remove special characters, numbers, and punctuation.\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "\n",
                                    "def clean_text(text):\n",
                                    "    # Remove URLs and emails\n",
                                    "    text = re.sub(r'http\\S+|www\\S+|[\\w.-]+@[\\w.-]+', '', text)\n",
                                    "    # TODO: Remove special characters, numbers, and punctuation\n",
                                    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
                                    "    return text\n",
                                    "\n",
                                    "# Example usage\n",
                                    "text_sample = \"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "\n",
                                    "cleaned_text = clean_text(text_sample)\n",
                                    "print(\"Final Cleaned Text:\", cleaned_text)\n",
                                    "```\n",
                                    "\n",
                                    "The output after completing the function will be: ` Final Cleaned Text: Hello Email me at  Visit  Natural language processing is amzing  `\n",
                                    "\n",
                                    "This shows how you can chain `re.sub()` calls to perform multiple cleaning operations on a single string, effectively preparing it for the next stages of NLP preprocessing."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Text Normalization in Action\n",
                                    "\n",
                                    "Nice job on learning about text normalization! Now, let's enhance your skills by applying what you've learned.\n",
                                    "\n",
                                    "Your task is to improve the text processing pipeline by:\n",
                                    "\n",
                                    "Converting all text to lowercase.\n",
                                    "Using unicodedata.normalize() to ensure proper Unicode representation.\n",
                                    "This will help you ensure that visually identical but differently encoded characters match after normalization. Give it a try and see the difference it makes!\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "import unicodedata\n",
                                    "\n",
                                    "# Example text\n",
                                    "text_sample = \"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "\n",
                                    "def clean_text(text):\n",
                                    "    # Remove URLs and emails\n",
                                    "    text = re.sub(r'http\\S+|www\\S+|[\\w.-]+@[\\w.-]+', '', text)\n",
                                    "    # Remove special characters, numbers, and punctuation\n",
                                    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
                                    "\n",
                                    "    # TODO: Convert to lowercase\n",
                                    "    # TODO: Normalize Unicode\n",
                                    "\n",
                                    "    return text\n",
                                    "\n",
                                    "# Clean the text using the function\n",
                                    "cleaned_text = clean_text(text_sample)\n",
                                    "\n",
                                    "# Print cleaned text\n",
                                    "print(\"Cleaned Text:\", cleaned_text)\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Hello\\!\n",
                                    "\n",
                                    "That's a great task to practice your text cleaning skills. The existing `clean_text` function already handles removing URLs, emails, and special characters. Your goal is to add the final touches by converting the text to lowercase and normalizing the Unicode.\n",
                                    "\n",
                                    "Here's how you can complete the code by adding those two steps. You can place them right before the `return` statement.\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "import unicodedata\n",
                                    "\n",
                                    "# Example text\n",
                                    "text_sample = \"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "\n",
                                    "def clean_text(text):\n",
                                    "    # Remove URLs and emails\n",
                                    "    text = re.sub(r'http\\S+|www\\S+|[\\w.-]+@[\\w.-]+', '', text)\n",
                                    "    # Remove special characters, numbers, and punctuation\n",
                                    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
                                    "    \n",
                                    "    # Convert to lowercase\n",
                                    "    text = text.lower()\n",
                                    "    \n",
                                    "    # Normalize Unicode\n",
                                    "    text = unicodedata.normalize('NFKC', text)\n",
                                    "    \n",
                                    "    return text\n",
                                    "\n",
                                    "# Clean the text using the function\n",
                                    "cleaned_text = clean_text(text_sample)\n",
                                    "\n",
                                    "# Print cleaned text\n",
                                    "print(\"Original Text:\", text_sample)\n",
                                    "print(\"Cleaned Text:\", cleaned_text)\n",
                                    "```\n",
                                    "\n",
                                    "**Explanation of the changes:**\n",
                                    "\n",
                                    "  * `text = text.lower()`: This line converts the entire string to lowercase. This is a fundamental step in text normalization to ensure that words like \"Hello\" and \"hello\" are treated as the same word.\n",
                                    "  * `text = unicodedata.normalize('NFKC', text)`: This line performs Unicode normalization. `NFKC` (Normalization Form Compatibility Composition) is a common form used for text processing because it handles a wide range of characters, including converting compatibility characters (like Roman numerals, superscript numbers, and ligatures) into their standard form. This helps in ensuring consistency, for example, converting `ﬁ` to `fi`.\n",
                                    "\n",
                                    "By adding these two lines, your `clean_text` function will be more robust and ready for further natural language processing tasks."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Refine Your Text Cleaning Skills\n",
                                    "\n",
                                    "Well done on mastering text normalization! Now, let's take it a step further by incorporating stopwords removal into your text-cleaning pipeline.\n",
                                    "\n",
                                    "Your task is to enhance the pipeline by:\n",
                                    "\n",
                                    "Using the NLTK stopwords list to filter out common, insignificant words.\n",
                                    "Ensuring the text is spell-checked after the stopwords are removed.\n",
                                    "This will refine your text data, making it more meaningful for NLP tasks. Dive in and see how much cleaner your text becomes!\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "import unicodedata\n",
                                    "import nltk\n",
                                    "from nltk.corpus import stopwords\n",
                                    "from autocorrect import Speller\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "\n",
                                    "# Initialize spell checker and stopwords\n",
                                    "spell = Speller(lang='en')\n",
                                    "stop_words = set(stopwords.words('english'))\n",
                                    "\n",
                                    "def clean_text(text):\n",
                                    "    # Normalize Unicode\n",
                                    "    text = unicodedata.normalize(\"NFKC\", text)\n",
                                    "    # Convert to lowercase\n",
                                    "    text = text.lower()\n",
                                    "    # Remove URLs and emails\n",
                                    "    text = re.sub(r'http\\S+|www\\S+|[\\w.-]+@[\\w.-]+', '', text)\n",
                                    "    # Remove special characters, numbers, and punctuation\n",
                                    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
                                    "    \n",
                                    "    # TODO: Split the text into words\n",
                                    "    # TODO: Remove stopwords from the list of words and spell-check each word\n",
                                    "    # TODO: Join the words back into a single string\n",
                                    "    \n",
                                    "    return text\n",
                                    "# Example text\n",
                                    "text_sample = \"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "\n",
                                    "# Print cleaned text\n",
                                    "print(\"Cleaned Text:\", clean_text(text_sample))\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Excellent\\! Incorporating stopwords removal and spell-checking will significantly improve the quality of your text data for any subsequent NLP analysis.\n",
                                    "\n",
                                    "Here is the completed code with the new steps integrated into the `clean_text` function.\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "import unicodedata\n",
                                    "import nltk\n",
                                    "from nltk.corpus import stopwords\n",
                                    "from autocorrect import Speller\n",
                                    "nltk.download('stopwords', quiet=True)\n",
                                    "\n",
                                    "# Initialize spell checker and stopwords\n",
                                    "spell = Speller(lang='en')\n",
                                    "stop_words = set(stopwords.words('english'))\n",
                                    "\n",
                                    "def clean_text(text):\n",
                                    "    # Normalize Unicode\n",
                                    "    text = unicodedata.normalize(\"NFKC\", text)\n",
                                    "    # Convert to lowercase\n",
                                    "    text = text.lower()\n",
                                    "    # Remove URLs and emails\n",
                                    "    text = re.sub(r'http\\S+|www\\S+|[\\w.-]+@[\\w.-]+', '', text)\n",
                                    "    # Remove special characters, numbers, and punctuation\n",
                                    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
                                    "    \n",
                                    "    # Split the text into words\n",
                                    "    words = text.split()\n",
                                    "\n",
                                    "    # Remove stopwords and spell-check each word\n",
                                    "    cleaned_words = [spell(word) for word in words if word not in stop_words]\n",
                                    "\n",
                                    "    # Join the words back into a single string\n",
                                    "    text = \" \".join(cleaned_words)\n",
                                    "    \n",
                                    "    return text\n",
                                    "\n",
                                    "# Example text\n",
                                    "text_sample = \"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "\n",
                                    "# Print cleaned text\n",
                                    "print(\"Original Text:\", text_sample)\n",
                                    "print(\"Cleaned Text:\", clean_text(text_sample))\n",
                                    "```\n",
                                    "\n",
                                    "**Explanation of the new code:**\n",
                                    "\n",
                                    "1.  **`words = text.split()`**: This line splits the input string into a list of individual words, making it easy to iterate over them.\n",
                                    "2.  **`cleaned_words = [spell(word) for word in words if word not in stop_words]`**: This is a powerful list comprehension that performs two key tasks simultaneously:\n",
                                    "      * `if word not in stop_words`: It filters out any word that is present in the NLTK stopwords set.\n",
                                    "      * `spell(word)`: For every word that is not a stopword, it applies the `autocorrect` spell checker.\n",
                                    "3.  **`text = \" \".join(cleaned_words)`**: Finally, this line joins the list of `cleaned_words` back into a single string, with each word separated by a space.\n",
                                    "\n",
                                    "When you run this code, you'll see a much cleaner output that is free of irrelevant words and contains a correctly spelled word (\"amazing\" instead of \"amzing\")."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Stemming vs Lemmatization Showdown\n",
                                    "\n",
                                    "You've done a fantastic job learning about text cleaning and normalization! Now, let's dive deeper into understanding stemming and lemmatization.\n",
                                    "\n",
                                    "Your task is to compare these two techniques by:\n",
                                    "\n",
                                    "Applying both stemming and lemmatization to a sample text.\n",
                                    "Examining the differences in the resulting words.\n",
                                    "This exercise will help you see the strengths and weaknesses of each approach in terms of accuracy and context preservation. Let's get started and see how these techniques transform your text!\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "import unicodedata\n",
                                    "import nltk\n",
                                    "from nltk.corpus import stopwords\n",
                                    "from autocorrect import Speller\n",
                                    "from nltk.stem import PorterStemmer\n",
                                    "from nltk.stem import WordNetLemmatizer\n",
                                    "\n",
                                    "# Download necessary NLTK data\n",
                                    "nltk.download('stopwords', quiet = True)\n",
                                    "nltk.download('wordnet', quiet = True)\n",
                                    "\n",
                                    "# Initialize spell checker and stopwords\n",
                                    "spell = Speller(lang='en')\n",
                                    "stop_words = set(stopwords.words('english'))\n",
                                    "\n",
                                    "# Example text\n",
                                    "text_sample = \"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "\n",
                                    "# Remove URLs and emails\n",
                                    "text_sample = re.sub(r'http\\S+|www\\S+|[\\w.-]+@[\\w.-]+', '', text_sample)\n",
                                    "# Remove special characters, numbers, and punctuation\n",
                                    "text_sample = re.sub(r'[^a-zA-Z\\s]', '', text_sample)\n",
                                    "# Normalize Unicode\n",
                                    "text_sample = unicodedata.normalize(\"NFKC\", text_sample)\n",
                                    "# Convert to lowercase\n",
                                    "text_sample = text_sample.lower()\n",
                                    "# Remove stopwords and spell-check\n",
                                    "words = text_sample.split()\n",
                                    "words = [spell(word) for word in words if word not in stop_words]\n",
                                    "text_sample = \" \".join(words)\n",
                                    "\n",
                                    "# Initialize stemmer and lemmatizer\n",
                                    "stemmer = PorterStemmer()\n",
                                    "lemmatizer = WordNetLemmatizer()\n",
                                    "\n",
                                    "# TODO: Apply stemming to the words\n",
                                    "\n",
                                    "# TODO: Apply lemmatization to the words\n",
                                    "\n",
                                    "# Print results\n",
                                    "print(\"Original Words:\", words)\n",
                                    "print(\"Stemmed Words:\", _____)\n",
                                    "print(\"Lemmatized Words:\", _____)\n",
                                    "```\n",
                                    "\n",
                                    "Excellent\\! This is an excellent exercise to highlight the key differences between these two common techniques. Stemming is a more aggressive, rule-based approach, while lemmatization is more conservative and relies on a dictionary to find the root form.\n",
                                    "\n",
                                    "Here is the completed code that applies both stemming and lemmatization to the pre-processed text, allowing you to compare the results directly.\n",
                                    "\n",
                                    "```python\n",
                                    "import re\n",
                                    "import unicodedata\n",
                                    "import nltk\n",
                                    "from nltk.corpus import stopwords\n",
                                    "from autocorrect import Speller\n",
                                    "from nltk.stem import PorterStemmer\n",
                                    "from nltk.stem import WordNetLemmatizer\n",
                                    "\n",
                                    "# Download necessary NLTK data\n",
                                    "nltk.download('stopwords', quiet = True)\n",
                                    "nltk.download('wordnet', quiet = True)\n",
                                    "\n",
                                    "# Initialize spell checker and stopwords\n",
                                    "spell = Speller(lang='en')\n",
                                    "stop_words = set(stopwords.words('english'))\n",
                                    "\n",
                                    "# Example text\n",
                                    "text_sample = \"Hello! Email me at example@mail.com. Visit https://example.com. Natural language processing is amzing! 😊\"\n",
                                    "\n",
                                    "# Remove URLs and emails\n",
                                    "text_sample = re.sub(r'http\\S+|www\\S+|[\\w.-]+@[\\w.-]+', '', text_sample)\n",
                                    "# Remove special characters, numbers, and punctuation\n",
                                    "text_sample = re.sub(r'[^a-zA-Z\\s]', '', text_sample)\n",
                                    "# Normalize Unicode\n",
                                    "text_sample = unicodedata.normalize(\"NFKC\", text_sample)\n",
                                    "# Convert to lowercase\n",
                                    "text_sample = text_sample.lower()\n",
                                    "# Remove stopwords and spell-check\n",
                                    "words = text_sample.split()\n",
                                    "words = [spell(word) for word in words if word not in stop_words]\n",
                                    "text_sample = \" \".join(words)\n",
                                    "\n",
                                    "# Initialize stemmer and lemmatizer\n",
                                    "stemmer = PorterStemmer()\n",
                                    "lemmatizer = WordNetLemmatizer()\n",
                                    "\n",
                                    "# Apply stemming to the words\n",
                                    "stemmed_words = [stemmer.stem(word) for word in words]\n",
                                    "\n",
                                    "# Apply lemmatization to the words\n",
                                    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
                                    "\n",
                                    "# Print results\n",
                                    "print(\"Original Words:\", words)\n",
                                    "print(\"Stemmed Words:\", stemmed_words)\n",
                                    "print(\"Lemmatized Words:\", lemmatized_words)\n",
                                    "```\n",
                                    "\n",
                                    "**What the Output Shows:**\n",
                                    "\n",
                                    "  * **Original Words**: `['hello', 'natural', 'language', 'processing', 'amazing']`\n",
                                    "      * This is the list of words after cleaning, stopword removal, and spell-checking.\n",
                                    "  * **Stemmed Words**: `['hello', 'natur', 'languag', 'process', 'amaz']`\n",
                                    "      * The stemmer applies a series of simple rules, often resulting in word fragments that are not actual dictionary words (e.g., `natur`, `languag`). This is because it truncates suffixes without considering the word's meaning.\n",
                                    "  * **Lemmatized Words**: `['hello', 'natural', 'language', 'processing', 'amazing']`\n",
                                    "      * The lemmatizer uses a dictionary to find the canonical base form (lemma) of the word. In this case, `processing` is kept as is because the default part of speech is a noun. Similarly, `amazing` is a verb form, but the lemmatizer defaults to a noun form, so it is unchanged. This output shows how lemmatization aims to return a valid word, unlike stemming.\n",
                                    "\n",
                                    "This example clearly demonstrates the trade-off: stemming is faster and simpler but can produce non-dictionary words, while lemmatization is more accurate and context-aware, returning a valid word from a dictionary."
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
