{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 4 Introduction to Word Embeddings\n",
                                    "\n",
                                    "# Introduction to Word Embeddings\n",
                                    "\n",
                                    "Welcome to the final lesson of the **\"Foundations of NLP Data Processing\"** course\\! So far, you've explored various techniques for representing text as numerical data, including **Bag-of-Words** and **TF-IDF**. While these methods are useful, they have a major limitation: they treat words as independent tokens without capturing their meanings or relationships.\n",
                                    "\n",
                                    "In this lesson, we introduce **word embeddings**, a powerful approach to representing words as continuous-valued vectors that encode semantic meaning. Word embeddings allow models to understand **relationships between words** based on their context in large text corpora. Unlike traditional text representations, embeddings can capture **synonymy, analogies, and contextual similarities**.\n",
                                    "\n",
                                    "## Why Do We Need Word Embeddings?\n",
                                    "\n",
                                    "Before diving into code, let's build an **intuition** for word embeddings by examining some limitations of traditional approaches:\n",
                                    "\n",
                                    "### Bag-of-Words (BoW) and TF-IDF ignore meaning\n",
                                    "\n",
                                    "In BoW and TF-IDF, words are represented as **isolated** units. Two words with similar meanings (e.g., \"king\" and \"queen\") have no direct relationship in these models.\n",
                                    "\n",
                                    "**Example:**\n",
                                    "\"I love NLP\" → `[1, 1, 1, 0, 0]`\n",
                                    "\"I enjoy NLP\" → `[1, 0, 1, 1, 0]`\n",
                                    "These two sentences are similar in meaning, but their vector representations don't capture this\\!\n",
                                    "\n",
                                    "### Word Order Matters\n",
                                    "\n",
                                    "\"The cat chased the dog\" vs. \"The dog chased the cat\" have different meanings, but traditional models treat them similarly.\n",
                                    "\n",
                                    "### No Concept of Context\n",
                                    "\n",
                                    "\"Apple\" (the fruit) and \"Apple\" (the company) are treated the same.\n",
                                    "\n",
                                    "Word embeddings solve these issues by representing words as **dense vectors** in a multi-dimensional space, where words with similar meanings have closer representations.\n",
                                    "\n",
                                    "## Understanding Word Embeddings: The Core Idea\n",
                                    "\n",
                                    "Word embeddings are generated by training models on large text data. The key idea is:\n",
                                    "\n",
                                    "> Words appearing in similar contexts should have similar vector representations.\n",
                                    "\n",
                                    "**Example:** The words \"king\" and \"queen\" often appear in similar contexts (e.g., \"the king rules the kingdom,\" \"the queen rules the kingdom\").\n",
                                    "Models like Word2Vec and GloVe learn to place these words **closer** together in the vector space.\n",
                                    "\n",
                                    "There are different approaches to training word embeddings:\n",
                                    "\n",
                                    "  * **Continuous Bag of Words (CBOW):** Predicts a word based on surrounding words.\n",
                                    "  * **Skip-gram:** Predicts surrounding words given a target word.\n",
                                    "  * **GloVe (Global Vectors for Word Representation):** Utilizes word co-occurrence statistics from a corpus.\n",
                                    "\n",
                                    "## Differences Between Word2Vec and GloVe\n",
                                    "\n",
                                    "  * **Word2Vec:** Developed by Google, it uses either the CBOW or Skip-gram approach. It focuses on predicting a word based on its context or vice versa, which makes it efficient for capturing local context.\n",
                                    "  * **GloVe:** Developed by Stanford, it uses global word co-occurrence statistics to learn embeddings. This approach captures both local and global context, making it effective for understanding broader semantic relationships.\n",
                                    "\n",
                                    "## Using Pre-trained Word Embeddings\n",
                                    "\n",
                                    "Instead of training from scratch, we can use **pre-trained embeddings**. Here, we'll use a smaller GloVe model (25-dimensional) for demonstration, but you can easily switch to other models like Word2Vec or FastText depending on your needs. If you want to suppress the output during model loading, you can use the following approach:\n",
                                    "\n",
                                    "```python\n",
                                    "import os\n",
                                    "import contextlib\n",
                                    "import gensim.downloader as api\n",
                                    "\n",
                                    "# Load a smaller GloVe model (25-dimensional) without printing output\n",
                                    "with open(os.devnull, 'w') as fnull:\n",
                                    "    with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
                                    "        pretrained_model = api.load(\"glove-twitter-25\")  # or any other model\n",
                                    "\n",
                                    "# Find similar words\n",
                                    "print(\"Most similar to 'apple':\", pretrained_model.most_similar(\"apple\"))\n",
                                    "\n",
                                    "# Compute similarity\n",
                                    "similarity = pretrained_model.similarity(\"queen\", \"king\")\n",
                                    "print(\"Similarity between 'queen' and 'king':\", similarity)\n",
                                    "\n",
                                    "# Perform analogy task\n",
                                    "result = pretrained_model.most_similar(positive=['dog', 'cats'], negative=['cat'])\n",
                                    "print(\"Result of analogy 'cat' is to 'cats' as 'dog' is to:\", result[0][0])\n",
                                    "```\n",
                                    "\n",
                                    "To suppress the output during model loading, you need to import the `os` and `contextlib` modules. The `os.devnull` is used to discard any output, while `contextlib.redirect_stdout` and `contextlib.redirect_stderr` are used to redirect standard output and error streams to `os.devnull`. This way, the model loads silently without printing any messages to the console.\n",
                                    "\n",
                                    "## When to Train a Custom Word Embedding Model\n",
                                    "\n",
                                    "While pre-trained models are powerful, there are scenarios where training a custom word embedding model might be beneficial:\n",
                                    "\n",
                                    "  * **Domain-Specific Vocabulary:** If your text data contains specialized vocabulary not well-represented in general corpora, a custom model can better capture these nuances.\n",
                                    "  * **Language Variants:** For dialects or less common languages, pre-trained models may not be available or effective.\n",
                                    "  * **Updated Contexts:** If your data reflects recent trends or changes in language use, a custom model can capture these shifts.\n",
                                    "\n",
                                    "## Visualizing Word Embeddings\n",
                                    "\n",
                                    "To understand the learned embeddings, we can visualize them using **Principal Component Analysis** (PCA), which reduces the high-dimensional vectors to 2D space:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "from sklearn.decomposition import PCA\n",
                                    "\n",
                                    "words = [\"dog\", \"cat\", \"computer\", \"town\", \"city\"]\n",
                                    "\n",
                                    "# Get Word Vectors\n",
                                    "vectors = np.array([pretrained_model.get_vector(word) for word in words if word in pretrained_model])\n",
                                    "\n",
                                    "# Reduce Dimensionality\n",
                                    "pca = PCA(n_components=2)\n",
                                    "reduced_vectors = pca.fit_transform(vectors)\n",
                                    "\n",
                                    "# Plot\n",
                                    "plt.figure(figsize=(6,6))\n",
                                    "for word, (x, y) in zip(words, reduced_vectors):\n",
                                    "    plt.scatter(x, y)\n",
                                    "    plt.text(x + 0.01, y + 0.01, word, fontsize=12)\n",
                                    "plt.title(\"Word Embeddings Visualization (PCA)\")\n",
                                    "plt.show()\n",
                                    "```\n",
                                    "\n",
                                    "This visualization helps us see how similar words are **grouped together** in the vector space. For instance, we can see that \"cat\" and \"dog\" are close to each other, as are \"city\" and \"town,\" while \"computer\" is positioned further away, indicating its distinct semantic meaning compared to the other words.\n",
                                    "\n",
                                    "## Summary and Next Steps\n",
                                    "\n",
                                    "Word embeddings capture the meaning of words based on their context, with models like Word2Vec and GloVe learning these embeddings by predicting words from their neighbors or using co-occurrence statistics. Pre-trained embeddings offer efficient word representation, and visualization aids in understanding word relationships. Moving forward, experiment with different datasets, explore models like GloVe and FastText, and apply word embeddings in NLP tasks such as sentiment analysis and text classification.\n",
                                    "\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Exploring Word Similarity with GloVe\n",
                                    "\n",
                                    "You've learned about using pre-trained word embeddings. Now, let's put that into practice!\n",
                                    "\n",
                                    "Your task is to load a smaller GloVe model using the gensim.downloader library. Then, choose two pairs of words: one pair with similar meanings and another with unrelated meanings.\n",
                                    "\n",
                                    "Use the model's similarity method to print their similarity scores.\n",
                                    "\n",
                                    "```python\n",
                                    "import gensim.downloader as api\n",
                                    "import contextlib\n",
                                    "import os\n",
                                    "\n",
                                    "# Suppress output while loading the model\n",
                                    "with open(os.devnull, 'w') as fnull:\n",
                                    "    with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
                                    "        # TODO: Load a smaller GloVe model (e.g., \"glove-wiki-gigaword-25\") \n",
                                    "        pretrained_model = api.load(\"\")  # or any other model\n",
                                    "\n",
                                    "\n",
                                    "# TODO: Choose word pairs\n",
                                    "similar_pair = (\"\", \"\")\n",
                                    "unrelated_pair = (\"\", \"\")\n",
                                    "\n",
                                    "# TODO: Compute similarity scores\n",
                                    "\n",
                                    "# TODO: Print similarity scores\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import gensim.downloader as api\n",
                                    "import contextlib\n",
                                    "import os\n",
                                    "\n",
                                    "# Suppress output while loading the model\n",
                                    "with open(os.devnull, 'w') as fnull:\n",
                                    "    with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
                                    "        # Load a smaller GloVe model (e.g., \"glove-wiki-gigaword-50\") \n",
                                    "        pretrained_model = api.load(\"glove-wiki-gigaword-50\")\n",
                                    "\n",
                                    "# Choose word pairs\n",
                                    "similar_pair = (\"king\", \"queen\")\n",
                                    "unrelated_pair = (\"apple\", \"car\")\n",
                                    "\n",
                                    "# Compute similarity scores\n",
                                    "similar_score = pretrained_model.similarity(similar_pair[0], similar_pair[1])\n",
                                    "unrelated_score = pretrained_model.similarity(unrelated_pair[0], unrelated_pair[1])\n",
                                    "\n",
                                    "# Print similarity scores\n",
                                    "print(f\"Similarity between '{similar_pair[0]}' and '{similar_pair[1]}': {similar_score:.4f}\")\n",
                                    "print(f\"Similarity between '{unrelated_pair[0]}' and '{unrelated_pair[1]}': {unrelated_score:.4f}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Analysis of the Results\n",
                                    "\n",
                                    "The output demonstrates how word embeddings capture semantic relationships. The similarity score for the pair (\"king\", \"queen\") will be significantly higher than the score for the pair (\"apple\", \"car\"). This is because \"king\" and \"queen\" often appear in similar contexts and share a conceptual relationship, so the model places them closer together in the vector space. In contrast, \"apple\" and \"car\" have very different meanings and contexts, resulting in a much lower similarity score. This highlights the effectiveness of word embeddings in representing the meaning of words beyond simple co-occurrence."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Exploring Word Synonyms with Embeddings\n",
                                    "\n",
                                    "You've just explored the power of pre-trained word embeddings. Now, let's dive deeper!\n",
                                    "\n",
                                    "Your task is to choose at least three distinct words (e.g., \"cat\", \"computer\", \"city\") and use the pre-trained model's most_similar function to find and print the top five synonyms for each word.\n",
                                    "\n",
                                    "Load the pre-trained model.\n",
                                    "Select your words.\n",
                                    "Use most_similar to find synonyms.\n",
                                    "Print the top five synonyms for each word.\n",
                                    "Good luck!\n",
                                    "\n",
                                    "```python\n",
                                    "import gensim.downloader as api\n",
                                    "import contextlib\n",
                                    "import os\n",
                                    "\n",
                                    "# Suppress output while loading the model\n",
                                    "with open(os.devnull, 'w') as fnull:\n",
                                    "    with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
                                    "        pretrained_model = api.load(\"glove-twitter-25\")  # or any other model\n",
                                    "\n",
                                    "# Choose words to find synonyms for \n",
                                    "words_to_check = [\"cat\", \"computer\", \"city\"]\n",
                                    "\n",
                                    "# TODO: Find and print top 5 synonyms for each word\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import gensim.downloader as api\n",
                                    "import contextlib\n",
                                    "import os\n",
                                    "\n",
                                    "# Suppress output while loading the model\n",
                                    "with open(os.devnull, 'w') as fnull:\n",
                                    "    with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
                                    "        pretrained_model = api.load(\"glove-twitter-25\")  # or any other model\n",
                                    "\n",
                                    "# Choose words to find synonyms for \n",
                                    "words_to_check = [\"cat\", \"computer\", \"city\"]\n",
                                    "\n",
                                    "# Find and print top 5 synonyms for each word\n",
                                    "for word in words_to_check:\n",
                                    "    print(f\"Top 5 most similar words to '{word}':\")\n",
                                    "    try:\n",
                                    "        similar_words = pretrained_model.most_similar(word, topn=5)\n",
                                    "        for i, (similar_word, score) in enumerate(similar_words):\n",
                                    "            print(f\"{i+1}. {similar_word} (Similarity: {score:.4f})\")\n",
                                    "    except KeyError:\n",
                                    "        print(f\"'{word}' not found in the vocabulary.\")\n",
                                    "    print(\"-\" * 30)\n",
                                    "```\n",
                                    "\n",
                                    "### Analysis of the Results\n",
                                    "\n",
                                    "The code demonstrates how the `most_similar` method uses the vector representations of words to find other words that are semantically close in the embedding space.\n",
                                    "\n",
                                    "  * For the word \"cat,\" the model correctly identifies other animals or related terms like \"dog\" and \"cats,\" showing its ability to capture high-level semantic categories.\n",
                                    "  * For \"computer,\" the model returns words like \"pc,\" \"laptop,\" and \"device,\" which are direct synonyms or related concepts in the context of technology.\n",
                                    "  * For \"city,\" the results will likely include other geographical or political entities like \"town,\" \"chicago,\" and \"new york,\" reflecting the contextual similarity of these words in the training data.\n",
                                    "\n",
                                    "This exercise proves the effectiveness of word embeddings in understanding and representing the nuances of language, a significant improvement over simple frequency-based methods like Bag-of-Words."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Word Analogy with GloVe\n",
                                    "\n",
                                    "You've just learned about the power of pre-trained word embeddings. Now, let's apply that knowledge!\n",
                                    "\n",
                                    "Your task is to create a word analogy using the pre-trained GloVe model. Use the most_similar function to find words similar to the result of \"bad\" minus \"big\" plus \"biggest\".\n",
                                    "\n",
                                    "Use most_similar to perform the analogy.\n",
                                    "Check if the result makes sense in the context of the analogy.\n",
                                    "This exercise will show you how well the model captures word relationships. Dive in and see the magic!\n",
                                    "\n",
                                    "```python\n",
                                    "import gensim.downloader as api\n",
                                    "import contextlib\n",
                                    "import os\n",
                                    "import sys\n",
                                    "\n",
                                    "# Suppress output while loading the model\n",
                                    "with open(os.devnull, 'w') as fnull:\n",
                                    "    with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
                                    "        # TODO: Load Pre-trained GloVe Model (Twitter 25)\n",
                                    "\n",
                                    "# TODO: Use the model's most_similar function to create a word analogy: \"bad\" - \"big\" + \"biggest\"\n",
                                    "\n",
                                    "# TODO: Print the top result\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import gensim.downloader as api\n",
                                    "import contextlib\n",
                                    "import os\n",
                                    "import sys\n",
                                    "\n",
                                    "# Suppress output while loading the model\n",
                                    "with open(os.devnull, 'w') as fnull:\n",
                                    "    with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
                                    "        # Load Pre-trained GloVe Model (Twitter 25)\n",
                                    "        glove_model = api.load(\"glove-twitter-25\")\n",
                                    "\n",
                                    "# Use the model's most_similar function to create a word analogy: \"bad\" - \"big\" + \"biggest\"\n",
                                    "analogy_result = glove_model.most_similar(positive=['bad', 'biggest'], negative=['big'], topn=1)\n",
                                    "\n",
                                    "# Print the top result\n",
                                    "print(f\"Result of analogy 'bad' - 'big' + 'biggest': {analogy_result[0][0]}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Analysis of the Results\n",
                                    "\n",
                                    "The code demonstrates a classic word analogy problem using vector arithmetic in the GloVe embedding space. The operation `vector(\"bad\") - vector(\"big\") + vector(\"biggest\")` attempts to solve the analogy: \"big\" is to \"biggest\" as \"bad\" is to...\n",
                                    "\n",
                                    "The result `analogy_result[0][0]` will be the word \"worst\". This makes perfect sense, as \"biggest\" is the superlative form of \"big,\" and \"worst\" is the superlative form of \"bad.\" This result shows that word embeddings are not just about finding synonyms but also about understanding and encoding complex semantic relationships like analogies and grammatical transformations. This is a powerful feature that goes far beyond what traditional methods like Bag-of-Words and TF-IDF can achieve."
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
