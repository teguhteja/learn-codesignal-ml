{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 2 Bag-of-Words and N-Grams in NLP\n",
                                    "\n",
                                    "Welcome to this lesson on **Bag-of-Words (BoW) and N-Grams**, a foundational technique in **Natural Language Processing (NLP)** for converting text data into numerical representations. Before we move on to more advanced text vectorization methods, understanding BoW is crucial because it provides the basic framework for handling textual data in machine learning models.\n",
                                    "\n",
                                    "In this lesson, you will learn:\n",
                                    "\n",
                                    "  * What the **Bag-of-Words model** is and why it’s useful.\n",
                                    "  * How **n-grams** enhance text representation.\n",
                                    "  * How to implement BoW with **scikit-learn’s CountVectorizer**.\n",
                                    "  * Challenges and best practices for working with BoW.\n",
                                    "\n",
                                    "### What is Bag-of-Words?\n",
                                    "\n",
                                    "The **Bag-of-Words (BoW)** model is a simple yet effective way to represent text data numerically. It converts text into a fixed-length numerical feature vector by counting word occurrences, disregarding grammar and word order.\n",
                                    "\n",
                                    "#### How BoW Works\n",
                                    "\n",
                                    "1.  **Tokenization**: Splitting text into individual words (tokens).\n",
                                    "2.  **Building a Vocabulary**: Creating a list of all unique words in the dataset.\n",
                                    "3.  **Encoding**: Counting how many times each word appears in a document.\n",
                                    "\n",
                                    "#### Example\n",
                                    "\n",
                                    "Consider these three sample documents:\n",
                                    "\n",
                                    "  * \"Machine learning is amazing.\"\n",
                                    "  * \"Bag-of-Words is a fundamental NLP technique.\"\n",
                                    "  * \"NLP models often rely on n-grams.\"\n",
                                    "\n",
                                    "**Step 1: Creating the Vocabulary**\n",
                                    "\n",
                                    "The unique words across all sentences are:\n",
                                    "\n",
                                    "`tokenized = [\"machine\", \"learning\", \"is\", \"amazing\", \"bag-of-words\", \"a\", \"fundamental\", \"nlp\", \"technique\", \"models\", \"often\", \"rely\", \"on\", \"n-grams\"]`\n",
                                    "\n",
                                    "**Step 2: Creating the Frequency Matrix**\n",
                                    "\n",
                                    "Each sentence is represented as a vector:\n",
                                    "\n",
                                    "| | Machine | Learning | Is | Amazing | Bag-of-Words | A | Fundamental | NLP | Technique | Models | Often | Rely | On | N-Grams |\n",
                                    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
                                    "| **Doc 1** | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
                                    "| **Doc 2** | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
                                    "| **Doc 3** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | 1 |\n",
                                    "\n",
                                    "Each row represents a document, and each column represents the occurrence of a word in that document.\n",
                                    "\n",
                                    "### Understanding N-Grams\n",
                                    "\n",
                                    "**N-Grams** are sequences of words that appear together in a document. The basic types include:\n",
                                    "\n",
                                    "  * **Unigrams**: Single words (e.g., \"machine\")\n",
                                    "  * **Bigrams**: Two consecutive words (e.g., \"machine learning\")\n",
                                    "  * **Trigrams**: Three consecutive words (e.g., \"learning is amazing\")\n",
                                    "\n",
                                    "Using n-grams helps capture context better than single-word unigrams. For example, \"not good\" is negative, but in a unigram model, \"not\" and \"good\" would be treated separately.\n",
                                    "\n",
                                    "#### Example of N-Grams\n",
                                    "\n",
                                    "Given the sentence:\n",
                                    "\n",
                                    "\"Natural Language Processing is fascinating.\"\n",
                                    "\n",
                                    "  * **Unigrams**: [\"Natural\", \"Language\", \"Processing\", \"is\", \"fascinating\"]\n",
                                    "  * **Bigrams**: [\"Natural Language\", \"Language Processing\", \"Processing is\", \"is fascinating\"]\n",
                                    "  * **Trigrams**: [\"Natural Language Processing\", \"Language Processing is\", \"Processing is fascinating\"]\n",
                                    "\n",
                                    "By using n-grams, models can capture **phrases** and **contextual meaning** instead of isolated words.\n",
                                    "\n",
                                    "### Implementing Bag-of-Words with Unigrams in Python\n",
                                    "\n",
                                    "Let's now implement BoW using **scikit-learn’s CountVectorizer**, focusing only on **unigrams**.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.feature_extraction.text import CountVectorizer\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "# Example Documents\n",
                                    "docs = [\n",
                                    "    \"Machine learning and NLP are closely related fields.\",\n",
                                    "    \"Bag-of-Words is a common technique in NLP.\",\n",
                                    "    \"NLP models often use n-grams to improve performance.\"\n",
                                    "]\n",
                                    "\n",
                                    "# Initialize CountVectorizer with n-gram range for unigrams only\n",
                                    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english') ## If we don't specify ngram_range=(1,1), it will by default understand it\n",
                                    "bow_matrix = vectorizer.fit_transform(docs)\n",
                                    "\n",
                                    "# Convert to DataFrame for better visualization\n",
                                    "df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
                                    "print(df)\n",
                                    "```\n",
                                    "\n",
                                    "**Step-by-Step Explanation**\n",
                                    "\n",
                                    "1.  **Import Libraries**:\n",
                                    "      * Import `CountVectorizer` from `sklearn.feature_extraction.text` for creating the BoW model.\n",
                                    "      * Import `pandas` for handling data in a tabular format.\n",
                                    "2.  **Define Example Documents**:\n",
                                    "      * Create a list `docs` containing three example text documents. These documents will be used to demonstrate the BoW model.\n",
                                    "3.  **Initialize CountVectorizer**:\n",
                                    "      * Create an instance of `CountVectorizer` with `ngram_range=(1,1)` to specify that only unigrams (single words) should be considered.\n",
                                    "      * Set `stop_words='english'` to remove common English stop words from the documents.\n",
                                    "4.  **Fit and Transform Documents**:\n",
                                    "      * Use the `fit_transform` method of `CountVectorizer` to learn the vocabulary from the documents and transform them into a BoW matrix. This matrix contains the frequency of each word in each document.\n",
                                    "5.  **Convert to DataFrame**:\n",
                                    "      * Convert the BoW matrix to a Pandas DataFrame for better visualization. The columns of the DataFrame represent the unique words (features) in the vocabulary, and the rows represent the documents.\n",
                                    "6.  **Print the DataFrame**:\n",
                                    "      * Print the DataFrame to display the BoW representation of the documents, showing the frequency of each unigram in each document.\n",
                                    "\n",
                                    "**Output Example**\n",
                                    "\n",
                                    "```\n",
                                    "   bag  closely  common  fields  grams  improve  learning  machine  models  nlp  performance  related  technique  use  words\n",
                                    "0    0        1       0       1      0        0         1        1       0    1            0        1          0    0      0\n",
                                    "1    1        0       1       0      0        0         0        0       0    1            0        0          1    0      1\n",
                                    "2    0        0       0       0      1        1         0        0       1    1            1        0          0    1      0\n",
                                    "```\n",
                                    "\n",
                                    "This matrix shows the frequency of different unigrams in each document. This information can be used to train machine learning models for classification, clustering, and other NLP tasks.\n",
                                    "\n",
                                    "### Practical Example: Text Classification with Bag-of-Words and N-Grams\n",
                                    "\n",
                                    "In this example, you will apply the Bag-of-Words and n-gram techniques to a simple text classification task using a dataset of categorized short texts. We will implement a complete pipeline that includes creating a BoW representation, splitting the data, training a classifier, and evaluating performance.\n",
                                    "\n",
                                    "#### Step-by-Step Implementation\n",
                                    "\n",
                                    "1.  **Dataset Preparation**:\n",
                                    "      * Use a dataset of categorized short texts, such as news headlines or product descriptions, with 2-3 different categories.\n",
                                    "2.  **BoW Representation with N-Grams**:\n",
                                    "      * Implement BoW with an appropriate n-gram range and preprocessing options like stop word removal and stemming/lemmatization.\n",
                                    "3.  **Data Splitting**:\n",
                                    "      * Split the data into training and testing sets.\n",
                                    "4.  **Model Training**:\n",
                                    "      * Train a simple classifier, such as Naive Bayes or Logistic Regression, on the BoW features.\n",
                                    "5.  **Performance Evaluation**:\n",
                                    "      * Evaluate the classification performance using metrics like accuracy, precision, recall, and F1-score.\n",
                                    "6.  **Experimentation and Analysis**:\n",
                                    "      * Experiment with different n-gram ranges and preprocessing options.\n",
                                    "      * Analyze how these choices affect classification accuracy.\n",
                                    "\n",
                                    "#### Example Code\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.feature_extraction.text import CountVectorizer\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.naive_bayes import MultinomialNB\n",
                                    "from sklearn.metrics import classification_report\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "# Example dataset\n",
                                    "data = {\n",
                                    "    'text': [\n",
                                    "        \"Breaking news: Market hits all-time high\",\n",
                                    "        \"New product launch: Innovative tech gadget\",\n",
                                    "        \"Sports update: Local team wins championship\",\n",
                                    "        \"Economy news: Inflation rates are rising\",\n",
                                    "        \"Tech news: New smartphone features unveiled\"\n",
                                    "    ],\n",
                                    "    'category': ['news', 'product', 'sports', 'news', 'tech']}\n",
                                    "\n",
                                    "df = pd.DataFrame(data)\n",
                                    "\n",
                                    "# BoW with n-grams\n",
                                    "# Initialize CountVectorizer with ngram_range=(1, 2) to include both unigrams and bigrams\n",
                                    "# This means the model will consider single words (unigrams) and pairs of consecutive words (bigrams)\n",
                                    "# stop_words='english' removes common English stop words to reduce noise in the data\n",
                                    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
                                    "X = vectorizer.fit_transform(df['text'])  # Transform the text data into a BoW matrix\n",
                                    "y = df['category']  # Target labels\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "# test_size=0.2 indicates 20% of the data will be used for testing, and 80% for training\n",
                                    "# random_state=42 ensures reproducibility of the data split\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# Train a Naive Bayes classifier on the training data\n",
                                    "classifier = MultinomialNB()\n",
                                    "classifier.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Evaluate performance on the test data\n",
                                    "y_pred = classifier.predict(X_test)\n",
                                    "print(classification_report(y_test, y_pred))  # Print classification metrics\n",
                                    "\n",
                                    "# Experiment with different n-gram ranges and preprocessing options\n",
                                    "# Analyze the impact on classification accuracy\n",
                                    "```\n",
                                    "\n",
                                    "By following this example, you can demonstrate your ability to apply BoW models to solve practical NLP tasks and understand the impact of different preprocessing and n-gram settings on classification performance.\n",
                                    "\n",
                                    "### Challenges\n",
                                    "\n",
                                    "While BoW is simple and effective, it has some challenges:\n",
                                    "\n",
                                    "  * **High Dimensionality**: With a large vocabulary, the feature space grows significantly.\n",
                                    "  * **Loss of Semantic Meaning**: BoW ignores the order and meaning of words.\n",
                                    "  * **Sparse Representation**: Most values in the feature matrix are zeros, making it inefficient for large datasets.\n",
                                    "\n",
                                    "### Best Practices\n",
                                    "\n",
                                    "✔ Use N-Grams: Bigrams and trigrams can improve context understanding.\n",
                                    "✔ Remove Stop Words: Reduces unnecessary features.\n",
                                    "✔ Apply Dimensionality Reduction: Use techniques like PCA or feature selection to manage feature explosion.\n",
                                    "✔ Consider Stemming/Lemmatization: Helps normalize words and reduce redundancy.\n",
                                    "\n",
                                    "### Summary\n",
                                    "\n",
                                    "In this lesson, we covered the **Bag-of-Words model** and how **n-grams** improve text representation. We implemented BoW using **scikit-learn** and explored practical examples.\n",
                                    "\n",
                                    "In the next lesson, we will continue our journey into more advanced text representation techniques. Apply what you've learned to real-world datasets and experiment with different n-gram settings to deepen your understanding\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Bag-of-Words Model Implementation\n",
                                    "\n",
                                    "You've learned about the Bag-of-Words model and its implementation. Now, let's put that knowledge into practice. Your task is to:\n",
                                    "\n",
                                    "Initialize the CountVectorizer for unigrams.\n",
                                    "Transform the text data into a numerical matrix.\n",
                                    "Print both the matrix and the vocabulary.\n",
                                    "Afterward, analyze the word frequency by identifying the most frequent words and determine which document contains specific keywords. This exercise will help you solidify your understanding of text representation. Dive in and see how well you can apply these concepts!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.feature_extraction.text import CountVectorizer\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "# Example Documents\n",
                                    "docs = [\n",
                                    "    \"Machine learning and NLP are closely related fields. Machine learning is powerful.\",\n",
                                    "    \"Bag-of-Words is a common technique in NLP. NLP is widely used.\",\n",
                                    "    \"NLP models often use n-grams to improve performance. N-grams are useful.\"\n",
                                    "]\n",
                                    "\n",
                                    "# TODO: Initialize CountVectorizer with n-gram range for unigrams only\n",
                                    "\n",
                                    "# TODO: Convert to DataFrame for better visualization\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Excellent\\! This is a great way to put your knowledge of the Bag-of-Words model into practice. By completing the code and analyzing the results, you will gain a deeper understanding of how text is transformed into a numerical representation.\n",
                                    "\n",
                                    "Here is the completed code, including the steps to initialize the `CountVectorizer`, transform the data, and print the results for analysis.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.feature_extraction.text import CountVectorizer\n",
                                    "import pandas as pd\n",
                                    "\n",
                                    "# Example Documents\n",
                                    "docs = [\n",
                                    "    \"Machine learning and NLP are closely related fields. Machine learning is powerful.\",\n",
                                    "    \"Bag-of-Words is a common technique in NLP. NLP is widely used.\",\n",
                                    "    \"NLP models often use n-grams to improve performance. N-grams are useful.\"\n",
                                    "]\n",
                                    "\n",
                                    "# Initialize CountVectorizer with n-gram range for unigrams only and remove stop words\n",
                                    "vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words='english')\n",
                                    "\n",
                                    "# Transform the text data into a BoW matrix\n",
                                    "bow_matrix = vectorizer.fit_transform(docs)\n",
                                    "\n",
                                    "# Convert to DataFrame for better visualization\n",
                                    "df_bow = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
                                    "\n",
                                    "# Print the resulting matrix as a DataFrame\n",
                                    "print(\"Bag-of-Words Matrix:\\n\")\n",
                                    "print(df_bow)\n",
                                    "\n",
                                    "# Print the vocabulary\n",
                                    "print(\"\\nVocabulary (sorted alphabetically):\\n\")\n",
                                    "print(vectorizer.get_feature_names_out())\n",
                                    "```\n",
                                    "\n",
                                    "### Analysis of the Results\n",
                                    "\n",
                                    "After running the code, you'll see a matrix and a vocabulary list. Let's analyze them to answer your questions:\n",
                                    "\n",
                                    "**1. Most Frequent Words:**\n",
                                    "By examining the matrix, you can identify the most frequent words by looking for the highest numbers in the columns.\n",
                                    "\n",
                                    "  * `learning` and `machine` each appear twice in Document 1, but only once across the entire corpus.\n",
                                    "  * `nlp` appears 3 times across all documents (1 in Doc 1, 1 in Doc 2, 1 in Doc 3).\n",
                                    "  * `n-grams` appears 2 times across all documents (1 in Doc 2, 1 in Doc 3).\n",
                                    "  * `powerful` appears once in Document 1.\n",
                                    "\n",
                                    "The most frequent words in the entire corpus are **nlp**, **n-grams**, **machine**, and **learning**.\n",
                                    "\n",
                                    "**2. Document Keywords:**\n",
                                    "By looking at the rows and columns in the DataFrame, you can determine which document contains specific keywords.\n",
                                    "\n",
                                    "  * `'powerful'` is in **Document 1** (as indicated by the `1` in the first row under the `'powerful'` column).\n",
                                    "  * `'technique'` is in **Document 2**.\n",
                                    "  * `'performance'` is in **Document 3**."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Enhance Text Analysis with N-Grams"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Text Classification with Bag-of-Words"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
