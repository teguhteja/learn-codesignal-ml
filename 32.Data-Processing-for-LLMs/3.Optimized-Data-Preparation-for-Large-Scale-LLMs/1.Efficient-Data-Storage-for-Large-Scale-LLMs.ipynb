{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Efficient Data Storage for Large-Scale LLMs\n",
                "\n",
                "# Introduction and Context Setting\n",
                "\n",
                "Welcome to the first lesson of our course on \"Optimized Data Preparation for Large-Scale LLMs\". In this lesson, we will explore the importance of **efficient data storage for large-scale language models (LLMs)**. As you may know, LLMs require vast amounts of data to train effectively. Therefore, choosing the right data storage format is crucial for handling these large datasets efficiently.\n",
                "\n",
                "We will focus on two popular storage formats: **JSONL** and **Parquet**. These formats are widely used due to their efficiency and ease of use, especially when dealing with large-scale datasets. By the end of this lesson, you will understand how to load, stream, and save large datasets using these formats, setting a strong foundation for your journey in data preparation for LLMs.\n",
                "\n",
                "-----\n",
                "\n",
                "### Loading Large Datasets with the `datasets` Library\n",
                "\n",
                "To handle large datasets efficiently, we will use the `datasets` library. This library is designed to work with large datasets by allowing you to **stream data**, which means you can process data in chunks rather than loading the entire dataset into memory at once.\n",
                "\n",
                "Let's start by loading a large dataset. In this example, we'll use the Wikipedia dataset:\n",
                "\n",
                "```python\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Load large dataset (Wikipedia)\n",
                "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True, trust_remote_code=True)\n",
                "```\n",
                "\n",
                "**Detailed Explanation of Parameters:**\n",
                "\n",
                "  * `load_dataset`: This function from the `datasets` library is used to load a dataset. It supports a wide range of datasets and provides options for customization.\n",
                "  * `\"wikipedia\"`: This is the name of the dataset you want to load. In this case, it specifies that we are loading the Wikipedia dataset.\n",
                "  * `\"20220301.en\"`: This parameter specifies the configuration or version of the dataset. Here, `\"20220301.en\"` indicates that we are using the English Wikipedia dump from March 1, 2022.\n",
                "  * `split=\"train\"`: This parameter specifies which subset of the dataset to load. Common splits include `\"train\"`, `\"test\"`, and `\"validation\"`. In this example, we are loading the training split of the dataset.\n",
                "  * `streaming=True`: This parameter enables **streaming mode**, which allows you to process the dataset in chunks rather than loading the entire dataset into memory at once. This is particularly useful for handling large datasets that may not fit into memory.\n",
                "  * `trust_remote_code=True`: This parameter is used to allow the execution of code from the dataset's repository. It is necessary when the dataset requires custom processing or transformations defined in its repository. Use this option with caution, as it executes code from an external source.\n",
                "\n",
                "-----\n",
                "\n",
                "### Streaming and Structuring Data\n",
                "\n",
                "Once we have the dataset loaded, the next step is to stream and structure the data. We will extract the text data and organize it into a list of dictionaries.\n",
                "\n",
                "```python\n",
                "# Stream and store in efficient formats\n",
                "data_list = [{\"text\": example[\"text\"]} for example in dataset.take(10000)]\n",
                "```\n",
                "\n",
                "In this code snippet, we use a list comprehension to iterate over the first 10,000 examples in the dataset. For each example, we extract the `\"text\"` field and store it in a dictionary with the key `\"text\"`. This results in a list of dictionaries, where each dictionary contains a single text entry.\n",
                "\n",
                "-----\n",
                "\n",
                "### Saving Data in JSONL Format\n",
                "\n",
                "Now that we have our data structured, we can save it in the **JSONL** format. JSONL, or JSON Lines, is a format where each line is a valid JSON object. This format is particularly useful for storing large text datasets.\n",
                "\n",
                "```python\n",
                "import json\n",
                "\n",
                "# Save as JSONL\n",
                "with open(\"dataset.jsonl\", \"w\") as f:\n",
                "    for line in data_list:\n",
                "        json.dump(line, f)\n",
                "        f.write(\"\\n\")\n",
                "```\n",
                "\n",
                "Here, we open a file named `\"dataset.jsonl\"` in write mode. We then iterate over our `data_list`, using `json.dump` to write each dictionary as a JSON object to the file, followed by a newline character. This creates a JSONL file where each line represents a single text entry.\n",
                "\n",
                "**When to Use JSONL**\n",
                "\n",
                "JSONL is ideal for datasets where each entry is independent and can be processed line by line. It is particularly useful for text data, logs, or any data that can be represented as a series of JSON objects. JSONL is easy to read and write, making it a good choice for data that needs to be human-readable or easily parsed by other systems.\n",
                "\n",
                "-----\n",
                "\n",
                "### Saving Data in Parquet Format\n",
                "\n",
                "Another efficient format for storing large datasets is **Parquet**. Parquet is a **columnar storage file format** that is highly efficient for both storage and retrieval.\n",
                "\n",
                "```python\n",
                "import pandas as pd\n",
                "\n",
                "# Save as Parquet\n",
                "df = pd.DataFrame(data_list)\n",
                "df.to_parquet(\"dataset.parquet\", engine=\"pyarrow\")\n",
                "```\n",
                "\n",
                "In this example, we first convert our `data_list` into a Pandas DataFrame. We then use the `to_parquet` method to save the DataFrame as a Parquet file named `\"dataset.parquet\"`. The `engine=\"pyarrow\"` parameter specifies the use of the PyArrow library, which is commonly used for handling Parquet files.\n",
                "\n",
                "**When to Use Parquet**\n",
                "\n",
                "Parquet is best suited for datasets that benefit from columnar storage, such as those with a large number of columns or when performing analytical queries. It is highly efficient for both storage and retrieval, making it ideal for large-scale data processing tasks. Parquet is also a good choice when working with data that needs to be compressed or when you need to perform complex queries on the data.\n",
                "\n",
                "-----\n",
                "\n",
                "### Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, we covered the essential steps for efficiently storing large-scale datasets for LLMs. We learned how to load and stream data using the `datasets` library, structure the data into a list of dictionaries, and save it in both **JSONL** and **Parquet** formats. These skills are crucial for managing large datasets and will serve as a foundation for more advanced data preparation techniques.\n",
                "\n",
                "As you move on to the practice exercises, you'll have the opportunity to apply these concepts and solidify your understanding. Remember, choosing the right data storage format is key to handling large-scale datasets efficiently. Keep exploring and experimenting with different datasets and formats to enhance your skills further.\n",
                "\n",
                "**View video lesson**\n",
                "\n",
                "**Start practice**\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Efficient Streaming of Wikipedia Dataset\n",
                "\n",
                "You've done well learning about streaming and structuring data. Now, let's put that knowledge to use by loading the Wikipedia dataset in streaming mode, similar to what we did in the lesson.\n",
                "\n",
                "Load the Wikipedia dataset using the Hugging Face datasets library.\n",
                "Use \"wikipedia\" as the dataset name.\n",
                "Use \"20220301.en\" as the configuration to specify the English Wikipedia dump from March 1, 2022.\n",
                "Set split=\"train\" to load the training subset of the dataset.\n",
                "Enable streaming=True to process data in chunks.\n",
                "Set trust_remote_code=True to allow execution of code from the dataset's repository.\n",
                "Extract the first 100 examples from the \"train\" split.\n",
                "Use a list comprehension to iterate over the first 100 examples.\n",
                "For each example, extract the \"text\" field and store it in a dictionary with the key \"text\".\n",
                "Store these examples in a list, with each item containing the text content.\n",
                "Print the first example to verify the output.\n",
                "This exercise will help you practice efficient data handling. Dive in and see how smoothly you can manage large datasets!\n",
                "\n",
                "```python\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Load the Wikipedia dataset in streaming mode\n",
                "# - \"wikipedia\": specifies the dataset to load\n",
                "# - \"20220301.en\": specifies the version/configuration of the dataset (English Wikipedia dump from March 1, 2022)\n",
                "# - split=\"train\": specifies the subset of the dataset to load (training split)\n",
                "# - streaming=True: enables streaming mode to process data in chunks\n",
                "# - trust_remote_code=True: allows execution of code from the dataset's repository\n",
                "\n",
                "# Extract the first 100 examples\n",
                "# Use a list comprehension to iterate over the first 100 examples in the dataset\n",
                "# For each example, extract the \"text\" field and store it in a dictionary with the key \"text\"\n",
                "\n",
                "# Print the first example to verify\n",
                "# This will output the text content of the first example in the data_list\n",
                "print(data_list[0])\n",
                "\n",
                "```\n",
                "\n",
                "Here is the completed code that efficiently loads the Wikipedia dataset in streaming mode and extracts the first 100 examples as requested.\n",
                "\n",
                "```python\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Load the Wikipedia dataset in streaming mode\n",
                "# - \"wikipedia\": specifies the dataset to load\n",
                "# - \"20220301.en\": specifies the version/configuration of the dataset (English Wikipedia dump from March 1, 2022)\n",
                "# - split=\"train\": specifies the subset of the dataset to load (training split)\n",
                "# - streaming=True: enables streaming mode to process data in chunks\n",
                "# - trust_remote_code=True: allows execution of code from the dataset's repository\n",
                "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True, trust_remote_code=True)\n",
                "\n",
                "# Extract the first 100 examples\n",
                "# Use a list comprehension to iterate over the first 100 examples in the dataset\n",
                "# For each example, extract the \"text\" field and store it in a dictionary with the key \"text\"\n",
                "data_list = [{\"text\": example[\"text\"]} for example in dataset.take(100)]\n",
                "\n",
                "# Print the first example to verify\n",
                "# This will output the text content of the first example in the data_list\n",
                "print(data_list[0])\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Saving Wikipedia Dataset in JSONL Format\n",
                "\n",
                "Great job learning about JSONL and Parquet formats! Now, let's apply that knowledge to a practical task. Your goal is to create a function that saves the Wikipedia dataset examples into a JSONL file.\n",
                "\n",
                "Use the pre-loaded Wikipedia dataset.\n",
                "Create a function to save the data into a JSONL file.\n",
                "Each line should be a JSON object with the text data.\n",
                "This exercise will help you understand how to format and serialize data efficiently. Dive in and see how well you can manage this task!\n",
                "\n",
                "```python\n",
                "from datasets import load_dataset\n",
                "import json\n",
                "\n",
                "# Load Wikipedia dataset\n",
                "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True, trust_remote_code=True)\n",
                "\n",
                "def save_to_jsonl(data_list, filename):\n",
                "    # TODO: Open the file in write mode and save each example as a JSON object\n",
                "    pass\n",
                "\n",
                "# Prepare data list with text\n",
                "data_list = [{\"text\": example[\"text\"]} for example in dataset.take(10000)]\n",
                "\n",
                "# TODO: Call the function to save data_list to \"wikipedia_texts.jsonl\"\n",
                "\n",
                "print(\"Wikipedia texts saved in JSONL format.\")\n",
                "\n",
                "```\n",
                "\n",
                "Here is the completed code with the `save_to_jsonl` function and the function call filled in. This solution will create a file named \"wikipedia\\_texts.jsonl\" with 10,000 JSON objects, each on a new line.\n",
                "\n",
                "```python\n",
                "from datasets import load_dataset\n",
                "import json\n",
                "\n",
                "# Load Wikipedia dataset\n",
                "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True, trust_remote_code=True)\n",
                "\n",
                "def save_to_jsonl(data_list, filename):\n",
                "    # Open the file in write mode and save each example as a JSON object\n",
                "    with open(filename, \"w\") as f:\n",
                "        for item in data_list:\n",
                "            json.dump(item, f)\n",
                "            f.write(\"\\n\")\n",
                "\n",
                "# Prepare data list with text\n",
                "data_list = [{\"text\": example[\"text\"]} for example in dataset.take(10000)]\n",
                "\n",
                "# Call the function to save data_list to \"wikipedia_texts.jsonl\"\n",
                "save_to_jsonl(data_list, \"wikipedia_texts.jsonl\")\n",
                "\n",
                "print(\"Wikipedia texts saved in JSONL format.\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Saving Wikipedia Data as Parquet\n",
                "\n",
                "Great job on understanding JSONL and Parquet formats! Now, let's take it a step further by working with the Wikipedia dataset.\n",
                "\n",
                "Load the Wikipedia dataset using the datasets library.\n",
                "Stream the data and extract the first 10,000 examples.\n",
                "Create a function to convert these examples into a pandas DataFrame.\n",
                "Save the DataFrame as a Parquet file named \"wikipedia_data.parquet\" using the PyArrow engine.\n",
                "This task will help you see the benefits of using Parquet for large datasets. Let's get started and see how efficiently you can handle this data!\n",
                "\n",
                "```python\n",
                "from datasets import load_dataset\n",
                "import pandas as pd\n",
                "\n",
                "# Load large dataset (Wikipedia)\n",
                "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True, trust_remote_code=True)\n",
                "\n",
                "# Stream and store in efficient formats\n",
                "data_list = [{\"text\": example[\"text\"]} for example in dataset.take(10000)]\n",
                "\n",
                "# TODO: Implement the function to convert the list of dictionaries (data_list) into a Pandas DataFrame\n",
                "def convert_to_df(data_list):\n",
                "    # TODO: Create a DataFrame from the data_list\n",
                "    \n",
                "    # TODO: Save the DataFrame as a Parquet file using the PyArrow engine\n",
                "\n",
                "# TODO: Call the function to save the data as Parquet\n",
                "\n",
                "print(\"Dataset saved in Parquet format.\")\n",
                "\n",
                "```\n",
                "\n",
                "Here is the completed code that implements the function to convert the data to a Pandas DataFrame and save it as a Parquet file. This method is highly efficient for handling and storing large-scale structured data.\n",
                "\n",
                "```python\n",
                "from datasets import load_dataset\n",
                "import pandas as pd\n",
                "\n",
                "# Load large dataset (Wikipedia)\n",
                "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True, trust_remote_code=True)\n",
                "\n",
                "# Stream and store in efficient formats\n",
                "data_list = [{\"text\": example[\"text\"]} for example in dataset.take(10000)]\n",
                "\n",
                "# Implement the function to convert the list of dictionaries (data_list) into a Pandas DataFrame\n",
                "def convert_to_df(data_list):\n",
                "    # Create a DataFrame from the data_list\n",
                "    df = pd.DataFrame(data_list)\n",
                "    \n",
                "    # Save the DataFrame as a Parquet file using the PyArrow engine\n",
                "    df.to_parquet(\"wikipedia_data.parquet\", engine=\"pyarrow\")\n",
                "\n",
                "# Call the function to save the data as Parquet\n",
                "convert_to_df(data_list)\n",
                "\n",
                "print(\"Dataset saved in Parquet format.\")\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
