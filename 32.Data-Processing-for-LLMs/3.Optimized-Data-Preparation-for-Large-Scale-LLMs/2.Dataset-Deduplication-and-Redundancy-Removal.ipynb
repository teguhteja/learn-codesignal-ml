{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Deduplication and Redundancy Removal\n",
                "\n",
                "In the world of large-scale language models (LLMs), the quality and uniqueness of your dataset are crucial. Duplicates and near-duplicates can skew the model's learning process, leading to inefficiencies and potential biases. This lesson focuses on **deduplication**, a key step in data preparation that ensures your dataset is as clean and efficient as possible. By the end of this lesson, you'll understand how to remove both exact and near-duplicates from your dataset, setting a strong foundation for building robust LLMs.\n",
                "\n",
                "### Recall: Basic Concepts of Hashing\n",
                "\n",
                "Before diving into deduplication, let's briefly revisit the concept of **hashing**. Hashing is a process that converts data into a fixed-size string of characters, which is typically a hash code. This is useful for quickly comparing data, as hash codes are unique to the data they represent. In previous lessons, we introduced the `hashlib` library in Python, which provides a simple way to generate hash codes. Remember, hashing is a fundamental tool in data processing, especially when dealing with large datasets.\n",
                "\n",
                "### Exact Deduplication Using Hashing\n",
                "\n",
                "Exact deduplication involves removing identical entries from your dataset. This is a straightforward process that can be efficiently handled using Python's `set` data structure. Let's walk through the steps:\n",
                "\n",
                "1.  **Identify Duplicates:** Start with a list of texts, some of which may be duplicates.\n",
                "\n",
                "    ```python\n",
                "    texts = [\n",
                "        \"Large language models require diverse datasets.\",\n",
                "        \"Language models need large and diverse datasets.\",\n",
                "        \"This is a duplicate sentence.\",\n",
                "        \"This is a duplicate sentence.\"\n",
                "    ]\n",
                "    ```\n",
                "\n",
                "2.  **Remove Duplicates:** Use a `set` to automatically filter out duplicate entries.\n",
                "\n",
                "    ```python\n",
                "    unique_texts = list(set(texts))\n",
                "    ```\n",
                "\n",
                "    By converting the list to a set and back to a list, you remove any duplicate entries. The `set` data structure inherently does not allow duplicates, making it perfect for this task.\n",
                "\n",
                "3.  **Result:** The `unique_texts` list now contains only unique entries.\n",
                "\n",
                "    ```python\n",
                "    print(unique_texts)\n",
                "    # Output: ['This is a duplicate sentence.', 'Large language models require diverse datasets.', 'Language models need large and diverse datasets.']\n",
                "    ```\n",
                "\n",
                "### Near-Duplicate Detection with MinHash\n",
                "\n",
                "MinHash is a technique used to approximate the similarity between sets, which is useful for detecting near-duplicates in large datasets.\n",
                "\n",
                "1.  **Setup MinHash:** Use the `datasketch` library to implement MinHash.\n",
                "\n",
                "    ```python\n",
                "    from datasketch import MinHash\n",
                "    minhash_dict = {}\n",
                "    ```\n",
                "\n",
                "2.  **Create MinHash Signatures:** For each unique text, create a MinHash signature.\n",
                "\n",
                "    ```python\n",
                "    for i, text in enumerate(unique_texts):\n",
                "        minhash = MinHash(num_perm=128)\n",
                "        for word in text.split():\n",
                "            minhash.update(word.encode('utf8'))\n",
                "        minhash_dict[i] = minhash\n",
                "    ```\n",
                "\n",
                "      * `num_perm=128`: This parameter specifies the number of permutations used in the MinHash algorithm. A higher number of permutations increases the accuracy of the similarity estimation but also increases the computational cost. In this context, `num_perm=128` strikes a balance between accuracy and efficiency, providing a reliable approximation of the Jaccard similarity between sets.\n",
                "      * `encode('utf8')`: The `encode('utf8')` method is used to convert each word in the text into a byte format, which is necessary for the MinHash object to process the data. UTF-8 is a standard encoding that supports a wide range of characters, ensuring that the text is correctly encoded regardless of its content.\n",
                "\n",
                "### Near-Duplicate Detection with Locality-Sensitive Hashing (LSH)\n",
                "\n",
                "Locality-Sensitive Hashing (LSH) efficiently finds similar items in large datasets.\n",
                "\n",
                "1.  **Setup LSH:** Initialize LSH with a similarity threshold.\n",
                "\n",
                "    ```python\n",
                "    from datasketch import MinHashLSH\n",
                "    lsh = MinHashLSH(threshold=0.8, num_perm=128)\n",
                "    ```\n",
                "\n",
                "    Here, `MinHashLSH` is initialized with a similarity threshold of 0.8, meaning it will consider items with 80% similarity as near-duplicates.\n",
                "\n",
                "2.  **Insert MinHash Signatures into LSH:** Insert each MinHash signature into the LSH.\n",
                "\n",
                "    ```python\n",
                "    for i, minhash in minhash_dict.items():\n",
                "        lsh.insert(f\"text_{i}\", minhash)\n",
                "    ```\n",
                "\n",
                "3.  **Query for Near-Duplicates with LSH:** Use the LSH to find near-duplicates.\n",
                "\n",
                "    ```python\n",
                "    for i, minhash in minhash_dict.items():\n",
                "        print(f\"Near duplicates for '{unique_texts[i]}':\", lsh.query(minhash))\n",
                "    ```\n",
                "\n",
                "    This code queries the LSH for each text's MinHash signature, returning a list of similar texts.\n",
                "\n",
                "### Recall: Basic Concepts of TF-IDF\n",
                "\n",
                "Before diving into near-duplicate detection using cosine similarity, let's briefly revisit the concept of **TF-IDF** (Term Frequency-Inverse Document Frequency). TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (or corpus). It is often used in text mining and information retrieval to convert text data into numerical vectors, which can then be used for various analyses, including similarity measurements.\n",
                "\n",
                "  * **Term Frequency (TF):** Measures how frequently a term appears in a document. It is calculated as the number of times a term appears in a document divided by the total number of terms in the document.\n",
                "  * **Inverse Document Frequency (IDF):** Measures how important a term is. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.\n",
                "\n",
                "The TF-IDF value is the product of these two metrics, providing a weight that indicates the importance of a term in a document relative to the entire corpus. This weighting helps in identifying the most relevant words for distinguishing between documents, making it a powerful tool for text vectorization.\n",
                "\n",
                "### Near-Duplicate Detection with Cosine Similarity\n",
                "\n",
                "Cosine Similarity measures the cosine of the angle between two vectors, providing a value between -1 and 1, which helps identify near-duplicates. The formula for cosine similarity between two vectors $A$ and $B$ is:\n",
                "\n",
                "$$\\text{Cosine Similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
                "\n",
                "where:\n",
                "\n",
                "  * $A \\\\cdot B$ is the dot product of the vectors.\n",
                "  * $|A|$ and $|B|$ are the magnitudes (or lengths) of the vectors.\n",
                "\n",
                "<!-- end list -->\n",
                "\n",
                "1.  **Vectorize Texts:** Convert texts into numerical vectors using techniques like TF-IDF.\n",
                "\n",
                "    ```python\n",
                "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "    vectorizer = TfidfVectorizer()\n",
                "    tfidf_matrix = vectorizer.fit_transform(unique_texts)\n",
                "    ```\n",
                "\n",
                "    The `TfidfVectorizer` converts the text data into a matrix of TF-IDF features.\n",
                "\n",
                "2.  **Calculate Cosine Similarity:** Use the cosine similarity function to find similar texts and round the similarity matrix for better readability.\n",
                "\n",
                "    ```python\n",
                "    from sklearn.metrics.pairwise import cosine_similarity\n",
                "    import numpy as np\n",
                "\n",
                "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
                "    print(np.round(cosine_sim, 2))\n",
                "    ```\n",
                "\n",
                "    This computes the cosine similarity between each pair of texts in the dataset and prints the similarity matrix rounded to two decimal places.\n",
                "\n",
                "3.  **Identify Near-Duplicates:** Set a threshold to determine near-duplicates based on cosine similarity.\n",
                "\n",
                "    ```python\n",
                "    # Initialize a set to keep track of already reported near-duplicates\n",
                "    reported_pairs = set()\n",
                "    threshold = 0.8\n",
                "    for i in range(len(unique_texts)):\n",
                "        for j in range(i + 1, len(unique_texts)):  # Start from i+1 to avoid duplicate pairs\n",
                "            if cosine_sim[i][j] > threshold and (j, i) not in reported_pairs:\n",
                "                print(f\"Near duplicates: '{unique_texts[i]}' and '{unique_texts[j]}'\")\n",
                "                reported_pairs.add((i, j))\n",
                "    ```\n",
                "\n",
                "    This modification ensures that each pair of near-duplicates is only reported once, avoiding repetition of the same text in the output.\n",
                "\n",
                "### Handling and Removing Duplicates: When and How\n",
                "\n",
                "#### When to Remove Duplicates\n",
                "\n",
                "  * **Data Quality Improvement:** Remove duplicates to enhance the quality of your dataset, ensuring that the model learns from diverse and unique examples.\n",
                "  * **Bias Reduction:** Duplicates can introduce bias, as repeated data points may skew the model's understanding. Removing them helps maintain a balanced dataset.\n",
                "  * **Efficiency:** Reducing redundancy decreases the dataset size, leading to faster processing and training times.\n",
                "\n",
                "#### How to Handle Duplicates\n",
                "\n",
                "  * **Exact Duplicates:** Use Python's `set` data structure to remove exact duplicates efficiently.\n",
                "  * **Near-Duplicates:** Implement MinHash, LSH, and Cosine Similarity to detect and handle near-duplicates, ensuring that similar but not identical entries are identified and managed.\n",
                "\n",
                "#### Cases to Consider\n",
                "\n",
                "  * **Domain-Specific Needs:** In some domains, duplicates might be necessary for emphasis or context. Evaluate the importance of duplicates based on your specific use case.\n",
                "  * **Data Augmentation:** If duplicates are part of a data augmentation strategy, consider their role in enhancing model robustness before removal.\n",
                "  * **Threshold Tuning:** Adjust similarity thresholds in MinHash, LSH, and Cosine Similarity based on the desired level of similarity detection, balancing between removing too many or too few entries.\n",
                "\n",
                "### Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, you learned how to perform both exact and near-duplicate deduplication on datasets, a crucial step in preparing data for large-scale language models. You now understand how to use Python's `set` for exact deduplication and the `datasketch` library for detecting near-duplicates with MinHash and LSH, as well as cosine similarity for an additional layer of precision. As you move on to the practice exercises, apply these techniques to different datasets and experiment with various parameters to deepen your understanding. This hands-on practice will reinforce your learning and prepare you for more advanced data preparation tasks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Removing Exact Duplicates Efficiently\n",
                "\n",
                "You've learned about exact deduplication using Python's set. Now, let's put that into practice. Your task is to create a function that removes exact duplicates from a list of text strings.\n",
                "\n",
                "Use Python's set to filter out duplicates.\n",
                "Return a list of unique texts.\n",
                "Calculate and print:\n",
                "The number of duplicates removed.\n",
                "The percentage of the original dataset that consisted of duplicates.\n",
                "This exercise will solidify your understanding of basic deduplication. Dive in and see how efficiently you can clean up the dataset!\n",
                "\n",
                "```python\n",
                "def deduplicate_texts(texts):\n",
                "    # TODO: Convert list to set to remove duplicates\n",
                "    \n",
                "    # TODO: Calculate number of duplicates removed\n",
                "    \n",
                "    # TODO: Calculate percentage of duplicates\n",
                "    \n",
                "    # TODO: Print the number and percentage of duplicates\n",
                "    print(f\"Number of duplicates removed: {______}\")\n",
                "    print(f\"Percentage of duplicates: {______:.2f}%\")\n",
                "    \n",
                "    return unique_texts\n",
                "\n",
                "# Sample dataset with duplicates\n",
                "texts = [\n",
                "    \"Natural language processing is an interesting field.\",\n",
                "    \"Natural language processing is an interesting field.\",\n",
                "    \"Natural langauge processing is an interesting field.\",  # Typo in \"language\"\n",
                "    \"Natural language processing is a fascinating field.\",   # Word change: \"interesting\" → \"fascinating\"\n",
                "    \"Processing of natural language is interesting.\",        # Word order changed\n",
                "    \"Deep learning is used in natural language processing.\"  # Different but related sentence\n",
                "]\n",
                "\n",
                "# Call the function and print unique texts\n",
                "unique_texts = deduplicate_texts(texts)\n",
                "print(\"Unique texts:\", unique_texts)\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "def deduplicate_texts(texts):\n",
                "    # TODO: Convert list to set to remove duplicates\n",
                "    initial_count = len(texts)\n",
                "    unique_texts = list(set(texts))\n",
                "    final_count = len(unique_texts)\n",
                "    \n",
                "    # TODO: Calculate number of duplicates removed\n",
                "    duplicates_removed = initial_count - final_count\n",
                "    \n",
                "    # TODO: Calculate percentage of duplicates\n",
                "    percentage_duplicates = (duplicates_removed / initial_count) * 100\n",
                "    \n",
                "    # TODO: Print the number and percentage of duplicates\n",
                "    print(f\"Number of duplicates removed: {duplicates_removed}\")\n",
                "    print(f\"Percentage of duplicates: {percentage_duplicates:.2f}%\")\n",
                "    \n",
                "    return unique_texts\n",
                "\n",
                "# Sample dataset with duplicates\n",
                "texts = [\n",
                "    \"Natural language processing is an interesting field.\",\n",
                "    \"Natural language processing is an interesting field.\",\n",
                "    \"Natural langauge processing is an interesting field.\",  # Typo in \"language\"\n",
                "    \"Natural language processing is a fascinating field.\",   # Word change: \"interesting\" → \"fascinating\"\n",
                "    \"Processing of natural language is interesting.\",        # Word order changed\n",
                "    \"Deep learning is used in natural language processing.\"  # Different but related sentence\n",
                "]\n",
                "\n",
                "# Call the function and print unique texts\n",
                "unique_texts = deduplicate_texts(texts)\n",
                "print(\"Unique texts:\", unique_texts)\n",
                "\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating MinHash Signatures\n",
                "\n",
                "Nice job on understanding exact deduplication! Now, let's take it a step further. Your task is to implement a function that creates MinHash signatures for a list of unique texts.\n",
                "\n",
                "Use the datasketch library to generate a MinHash signature for each text.\n",
                "Split each text into words, convert them to bytes, and update the MinHash object.\n",
                "Return a dictionary mapping text indices to their MinHash signatures.\n",
                "This exercise will help you grasp the first step in detecting near duplicates. Dive in and see how you can apply MinHash to your dataset!\n",
                "\n",
                "```python\n",
                "from datasketch import MinHash\n",
                "\n",
                "def create_minhash_signatures(texts):\n",
                "    minhash_dict = {}\n",
                "    for i, text in enumerate(texts):\n",
                "        minhash = MinHash(num_perm=128)\n",
                "        # TODO: Split the text into words\n",
                "        # TODO: Convert each word to bytes and update the MinHash object\n",
                "        minhash_dict[i] = minhash\n",
                "    return minhash_dict\n",
                "\n",
                "# Example usage\n",
                "unique_texts = [\n",
                "    \"Natural language processing is an interesting field.\",\n",
                "    \"Natural langauge processing is an interesting field.\",  # Typo in \"language\"\n",
                "    \"Natural language processing is a fascinating field.\",   # Word change: \"interesting\" → \"fascinating\"\n",
                "    \"Processing of natural language is interesting.\",        # Word order changed\n",
                "    \"Deep learning is used in natural language processing.\"  # Different but related sentence\n",
                "]\n",
                "\n",
                "minhash_signatures = create_minhash_signatures(unique_texts)\n",
                "for index, minhash in minhash_signatures.items():\n",
                "    print(f\"Text {index}: MinHash signature created.\")\n",
                "```\n",
                "\n",
                "```python\n",
                "from datasketch import MinHash\n",
                "\n",
                "def create_minhash_signatures(texts):\n",
                "    minhash_dict = {}\n",
                "    for i, text in enumerate(texts):\n",
                "        minhash = MinHash(num_perm=128)\n",
                "        # TODO: Split the text into words\n",
                "        words = text.split()\n",
                "        # TODO: Convert each word to bytes and update the MinHash object\n",
                "        for word in words:\n",
                "            minhash.update(word.encode('utf8'))\n",
                "        minhash_dict[i] = minhash\n",
                "    return minhash_dict\n",
                "\n",
                "# Example usage\n",
                "unique_texts = [\n",
                "    \"Natural language processing is an interesting field.\",\n",
                "    \"Natural langauge processing is an interesting field.\",  # Typo in \"language\"\n",
                "    \"Natural language processing is a fascinating field.\",   # Word change: \"interesting\" → \"fascinating\"\n",
                "    \"Processing of natural language is interesting.\",        # Word order changed\n",
                "    \"Deep learning is used in natural language processing.\"  # Different but related sentence\n",
                "]\n",
                "\n",
                "minhash_signatures = create_minhash_signatures(unique_texts)\n",
                "for index, minhash in minhash_signatures.items():\n",
                "    print(f\"Text {index}: MinHash signature created.\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Detect Near-Duplicates with LSH\n",
                "\n",
                "Well done on mastering MinHash signatures! Now, let's explore Locality-Sensitive Hashing (LSH) to detect near-duplicates in a dataset. Your task is to implement a function that:\n",
                "\n",
                "Initializes a MinHashLSH object with two given similarity thresholds: 0.8 and 0.5.\n",
                "Inserts MinHash signatures into the LSH.\n",
                "Queries the LSH to find near-duplicates for each text at both thresholds.\n",
                "Return a dictionary mapping text indices to their near-duplicates for each threshold. Also, print examples of identified near-duplicate text indices along with their similarity thresholds. This exercise will deepen your understanding of near-duplicate detection and help you understand how thresholds affect the detection process. Let's see how effectively you can apply LSH!\n",
                "\n",
                "```python\n",
                "from datasketch import MinHash, MinHashLSH\n",
                "\n",
                "def detect_near_duplicates(minhash_dict, texts, threshold=0.8):\n",
                "    # TODO: Initialize MinHashLSH with the given similarity threshold\n",
                "    # num_perm=128 specifies the number of permutations used in MinHash, affecting the accuracy of similarity estimation\n",
                "    \n",
                "    # TODO: Insert MinHash signatures into LSH\n",
                "    \n",
                "    # Query the LSH to find near-duplicates for each text\n",
                "    near_duplicates = {}\n",
                "    for i, minhash in minhash_dict.items():\n",
                "        # TODO: Query the LSH with the current MinHash signature to find similar items        \n",
                "        # TODO: Store the result in the near_duplicates dictionary with the current index as the key        \n",
                "        # TODO: Print examples of identified near-duplicate text indices along with their similarity thresholds\n",
                "        \n",
                "    return near_duplicates\n",
                "\n",
                "# Example usage\n",
                "texts = [\n",
                "    \"Natural language processing is an interesting field.\",\n",
                "    \"Natural langauge processing is an interesting field.\",  # Typo in \"language\"\n",
                "    \"Natural language processing is a fascinating field.\",   # Word change: \"interesting\" → \"fascinating\"\n",
                "    \"Processing of natural language is interesting.\",        # Word order changed\n",
                "    \"Deep learning is used in natural language processing.\"  # Different but related sentence\n",
                "]\n",
                "\n",
                "# Create MinHash signatures\n",
                "minhash_dict = {}\n",
                "for i, text in enumerate(texts):\n",
                "    minhash = MinHash(num_perm=128)\n",
                "    for word in text.split():\n",
                "        minhash.update(word.encode('utf8'))\n",
                "    minhash_dict[i] = minhash\n",
                "\n",
                "# Detect near-duplicates\n",
                "print(\"Results for near-duplicates with a similarity threshold of 0.8:\")\n",
                "detect_near_duplicates(minhash_dict, texts)\n",
                "print(\"-----------------------------------------------------------------\")\n",
                "# TODO: Detect near-duplicates with threshold = 0.5\n",
                "print(\"Results for near-duplicates with a similarity threshold of 0.5:\")\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from datasketch import MinHash, MinHashLSH\n",
                "\n",
                "def detect_near_duplicates(minhash_dict, texts, threshold=0.8):\n",
                "    # TODO: Initialize MinHashLSH with the given similarity threshold\n",
                "    # num_perm=128 specifies the number of permutations used in MinHash, affecting the accuracy of similarity estimation\n",
                "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
                "    \n",
                "    # TODO: Insert MinHash signatures into LSH\n",
                "    for i, minhash in minhash_dict.items():\n",
                "        lsh.insert(f\"text_{i}\", minhash)\n",
                "    \n",
                "    # Query the LSH to find near-duplicates for each text\n",
                "    near_duplicates = {}\n",
                "    for i, minhash in minhash_dict.items():\n",
                "        # TODO: Query the LSH with the current MinHash signature to find similar items\n",
                "        result = lsh.query(minhash)\n",
                "        # TODO: Store the result in the near_duplicates dictionary with the current index as the key\n",
                "        near_duplicates[i] = result\n",
                "        # TODO: Print examples of identified near-duplicate text indices along with their similarity thresholds\n",
                "        if len(result) > 1:\n",
                "            print(f\"Near duplicates for text_{i} (threshold={threshold}): {result}\")\n",
                "            \n",
                "    return near_duplicates\n",
                "\n",
                "# Example usage\n",
                "texts = [\n",
                "    \"Natural language processing is an interesting field.\",\n",
                "    \"Natural langauge processing is an interesting field.\",  # Typo in \"language\"\n",
                "    \"Natural language processing is a fascinating field.\",   # Word change: \"interesting\" → \"fascinating\"\n",
                "    \"Processing of natural language is interesting.\",        # Word order changed\n",
                "    \"Deep learning is used in natural language processing.\"  # Different but related sentence\n",
                "]\n",
                "\n",
                "# Create MinHash signatures\n",
                "minhash_dict = {}\n",
                "for i, text in enumerate(texts):\n",
                "    minhash = MinHash(num_perm=128)\n",
                "    for word in text.split():\n",
                "        minhash.update(word.encode('utf8'))\n",
                "    minhash_dict[i] = minhash\n",
                "\n",
                "# Detect near-duplicates\n",
                "print(\"Results for near-duplicates with a similarity threshold of 0.8:\")\n",
                "detect_near_duplicates(minhash_dict, texts, threshold=0.8)\n",
                "print(\"-----------------------------------------------------------------\")\n",
                "# TODO: Detect near-duplicates with threshold = 0.5\n",
                "print(\"Results for near-duplicates with a similarity threshold of 0.5:\")\n",
                "detect_near_duplicates(minhash_dict, texts, threshold=0.5)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Detecting Near-Duplicates with Cosine Similarity\n",
                "\n",
                "You've done well with MinHash! Now, let's explore Cosine Similarity for detecting near-duplicates. Your task is to complete the missing parts in the starter code to:\n",
                "\n",
                "Use TfidfVectorizer to convert texts into TF-IDF vectors.\n",
                "Calculate the cosine similarity matrix and print it.\n",
                "Identify text pairs with scores above two different thresholds.\n",
                "Return two dictionaries mapping each text index to a list of its near duplicates for each threshold.\n",
                "This exercise will deepen your understanding of different deduplication techniques. Dive in and see how Cosine Similarity can enhance your dataset analysis!\n",
                "\n",
                "```python\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import numpy as np\n",
                "\n",
                "def find_near_duplicates(texts, threshold1, threshold2):\n",
                "    # TODO: Use TfidfVectorizer to convert texts into TF-IDF vectors\n",
                "    vectorizer = _______________\n",
                "    tfidf_matrix = _______.fit_transform(texts)\n",
                "    \n",
                "    # TODO: Calculate the cosine similarity matrix and print it\n",
                "    cosine_sim = cosine_similarity(_________, __________)\n",
                "    print(\"Cosine Similarity Matrix:\")\n",
                "    print(np.round(cosine_sim, 2))\n",
                "\n",
                "    near_duplicates1 = {}\n",
                "    near_duplicates2 = {}\n",
                "    for i in range(len(texts)):\n",
                "        # TODO: Identify text pairs with similarity scores above the first threshold\n",
                "        similar_indices1 = [j for j in range(len(texts)) if i != j and _______ > threshold1]\n",
                "        near_duplicates1[i] = similar_indices1\n",
                "        \n",
                "        # TODO: Identify text pairs with similarity scores above the second threshold\n",
                "        similar_indices2 = [j for j in range(len(texts)) if i != j and  _______ > threshold2]\n",
                "        near_duplicates2[i] = similar_indices2\n",
                "    return near_duplicates1, near_duplicates2\n",
                "\n",
                "# Sample dataset with duplicates\n",
                "texts = [\n",
                "    \"Natural language processing is an interesting field.\",\n",
                "    \"Natural language processing is an interesting field.\",  # This is a duplicate\n",
                "    \"Natural langauge processing is an interesting field.\",  # Typo in \"language\"\n",
                "    \"Natural language processing is a fascinating field.\",   # Word change: \"interesting\" → \"fascinating\"\n",
                "    \"Processing of natural language is interesting.\",        # Word order changed\n",
                "    \"Deep learning is used in natural language processing.\"  # Different but related sentence\n",
                "]\n",
                "\n",
                "# Example usage\n",
                "threshold1 = 0.8\n",
                "threshold2 = 0.5\n",
                "near_duplicates1, near_duplicates2 = find_near_duplicates(texts, threshold1, threshold2)\n",
                "print(\"Near duplicates with threshold 0.8:\")\n",
                "for index, duplicates in near_duplicates1.items():\n",
                "    print(f\"Text {index}: {duplicates}\")\n",
                "\n",
                "print(\"Near duplicates with threshold 0.5:\")\n",
                "for index, duplicates in near_duplicates2.items():\n",
                "    print(f\"Text {index}: {duplicates}\")\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
