{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Filtering and Toxicity Detection\n",
                "\n",
                "### Dataset Filtering and Toxicity Detection\n",
                "\n",
                "**Introduction and Context Setting**\n",
                "\n",
                "Welcome to the lesson on Dataset Filtering and Toxicity Detection. In the previous lessons, we explored efficient data storage and deduplication techniques for preparing datasets for large-scale language models (LLMs). Now, we will focus on filtering datasets to remove non-English and toxic content. This step is crucial to ensure the quality and safety of the data used to train LLMs. By the end of this lesson, you will be able to implement a function that filters out unwanted content from a dataset.\n",
                "\n",
                "### Language Detection with `langdetect`\n",
                "\n",
                "To filter out non-English content, we will use the `langdetect` library. This library helps identify the language of a given text.\n",
                "\n",
                "**Step-by-Step Explanation**\n",
                "\n",
                "1.  **Import the Library:** First, we need to import the `detect` function from the `langdetect` library.\n",
                "\n",
                "    ```python\n",
                "    from langdetect import detect\n",
                "    ```\n",
                "\n",
                "2.  **Detect Language:** Use the `detect` function to identify the language of a text. It returns a language code, such as \"en\" for English.\n",
                "\n",
                "    ```python\n",
                "    text = \"Je ne parle pas anglais.\"\n",
                "    language = detect(text)\n",
                "    print(\"Detected Language:\", language)\n",
                "    ```\n",
                "\n",
                "    Output:\n",
                "\n",
                "    ```\n",
                "    Detected Language: fr\n",
                "    ```\n",
                "\n",
                "    Here, the text is in French, so the detected language code is \"fr\".\n",
                "\n",
                "### Toxicity Detection with `Detoxify`\n",
                "\n",
                "Next, we will use the `Detoxify` library to detect toxic language in the text. This library provides a model that predicts toxicity scores.\n",
                "\n",
                "**Step-by-Step Explanation**\n",
                "\n",
                "1.  **Import the Library:** Import the `Detoxify` class from the `detoxify` library.\n",
                "\n",
                "    ```python\n",
                "    from detoxify import Detoxify\n",
                "    ```\n",
                "\n",
                "2.  **Predict Toxicity:** Use the `Detoxify` model to predict the toxicity score of a text. A higher score indicates more toxic content.\n",
                "\n",
                "    ```python\n",
                "    text = \"I hate this group of people!\"\n",
                "    toxicity_scores = Detoxify(\"original\").predict(text)\n",
                "    print(\"Toxicity Score:\", toxicity_scores[\"toxicity\"])\n",
                "    ```\n",
                "\n",
                "    Output:\n",
                "\n",
                "    ```\n",
                "    Toxicity Score: 0.85\n",
                "    ```\n",
                "\n",
                "    In this example, the text is considered highly toxic with a score of 0.85.\n",
                "\n",
                "### Implementing the Filtering Function\n",
                "\n",
                "Now, let's implement a function that combines language and toxicity detection to filter a dataset.\n",
                "\n",
                "**Step-by-Step Explanation**\n",
                "\n",
                "1.  **Define the Function:** Create a function `filter_text` that takes a text as input and returns `None` if the text is non-English or highly toxic.\n",
                "\n",
                "    ```python\n",
                "    def filter_text(text):\n",
                "        # Language Detection\n",
                "        if detect(text) != \"en\":\n",
                "            return None  # Remove non-English text\n",
                "        # Toxicity Detection\n",
                "        toxicity_scores = Detoxify(\"original\").predict(text)\n",
                "        if toxicity_scores[\"toxicity\"] > 0.7:\n",
                "            return None  # Remove highly toxic content\n",
                "        return text  # Keep clean text\n",
                "    ```\n",
                "\n",
                "    The function first checks if the text is in English. If not, it returns `None`. It then checks the toxicity score. If the score is above 0.7, it returns `None`. If the text passes both checks, it is returned as clean text.\n",
                "\n",
                "### Applying the Filtering Function to a Dataset\n",
                "\n",
                "Finally, we will apply the `filter_text` function to a list of texts using list comprehension.\n",
                "\n",
                "**Step-by-Step Explanation**\n",
                "\n",
                "1.  **Sample Dataset:** Define a list of sample texts.\n",
                "\n",
                "    ```python\n",
                "    texts = [\n",
                "        \"I hate this group of people!\",  # Toxic statement\n",
                "        \"This is a normal sentence.\",\n",
                "        \"Je ne parle pas anglais.\"  # Non-English\n",
                "    ]\n",
                "    ```\n",
                "\n",
                "2.  **Apply Filtering:** Use list comprehension to filter the dataset.\n",
                "\n",
                "    ```python\n",
                "    filtered_texts = [filter_text(text) for text in texts if filter_text(text)]\n",
                "    print(\"Filtered Dataset:\", filtered_texts)\n",
                "    ```\n",
                "\n",
                "    Output:\n",
                "\n",
                "    ```\n",
                "    Filtered Dataset: ['This is a normal sentence.']\n",
                "    ```\n",
                "\n",
                "    The list comprehension iterates over each text, applies the `filter_text` function, and includes only the texts that are not `None`.\n",
                "\n",
                "### Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, you learned how to filter a dataset by removing non-English and toxic content using the `langdetect` and `Detoxify` libraries. We implemented a function that combines these checks and applied it to a sample dataset. This filtering process is essential for maintaining the quality and safety of data used in training large-scale language models.\n",
                "\n",
                "As you move on to the practice exercises, try experimenting with different datasets and filtering criteria. This hands-on practice will reinforce your understanding and help you apply these techniques to real-world scenarios. Congratulations on reaching this point in the course, and keep up the great work\\!\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Language Detection and Reporting\n",
                "\n",
                "Now that you've learned about the importance of filtering datasets for LLM training, let's put your knowledge into practice! In this exercise, you'll focus specifically on the language detection aspect we just covered.\n",
                "\n",
                "Your task is to complete the detect_languages function that processes a list of text samples and returns a list of tuples, each containing a text and its detected language. You'll use the langdetect library's detect function to identify each text's language.\n",
                "\n",
                "To complete this exercise:\n",
                "\n",
                "Implement the logic to detect the language of each text\n",
                "Return a list of tuples, each containing the text and its detected language\n",
                "This exercise builds a foundation for the more complex filtering we'll do later when we combine language detection with toxicity filtering. Successfully implementing this function will give you the skills needed for real-world dataset preparation tasks!\n",
                "\n",
                "```python\n",
                "from langdetect import detect\n",
                "\n",
                "# Sample dataset with texts in different languages\n",
                "texts = [\n",
                "    \"Hello, this is a sample text in English.\",  # English\n",
                "    \"Hola, este es un ejemplo de texto en español.\",  # Spanish\n",
                "    \"Bonjour, ceci est un exemple de texte en français.\",  # French\n",
                "    \"Hallo, dies ist ein Beispieltext auf Deutsch.\",  # German\n",
                "    \"This is another English text sample.\",  # English\n",
                "    \"Questo è un esempio di testo in italiano.\",  # Italian\n",
                "    \"The quick brown fox jumps over the lazy dog.\",  # English\n",
                "    \"私はあなたの言語を話せません。\",  # Japanese\n",
                "    \"English is a West Germanic language.\"  # English\n",
                "]\n",
                "\n",
                "def detect_languages(text_list):\n",
                "    \"\"\"\n",
                "    Detect the language of each text in the list and return the results.\n",
                "    \n",
                "    Args:\n",
                "        text_list (list): A list of text strings in various languages\n",
                "    \n",
                "    Returns:\n",
                "        list: A list of tuples containing each text and its detected language\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    for text in text_list:\n",
                "        # TODO: Detect the language of the text\n",
                "        # TODO: Append the text and its detected language to the results list\n",
                "        pass\n",
                "    return results\n",
                "\n",
                "# Get the detection results\n",
                "detection_results = detect_languages(texts)\n",
                "\n",
                "# Print the results\n",
                "for text, language in detection_results:\n",
                "    print(f\"Text: {text} | Detected Language: {language}\")\n",
                "```\n",
                "\n",
                "```python\n",
                "from langdetect import detect\n",
                "\n",
                "# Sample dataset with texts in different languages\n",
                "texts = [\n",
                "    \"Hello, this is a sample text in English.\",  # English\n",
                "    \"Hola, este es un ejemplo de texto en español.\",  # Spanish\n",
                "    \"Bonjour, ceci est un exemple de texte en français.\",  # French\n",
                "    \"Hallo, dies ist ein Beispieltext auf Deutsch.\",  # German\n",
                "    \"This is another English text sample.\",  # English\n",
                "    \"Questo è un esempio di testo in italiano.\",  # Italian\n",
                "    \"The quick brown fox jumps over the lazy dog.\",  # English\n",
                "    \"私はあなたの言語を話せません。\",  # Japanese\n",
                "    \"English is a West Germanic language.\"  # English\n",
                "]\n",
                "\n",
                "def detect_languages(text_list):\n",
                "    \"\"\"\n",
                "    Detect the language of each text in the list and return the results.\n",
                "    \n",
                "    Args:\n",
                "        text_list (list): A list of text strings in various languages\n",
                "    \n",
                "    Returns:\n",
                "        list: A list of tuples containing each text and its detected language\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    for text in text_list:\n",
                "        # TODO: Detect the language of the text\n",
                "        try:\n",
                "            language = detect(text)\n",
                "        except:\n",
                "            language = \"unknown\" # Handle cases where language cannot be detected\n",
                "        # TODO: Append the text and its detected language to the results list\n",
                "        results.append((text, language))\n",
                "    return results\n",
                "\n",
                "# Get the detection results\n",
                "detection_results = detect_languages(texts)\n",
                "\n",
                "# Print the results\n",
                "for text, language in detection_results:\n",
                "    print(f\"Text: {text} | Detected Language: {language}\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Filter English Texts with Langdetect\n",
                "\n",
                "You've done well learning about language detection! Now, let's put that knowledge to use. Your task is to create a function using the langdetect library to filter a list of text samples.\n",
                "\n",
                "Import the detect function from langdetect.\n",
                "Use it to identify the language of each text.\n",
                "Return only the English texts from the list.\n",
                "Dive in and see how effectively you can filter out non-English content!\n",
                "\n",
                "```python\n",
                "from langdetect import detect\n",
                "\n",
                "# Sample dataset\n",
                "texts = [\n",
                "    \"Hello, how are you?\",  # English\n",
                "    \"Hola, ¿cómo estás?\",   # Spanish\n",
                "    \"Bonjour, comment ça va?\",  # French\n",
                "    \"This is a test sentence.\",  # English\n",
                "    \"C'est une phrase de test.\"  # French\n",
                "]\n",
                "\n",
                "# Function to filter English texts\n",
                "def filter_english_texts(texts):\n",
                "    # TODO: Use list comprehension to filter only English texts (en)\n",
                "    return english_texts\n",
                "\n",
                "# Apply filtering\n",
                "filtered_texts = filter_english_texts(texts)\n",
                "print(\"Filtered English Texts:\", filtered_texts)\n",
                "```\n",
                "\n",
                "```python\n",
                "from langdetect import detect\n",
                "\n",
                "# Sample dataset\n",
                "texts = [\n",
                "    \"Hello, how are you?\",  # English\n",
                "    \"Hola, ¿cómo estás?\",   # Spanish\n",
                "    \"Bonjour, comment ça va?\",  # French\n",
                "    \"This is a test sentence.\",  # English\n",
                "    \"C'est une phrase de test.\"  # French\n",
                "]\n",
                "\n",
                "# Function to filter English texts\n",
                "def filter_english_texts(texts):\n",
                "    # Use list comprehension to filter only English texts (en)\n",
                "    english_texts = [text for text in texts if detect(text) == 'en']\n",
                "    return english_texts\n",
                "\n",
                "# Apply filtering\n",
                "filtered_texts = filter_english_texts(texts)\n",
                "print(\"Filtered English Texts:\", filtered_texts)\n",
                "```\n",
                "\n",
                "**Output:**\n",
                "\n",
                "```\n",
                "Filtered English Texts: ['Hello, how are you?', 'This is a test sentence.']\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Detect and Filter Toxic Texts\n",
                "\n",
                "Nice progress in understanding toxicity detection! Now, let's focus on filtering English texts based on their toxicity levels.\n",
                "\n",
                "Use the Detoxify library to analyze a list of texts.\n",
                "Calculate toxicity scores for each text.\n",
                "Remove texts with scores above 0.7.\n",
                "This exercise will help you refine your skills in ensuring data quality. Dive in and see how well you can clean up the dataset!\n",
                "\n",
                "```python\n",
                "import os\n",
                "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
                "\n",
                "from detoxify import Detoxify\n",
                "\n",
                "# Sample dataset\n",
                "texts = [\n",
                "    \"I hate this group of people!\",  # Toxic statement\n",
                "    \"This is a normal sentence.\",\n",
                "    \"I love everyone here!\"  # Positive statement\n",
                "]\n",
                "\n",
                "# Function to filter content based on toxicity\n",
                "def filter_toxic_texts(texts, threshold=0.7):\n",
                "    filtered_texts = []\n",
                "    for text in texts:\n",
                "        # TODO: Calculate toxicity scores using Detoxify\n",
                "        # TODO: Append text to filtered_texts if toxicity score is below or equal to threshold\n",
                "    return filtered_texts\n",
                "\n",
                "# Apply filtering\n",
                "filtered_texts = filter_toxic_texts(texts)\n",
                "print(\"Filtered Dataset:\", filtered_texts)\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "import os\n",
                "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
                "\n",
                "from detoxify import Detoxify\n",
                "\n",
                "# Sample dataset\n",
                "texts = [\n",
                "    \"I hate this group of people!\",  # Toxic statement\n",
                "    \"This is a normal sentence.\",\n",
                "    \"I love everyone here!\"  # Positive statement\n",
                "]\n",
                "\n",
                "# Function to filter content based on toxicity\n",
                "def filter_toxic_texts(texts, threshold=0.7):\n",
                "    detoxifier = Detoxify('unbiased')\n",
                "    filtered_texts = []\n",
                "    for text in texts:\n",
                "        # Calculate toxicity scores using Detoxify\n",
                "        results = detoxifier.predict(text)\n",
                "        toxicity_score = results['toxicity']\n",
                "        \n",
                "        # Append text to filtered_texts if toxicity score is below or equal to threshold\n",
                "        if toxicity_score <= threshold:\n",
                "            filtered_texts.append(text)\n",
                "            \n",
                "    return filtered_texts\n",
                "\n",
                "# Apply filtering\n",
                "filtered_texts = filter_toxic_texts(texts)\n",
                "print(\"Filtered Dataset:\", filtered_texts)\n",
                "\n",
                "```\n",
                "\n",
                "**Output:**\n",
                "\n",
                "```\n",
                "Filtered Dataset: ['This is a normal sentence.', 'I love everyone here!']\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Filter English and Non-Toxic Texts\n",
                "\n",
                "You've learned how to detect languages and filter toxic content. Now, let's combine these skills! Your task is to create a filter_dataset function that processes a list of texts to return only English, non-toxic content.\n",
                "\n",
                "Use langdetect to check whether each text is in English.\n",
                "Use Detoxify to evaluate toxicity, filtering out texts with scores above 0.7.\n",
                "Implement the filtering pipeline to ensure that only clean texts are returned.\n",
                "This exercise will solidify your understanding of dataset filtering. Dive in and see how effectively you can clean up the dataset!\n",
                "\n",
                "```python\n",
                "import os\n",
                "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
                "\n",
                "from detoxify import Detoxify\n",
                "from langdetect import detect\n",
                "\n",
                "# Sample dataset\n",
                "texts = [\n",
                "    \"I hate this group of people!\",  # Toxic statement\n",
                "    \"This is a normal sentence.\",\n",
                "    \"Je ne parle pas anglais.\"  # Non-English\n",
                "]\n",
                "\n",
                "# Function to filter content\n",
                "def filter_dataset(texts):\n",
                "    def filter_text(text):\n",
                "        # TODO: Detect the language of the text\n",
                "        # TODO: Compare the detected language with \"en\" to check if it's English\n",
                "            return None  # Remove non-English text\n",
                "        \n",
                "        # TODO: Initialize Detoxify model\n",
                "        # TODO: Predict the toxicity score of the text\n",
                "        # TODO: Check if the toxicity score is greater than 0.7\n",
                "            return None  # Remove highly toxic content\n",
                "        \n",
                "        # TODO: Return the text if it passes both language and toxicity checks\n",
                "        return text  # Keep clean text\n",
                "\n",
                "    # TODO: Create a list comprehension to filter texts using filter_text function\n",
                "    return [filter_text(text) for text in texts if filter_text(text)]\n",
                "\n",
                "# TODO: Execute the filter_dataset function with the sample dataset\n",
                "# TODO: Print the filtered dataset to verify the results\n",
                "filtered_texts = filter_dataset(texts)\n",
                "print(\"Filtered Dataset:\", filtered_texts)\n",
                "\n",
                "```\n",
                "\n",
                "\n",
                "This traceback indicates a `SyntaxError` in your code. The error message `assignment expression cannot rebind comprehension iteration variable 'text'` means you can't use the walrus operator `:=` to reassign the variable that's being iterated over within a list comprehension.\n",
                "\n",
                "The problem lies in this line:\n",
                "\n",
                "```python\n",
                "return [text for text in texts if (text := filter_text(text)) is not None]\n",
                "```\n",
                "\n",
                "To fix this, you need to store the result of `filter_text(text)` in a separate variable within the list comprehension. However, a list comprehension isn't designed for this type of logic.\n",
                "\n",
                "A better way to solve this is to refactor the code to a standard `for` loop, which is much more readable and doesn't run into this syntax limitation.\n",
                "\n",
                "Here is the corrected and more Pythonic code for the `filter_dataset` function:\n",
                "\n",
                "-----\n",
                "\n",
                "### Corrected Code\n",
                "\n",
                "```python\n",
                "import os\n",
                "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
                "\n",
                "from detoxify import Detoxify\n",
                "from langdetect import detect\n",
                "\n",
                "# Sample dataset\n",
                "texts = [\n",
                "    \"I hate this group of people!\",  # Toxic statement\n",
                "    \"This is a normal sentence.\",\n",
                "    \"Je ne parle pas anglais.\"  # Non-English\n",
                "]\n",
                "\n",
                "# Initialize Detoxify once to avoid reloading the model for each text\n",
                "detoxify_model = Detoxify('unbiased')\n",
                "\n",
                "# Function to filter content\n",
                "def filter_dataset(texts):\n",
                "    filtered_list = []\n",
                "    for text in texts:\n",
                "        # Check language\n",
                "        try:\n",
                "            if detect(text) != \"en\":\n",
                "                continue # Skip to the next text if not English\n",
                "        except:\n",
                "            continue # Skip if language detection fails\n",
                "\n",
                "        # Check toxicity\n",
                "        results = detoxify_model.predict(text)\n",
                "        if results['toxicity'] > 0.7:\n",
                "            continue # Skip if highly toxic\n",
                "\n",
                "        # Add to the filtered list if it passes all checks\n",
                "        filtered_list.append(text)\n",
                "\n",
                "    return filtered_list\n",
                "\n",
                "# Execute the filter_dataset function with the sample dataset\n",
                "filtered_texts = filter_dataset(texts)\n",
                "\n",
                "# Print the filtered dataset to verify the results\n",
                "print(\"Filtered Dataset:\", filtered_texts)\n",
                "```\n",
                "\n",
                "By switching to a standard `for` loop, the logic becomes straightforward: iterate through each `text`, apply your checks, and if a text passes, add it to a new list. This avoids the `SyntaxError` and makes the code easier to understand and debug."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
