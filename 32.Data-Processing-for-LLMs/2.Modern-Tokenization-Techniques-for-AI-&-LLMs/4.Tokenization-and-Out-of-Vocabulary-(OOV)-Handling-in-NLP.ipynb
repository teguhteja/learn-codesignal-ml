{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 4"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization and Out-of-Vocabulary (OOV) Handling in NLP\n",
                "\n",
                "# Introduction to Tokenization and OOV Handling\n",
                "\n",
                "Welcome to this lesson on **tokenization** and handling **Out-of-Vocabulary (OOV)** words. Tokenization is a fundamental step in Natural Language Processing (NLP) that involves breaking down text into smaller units called **tokens**. This process is crucial for AI and Large Language Models (LLMs) as it allows them to understand and process text data effectively.\n",
                "\n",
                "However, a common challenge in tokenization is dealing with OOV words‚Äîwords that are not present in the model's vocabulary. Handling these words is essential for maintaining the performance and accuracy of language models. Additionally, **text cleaning** before tokenization‚Äîsuch as removing unnecessary symbols, handling case sensitivity, and ensuring proper encoding‚Äîcan significantly improve tokenization quality. Another important aspect is selecting the right model for the language, as some tokenizers are better suited for multilingual text.\n",
                "\n",
                "-----\n",
                "\n",
                "### How Tokenizers Handle OOV Words\n",
                "\n",
                "Different tokenization methods handle OOV words in distinct ways:\n",
                "\n",
                "| Tokenizer Type | Method | OOV Handling Strategy |\n",
                "| :--- | :--- | :--- |\n",
                "| **WordPiece (BERT)** | Subword tokenization | Uses `[UNK]` if no match is found |\n",
                "| **Byte-Pair Encoding (GPT-2, RoBERTa)** | Merges frequent character pairs | Breaks OOV words into smaller subwords |\n",
                "| **SentencePiece (T5, mT5, XLM-R)** | Probabilistic model-based | Keeps rare words but splits them into known subwords |\n",
                "\n",
                "-----\n",
                "\n",
                "### Tokenization with BERT, GPT-2, and T5\n",
                "\n",
                "Let's explore how different tokenization methods handle a complex text containing Korean words, emojis, and links. The text we will use is:\n",
                "\n",
                "```\n",
                "\"üöÄ The new XZ-900 Ïä§ÎßàÌä∏Ìè∞ is absolutely ultrahyperfast! Only ‚Ç¨799 üí∞. Get yours now at www.techstore.aiudjashdf!\"\n",
                "```\n",
                "\n",
                "#### 1\\. WordPiece Tokenization with BERT\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "# Load WordPiece tokenizer (BERT)\n",
                "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "# Tokenize the sample text\n",
                "text = \"üöÄ The new XZ-900 Ïä§ÎßàÌä∏Ìè∞ is absolutely ultrahyperfast! Only ‚Ç¨799 üí∞. Get yours now at www.techstore.aiudjashdf!\"\n",
                "tokens_bert = tokenizer_bert.tokenize(text)\n",
                "print(\"BERT Tokenization:\", tokens_bert)\n",
                "```\n",
                "\n",
                "**Output:**\n",
                "\n",
                "```\n",
                "BERT Tokenization: ['[UNK]', 'the', 'new', 'x', '##z', '-', '900', '·Ñâ', '##·Ö≥', '##·ÑÜ', '##·Ö°', '##·Ñê', '##·Ö≥', '##·Ñë', '##·Ö©', '##·Ü´', 'is', 'absolutely', 'ultra', '##hy', '##per', '##fast', '!', 'only', '‚Ç¨', '##7', '##9', '##9', '[UNK]', '.', 'get', 'yours', 'now', 'at', 'www', '.', 'tech', '##stor', '##e', '.', 'ai', '##ud', '##jas', '##hd', '##f', '!']\n",
                "```\n",
                "\n",
                "**BERT Tokenization Output Explanation:**\n",
                "\n",
                "  * Breaks words into subwords using `##` to mark subword units.\n",
                "  * Uses `[UNK]` for unknown tokens (e.g., emojis, non-Latin scripts like Korean).\n",
                "  * If working with multilingual text, using `bert-base-multilingual-cased` instead of `bert-base-uncased` can significantly improve tokenization accuracy for non-English languages.\n",
                "\n",
                "-----\n",
                "\n",
                "#### 2\\. Byte Pair Encoding (BPE) with GPT-2\n",
                "\n",
                "```python\n",
                "# Load BPE tokenizer (GPT-2)\n",
                "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "# Tokenize the sample text\n",
                "tokens_gpt2 = tokenizer_gpt2.tokenize(text)\n",
                "print(\"GPT-2 Tokenization:\", tokens_gpt2)\n",
                "```\n",
                "\n",
                "**Output:**\n",
                "\n",
                "```\n",
                "GPT-2 Tokenization: ['√∞≈Å', 'ƒº', 'ƒ¢', 'ƒ†The', 'ƒ†new', 'ƒ†X', 'Z', '-', '900', 'ƒ†√¨', 'ƒ¨', '¬§', '√´', '¬ß', 'ƒ™', '√≠', 'ƒ¨', '¬∏', '√≠', 'ƒ±', '¬∞', 'ƒ†is', 'ƒ†absolutely', 'ƒ†ult', 'rah', 'y', 'per', 'fast', '!', 'ƒ†Only', 'ƒ†√¢ƒ§¬¨', '799', 'ƒ†√∞≈Å', 'ƒ¥', '¬∞', '.', 'ƒ†Get', 'ƒ†yours', 'ƒ†now', 'ƒ†at', 'ƒ†www', '.', 'tech', 'store', '.', 'ai', 'ud', 'j', 'ash', 'df', '!']\n",
                "```\n",
                "\n",
                "**GPT-2 Tokenization Output Explanation:**\n",
                "\n",
                "  * No `[UNK]` tokens, as BPE splits unknown words into frequent subword pairs.\n",
                "  * Handles emojis, Korean text, and URLs more effectively than WordPiece, but still not optimized for non-English languages.\n",
                "\n",
                "-----\n",
                "\n",
                "#### 3\\. SentencePiece Tokenization with T5\n",
                "\n",
                "```python\n",
                "# Load SentencePiece tokenizer (T5)\n",
                "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
                "# Tokenize the sample text\n",
                "tokens_t5 = tokenizer_t5.tokenize(text)\n",
                "print(\"T5 Tokenization:\", tokens_t5)\n",
                "```\n",
                "\n",
                "**Output:**\n",
                "\n",
                "```\n",
                "T5 Tokenization: [' ', 'üöÄ', ' The', ' new', ' ', 'Ïä§ÎßàÌä∏Ìè∞', ' is', ' absolutely', ' ultra', 'hyp', 'er', 'fast', '!', ' Only', ' ‚Ç¨', '7', '99', ' ', 'üí∞', '.', ' Get', ' your', 's', ' now', ' at', ' www', '.', 'tech', 'store', '.', 'a', 'i', 'u', 'd', 'j', 'ash', 'd', 'f', '!']\n",
                "```\n",
                "\n",
                "**T5 Tokenization Output Explanation:**\n",
                "\n",
                "  * Uses SentencePiece, a flexible tokenization approach that supports diverse characters.\n",
                "  * Adds  markers to indicate new words.\n",
                "  * Handles non-English text more effectively compared to WordPiece and BPE.\n",
                "\n",
                "-----\n",
                "\n",
                "### Comparison of WordPiece, BPE, and SentencePiece Tokenization\n",
                "\n",
                "| Feature | BERT (WordPiece) | GPT-2 (BPE) | T5 (SentencePiece) |\n",
                "| :--- | :--- | :--- | :--- |\n",
                "| **Handles OOV words** | Replaces with `[UNK]` | Breaks into subwords | Splits into subwords without `[UNK]` |\n",
                "| **Emoji Support** | `[UNK]` | Keeps intact | Keeps intact |\n",
                "| **Non-Latin Text (e.g., Korean)** | `[UNK]` | Splits into known subwords | Keeps as a whole word |\n",
                "| **Number Handling** | Keeps whole | Splits into sub-tokens | Splits into sub-tokens |\n",
                "| **Hyphenated Words** | Sometimes splits | Often keeps intact | Splits smartly |\n",
                "\n",
                "-----\n",
                "\n",
                "### How to Improve OOV Handling?\n",
                "\n",
                "To reduce OOV issues, you can:\n",
                "\n",
                "  * Use **Multilingual Tokenizers** (`xlm-roberta-base`, `bert-base-multilingual-cased`).\n",
                "  * **Train a Custom Tokenizer** (e.g., `sentencepiece`, `BPE`) on domain-specific text.\n",
                "  * **Expand Vocabulary** by pretraining on larger datasets.\n",
                "  * Ensure **Proper Text Cleaning** before tokenization (e.g., removing unnecessary symbols, handling casing, ensuring correct encoding).\n",
                "\n",
                "-----\n",
                "\n",
                "### Example: Handling Chinese Text with XLM-RoBERTa\n",
                "\n",
                "```python\n",
                "# Load XLM-RoBERTa tokenizer\n",
                "tokenizer_xlm = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
                "# Sample Chinese text\n",
                "chinese_text = \"ËøôÊòØ‰∏Ä‰∏™ÊµãËØï„ÄÇ\"\n",
                "# Tokenize the Chinese text\n",
                "tokens_xlm = tokenizer_xlm.tokenize(chinese_text)\n",
                "print(\"XLM-RoBERTa Tokenization:\", tokens_xlm)\n",
                "```\n",
                "\n",
                "**Output:**\n",
                "\n",
                "```\n",
                "XLM-RoBERTa Tokenization: [' ', 'ËøôÊòØ‰∏Ä‰∏™', 'ÊµãËØï', '„ÄÇ']\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### Summary and Next Steps\n",
                "\n",
                "In this lesson, we explored different tokenization methods and their strategies for handling OOV words. We compared WordPiece, BPE, and SentencePiece tokenizers and discussed how to improve OOV handling. As a next step, practice implementing these tokenization techniques on various text samples, including multilingual data, to better understand their strengths and limitations.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization Showdown BERT vs GPT2\n",
                "\n",
                "\n",
                "You've learned about different tokenization methods and how they handle OOV (Out-Of-Vocabulary) words. Now, let's put that knowledge into practice!\n",
                "\n",
                "In this task, you will:\n",
                "\n",
                "Compare how BERT (WordPiece) and GPT-2 (BPE) tokenizers handle OOV words.\n",
                "Tokenize multiple example phrases with OOV words, including technical terms and combined words.\n",
                "Analyze the differences and count UNK tokens for BERT to see how it handles OOV words.\n",
                "Dive in and see how these tokenizers perform!\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Example phrases with OOV words\n",
                "phrases = [\n",
                "    \"The new hyperlooptechnology is groundbreaking.\",\n",
                "    \"The XZ-900 Ïä§ÎßàÌä∏Ìè∞ is ultrahyperfast!\"\n",
                "    \"Heading to the beach! üåä‚òÄÔ∏è Can‚Äôt wait!\"\n",
                "]\n",
                "\n",
                "# TODO: Load WordPiece tokenizer (BERT)\n",
                "\n",
                "# TODO: Load BPE tokenizer (GPT-2)\n",
                "\n",
                "for text in phrases:\n",
                "    tokens_bert = tokenizer_bert.tokenize(text)\n",
                "    tokens_gpt2 = tokenizer_gpt2.tokenize(text)\n",
                "    \n",
                "    # TODO: Count UNK tokens only for BERT\n",
                "    unk_count_bert = tokens_bert.count('____')\n",
                "    \n",
                "    print(f\"Text: {text}\")\n",
                "    print(\"BERT Tokenization:\", tokens_bert)\n",
                "    print(\"GPT-2 Tokenization:\", tokens_gpt2)\n",
                "    print(f\"BERT UNK Tokens: {unk_count_bert}\")\n",
                "    print(\"-\" * 50)\n",
                "\n",
                "```\n",
                "\n",
                "To correctly fill in the code, you need to know how to load the BERT and GPT-2 tokenizers and what the UNK token is for BERT's WordPiece tokenizer. The `TODO` comments in the code snippet guide you to:\n",
                "\n",
                "1.  **Load WordPiece tokenizer (BERT):** You need to instantiate an `AutoTokenizer` and specify the model name for BERT.\n",
                "2.  **Load BPE tokenizer (GPT-2):** Similarly, you need to load the tokenizer for GPT-2.\n",
                "3.  **Count UNK tokens only for BERT:** You need to replace the placeholder `____` with the actual UNK token used by BERT's tokenizer to get the correct count.\n",
                "\n",
                "Once you have the completed code, running it will provide the analysis you need to compare how the two tokenizers handle the provided phrases.\n",
                "\n",
                "Here is the completed code with the necessary changes:\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Example phrases with OOV words\n",
                "phrases = [\n",
                "    \"The new hyperlooptechnology is groundbreaking.\",\n",
                "    \"The XZ-900 Ïä§ÎßàÌä∏Ìè∞ is ultrahyperfast!\",\n",
                "    \"Heading to the beach! üåä‚òÄÔ∏è Can‚Äôt wait!\"\n",
                "]\n",
                "\n",
                "# Load WordPiece tokenizer (BERT)\n",
                "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "\n",
                "# Load BPE tokenizer (GPT-2)\n",
                "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "\n",
                "for text in phrases:\n",
                "    tokens_bert = tokenizer_bert.tokenize(text)\n",
                "    tokens_gpt2 = tokenizer_gpt2.tokenize(text)\n",
                "    \n",
                "    # Count UNK tokens only for BERT\n",
                "    unk_count_bert = tokens_bert.count('[UNK]')\n",
                "    \n",
                "    print(f\"Text: {text}\")\n",
                "    print(\"BERT Tokenization:\", tokens_bert)\n",
                "    print(\"GPT-2 Tokenization:\", tokens_gpt2)\n",
                "    print(f\"BERT UNK Tokens: {unk_count_bert}\")\n",
                "    print(\"-\" * 50)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Multilingual Tokenization Challenge\n",
                "\n",
                "Nice job exploring tokenization methods! Now, let's dive deeper into how different tokenizers handle multilingual text.\n",
                "\n",
                "In this task, you will:\n",
                "Add the¬†T5¬†(SentencePiece) tokenizer to your comparison.\n",
                "Test¬†BERT,¬†GPT-2, and¬†T5¬†tokenizers on multilingual text samples with different writing systems (e.g., Latin, Korean, Arabic, Chinese).\n",
                "Examine how each tokenizer handles OOV tokens and compare tokenization patterns.\n",
                "Analyze which tokenizer is most effective for non-English text. This will help you understand the strengths and limitations of each method. Let's see how these tokenizers perform!\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Sample multilingual text\n",
                "text = \"Hello, ÏïàÎÖïÌïòÏÑ∏Ïöî, ŸÖÿ±ÿ≠ÿ®ÿß, ‰Ω†Â•Ω\"\n",
                "\n",
                "# Load WordPiece tokenizer (BERT)\n",
                "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "tokens_bert = tokenizer_bert.tokenize(text)\n",
                "print(\"BERT Tokenization:\", tokens_bert)\n",
                "\n",
                "# Load BPE tokenizer (GPT-2)\n",
                "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "tokens_gpt2 = tokenizer_gpt2.tokenize(text)\n",
                "print(\"GPT-2 Tokenization:\", tokens_gpt2)\n",
                "\n",
                "# TODO: Load SentencePiece tokenizer (T5)\n",
                "# TODO: Tokenize the sample multilingual text using the T5 tokenizer\n",
                "# TODO: Print the tokenization result for T5\n",
                "\n",
                "```\n",
                "\n",
                "That's an excellent next step\\! Expanding the comparison to include a multilingual sample will clearly demonstrate the differences between these tokenization methods.\n",
                "\n",
                "The key to completing the code is to load the `t5-base` model with `AutoTokenizer`, apply it to the multilingual text, and then print the result. This will allow for a direct comparison of how BERT, GPT-2, and T5 each handle non-Latin characters.\n",
                "\n",
                "Here is the completed code with the T5 tokenizer added as requested:\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Sample multilingual text\n",
                "text = \"Hello, ÏïàÎÖïÌïòÏÑ∏Ïöî, ŸÖÿ±ÿ≠ÿ®ÿß, ‰Ω†Â•Ω\"\n",
                "\n",
                "# Load WordPiece tokenizer (BERT)\n",
                "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "tokens_bert = tokenizer_bert.tokenize(text)\n",
                "print(\"BERT Tokenization:\", tokens_bert)\n",
                "\n",
                "# Load BPE tokenizer (GPT-2)\n",
                "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "tokens_gpt2 = tokenizer_gpt2.tokenize(text)\n",
                "print(\"GPT-2 Tokenization:\", tokens_gpt2)\n",
                "\n",
                "# Load SentencePiece tokenizer (T5)\n",
                "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
                "# Tokenize the sample multilingual text using the T5 tokenizer\n",
                "tokens_t5 = tokenizer_t5.tokenize(text)\n",
                "# Print the tokenization result for T5\n",
                "print(\"T5 Tokenization:\", tokens_t5)\n",
                "\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Multilingual Tokenization and OOV Reduction\n",
                "\n",
                "Excellent work with the multilingual tokenization comparison! Now, let's take it a step further by implementing practical strategies to reduce those pesky unknown tokens.\n",
                "\n",
                "In this exercise, you'll use the same multilingual text to compare how a standard RoBERTa tokenizer and a multilingual XLM-RoBERTa tokenizer handle different languages.\n",
                "\n",
                "This hands-on exercise will give you practical skills for choosing the right tokenizer and preprocessing techniques for real-world multilingual NLP applications.\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Sample multilingual text (same as previous exercise)\n",
                "text = \"Hello, ÏïàÎÖïÌïòÏÑ∏Ïöî, ŸÖÿ±ÿ≠ÿ®ÿß, ‰Ω†Â•Ω\"\n",
                "\n",
                "# TODO: Load standard RoBERTa tokenizer\n",
                "# TODO: Load multilingual XLM-RoBERTa tokenizer\n",
                "\n",
                "# TODO: Tokenize text with RoBERTa tokenizer\n",
                "# TODO: Tokenize text with XLM-RoBERTa tokenizer\n",
                "\n",
                "# TODO: Print RoBERTa tokenization \n",
                "# TODO: Print XLM-RoBERTa tokenization\n",
                "\n",
                "```\n",
                "\n",
                "That's an excellent final step to solidify your understanding of multilingual tokenization. The difference between a standard and a multilingual model's tokenizer is one of the most important concepts for handling diverse text data.\n",
                "\n",
                "To complete this task, you'll need to load the `roberta-base` model for the standard tokenizer and the `xlm-roberta-base` model for the multilingual one using `AutoTokenizer`. Once loaded, you can apply them to the provided text and print the results to see the difference firsthand.\n",
                "\n",
                "Here is the completed code:\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Sample multilingual text (same as previous exercise)\n",
                "text = \"Hello, ÏïàÎÖïÌïòÏÑ∏Ïöî, ŸÖÿ±ÿ≠ÿ®ÿß, ‰Ω†Â•Ω\"\n",
                "\n",
                "# Load standard RoBERTa tokenizer\n",
                "tokenizer_roberta = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
                "\n",
                "# Load multilingual XLM-RoBERTa tokenizer\n",
                "tokenizer_xlm_roberta = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
                "\n",
                "# Tokenize text with RoBERTa tokenizer\n",
                "tokens_roberta = tokenizer_roberta.tokenize(text)\n",
                "\n",
                "# Tokenize text with XLM-RoBERTa tokenizer\n",
                "tokens_xlm_roberta = tokenizer_xlm_roberta.tokenize(text)\n",
                "\n",
                "# Print RoBERTa tokenization \n",
                "print(\"RoBERTa Tokenization:\", tokens_roberta)\n",
                "\n",
                "# Print XLM-RoBERTa tokenization\n",
                "print(\"XLM-RoBERTa Tokenization:\", tokens_xlm_roberta)\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
