{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction to Tokenization (Rule-Based Tokenization)\n",
                "\n",
                "# Introduction to Tokenization\n",
                "\n",
                "Welcome to the first lesson of our course on **Modern Tokenization Techniques for AI & LLMs**. In this lesson, we will explore the concept of **tokenization**, a fundamental step in **Natural Language Processing** (NLP). Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, sentences, or even characters, depending on the level of granularity required.\n",
                "\n",
                "Tokenization is crucial because it transforms raw text into a format that can be easily processed by AI models, enabling them to understand and generate human language. Additionally, tokenization helps in reducing the complexity of text data, making it easier to analyze and manipulate. It is the first step in many NLP pipelines, serving as the foundation for tasks such as parsing, part-of-speech tagging, and named entity recognition.\n",
                "\n",
                "## Recall: Python Libraries for NLP\n",
                "\n",
                "Before we dive into tokenization techniques, let's briefly recall the importance of Python libraries in NLP. Libraries like NLTK (Natural Language Toolkit) and spaCy provide powerful tools for text processing, making complex tasks like tokenization more manageable. While we have touched on these libraries before, it's important to remember that they offer pre-built functions that save time and effort, allowing us to focus on building and refining our models. These libraries also come with extensive documentation and community support, which can be invaluable when troubleshooting or seeking to extend their functionality. Furthermore, they are optimized for performance, enabling efficient processing of large datasets, which is essential when working with LLMs.\n",
                "\n",
                "## Understanding Rule-Based Tokenization\n",
                "\n",
                "Rule-based tokenization involves using predefined rules to split text into tokens. This method is straightforward and effective for many applications. Unlike statistical or machine learning-based tokenization, rule-based tokenization relies on patterns such as spaces, punctuation, or regular expressions to identify token boundaries. While it is fast and easy to implement, it may not handle all edge cases, such as contractions or special characters, as effectively as more advanced methods. Rule-based tokenization is often used in scenarios where the text structure is predictable and consistent, such as processing log files or structured documents. However, it may require manual adjustments to handle language-specific nuances or domain-specific jargon.\n",
                "\n",
                "## NLTK Tokenization Techniques\n",
                "\n",
                "Let's explore how to perform tokenization using **NLTK**, a popular library for NLP tasks. NLTK provides a variety of tokenization methods, each suited for different types of text and analysis needs. It is widely used in academic research and educational settings due to its comprehensive suite of tools and ease of use.\n",
                "\n",
                "### Word Tokenization with NLTK\n",
                "\n",
                "First, we'll use NLTK's `word_tokenize` function to split a sentence into individual words.\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "from nltk.tokenize import word_tokenize\n",
                "\n",
                "nltk.download('punkt_tab')\n",
                "\n",
                "# Sample text with two sentences\n",
                "txt = \"Dr. John O'Reilly’s AI-based startup raised $10M in 2023. The company plans to expand globally next year.\"\n",
                "\n",
                "# NLTK word tokenization\n",
                "nltk_word_tokens = word_tokenize(txt)\n",
                "\n",
                "print(\"NLTK Word Tokenization:\", nltk_word_tokens)\n",
                "```\n",
                "\n",
                "**Explanation:**\n",
                "\n",
                "  - We import the necessary functions from NLTK and download the `punkt` package, which is required for tokenization.\n",
                "  - The `word_tokenize` function splits the text into words, handling punctuation and special characters.\n",
                "  - The output is a list of words: `['Dr.', 'John', \"O'Reilly\", '’', 's', 'AI-based', 'startup', 'raised', '$', '10M', 'in', '2023', '.', 'The', 'company', 'plans', 'to', 'expand', 'globally', 'next', 'year', '.']`.\n",
                "  - This method is particularly useful for tasks that require word-level analysis, such as sentiment analysis or word frequency counting.\n",
                "\n",
                "### Sentence Tokenization with NLTK\n",
                "\n",
                "Next, we'll use `sent_tokenize` to split text into sentences.\n",
                "\n",
                "```python\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "txt = \"Dr. John O'Reilly’s AI-based startup raised $10M in 2023. The company plans to expand globally next year.\"\n",
                "\n",
                "# NLTK sentence tokenization\n",
                "nltk_sentence_tokens = sent_tokenize(txt)\n",
                "\n",
                "print(\"NLTK Sentence Tokenization:\", nltk_sentence_tokens)\n",
                "```\n",
                "\n",
                "**Explanation:**\n",
                "\n",
                "  - The `sent_tokenize` function divides the text into sentences.\n",
                "  - The output is a list containing the sentences: `[\"Dr. John O'Reilly’s AI-based startup raised $10M in 2023.\", \"The company plans to expand globally next year.\"]`\n",
                "  - Sentence tokenization is crucial for tasks that require understanding the context or flow of information, such as summarization or translation.\n",
                "\n",
                "### Regex-Based Tokenization with NLTK\n",
                "\n",
                "Finally, we'll use `regexp_tokenize` to tokenize text based on a regular expression pattern.\n",
                "\n",
                "```python\n",
                "from nltk.tokenize import regexp_tokenize\n",
                "\n",
                "txt = \"Dr. John O'Reilly’s AI-based startup raised $10M in 2023. The company plans to expand globally next year.\"\n",
                "\n",
                "# NLTK regex-based tokenization\n",
                "nltk_regex_tokens = regexp_tokenize(txt, pattern=r'\\w+|\\$[\\d\\.]+|\\S')\n",
                "\n",
                "print(\"NLTK Regex Tokenization:\", nltk_regex_tokens)\n",
                "```\n",
                "\n",
                "**Explanation:**\n",
                "\n",
                "  - The `regexp_tokenize` function uses a regular expression to define token boundaries.\n",
                "  - The pattern `r'\\w+|\\$[\\d\\.]+|\\S'` matches words, dollar amounts, and non-whitespace characters.\n",
                "  - The output is a list of tokens: `['Dr', '.', 'John', 'O', \"'\", 'Reilly', '’', 's', 'AI', '-', 'based', 'startup', 'raised', '$10M', 'in', '2023', '.', 'The', 'company', 'plans', 'to', 'expand', 'globally', 'next', 'year', '.']`.\n",
                "  - Regex-based tokenization offers flexibility and precision, allowing customization for specific tokenization needs, such as extracting dates, numbers, or specific patterns.\n",
                "\n",
                "## spaCy Tokenization\n",
                "\n",
                "Now, let's briefly see how **spaCy** handles tokenization. spaCy is known for its speed and efficiency in processing large volumes of text. It is designed for production use and offers a range of features beyond tokenization, such as part-of-speech tagging, dependency parsing, and named entity recognition.\n",
                "\n",
                "```python\n",
                "import spacy\n",
                "\n",
                "txt = \"Dr. John O'Reilly’s AI-based startup raised $10M in 2023. The company plans to expand globally next year.\"\n",
                "\n",
                "# Load the spaCy model\n",
                "nlp = spacy.load(\"en_core_web_sm\")\n",
                "\n",
                "# spaCy tokenization\n",
                "doc = nlp(txt)\n",
                "spacy_tokens = [token.text for token in doc]\n",
                "\n",
                "print(\"spaCy Tokenization:\", spacy_tokens)\n",
                "```\n",
                "\n",
                "**Explanation:**\n",
                "\n",
                "  - We load the spaCy model `en_core_web_sm`, which is a small English model.\n",
                "  - The `nlp` function processes the text, and we extract tokens using a list comprehension.\n",
                "  - The output is a list of tokens: `['Dr.', 'John', 'O', \"'\", 'Reilly', '’s', 'AI', '-', 'based', 'startup', 'raised', '$', '10M', 'in', '2023', '.', 'The', 'company', 'plans', 'to', 'expand', 'globally', 'next', 'year', '.']`.\n",
                "  - spaCy's tokenization is highly efficient and can handle large datasets quickly, making it suitable for real-time applications.\n",
                "\n",
                "## Comparing NLTK and spaCy Tokenization\n",
                "\n",
                "Let's compare how NLTK and spaCy handle the tokenization of \"O'Reilly’s\". Both libraries provide similar outputs, but there are subtle differences in how they handle punctuation and special characters. Here's a side-by-side comparison of the tokenization results for \"O'Reilly’s\":\n",
                "\n",
                "  - **NLTK Tokens:** `[\"O'Reilly\", '’', 's']`\n",
                "  - **spaCy Tokens:** `['O', \"'\", 'Reilly', '’s']`\n",
                "\n",
                "The choice between NLTK and spaCy depends on the specific requirements of your project, such as speed, accuracy, and ease of use. NLTK is often preferred for educational purposes and research, while spaCy is favored in industry settings for its performance and additional NLP capabilities.\n",
                "\n",
                "## Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, we introduced the concept of tokenization and explored rule-based tokenization techniques using NLTK and spaCy. We learned how to tokenize text into words and sentences and compared the outputs of both libraries. As you move on to the practice exercises, focus on applying these techniques to different text samples and observe how tokenization affects the structure and meaning of the text. This foundational knowledge will be crucial as we delve deeper into data processing for LLMs in future lessons. Understanding the nuances of tokenization will also help you make informed decisions when selecting or designing tokenization strategies for specific NLP tasks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenize Text with NLTK\n",
                "\n",
                "You've done well exploring tokenization techniques! Now, let's put your skills to the test. Your task is to:\n",
                "\n",
                "Use NLTK's word_tokenize function to break down a text paragraph into words.\n",
                "Count the total number of tokens generated.\n",
                "This exercise will deepen your understanding of tokenization. Dive in and see how NLTK handles different text elements!\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "from nltk.tokenize import word_tokenize\n",
                "nltk.download('punkt_tab', quiet = True)\n",
                "\n",
                "# Sample text\n",
                "txt = \"Dr. John O'Reilly’s AI-based startup raised $10M in 2023. It’s amazing how quickly they’ve grown! They’re planning to expand to Europe, Asia, and beyond. Isn’t it exciting?\"\n",
                "\n",
                "# TODO: Use NLTK's word_tokenize function to tokenize the text\n",
                "\n",
                "# TODO: Count the total number of tokens\n",
                "```\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "from nltk.tokenize import word_tokenize\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "# Sample text\n",
                "txt = \"Dr. John O'Reilly’s AI-based startup raised $10M in 2023. It’s amazing how quickly they’ve grown! They’re planning to expand to Europe, Asia, and beyond. Isn’t it exciting?\"\n",
                "\n",
                "# Use NLTK's word_tokenize function to tokenize the text\n",
                "nltk_word_tokens = word_tokenize(txt)\n",
                "\n",
                "# Count the total number of tokens\n",
                "token_count = len(nltk_word_tokens)\n",
                "\n",
                "print(\"NLTK Word Tokenization:\", nltk_word_tokens)\n",
                "print(\"Total number of tokens:\", token_count)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sentence Tokenization with NLTK\n",
                "\n",
                "Nice job exploring tokenization techniques! Now, let's apply what you've learned. Your task is to:\n",
                "\n",
                "Use NLTK's sent_tokenize function to split a complex paragraph into sentences.\n",
                "Print the resulting sentences.\n",
                "Compare the number of sentences identified by NLTK with what a human might consider distinct sentences.\n",
                "This exercise will help you understand how NLTK handles tricky sentence boundaries. Give it a try and see how well it performs!\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "nltk.download('punkt_tab', quiet = True)\n",
                "\n",
                "# Sample complex text\n",
                "txt = \"Dr. Smith graduated from the University. He earned his Ph.D. in 2010! Can you believe it? 'Yes,' she replied. 'It's true.'\"\n",
                "\n",
                "# TODO: Use sent_tokenize to split the text into sentences\n",
                "\n",
                "# TODO: Compare the number of sentences identified by NLTK with human perception\n",
                "print(\"Number of sentences a human might consider:\", 5)  # Example human count\n",
                "\n",
                "```\n",
                "\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "# Sample complex text\n",
                "txt = \"Dr. Smith graduated from the University. He earned his Ph.D. in 2010! Can you believe it? 'Yes,' she replied. 'It's true.'\"\n",
                "\n",
                "# Use sent_tokenize to split the text into sentences\n",
                "nltk_sentences = sent_tokenize(txt)\n",
                "\n",
                "# Compare the number of sentences identified by NLTK with human perception\n",
                "print(\"NLTK identified the following sentences:\")\n",
                "for i, sentence in enumerate(nltk_sentences):\n",
                "    print(f\"{i + 1}. {sentence}\")\n",
                "\n",
                "print(\"\\nNumber of sentences identified by NLTK:\", len(nltk_sentences))\n",
                "print(\"Number of sentences a human might consider:\", 5)\n",
                "\n",
                "# NLTK's sent_tokenize handles cases like \"Dr.\" and \"Ph.D.\" correctly by not splitting them,\n",
                "# and it also correctly identifies sentence boundaries after punctuation like '!', '?', and '.', even within quotes.\n",
                "# This results in a count that aligns well with human perception for this specific text."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extract Monetary Values with Regex\n",
                "\n",
                "Cosmo\n",
                "Just now\n",
                "Read message aloud\n",
                "You've done a great job learning about tokenization! Now, let's dive into a practical task. Your objective is to:\n",
                "\n",
                "Modify the regular expression pattern in the regexp_tokenize function to extract monetary values from a text.\n",
                "Identify and extract amounts like \"$10\", \"$10.5M\", \"€20\", etc.\n",
                "Ensure that other numerical data is ignored.\n",
                "To help you get started, here's a brief guide on how to construct the regex pattern:\n",
                "\n",
                "Use [\\$€] to match the currency symbols $ or €.\n",
                "Follow it with \\d+ to match one or more digits.\n",
                "Use (?:\\.\\d+)? to optionally match a decimal point followed by one or more digits. The ?: makes it a non-capturing group.\n",
                "Add [MK]? to optionally match the letters M or K, which often denote million or thousand.\n",
                "This exercise will enhance your skills in customizing tokenization for specific information extraction. Let's see how well you can tailor the regex pattern!\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "from nltk.tokenize import regexp_tokenize\n",
                "nltk.download('punkt_tab', quiet = True)\n",
                "\n",
                "# Sample text with various information\n",
                "txt =\"Dr. John O'Reilly’s AI-based startup raised $10M in 2023. The company plans to expand globally next year. They also received €20 million from investors and have a 5% growth rate. Additionally, they secured $100K from a local grant. Contact: john@example.com\"\n",
                "\n",
                "# TODO: Modify the regex pattern to extract monetary values\n",
                "pattern = _____\n",
                "\n",
                "# TODO: Use NLTK's regexp_tokenize function with  regex pattern to extract monetary values from the text\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "import nltk\n",
                "from nltk.tokenize import regexp_tokenize\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "# Sample text with various information\n",
                "txt = \"Dr. John O'Reilly’s AI-based startup raised $10M in 2023. The company plans to expand globally next year. They also received €20 million from investors and have a 5% growth rate. Additionally, they secured $100K from a local grant. Contact: john@example.com\"\n",
                "\n",
                "# TODO: Modify the regex pattern to extract monetary values\n",
                "pattern = r'[\\$€]\\d+(?:\\.\\d+)?[MK]?'\n",
                "\n",
                "# TODO: Use NLTK's regexp_tokenize function with the regex pattern to extract monetary values from the text\n",
                "monetary_tokens = regexp_tokenize(txt, pattern)\n",
                "\n",
                "print(\"Extracted Monetary Values:\", monetary_tokens)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization Showdown with NLTK and spaCy\n",
                "\n",
                "You've done well exploring NLTK's tokenization techniques! Now, let's expand your skills by focusing on spaCy. Your task is to:\n",
                "\n",
                "Tokenize a specialized text using spaCy.\n",
                "Identify how spaCy handles contractions, hyphenated words, and special characters.\n",
                "This exercise will deepen your understanding of spaCy's tokenization approach. Dive in and see how this library handles text!\n",
                "\n",
                "```python\n",
                "import spacy\n",
                "\n",
                "# Sample text\n",
                "txt = \"OpenAI's GPT-4 is an AI-powered tool that won't disappoint.\"\n",
                "\n",
                "# spaCy tokenization\n",
                "# TODO: Load the spaCy model and tokenize the text\n",
                "\n",
                "```\n",
                "```python\n",
                "import spacy\n",
                "\n",
                "# Sample text\n",
                "txt = \"OpenAI's GPT-4 is an AI-powered tool that won't disappoint.\"\n",
                "\n",
                "# spaCy tokenization\n",
                "# Load the spaCy English model. Make sure you have it installed:\n",
                "# python -m spacy download en_core_web_sm\n",
                "nlp = spacy.load(\"en_core_web_sm\")\n",
                "\n",
                "# Process the text to create a Doc object\n",
                "doc = nlp(txt)\n",
                "\n",
                "# Iterate over the tokens and print their text\n",
                "print(\"spaCy Tokens:\")\n",
                "for token in doc:\n",
                "    print(token.text)\n",
                "```\n",
                "\n",
                "### Explanation of spaCy's Tokenization\n",
                "\n",
                "The output of the code will be:\n",
                "\n",
                "```\n",
                "spaCy Tokens:\n",
                "OpenAI\n",
                "'s\n",
                "GPT-4\n",
                "is\n",
                "an\n",
                "AI-powered\n",
                "tool\n",
                "that\n",
                "wo\n",
                "n't\n",
                "disappoint\n",
                ".\n",
                "```\n",
                "\n",
                "Here's a breakdown of how spaCy's tokenization handles the specific elements in the sample text:\n",
                "\n",
                "1.  **Contractions:** spaCy's default behavior is to split contractions like `\"won't\"` into separate tokens for the root word and the contraction part. In this case, `\"won't\"` is tokenized as `\"wo\"` and `\"n't\"`. This is a linguistically informed decision that separates the verb stem from the negation token.\n",
                "\n",
                "2.  **Hyphenated Words:** spaCy treats `\"AI-powered\"` as a single token. This is often the desired behavior for compound adjectives, as the entire phrase functions as a single unit to describe the \"tool.\" Unlike some simpler tokenizers, spaCy doesn't automatically split on the hyphen, recognizing that the words are closely related.\n",
                "\n",
                "3.  **Special Characters and Punctuation:**\n",
                "\n",
                "      * The possessive `'s` in `\"OpenAI's\"` is tokenized as a separate token (`'s`). This is a standard linguistic tokenization practice that separates the base noun from the possessive marker.\n",
                "      * The number-letter combination `\"GPT-4\"` is correctly identified and kept as a single token.\n",
                "      * The period at the end of the sentence (`.`) is correctly treated as its own token.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization Showdown with NLTK and spaCy"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
