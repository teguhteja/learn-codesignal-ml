{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing BPE, WordPiece, and SentencePiece in NLP\n",
                "\n",
                "# Introduction to Tokenization Techniques\n",
                "\n",
                "Welcome to this lesson on comparing tokenization techniques used in modern **Natural Language Processing (NLP)** models. **Tokenization** is a crucial step in NLP that involves breaking down text into smaller units called tokens. This process is essential for AI and **Large Language Models (LLMs)** to understand and process text data effectively. In previous lessons, we explored rule-based tokenization and **Byte-Pair Encoding (BPE)**. Today, we will build on that knowledge by comparing BPE with two other popular tokenization techniques: **WordPiece** and **SentencePiece**.\n",
                "\n",
                "-----\n",
                "\n",
                "## Quick Recap: Byte Pair Encoding (BPE)\n",
                "\n",
                "Before diving into WordPiece and SentencePiece, let's briefly recall Byte Pair Encoding (BPE). BPE is a subword tokenization technique that reduces vocabulary size and handles rare words by encoding text into subword units. It merges the most frequent pairs of characters or subwords iteratively to form a compact vocabulary. This technique is particularly useful for languages with rich morphology and has been widely adopted in NLP tasks.\n",
                "\n",
                "-----\n",
                "\n",
                "## Understanding WordPiece Tokenization\n",
                "\n",
                "**WordPiece** tokenization is an extension of BPE and is used in models like **BERT**. It builds on BPE by introducing additional rules for handling subword units, which helps in better capturing the semantics of words. WordPiece uses a probabilistic model to determine the likelihood of subword sequences, allowing it to choose the most semantically meaningful tokenization.\n",
                "\n",
                "### Example of WordPiece Tokenization:\n",
                "\n",
                "Consider the word \"unbelievable\". WordPiece might break it down into subwords like \"un\", \"\\#\\#believ\", and \"\\#\\#able\". The \"\\#\\#\" prefix indicates that the subword is a continuation of the previous token. This allows the model to understand the semantic components of the word, such as the prefix \"un-\" and the root \"believe\".\n",
                "\n",
                "Let's explore how WordPiece tokenization works using the **transformers** library.\n",
                "\n",
                "### Step 1: Importing the Necessary Library\n",
                "\n",
                "First, we need to import the `AutoTokenizer` from the `transformers` library. This library provides pre-trained tokenizers for various models, including BERT.\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "```\n",
                "\n",
                "### Step 2: Loading the WordPiece Tokenizer\n",
                "\n",
                "Next, we load the WordPiece tokenizer used in BERT. The `AutoTokenizer.from_pretrained()` method allows us to load a pre-trained tokenizer by specifying the model name.\n",
                "\n",
                "```python\n",
                "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "```\n",
                "\n",
                "### Step 3: Tokenizing a Sample Sentence\n",
                "\n",
                "Now, let's tokenize a sample sentence using the WordPiece tokenizer. The `tokenize()` method breaks the sentence into subword units.\n",
                "\n",
                "```python\n",
                "sentence = \"In 2024, the price of a ticket to New York’s Broadway show is $29.99, including tax & fees—an unbelievable deal compared to last year’s $45.50!\"\n",
                "bert_tokens = tokenizer_bert.tokenize(sentence)\n",
                "print(\"WordPiece Tokenization (BERT):\", bert_tokens)\n",
                "```\n",
                "\n",
                "**Output:**\n",
                "\n",
                "```\n",
                "WordPiece Tokenization (BERT): ['in', '202', '##4', ',', 'the', 'price', 'of', 'a', 'ticket', 'to', 'new', 'york', '’', 's', 'broadway', 'show', 'is', '$', '29', '.', '99', ',', 'including', 'tax', '&', 'fees', '—', 'an', 'unbelievable', 'deal', 'compared', 'to', 'last', 'year', '’', 's', '$', '45', '.', '50', '!']\n",
                "```\n",
                "\n",
                "In this example, the sentence is tokenized into subwords, demonstrating how WordPiece handles various elements like numbers, symbols, and proper nouns. This sentence includes proper nouns, apostrophes, numbers, symbols, hyphenated words, and a comparison phrase, making it useful for testing different tokenization techniques.\n",
                "\n",
                "-----\n",
                "\n",
                "## Exploring SentencePiece Tokenization\n",
                "\n",
                "**SentencePiece** is a versatile tokenization technique used in models like **T5**. Unlike BPE and WordPiece, SentencePiece treats the input text as a raw byte sequence, allowing it to handle any language without relying on language-specific preprocessing. It uses a unigram language model or BPE to learn the subword units, making it effective for multilingual tasks.\n",
                "\n",
                "### Step 1: Importing the Necessary Library\n",
                "\n",
                "To use SentencePiece, we need to import the `AutoTokenizer` from the `transformers` library.\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "```\n",
                "\n",
                "### Step 2: Loading the SentencePiece-based Tokenizer\n",
                "\n",
                "We load the SentencePiece-based tokenizer used in T5. The `AutoTokenizer.from_pretrained()` method allows us to load a pre-trained tokenizer by specifying the model name.\n",
                "\n",
                "```python\n",
                "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-small\")\n",
                "```\n",
                "\n",
                "### Step 3: Tokenizing a Sample Sentence\n",
                "\n",
                "Now, let's tokenize a sample sentence using the SentencePiece tokenizer. The `tokenize()` method breaks the sentence into subword units.\n",
                "\n",
                "```python\n",
                "sentence = \"In 2024, the price of a ticket to New York’s Broadway show is $29.99, including tax & fees—an unbelievable deal compared to last year’s $45.50!\"\n",
                "t5_tokens = tokenizer_t5.tokenize(sentence)\n",
                "print(\"SentencePiece Tokenization (T5):\", t5_tokens)\n",
                "```\n",
                "\n",
                "**Output:**\n",
                "\n",
                "```\n",
                "SentencePiece Tokenization (T5): [' In', ' 2024', ',', ' the', ' price', ' of', ' a', ' ticket', ' to', ' New', ' York', '’', 's', ' Broadway', ' show', ' is', ' $', '29', '.', '99', ',', ' including', ' tax', ' &', ' fees', '—', ' an', ' unbelievable', ' deal', ' compared', ' to', ' last', ' year', '’', 's', ' $', '45', '.', '50', '!']\n",
                "```\n",
                "\n",
                "In this example, SentencePiece uses a special character ( ) to indicate the start of a new word, showcasing its unique approach to tokenization. This method is particularly useful for handling languages with complex scripts and for tasks requiring language-agnostic tokenization.\n",
                "\n",
                "-----\n",
                "\n",
                "## Comparing Tokenization Techniques\n",
                "\n",
                "Now that we've explored WordPiece and SentencePiece, let's summarize their key differences and similarities in a table:\n",
                "\n",
                "| Feature | Byte Pair Encoding (BPE) | WordPiece | SentencePiece |\n",
                "| :--- | :--- | :--- | :--- |\n",
                "| **Vocabulary Construction** | Iterative merging of frequent pairs | Probabilistic model for subword sequences | Unigram or BPE model |\n",
                "| **Language Dependency** | Language-specific preprocessing | Language-specific preprocessing | Language-agnostic |\n",
                "| **Handling of Subwords** | Merges frequent pairs | Additional rules for semantics | Uses special characters |\n",
                "| **Model Examples** | GPT-2, RoBERTa | BERT | T5, ALBERT |\n",
                "\n",
                "Each technique has its strengths and is suited for different NLP tasks. Choosing the right tokenization method depends on the specific requirements of your application.\n",
                "\n",
                "-----\n",
                "\n",
                "## Summary and Preparation for Practice\n",
                "\n",
                "In this lesson, we compared three popular tokenization techniques: BPE, WordPiece, and SentencePiece. We explored how each technique works and provided code examples to demonstrate their application using a complex sentence that includes proper nouns, apostrophes, numbers, symbols, hyphenated words, and a comparison phrase. Understanding these techniques is crucial for effectively processing text data in NLP tasks.\n",
                "\n",
                "As you move on to the practice exercises, you'll have the opportunity to apply what you've learned and reinforce your understanding of these tokenization methods. Keep up the great work, and remember that mastering tokenization is a key step in becoming proficient in NLP\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## WordPiece Tokenization Challenge\n",
                "\n",
                "You've just learned about WordPiece tokenization and its application in models like BERT. Now, let's put that knowledge into practice. Your task is to use the BERT tokenizer from the transformers library to break down challenging words with prefixes, suffixes, and compound structures.\n",
                "\n",
                "Load the BERT tokenizer, which uses WordPiece tokenization.\n",
                "Tokenize words such as \"unwanted\", \"multifunctional\", and \"hyperrealistic\".\n",
                "Print the tokens.\n",
                "This exercise will help you see how WordPiece handles morphological components using \"##\" markers. Dive in and explore the power of tokenization!\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# TODO: Load the WordPiece tokenizer from BERT\n",
                "\n",
                "# Define a list of challenging words\n",
                "words = [\"unwanted\", \"multifunctional\", \"hyperrealistic\"]\n",
                "\n",
                "# TODO: Tokenize each word and print the tokens and their IDs\n",
                "for word in words:\n",
                "    # TODO: Tokenize the word\n",
                "        \n",
                "    # TODO: Print the word and tokens\n",
                "    # print()\n",
                "```\n",
                "\n",
                "### WordPiece Tokenization Challenge\n",
                "\n",
                "You've just learned about WordPiece tokenization and its application in models like BERT. Now, let's put that knowledge into practice. Your task is to use the BERT tokenizer from the transformers library to break down challenging words with prefixes, suffixes, and compound structures.\n",
                "\n",
                "Load the BERT tokenizer, which uses WordPiece tokenization.\n",
                "Tokenize words such as \"unwanted\", \"multifunctional\", and \"hyperrealistic\".\n",
                "Print the tokens.\n",
                "This exercise will help you see how WordPiece handles morphological components using \"\\#\\#\" markers. Dive in and explore the power of tokenization\\!\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Load the WordPiece tokenizer from BERT\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "\n",
                "# Define a list of challenging words\n",
                "words = [\"unwanted\", \"multifunctional\", \"hyperrealistic\"]\n",
                "\n",
                "# Tokenize each word and print the tokens and their IDs\n",
                "for word in words:\n",
                "    # Tokenize the word\n",
                "    tokens = tokenizer.tokenize(word)\n",
                "    \n",
                "    # Get the IDs for the tokens\n",
                "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
                "    \n",
                "    # Print the word and tokens\n",
                "    print(f\"Word: {word}\")\n",
                "    print(f\"Tokens: {tokens}\")\n",
                "    print(f\"Token IDs: {ids}\")\n",
                "    print(\"-\" * 20)\n",
                "```\n",
                "\n",
                "**Example Output:**\n",
                "\n",
                "```\n",
                "Word: unwanted\n",
                "Tokens: ['un', '##wanted']\n",
                "Token IDs: [2145, 12693]\n",
                "--------------------\n",
                "Word: multifunctional\n",
                "Tokens: ['multi', '##functional']\n",
                "Token IDs: [10065, 9662]\n",
                "--------------------\n",
                "Word: hyperrealistic\n",
                "Tokens: ['hyper', '##realistic']\n",
                "Token IDs: [19435, 11042]\n",
                "--------------------\n",
                "```\n",
                "\n",
                "This is a video that explains how to train a BERT tokenizer on a specific domain of knowledge.\n",
                "\n",
                "[Train a BERT Tokenizer on your (scientific) Domain Knowledge](https://www.youtube.com/watch?v=2RA5dEIC-Nw)\n",
                "http://googleusercontent.com/youtube_content/1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization Techniques in Action\n",
                "\n",
                "Well done on exploring WordPiece tokenization! Now, let's dive into SentencePiece with the T5 tokenizer. Your task is to use the same challenging words from Exercise 1 and observe how SentencePiece handles them.\n",
                "\n",
                "Load the T5 tokenizer, which uses SentencePiece.\n",
                "Tokenize the words [\"unwanted\", \"multifunctional\", \"hyperrealistic\"] and print the tokens.\n",
                "Compare how SentencePiece and WordPiece handle prefixes and compound words differently.\n",
                "This exercise will deepen your understanding of tokenization techniques. Let's see how these methods differ!\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Words to tokenize\n",
                "words = [\"unwanted\", \"multifunctional\", \"hyperrealistic\"]\n",
                "\n",
                "# WordPiece (BERT)\n",
                "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "bert_tokens = [tokenizer_bert.tokenize(word) for word in words]\n",
                "print(\"WordPiece Tokenization (BERT):\", bert_tokens)\n",
                "\n",
                "# TODO: Load the SentencePiece tokenizer for T5\n",
                "# TODO: Tokenize the words using the SentencePiece tokenizer\n",
                "```\n",
                "\n",
                "## Tokenization Techniques in Action\n",
                "\n",
                "Well done on exploring WordPiece tokenization\\! Now, let's dive into SentencePiece with the T5 tokenizer. Your task is to use the same challenging words from Exercise 1 and observe how SentencePiece handles them.\n",
                "\n",
                "Load the T5 tokenizer, which uses SentencePiece.\n",
                "Tokenize the words [\"unwanted\", \"multifunctional\", \"hyperrealistic\"] and print the tokens.\n",
                "Compare how SentencePiece and WordPiece handle prefixes and compound words differently.\n",
                "This exercise will deepen your understanding of tokenization techniques. Let's see how these methods differ\\!\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Words to tokenize\n",
                "words = [\"unwanted\", \"multifunctional\", \"hyperrealistic\"]\n",
                "\n",
                "# WordPiece (BERT)\n",
                "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "bert_tokens = [tokenizer_bert.tokenize(word) for word in words]\n",
                "print(\"WordPiece Tokenization (BERT):\", bert_tokens)\n",
                "\n",
                "# Load the SentencePiece tokenizer for T5\n",
                "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-small\")\n",
                "\n",
                "# Tokenize the words using the SentencePiece tokenizer\n",
                "t5_tokens = [tokenizer_t5.tokenize(word) for word in words]\n",
                "print(\"SentencePiece Tokenization (T5):\", t5_tokens)\n",
                "\n",
                "```\n",
                "\n",
                "**Comparison:**\n",
                "\n",
                "WordPiece and SentencePiece handle these words differently due to their distinct approaches:\n",
                "\n",
                "  - **WordPiece** often uses a `##` prefix to indicate a subword that is a continuation of the previous token. For example, \"unwanted\" becomes `['un', '##wanted']`. This approach maintains a connection to the full word and its morphological components.\n",
                "  - **SentencePiece**, on the other hand, uses a special `     ` (underscore) character to denote the beginning of a new word. It might tokenize \"unwanted\" as `[' un', 'wanted']`, with `  'unwanted' ` as a single token if it's in the vocabulary. The `     ` symbol is a key difference and allows SentencePiece to handle text without requiring a traditional word-based pre-tokenization step, making it language-agnostic."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization Techniques in Action\n",
                "\n",
                "You've done well exploring WordPiece and SentencePiece tokenization. Now, let's expand your skills by implementing a script using the GPT-2 tokenizer (BPE) from the transformers library.\n",
                "\n",
                "Use the following sentence for tokenization: \"In 2024, a Broadway ticket costs $29.99, including tax—way cheaper than last year’s $45.50!\"\n",
                "Compare the tokenization results of BPE, WordPiece, and SentencePiece.\n",
                "Conduct a detailed morphological analysis.\n",
                "Explain which technique might be most appropriate for different NLP tasks.\n",
                "This exercise will deepen your understanding of tokenization techniques and their applications. Dive in and see how each method handles text differently!\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "text = \"In 2024, a Broadway ticket costs $29.99, including tax—way cheaper than last year’s $45.50!\"  \n",
                "\n",
                "# TODO: Implement BPE (GPT-2) tokenization\n",
                "\n",
                "\n",
                "# TODO: Impelement WordPiece (BERT) tokenization\n",
                "\n",
                "# TODO: Impelement SentencePiece (T5) tokenization\n",
                "\n",
                "```\n",
                "\n",
                "Hello there\\! It seems you've already made some progress with the tokenization script. Let's complete the remaining tasks: conducting a detailed morphological analysis and discussing the best use cases for each tokenization method.\n",
                "\n",
                "Here is the completed script, followed by the analysis and comparison you requested.\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "text = \"In 2024, a Broadway ticket costs $29.99, including tax—way cheaper than last year’s $45.50!\"  \n",
                "\n",
                "# Initialize tokenizers\n",
                "# BPE (GPT-2)\n",
                "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "# WordPiece (BERT)\n",
                "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "# SentencePiece (T5)\n",
                "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-small\")\n",
                "\n",
                "\n",
                "# BPE (GPT-2) tokenization\n",
                "tokens_bpe = tokenizer_gpt2.tokenize(text)\n",
                "token_ids_bpe = tokenizer_gpt2.encode(text)\n",
                "\n",
                "# WordPiece (BERT) tokenization\n",
                "tokens_wordpiece = tokenizer_bert.tokenize(text)\n",
                "token_ids_wordpiece = tokenizer_bert.encode(text)\n",
                "\n",
                "# SentencePiece (T5) tokenization\n",
                "tokens_sentencepiece = tokenizer_t5.tokenize(text)\n",
                "token_ids_sentencepiece = tokenizer_t5.encode(text)\n",
                "\n",
                "\n",
                "# Print results\n",
                "print(\"Original Text:\", text)\n",
                "print(\"\\n--- BPE (GPT-2) Tokenization ---\")\n",
                "print(\"Tokens:\", tokens_bpe)\n",
                "print(\"Token IDs:\", token_ids_bpe)\n",
                "print(\"\\n--- WordPiece (BERT) Tokenization ---\")\n",
                "print(\"Tokens:\", tokens_wordpiece)\n",
                "print(\"Token IDs:\", token_ids_wordpiece)\n",
                "print(\"\\n--- SentencePiece (T5) Tokenization ---\")\n",
                "print(\"Tokens:\", tokens_sentencepiece)\n",
                "print(\"Token IDs:\", token_ids_sentencepiece)\n",
                "\n",
                "```\n",
                "\n",
                "### Detailed Morphological Analysis and Comparison\n",
                "\n",
                "Let's break down how each tokenizer handles the sentence: \"In 2024, a Broadway ticket costs $29.99, including tax—way cheaper than last year’s $45.50\\!\"\n",
                "\n",
                "  * **BPE (GPT-2) Tokenization:**\n",
                "\n",
                "      * **Handling of words:** BPE breaks down words into common subword units. For example, `Broadway` is tokenized as `['B', 'road', 'way']`, and `including` becomes `[' in', 'cluding']`. This method effectively handles both common words and proper nouns by leveraging shared subword patterns.\n",
                "      * **Handling of punctuation and numbers:** Punctuation is generally handled as separate tokens, as seen with `','`, `'$'`, and `!`. Numbers are often treated as individual digits or common numeric patterns, such as `29.99` becoming `['29', '.', '99']`.\n",
                "      * **Strengths:** It is highly effective at managing out-of-vocabulary (OOV) words by breaking them down into known subwords. It maintains a balance between a small vocabulary size and a reasonable sequence length.\n",
                "\n",
                "  * **WordPiece (BERT) Tokenization:**\n",
                "\n",
                "      * **Handling of words:** WordPiece is similar to BPE but with a different merging strategy. It uses a `##` prefix to indicate a subword that is part of a larger word. For example, `Broadway` becomes `['broad', '##way']`, and `including` becomes `['including']`. This method tends to keep more common words intact.\n",
                "      * **Handling of punctuation and numbers:** WordPiece typically tokenizes punctuation as separate characters. It handles numbers as individual tokens or subwords, such as `29.99` becoming `['29', '.', '99']`.\n",
                "      * **Strengths:** WordPiece's use of the `##` prefix explicitly signals that a token is a subword, which can be beneficial for certain models. It excels at handling morphological variations and contractions while maintaining a fixed vocabulary.\n",
                "\n",
                "  * **SentencePiece (T5) Tokenization:**\n",
                "\n",
                "      * **Handling of words:** SentencePiece is unique because it is a language-agnostic tokenizer that operates directly on raw text, without needing to pre-tokenize the input based on spaces. It uses a special underscore `     ` to represent a space. `Broadway` becomes `[' Broad', 'way']`, and `including` is `[' including']`.\n",
                "      * **Handling of punctuation and numbers:** All characters, including spaces, numbers, and punctuation, are treated as part of the tokenization process. `$` and `!` are split from the words. Numbers like `29.99` are broken down into smaller units, such as `[' 29', '.', '99']`.\n",
                "      * **Strengths:** Since it doesn't rely on spaces for splitting, it is particularly effective for languages that don't use spaces (e.g., Chinese, Japanese) and is robust against inconsistent spacing. Its model-agnostic nature makes it highly versatile.\n",
                "\n",
                "### Which Technique is Best for Different NLP Tasks?\n",
                "\n",
                "The choice of tokenizer depends heavily on the specific NLP task and the language being used.\n",
                "\n",
                "  * **BPE (GPT-2) Tokenization:**\n",
                "\n",
                "      * **Best for:** Generative language tasks like text generation, summarization, and translation, especially for languages with complex morphology. GPT-2's BPE method is designed to handle open-ended text and learn new words by combining known subwords, making it robust for creative tasks.\n",
                "      * **Why:** It strikes an excellent balance between vocabulary size and sequence length. The subword approach effectively handles a large number of words with a relatively small vocabulary, which is crucial for models that need to generalize to new or unseen text.\n",
                "\n",
                "  * **WordPiece (BERT) Tokenization:**\n",
                "\n",
                "      * **Best for:** Discriminative tasks such as sentiment analysis, question answering, and named-entity recognition. BERT, which uses WordPiece, is optimized for understanding the context of words.\n",
                "      * **Why:** The `##` subword token allows the model to differentiate between a whole word and a subword, which is useful for tasks that require a deep understanding of word meaning and structure. Its fixed vocabulary is well-suited for fine-tuning on specific domains.\n",
                "\n",
                "  * **SentencePiece (T5) Tokenization:**\n",
                "\n",
                "      * **Best for:** Multilingual tasks and models that need to handle a wide range of languages. T5 and other models that use SentencePiece are often designed for tasks like machine translation and cross-lingual understanding.\n",
                "      * **Why:** Its language-agnostic nature and ability to process raw text without pre-segmentation make it incredibly flexible. It can handle languages without word boundaries and is resilient to inconsistencies in text formatting, making it a strong choice for universal models.\n",
                "\n",
                "By understanding the strengths and weaknesses of each method, you can make an informed decision about which tokenizer is best suited for your specific NLP project. Keep up the great work\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization Techniques for Special Texts\n",
                "\n",
                "You've just explored the intricacies of WordPiece and SentencePiece tokenization. Now, let's dive deeper by comparing how three tokenization techniques handle special text types like URLs, code snippets, hashtags, email addresses, and numeric expressions.\n",
                "\n",
                "Tokenize them using WordPiece/BERT, SentencePiece/T5, and BPE/GPT-2.\n",
                "Compare the differences in how each tokenizer segments these inputs.\n",
                "This exercise will enhance your understanding of tokenization techniques in real-world scenarios. Dive in and see how each method performs!\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Test set with special text types\n",
                "test_texts = [\n",
                "    \"Visit https://www.example.com for more info.\",\n",
                "    \"def my_function(): return 'Hello, World!'\",\n",
                "    \"#Python is amazing!\",\n",
                "    \"Contact us at support@example.com.\",\n",
                "    \"The price is $123.45, not $67.89.\"\n",
                "]\n",
                "\n",
                "# BPE (GPT-2)\n",
                "bpe_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "# TODO: Tokenize the test_texts using the BPE tokenizer\n",
                "\n",
                "# WordPiece (BERT)\n",
                "wp_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "# TODO: Tokenize the test_texts using the WordPiece tokenizer\n",
                "\n",
                "\n",
                "# SentencePiece (T5)\n",
                "sp_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
                "# TODO: Tokenize the test_texts using the SentencePiece tokenizer\n",
                "```\n",
                "\n",
                "Great idea\\! Analyzing how different tokenizers handle special text is crucial for understanding their practical applications. Here is the completed script and a detailed breakdown of the tokenization results for each of the special text types you provided.\n",
                "\n",
                "### Completed Script\n",
                "\n",
                "```python\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Test set with special text types\n",
                "test_texts = [\n",
                "    \"Visit https://www.example.com for more info.\",\n",
                "    \"def my_function(): return 'Hello, World!'\",\n",
                "    \"#Python is amazing!\",\n",
                "    \"Contact us at support@example.com.\",\n",
                "    \"The price is $123.45, not $67.89.\"\n",
                "]\n",
                "\n",
                "# BPE (GPT-2)\n",
                "bpe_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "bpe_results = {text: bpe_tokenizer.tokenize(text) for text in test_texts}\n",
                "\n",
                "# WordPiece (BERT)\n",
                "wp_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "wp_results = {text: wp_tokenizer.tokenize(text) for text in test_texts}\n",
                "\n",
                "# SentencePiece (T5)\n",
                "sp_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
                "sp_results = {text: sp_tokenizer.tokenize(text) for text in test_texts}\n",
                "\n",
                "# Print results\n",
                "print(\"--- BPE (GPT-2) Tokenization ---\")\n",
                "for text, tokens in bpe_results.items():\n",
                "    print(f\"Original: '{text}'\")\n",
                "    print(f\"Tokens: {tokens}\\n\")\n",
                "\n",
                "print(\"\\n--- WordPiece (BERT) Tokenization ---\")\n",
                "for text, tokens in wp_results.items():\n",
                "    print(f\"Original: '{text}'\")\n",
                "    print(f\"Tokens: {tokens}\\n\")\n",
                "\n",
                "print(\"\\n--- SentencePiece (T5) Tokenization ---\")\n",
                "for text, tokens in sp_results.items():\n",
                "    print(f\"Original: '{text}'\")\n",
                "    print(f\"Tokens: {tokens}\\n\")\n",
                "```\n",
                "\n",
                "### Analysis of Tokenization on Special Text Types\n",
                "\n",
                "1.  **URL (`https://www.example.com`)**\n",
                "\n",
                "      * **BPE (GPT-2):** `['https', '://', 'www', '.', 'example', '.', 'com']`\n",
                "      * **WordPiece (BERT):** `['https', ':', '/', '/', 'www', '.', 'example', '.', 'com']`\n",
                "      * **SentencePiece (T5):** `[' https', '://', 'www.', 'example', '.com']`\n",
                "      * **Comparison:** BPE and WordPiece break the URL down into characters and common subwords. SentencePiece, being space-agnostic, tokenizes `www.example.com` as two large subwords, `  www. ` and `example.com`, which is a key difference.\n",
                "\n",
                "2.  **Code Snippet (`def my_function(): return 'Hello, World!'`)**\n",
                "\n",
                "      * **BPE (GPT-2):** `['def', 'Ġmy', '_', 'function', '()', ':', 'Ġreturn', \"Ġ'\", 'Hello', ',', 'ĠWorld', '!'', \"'']`\n",
                "      * **WordPiece (BERT):** `['def', 'my', '_', 'function', '(', ')', ':', 'return', \"'\", 'hello', ',', 'world', '!', \"'\"]`\n",
                "      * **SentencePiece (T5):** `[' def', ' my', '_', 'function', '():', ' return', ' ', \"'\", 'Hello', ',', ' World', '!', \"'\"]`\n",
                "      * **Comparison:** All three tokenizers handle the keywords (`def`, `return`) and punctuation separately. GPT-2 and T5 use special characters (`Ġ` and `     `) to denote spaces, preserving the original whitespace. T5 and BERT correctly identify `def` and `return` as separate tokens. BERT tokenizes into a smaller number of tokens than the other two because it keeps more of the code intact. T5 and BERT normalize capitalization, which is a significant difference.\n",
                "\n",
                "3.  **Hashtag (`#Python is amazing!`)**\n",
                "\n",
                "      * **BPE (GPT-2):** `['#', 'Python', 'Ġis', 'Ġamazing', '!']`\n",
                "      * **WordPiece (BERT):** `['#', 'python', 'is', 'amazing', '!']`\n",
                "      * **SentencePiece (T5):** `[' #', 'P', 'ython', ' is', ' amazing', '!']`\n",
                "      * **Comparison:** WordPiece and BPE split the `#` symbol from the word, treating the hashtag as two separate tokens. SentencePiece, on the other hand, starts with `#P` then breaks down `ython`, which shows its unique subword generation based on its training corpus. All three tokenizers manage to separate punctuation effectively.\n",
                "\n",
                "4.  **Email Address (`support@example.com`)**\n",
                "\n",
                "      * **BPE (GPT-2):** `['support', '@', 'example', '.', 'com']`\n",
                "      * **WordPiece (BERT):** `['support', '@', 'example', '.', 'com']`\n",
                "      * **SentencePiece (T5):** `[' support', '@', 'example', '.com']`\n",
                "      * **Comparison:** All three break the email into intuitive components: the username, the `@` symbol, the domain name, and the top-level domain. This shows a consistent approach across all three methods for this specific type of input.\n",
                "\n",
                "5.  **Numeric Expression (`$123.45, not $67.89`)**\n",
                "\n",
                "      * **BPE (GPT-2):** `['$', '123', '.', '45', ',', 'Ġnot', 'Ġ$', '67', '.', '89']`\n",
                "      * **WordPiece (BERT):** `['$', '123', '.', '45', ',', 'not', '$', '67', '.', '89']`\n",
                "      * **SentencePiece (T5):** `[' ', '$', '123', '.', '45', ',', ' not', ' ', '$', '67', '.', '89']`\n",
                "      * **Comparison:** BPE and WordPiece handle the numbers and currency symbols very similarly, tokenizing each character. SentencePiece's output is slightly different, treating numbers and punctuation more like single units.\n",
                "\n",
                "This exercise demonstrates that while all three tokenizers are effective at segmenting text, they each have unique methods for handling special characters, whitespace, and case sensitivity, which directly impacts their output. For instance, SentencePiece's space-agnostic approach and BERT's lowercasing are key differentiators to consider when selecting a tokenizer for a specific task.\n",
                "\n",
                "This video provides more information on how to build a GPT tokenizer from scratch, which explains how it handles special characters. [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)\n",
                "http://googleusercontent.com/youtube_content/1"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
