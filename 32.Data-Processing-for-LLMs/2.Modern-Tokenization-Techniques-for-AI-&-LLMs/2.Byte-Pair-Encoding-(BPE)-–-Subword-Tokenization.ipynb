{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unit 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Byte-Pair Encoding (BPE) – Subword Tokenization\n",
                "\n",
                "Here is the text converted into Markdown format.\n",
                "\n",
                "# Introduction to Subword Tokenization and Byte-Pair Encoding (BPE)\n",
                "\n",
                "Welcome to the next step in your journey through **Natural Language Processing (NLP)**. In this lesson, we will explore **subword tokenization**, a technique that helps reduce vocabulary size and handle out-of-vocabulary words, making it a crucial tool for modern NLP models. We will focus on **Byte-Pair Encoding (BPE)**, a popular subword tokenization method.\n",
                "\n",
                "-----\n",
                "\n",
                "### Why Subword Tokenization and Understanding BPE\n",
                "\n",
                "Subword tokenization is essential because it offers more flexibility and efficiency compared to traditional tokenization methods. It allows us to break down words into smaller units, which is particularly useful for handling rare and out-of-vocabulary words. This approach improves model performance and reduces the overall vocabulary size.\n",
                "\n",
                "**Byte-Pair Encoding (BPE)** is a widely-used subword tokenization method that iteratively merges the most frequent pairs of bytes or characters in a text corpus. This process continues until a predefined vocabulary size is reached.\n",
                "\n",
                "-----\n",
                "\n",
                "### Example of Subword Tokenization and BPE\n",
                "\n",
                "Consider the word \"unhappiness\". Traditional tokenization might treat it as a single token, but subword tokenization can break it down into smaller units like **\"un\"**, **\"happi\"**, and **\"ness\"**. This breakdown allows the model to understand and process parts of the word even if the entire word is rare or unseen.\n",
                "\n",
                "Let's say we have a corpus with the words \"low\", \"lowest\", and \"newer\". BPE might start by merging frequent pairs like \"lo\" and \"we\", eventually creating subword units like \"low\", \"est\", and \"new\". This process allows the model to efficiently handle variations of words.\n",
                "\n",
                "### Advantages of BPE\n",
                "\n",
                "  * **Reduces Vocabulary Size:** By merging frequent pairs, BPE creates a compact vocabulary.\n",
                "  * **Handles Rare Words:** Breaks down rare words into known subword units, improving model performance.\n",
                "  * **Improves Efficiency:** Smaller vocabularies lead to faster and more efficient model training and inference.\n",
                "\n",
                "-----\n",
                "\n",
                "### Implementing BPE with Pretrained Models\n",
                "\n",
                "In most real-world applications, training a BPE model from scratch is not necessary. Instead, we can leverage pretrained models that already utilize BPE for tokenization. However, there are specific cases where training your own BPE tokenizer might be beneficial:\n",
                "\n",
                "  * **Domain-Specific Language:** If your application involves a specialized domain with unique vocabulary, training a BPE tokenizer on a domain-specific corpus can improve performance.\n",
                "  * **Low-Resource Languages:** For languages with limited available data, a custom BPE tokenizer can be tailored to better handle linguistic nuances.\n",
                "  * **Research and Experimentation:** If you're conducting research or experimenting with novel NLP techniques, training your own BPE tokenizer can provide insights and flexibility.\n",
                "\n",
                "For this lesson, we will focus on using pretrained models, which are efficient and widely applicable. Pretrained models come with a predefined vocabulary size, which is crucial for balancing model performance and computational efficiency. A larger vocabulary size can capture more linguistic nuances but may increase computational requirements, while a smaller vocabulary size can improve efficiency but might miss some details.\n",
                "\n",
                "### Pretrained Models Using BPE\n",
                "\n",
                "Byte-Pair Encoding is widely used in many state-of-the-art pretrained language models due to its efficiency in handling subword tokenization. Here are a few notable models that utilize BPE:\n",
                "\n",
                "  * **GPT-2 (Generative Pre-trained Transformer 2):** Developed by OpenAI, GPT-2 uses BPE to tokenize text, allowing it to handle a vast array of vocabulary efficiently. This model is known for its ability to generate coherent and contextually relevant text.\n",
                "  * **BERT (Bidirectional Encoder Representations from Transformers):** BERT, developed by Google, employs a variant of BPE known as **WordPiece**. While not exactly BPE, WordPiece shares similar principles of subword tokenization, breaking down words into smaller units to improve understanding and context.\n",
                "  * **RoBERTa (A Robustly Optimized BERT Pretraining Approach):** RoBERTa, an optimized version of BERT, also uses BPE for tokenization. It builds on BERT's architecture and training methodology, achieving improved performance on various NLP tasks.\n",
                "\n",
                "These models demonstrate the effectiveness of BPE in handling diverse linguistic structures and improving the performance of NLP applications. By leveraging BPE, these models can efficiently process and understand text, making them powerful tools for a wide range of language tasks.\n",
                "\n",
                "-----\n",
                "\n",
                "### Step-by-Step Implementation with Pretrained Models:\n",
                "\n",
                "To see BPE in action with a pretrained model, we can use the **transformers** library by Hugging Face, which provides easy access to many pretrained models. Below is an example of how to use GPT-2 with BPE tokenization:\n",
                "\n",
                "#### 1\\. Load a Pretrained Model and Tokenizer:\n",
                "\n",
                "Use the `transformers` library to load GPT-2 and its tokenizer.\n",
                "\n",
                "```python\n",
                "from transformers import GPT2Tokenizer\n",
                "\n",
                "# Load the tokenizer\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
                "```\n",
                "\n",
                "*To use RoBERTa instead of GPT-2, you would load the RoBERTa tokenizer by replacing `GPT2Tokenizer` with `RobertaTokenizer` and specifying `\"roberta-base\"` as the model name.*\n",
                "\n",
                "#### 2\\. Tokenize and Encode Text:\n",
                "\n",
                "Use the tokenizer to encode a sentence, which will demonstrate BPE in action.\n",
                "\n",
                "```python\n",
                "input_text = \"Tokenization is essential\"\n",
                "encoded_input = tokenizer.encode(input_text)\n",
                "\n",
                "print(\"Encoded input:\", encoded_input)\n",
                "print(\"Tokenized output:\", tokenizer.convert_ids_to_tokens(encoded_input))\n",
                "```\n",
                "\n",
                "  * **`encoded_input`:** This line prints the encoded input, which is a list containing the token IDs. Each number in the list represents a specific subword token in the vocabulary used by the GPT-2 model. These IDs are used internally by the model to process the input text.\n",
                "  * **`tokenizer.convert_ids_to_tokens(encoded_input)`:** This line converts the token IDs back to their corresponding subword tokens. It helps in understanding how the input text is broken down into subword units by the BPE tokenizer. The output shows the actual tokens that correspond to the encoded input IDs.\n",
                "\n",
                "#### Output:\n",
                "\n",
                "The output will display the tokenized version of the input sentence, showing how BPE breaks it into subword units.\n",
                "\n",
                "```text\n",
                "Encoded input: [464, 9220, 318, 13779]\n",
                "Tokenized output: ['Token', 'ization', 'Ġis', 'Ġessential']\n",
                "```\n",
                "\n",
                "  * The `Ġ` in the tokenized output represents a space character. In the BPE tokenization used by models like GPT-2, spaces are often represented by a special character (in this case, `Ġ`) to indicate the start of a new word or subword following a space. This helps the model distinguish between words that appear at the beginning of a sentence or after a space and those that are part of a compound word or subword.\n",
                "\n",
                "-----\n",
                "\n",
                "### Summary and Next Steps\n",
                "\n",
                "In this lesson, we introduced the concept of **subword tokenization**, highlighting its importance in handling rare and out-of-vocabulary words while reducing vocabulary size. We explored **Byte-Pair Encoding (BPE)**, a widely-used subword tokenization technique, and demonstrated its implementation using pretrained models. We also discussed how pretrained models like GPT-2 leverage BPE for efficient tokenization and processing of text.\n",
                "\n",
                "As you move on to the practice exercises, focus on applying these concepts to gain hands-on experience. Experiment with different corpora and vocabulary sizes to see how BPE affects tokenization. This practical application will solidify your understanding and prepare you for more advanced NLP tasks. Keep up the great work, and continue to build on your knowledge of tokenization techniques\\!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exploring Pre-trained Tokenizers with GPT-2\n",
                "\n",
                "In the previous lesson, you explored how pre-trained tokenizers utilize BPE for subword tokenization. Now, let's apply this knowledge to a practical task.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Use the pre-trained GPT2Tokenizer to encode the sentence \"Understanding tokenization is crucial.\"\n",
                "Print the resulting subword tokens.\n",
                "This exercise will help reinforce your understanding of how pre-trained tokenizers work. Dive in and examine the results!\n",
                "\n",
                "```python\n",
                "from transformers import GPT2Tokenizer\n",
                "\n",
                "\n",
                "# TODO: Initialize the pre-trained GPT-2 tokenizer\n",
                "# TODO: Encode the sentence  \"Understanding tokenization is crucial.\" using the pre-trained tokenizer\n",
                "# TODO: Print the tokenized output using the pre-trained tokenizer\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from transformers import GPT2Tokenizer\n",
                "\n",
                "# Initialize the pre-trained GPT-2 tokenizer\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
                "\n",
                "# Encode the sentence \"Understanding tokenization is crucial.\"\n",
                "input_text = \"Understanding tokenization is crucial.\"\n",
                "encoded_input = tokenizer.encode(input_text)\n",
                "\n",
                "# Convert the token IDs back to human-readable subword tokens\n",
                "tokenized_output = tokenizer.convert_ids_to_tokens(encoded_input)\n",
                "\n",
                "# Print the tokenized output\n",
                "print(\"Original sentence:\", input_text)\n",
                "print(\"Tokenized output:\", tokenized_output)\n",
                "```\n",
                "\n",
                "### Explanation of the Output\n",
                "\n",
                "When you run the code, you'll see the following output:\n",
                "\n",
                "```\n",
                "Original sentence: Understanding tokenization is crucial.\n",
                "Tokenized output: ['Understanding', 'Ġtokenization', 'Ġis', 'Ġcrucial', '.']\n",
                "```\n",
                "\n",
                "The output demonstrates how the **GPT-2 BPE tokenizer** handles the sentence:\n",
                "\n",
                "  * **\"Understanding\"** is recognized as a single, complete word and tokenized as such.\n",
                "  * **\"Ġtokenization\"** is a great example of subword tokenization. The word \"tokenization\" is split into a subword piece. The special character `Ġ` (U+2581) is a common way in BPE tokenizers to represent a space and indicates that this subword is the start of a new word.\n",
                "  * **\"Ġis\"** and **\"Ġcrucial\"** are also prefaced with `Ġ`, showing that they are complete words following a space.\n",
                "  * The final period **\".\"** is treated as its own separate token, which is standard for punctuation in most tokenizers.\n",
                "\n",
                "This exercise shows how a powerful, pre-trained tokenizer breaks down text into meaningful units that are efficient for a language model to process."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using Pre-trained Tokenizers with RoBERTa\n",
                "\n",
                "You've just learned about the power of subword tokenization and BPE. Now, let's dive into using a pre-trained tokenizer to see these concepts in action.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Import the RobertaTokenizer from the transformers library.\n",
                "Load the tokenizer using RobertaTokenizer.from_pretrained(\"roberta-base\").\n",
                "Encode the provided sentence.\n",
                "Print the resulting subword tokens.\n",
                "This hands-on exercise will help you see how pre-trained tokenizers work. Let's get started and see what insights you can uncover!\n",
                "\n",
                "```python\n",
                "# TODO: Import the RobertaTokenizer from the transformers library\n",
                "\n",
                "# TODO: Load the tokenizer using RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
                "\n",
                "# TODO: Encode the input text\n",
                "input_text = \"I usually wake up at 7:30 AM, grab a coffee, and check my emails before starting work.\"\n",
                "# TODO: Use the tokenizer to encode the input_text\n",
                "\n",
                "# TODO: Print the tokenized output using tokenizer.convert_ids_to_tokens(encoded_input)\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from transformers import RobertaTokenizer\n",
                "\n",
                "# Load the tokenizer using RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
                "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
                "\n",
                "# Encode the input text\n",
                "input_text = \"I usually wake up at 7:30 AM, grab a coffee, and check my emails before starting work.\"\n",
                "encoded_input = tokenizer.encode(input_text)\n",
                "\n",
                "# Print the tokenized output\n",
                "tokenized_output = tokenizer.convert_ids_to_tokens(encoded_input)\n",
                "print(\"Original sentence:\", input_text)\n",
                "print(\"Tokenized output:\", tokenized_output)\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### Explanation of the Tokenized Output\n",
                "\n",
                "The output of the code will look like this:\n",
                "\n",
                "```\n",
                "Original sentence: I usually wake up at 7:30 AM, grab a coffee, and check my emails before starting work.\n",
                "Tokenized output: ['<s>', 'I', 'Ġusually', 'Ġwake', 'Ġup', 'Ġat', 'Ġ7', ':', '30', 'ĠAM', ',', 'Ġgrab', 'Ġa', 'Ġcoffee', ',', 'Ġand', 'Ġcheck', 'Ġmy', 'Ġemails', 'Ġbefore', 'Ġstarting', 'Ġwork', '.', '</s>']\n",
                "```\n",
                "\n",
                "Here's a breakdown of how the **RoBERTa tokenizer** handles the sentence:\n",
                "\n",
                "  * **Special Tokens:** The output starts with `<s>` and ends with `</s>`. These are special tokens used by the RoBERTa model to mark the beginning and end of a sentence.\n",
                "  * **Space Handling:** The `Ġ` character is used to represent a space. This is a common feature in many BPE-based tokenizers like RoBERTa's. For example, `Ġusually` and `Ġwake` are full words that follow a space. This helps the model accurately reconstruct the original text.\n",
                "  * **Punctuation and Numbers:** Punctuation like the comma (`,`) and period (`.`) are tokenized separately. Interestingly, the time `7:30` is split into `7`, `:`, and `30`, demonstrating how the tokenizer handles numerical data and special characters.\n",
                "  * **Contractions and Hyphenated Words:** RoBERTa's tokenizer, like GPT-2's, is designed to handle common linguistic patterns. While this specific sentence doesn't have contractions or hyphenated words, you can see its ability to break down text into logical subwords and full words."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Tokenization with GPT-2 and RoBERTa\n",
                "\n",
                "You've just seen how pretrained models use BPE for tokenization. In this last task for this unit, let's compare how different models handle sentences from various contexts.\n",
                "\n",
                "Your task is to:\n",
                "\n",
                "Use the pretrained GPT2Tokenizer to encode the following sentences:\n",
                "\n",
                "Casual: \"Hey, how's it going? I was thinking we could catch up over coffee sometime soon.\"\n",
                "Physics: \"The gravitational force between two masses is inversely proportional to the square of the distance between them.\"\n",
                "Rare Words: \"The quizzaciously zephyrous xylophonist played a mellifluous tune, captivating the audience with its ethereal beauty.\"\n",
                "Use the pretrained RobertaTokenizer to encode the same sentences.\n",
                "\n",
                "Print and compare the tokenized outputs from both models for each sentence.\n",
                "\n",
                "This exercise will help you understand the differences in tokenization between GPT-2 and RoBERTa across different styles and contexts. Dive in and see how each model processes the text!\n",
                "\n",
                "```python\n",
                "from transformers import GPT2Tokenizer, RobertaTokenizer\n",
                "\n",
                "# Load the GPT-2 tokenizer\n",
                "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
                "\n",
                "# Load the RoBERTa tokenizer\n",
                "# TODO: Load the RoBERTa tokenizer\n",
                "\n",
                "# Define the input texts\n",
                "casual_text = \"Hey, how's it going? I was thinking we could catch up over coffee sometime soon.\"\n",
                "physics_text = \"The gravitational force between two masses is inversely proportional to the square of the distance between them.\"\n",
                "rare_words_text = \"The quizzaciously zephyrous xylophonist played a mellifluous tune, captivating the audience with its ethereal beauty.\"\n",
                "\n",
                "# TODO: Encode the input texts using GPT-2 tokenizer\n",
                "# TODO: Encode the input texts using GPT-2 tokenizer and print the tokenized outputs\n",
                "\n",
                "# TODO: Encode the input texts using RoBERTa tokenizer\n",
                "# TODO: Encode the input texts using RoBERTa tokenizer and print the tokenized outputs\n",
                "\n",
                "```\n",
                "\n",
                "```python\n",
                "from transformers import GPT2Tokenizer, RobertaTokenizer\n",
                "\n",
                "# Load the GPT-2 tokenizer\n",
                "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
                "\n",
                "# Load the RoBERTa tokenizer\n",
                "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
                "\n",
                "# Define the input texts\n",
                "casual_text = \"Hey, how's it going? I was thinking we could catch up over coffee sometime soon.\"\n",
                "physics_text = \"The gravitational force between two masses is inversely proportional to the square of the distance between them.\"\n",
                "rare_words_text = \"The quizzaciously zephyrous xylophonist played a mellifluous tune, captivating the audience with its ethereal beauty.\"\n",
                "\n",
                "# Encode and print outputs for GPT-2\n",
                "print(\"--- GPT-2 Tokenization ---\")\n",
                "gpt2_casual = gpt2_tokenizer.convert_ids_to_tokens(gpt2_tokenizer.encode(casual_text))\n",
                "print(\"Casual Text:\", gpt2_casual)\n",
                "gpt2_physics = gpt2_tokenizer.convert_ids_to_tokens(gpt2_tokenizer.encode(physics_text))\n",
                "print(\"Physics Text:\", gpt2_physics)\n",
                "gpt2_rare = gpt2_tokenizer.convert_ids_to_tokens(gpt2_tokenizer.encode(rare_words_text))\n",
                "print(\"Rare Words Text:\", gpt2_rare)\n",
                "\n",
                "print(\"\\n\")\n",
                "\n",
                "# Encode and print outputs for RoBERTa\n",
                "print(\"--- RoBERTa Tokenization ---\")\n",
                "roberta_casual = roberta_tokenizer.convert_ids_to_tokens(roberta_tokenizer.encode(casual_text))\n",
                "print(\"Casual Text:\", roberta_casual)\n",
                "roberta_physics = roberta_tokenizer.convert_ids_to_tokens(roberta_tokenizer.encode(physics_text))\n",
                "print(\"Physics Text:\", roberta_physics)\n",
                "roberta_rare = roberta_tokenizer.convert_ids_to_tokens(roberta_tokenizer.encode(rare_words_text))\n",
                "print(\"Rare Words Text:\", roberta_rare)\n",
                "```\n",
                "\n",
                "-----\n",
                "\n",
                "### Comparison and Analysis\n",
                "\n",
                "This exercise highlights some key differences in how the GPT-2 and RoBERTa tokenizers operate.\n",
                "\n",
                "#### GPT-2 Tokenizer\n",
                "\n",
                "  * **Contractions:** GPT-2 splits **`how's`** into `how` and `'s`. This is a common and linguistically sound approach.\n",
                "  * **Subword Handling:** It effectively tokenizes complex words. Notice how the rare word **`quizzaciously`** is broken down into `quizz`, `ac`, `iously`, while **`zephyrous`** is split into `ze`, `phy`, and `rous`. This demonstrates its ability to handle unfamiliar vocabulary by relying on common subword components.\n",
                "  * **Space Handling:** GPT-2 uses a leading space `Ġ` for most words that follow a space, which helps preserve word boundaries.\n",
                "\n",
                "#### RoBERTa Tokenizer\n",
                "\n",
                "  * **Special Tokens:** RoBERTa adds `<s>` and `</s>` tokens to the beginning and end of each sentence, which are used to mark sentence boundaries during model training.\n",
                "  * **Subword Handling:** RoBERTa shows a slightly different approach to subword tokenization, particularly with the rare words. For example, it splits **`zephyrous`** into `ze`, `ph`, `yr`, `ous`.\n",
                "  * **Contractions:** Similar to GPT-2, RoBERTa splits **`how's`** into `how` and `'s`.\n",
                "  * **Consistency:** Both tokenizers handle the **Physics** and **Casual** texts very similarly, treating words as single tokens and separating punctuation. The main differences are in their handling of special boundary tokens and the specific subword splits for rare words.\n",
                "\n",
                "The outputs show that while both models use BPE for subword tokenization, the specific vocabulary and subword rules they learned during their pre-training are slightly different. This is why a model and its tokenizer are always used as a pair—the model is trained to interpret tokens precisely as its tokenizer generates them."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
