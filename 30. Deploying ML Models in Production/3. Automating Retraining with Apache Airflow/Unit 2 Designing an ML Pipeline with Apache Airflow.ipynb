{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 2 Designing an ML Pipeline with Apache Airflow\n",
                                    "\n",
                                    "Here is the text converted to Markdown:\n",
                                    "\n",
                                    "# Welcome to the second lesson in our \"Automating Retraining with Apache Airflow\" course\\!\n",
                                    "\n",
                                    "In the previous lesson, we introduced Apache Airflow and built a simple \"hello world\" DAG. Now that we understand the basics, we're ready to take a significant step forward and explore how to structure more complex workflows specifically designed for machine learning pipelines.\n",
                                    "\n",
                                    "Machine learning workflows typically involve several distinct stages, from data extraction to model deployment. In this lesson, we'll build a complete ML pipeline using Airflow's TaskFlow API, learning how to orchestrate the various steps needed to train and deploy a model. We'll see how Airflow can help us manage dependencies between tasks, pass data between steps, and implement conditional logic based on model performance.\n",
                                    "\n",
                                    "By the end of this lesson, you'll understand how to design a functional DAG that represents an end-to-end ML training pipeline, giving you a solid foundation for implementing actual ML code in future lessons.\n",
                                    "\n",
                                    "## Understanding ML Workflows in Airflow\n",
                                    "\n",
                                    "Before diving into code, let's consider what an ML pipeline typically involves and how we can represent it in Airflow. A standard machine learning workflow often includes these key stages:\n",
                                    "\n",
                                    "  * **Data extraction:** Collecting data from various sources.\n",
                                    "  * **Data transformation:** Cleaning, preprocessing, and feature engineering.\n",
                                    "  * **Model training:** Using the prepared data to train a machine learning model.\n",
                                    "  * **Model validation:** Evaluating the model's performance against a validation set.\n",
                                    "  * **Model deployment:** Deploying the model to production (if it meets quality thresholds).\n",
                                    "\n",
                                    "Each of these stages can be represented as a task in our Airflow DAG, with clear dependencies between them. For example, we can't train a model until the data has been extracted and transformed, or we can't deploy it until it has been trained and evaluated. Airflow is particularly well-suited for ML workflows because it:\n",
                                    "\n",
                                    "  * Handles task scheduling and retries automatically;\n",
                                    "  * Provides visibility into the execution of each step;\n",
                                    "  * Allows us to pass data between tasks;\n",
                                    "  * Enables conditional execution based on the results of previous tasks;\n",
                                    "  * Maintains a record of runs for reproducibility and auditing.\n",
                                    "\n",
                                    "## Designing an ML Pipeline DAG\n",
                                    "\n",
                                    "Let's start designing our ML pipeline by setting up the basic DAG structure:\n",
                                    "\n",
                                    "```python\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),  # Longer retry delay for ML tasks\n",
                                    "}\n",
                                    "```\n",
                                    "\n",
                                    "This code should look familiar from our first lesson. We import the necessary modules and set up default arguments. Notice that we've increased the `retry_delay` to 5 minutes, which is more appropriate for ML tasks that might take longer to execute than simple examples.\n",
                                    "\n",
                                    "Now, let's define our DAG using the `@dag` decorator:\n",
                                    "\n",
                                    "```python\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"\n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the later units.\n",
                                    "    \"\"\"\n",
                                    "```\n",
                                    "\n",
                                    "We've named our DAG `mlops_pipeline` and set it to run daily. While scheduling a DAG to run at regular intervals (like daily) is common, Airflow also supports alternative triggering mechanisms: for example, you can use triggers to launch a DAG in response to external events, or sensors to wait for specific conditions (such as the arrival of a file or completion of another process) before starting the workflow. This flexibility allows you to tailor your ML pipeline's execution to your specific operational requirements.\n",
                                    "\n",
                                    "The `catchup=False` parameter ensures that Airflow won't execute the DAG for past dates, which is important for ML pipelines where we typically only want to train with the most recent data. Finally, the tags help categorize our DAG in the Airflow UI, making it easier to find among many workflows.\n",
                                    "\n",
                                    "## Implementing Data Extraction and Transformation\n",
                                    "\n",
                                    "Now let's implement the first two tasks of our ML pipeline:\n",
                                    "\n",
                                    "```python\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "```\n",
                                    "\n",
                                    "This task simulates extracting data from a source system. In a real ML pipeline, this might involve querying a database, accessing an API, or reading files from a data lake. The task returns a dictionary with metadata about the extraction process, including a flag indicating success and the number of records extracted. This information will be useful for downstream tasks.\n",
                                    "\n",
                                    "Next, let's implement the transformation task:\n",
                                    "\n",
                                    "```python\n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "```\n",
                                    "\n",
                                    "The `transform_data` task takes the result from the extraction task as an input parameter, demonstrating how the TaskFlow API makes it easy to pass data between tasks. The task checks if extraction was successful, performs its transformation (simulated in this case), and returns information about the features generated. Note how we're raising an exception if extraction failed — this ensures that our pipeline will fail appropriately if a critical step doesn't complete successfully.\n",
                                    "\n",
                                    "## Building Model Training and Validation Tasks\n",
                                    "\n",
                                    "With our data prepared, let's implement the next stages of our ML pipeline:\n",
                                    "\n",
                                    "```python\n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "```\n",
                                    "\n",
                                    "The `train_model` task simulates training a machine learning model using the transformed data. In a real pipeline, this is where you would implement your model training code using frameworks like `scikit-learn`, `TensorFlow`, or `PyTorch`. Our simulated task returns a dictionary with the training results, including an accuracy metric that will be used for model validation.\n",
                                    "\n",
                                    "Now for the validation task:\n",
                                    "\n",
                                    "```python\n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            # Determine if model meets quality threshold\n",
                                    "            if accuracy >= 0.8:\n",
                                    "                return {\"validation\": \"passed\", \"accuracy\": accuracy}\n",
                                    "            else:\n",
                                    "                return {\"validation\": \"failed\", \"accuracy\": accuracy}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "```\n",
                                    "\n",
                                    "The `validate_model` task evaluates the model's performance against a predefined threshold (0.8 in this case). Rather than failing the task if the model doesn't meet the threshold, it returns a status indicating whether validation passed or failed. This approach gives us flexibility in how we handle underperforming models — we might choose to deploy them anyway with monitoring, or we might prevent deployment entirely.\n",
                                    "\n",
                                    "**Note:** in general, model validation could be more complex and involve different metrics than simple accuracy. We'll delve into this topic in more detail in a later lesson in the course.\n",
                                    "\n",
                                    "## Implementing Conditional Deployment Logic\n",
                                    "\n",
                                    "The final task in our ML pipeline is model deployment, which should only happen if the model passes validation:\n",
                                    "\n",
                                    "```python\n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            print(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "```\n",
                                    "\n",
                                    "This task introduces an important concept: the `trigger_rule` parameter. By setting `trigger_rule=\"none_failed\"`, we're telling Airflow to run this task as long as no upstream tasks failed (even if some skipped). This ensures our deployment task runs as long as no upstream tasks have failed (i.e., raised an exception), even if the validation task returns a result indicating the model isn't good enough to deploy. The deployment logic inside the task will then decide whether to actually deploy the model based on the validation result.\n",
                                    "\n",
                                    "Inside the task, we check the validation result and either deploy the model or log the failure. In both cases, the task itself succeeds, but the return value indicates whether deployment actually occurred. This pattern is useful for conditional logic that doesn't necessarily represent a failure of the pipeline itself.\n",
                                    "\n",
                                    "## Defining Task Dependencies\n",
                                    "\n",
                                    "Now that we've defined all our tasks, we need to establish the relationships between them:\n",
                                    "\n",
                                    "```python\n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_model(validation_result)\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "```\n",
                                    "\n",
                                    "When we call `extract_data()`, it returns a value which we pass to `transform_data()`, creating a dependency between them. This pattern continues through our entire workflow, creating a linear sequence of tasks.\n",
                                    "\n",
                                    "The final line `training_pipeline()` instantiates our DAG object, making it discoverable by the Airflow scheduler. This is a critical step — without it, our DAG won't be registered with Airflow.\n",
                                    "\n",
                                    "## Conclusion and Next Steps\n",
                                    "\n",
                                    "In this lesson, you've built a complete ML pipeline using Apache Airflow's TaskFlow API. You've learned how to structure a DAG for machine learning workflows, create tasks that represent each stage of an ML pipeline, pass data between tasks, implement conditional logic, and use trigger rules to control task execution. These concepts form the foundation of production-ready ML pipelines that can reliably train and deploy models on a schedule.\n",
                                    "\n",
                                    "In the upcoming practice exercises, you'll have the opportunity to apply these concepts by creating your own ML pipeline DAG. You'll experiment with different task configurations, implement conditional logic, and explore how Airflow can help make your ML workflows more robust and manageable. This hands-on experience will solidify your understanding of how Airflow can serve as a powerful orchestration tool for machine learning operations."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Model Validation Logic in Airflow\n",
                                    "\n",
                                    "You’ve just seen how each stage of an ML pipeline can be represented as a task in Airflow, and how data and results flow from one step to the next. Now it’s your turn to put this into practice by completing the validation logic for the model evaluation step.\n",
                                    "\n",
                                    "In the validate_model task, you need to decide if the trained model is good enough to move forward. Your job is to:\n",
                                    "\n",
                                    "Add a conditional check to see if the model’s accuracy is at least 0.8.\n",
                                    "If the accuracy meets or exceeds this threshold, return a dictionary with \"validation\": \"passed\" and include the accuracy.\n",
                                    "If the accuracy is below the threshold, return a dictionary with \"validation\": \"failed\" and include the accuracy as well.\n",
                                    "This step is important because it controls whether the model will be deployed in the next stage. Give it a try and see how your logic shapes the pipeline’s outcome!\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Introduction to Airflow for MLOps - Sample DAG\n",
                                    "\n",
                                    "This module defines a basic Airflow DAG that demonstrates core concepts\n",
                                    "for ML workflows using the TaskFlow API. This serves as an introduction \n",
                                    "to Airflow before integrating our actual ML components.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "import logging\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule_interval='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date (in the past)\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"\n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the later units.\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define tasks using the TaskFlow API\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "    \n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            # TODO: Add a conditional check to determine if the model meets quality standards\n",
                                    "            # Hint: Check if accuracy is greater than or equal to 0.8\n",
                                    "\n",
                                    "            # TODO: If the model meets quality standards, return a dictionary with validation status \"passed\"\n",
                                    "            # TODO: If the model doesn't meet quality standards, return a dictionary with validation status \"failed\"\n",
                                    "            # Remember to include the accuracy in both cases\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            logging.warning(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "    \n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_model(validation_result)\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "```\n",
                                    "\n",
                                    "I will complete the `validate_model` task as requested.\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Introduction to Airflow for MLOps - Sample DAG\n",
                                    "\n",
                                    "This module defines a basic Airflow DAG that demonstrates core concepts\n",
                                    "for ML workflows using the TaskFlow API. This serves as an introduction \n",
                                    "to Airflow before integrating our actual ML components.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "import logging\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule_interval='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date (in the past)\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"\n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the later units.\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define tasks using the TaskFlow API\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "    \n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            \n",
                                    "            # Add a conditional check to determine if the model meets quality standards\n",
                                    "            # If the model meets quality standards, return a dictionary with validation status \"passed\"\n",
                                    "            if accuracy >= 0.8:\n",
                                    "                return {\"validation\": \"passed\", \"accuracy\": accuracy}\n",
                                    "            # If the model doesn't meet quality standards, return a dictionary with validation status \"failed\"\n",
                                    "            else:\n",
                                    "                return {\"validation\": \"failed\", \"accuracy\": accuracy}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            logging.warning(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "    \n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_model(validation_result)\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Archiving Models in Your Pipeline\n",
                                    "\n",
                                    "You’ve just seen how to validate and deploy a model in your Airflow ML pipeline. Now, let’s take it a step further by ensuring your deployed models are properly archived for future reference.\n",
                                    "\n",
                                    "Your task is to add a new step to the pipeline that archives the model details after a successful deployment. This helps keep track of which models have been put into production.\n",
                                    "\n",
                                    "Here’s what you need to do:\n",
                                    "\n",
                                    "Create a new task called archive_model using the TaskFlow API.\n",
                                    "This task should:\n",
                                    "Take the output from the deploy_model task as its input.\n",
                                    "Check if the deployment was successful.\n",
                                    "If it was, print a message about archiving and return a dictionary with \"archived\": True.\n",
                                    "If not, print a message about skipping archiving and return a dictionary with \"archived\": False.\n",
                                    "Update the dependency chain so that archive_model runs after deploy_model and receives its result.\n",
                                    "Adding this step will help you see how to extend your pipeline with new tasks and manage the flow of information between them.\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Introduction to Airflow for MLOps - Sample DAG\n",
                                    "\n",
                                    "This module defines a basic Airflow DAG that demonstrates core concepts\n",
                                    "for ML workflows using the TaskFlow API. This serves as an introduction \n",
                                    "to Airflow before integrating our actual ML components.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "import logging\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule_interval='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date (in the past)\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"\n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the later units.\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define tasks using the TaskFlow API\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "    \n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            # Determine if model meets quality threshold\n",
                                    "            if accuracy >= 0.8:\n",
                                    "                return {\"validation\": \"passed\", \"accuracy\": accuracy}\n",
                                    "            else:\n",
                                    "                return {\"validation\": \"failed\", \"accuracy\": accuracy}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            logging.warning(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "\n",
                                    "    # TODO: Add a new task called \"archive_model\" that simulates archiving model details after deployment.\n",
                                    "    # The task should:\n",
                                    "    #   - Accept the output from deploy_model as input\n",
                                    "    #   - Check if deployment was successful \n",
                                    "    #   - If successful, print a message about archiving and return {\"archived\": True}\n",
                                    "    #   - If not, print a message about skipping archiving and return {\"archived\": False}\n",
                                    "\n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_result = deploy_model(validation_result)\n",
                                    "    # TODO: Update the dependency chain to include the new archive_model task after deploy_model.\n",
                                    "    # The archive_model task should receive deploy_result as its input.\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "I will add a new task `archive_model` and update the dependency chain to include it after the `deploy_model` task.\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Introduction to Airflow for MLOps - Sample DAG\n",
                                    "\n",
                                    "This module defines a basic Airflow DAG that demonstrates core concepts\n",
                                    "for ML workflows using the TaskFlow API. This serves as an introduction \n",
                                    "to Airflow before integrating our actual ML components.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "import logging\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule_interval='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date (in the past)\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"\n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the later units.\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define tasks using the TaskFlow API\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "    \n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            # Determine if model meets quality threshold\n",
                                    "            if accuracy >= 0.8:\n",
                                    "                return {\"validation\": \"passed\", \"accuracy\": accuracy}\n",
                                    "            else:\n",
                                    "                return {\"validation\": \"failed\", \"accuracy\": accuracy}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            logging.warning(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "\n",
                                    "    # Add a new task called \"archive_model\" that simulates archiving model details after deployment.\n",
                                    "    @task(task_id=\"archive_model\")\n",
                                    "    def archive_model(deploy_result):\n",
                                    "        \"\"\"Simulate archiving the model details.\"\"\"\n",
                                    "        if deploy_result.get(\"deployed\"):\n",
                                    "            print(\"Model deployment was successful. Archiving model details...\")\n",
                                    "            return {\"archived\": True}\n",
                                    "        else:\n",
                                    "            print(\"Model was not deployed. Skipping archiving.\")\n",
                                    "            return {\"archived\": False}\n",
                                    "\n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_result = deploy_model(validation_result)\n",
                                    "    # Update the dependency chain to include the new archive_model task after deploy_model.\n",
                                    "    # The archive_model task should receive deploy_result as its input.\n",
                                    "    archive_model(deploy_result)\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adding Rollback to Your ML Pipeline\n",
                                    "\n",
                                    "Now, let’s make your pipeline even more reliable by adding a way to respond if deployment fails.\n",
                                    "\n",
                                    "Your task is to add a new step called rollback_model to the pipeline. This step should check if the deployment was unsuccessful and, if so, perform a simulated rollback. If deployment was successful, it should simply note that no rollback is needed. Here’s what you need to do:\n",
                                    "\n",
                                    "Create a new task named rollback_model.\n",
                                    "This task should:\n",
                                    "Take the output from the deploy_model task as its input.\n",
                                    "Check if deployment failed (when deploy_result[\"deployed\"] is False).\n",
                                    "If deployment failed, print a message about rolling back and return {\"rollback_performed\": True}.\n",
                                    "If deployment succeeded, print a message that no rollback is needed and return {\"rollback_performed\": False}.\n",
                                    "Update the pipeline so that rollback_model runs after deploy_model and receives its result as input.\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Introduction to Airflow for MLOps - Sample DAG\n",
                                    "\n",
                                    "This module defines a basic Airflow DAG that demonstrates core concepts\n",
                                    "for ML workflows using the TaskFlow API. This serves as an introduction \n",
                                    "to Airflow before integrating our actual ML components.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "import logging\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule_interval='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date (in the past)\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"\n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the later units.\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define tasks using the TaskFlow API\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "    \n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            # Determine if model meets quality threshold\n",
                                    "            if accuracy >= 0.8:\n",
                                    "                return {\"validation\": \"passed\", \"accuracy\": accuracy}\n",
                                    "            else:\n",
                                    "                return {\"validation\": \"failed\", \"accuracy\": accuracy}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            logging.warning(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "\n",
                                    "    # TODO: Add a new task called \"rollback_model\" using the @task decorator.\n",
                                    "    # This task should:\n",
                                    "    #   - Take the output from deploy_model as its input\n",
                                    "    #   - Check if deployment was unsuccessful\n",
                                    "    #   - If deployment failed, print a message about rolling back and return {\"rollback_performed\": True}\n",
                                    "    #   - If deployment succeeded, print a message that no rollback is needed and return {\"rollback_performed\": False}\n",
                                    "\n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_result = deploy_model(validation_result)\n",
                                    "    # TODO: Call your rollback_model task here, passing deploy_result as input.\n",
                                    "    # The rollback_model task should always run after deploy_model,\n",
                                    "    # but only perform a rollback if deployment failed.\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "```\n",
                                    "\n",
                                    "I will add a `rollback_model` task and update the dependency chain to include it.\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Introduction to Airflow for MLOps - Sample DAG\n",
                                    "\n",
                                    "This module defines a basic Airflow DAG that demonstrates core concepts\n",
                                    "for ML workflows using the TaskFlow API. This serves as an introduction \n",
                                    "to Airflow before integrating our actual ML components.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "import logging\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule_interval='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date (in the past)\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"\n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the later units.\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define tasks using the TaskFlow API\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "    \n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            # Determine if model meets quality threshold\n",
                                    "            if accuracy >= 0.8:\n",
                                    "                return {\"validation\": \"passed\", \"accuracy\": accuracy}\n",
                                    "            else:\n",
                                    "                return {\"validation\": \"failed\", \"accuracy\": accuracy}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            logging.warning(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "\n",
                                    "    # Add a new task called \"rollback_model\" using the @task decorator.\n",
                                    "    @task(task_id=\"rollback_model\")\n",
                                    "    def rollback_model(deploy_result):\n",
                                    "        \"\"\"Simulates a rollback if deployment failed.\"\"\"\n",
                                    "        if deploy_result[\"deployed\"]:\n",
                                    "            print(\"Model deployed successfully. No rollback needed.\")\n",
                                    "            return {\"rollback_performed\": False}\n",
                                    "        else:\n",
                                    "            print(\"Model deployment failed. Performing rollback...\")\n",
                                    "            return {\"rollback_performed\": True}\n",
                                    "            \n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_result = deploy_model(validation_result)\n",
                                    "    # Call your rollback_model task here, passing deploy_result as input.\n",
                                    "    rollback_model(deploy_result)\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Build a Complete ML Pipeline DAG"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
