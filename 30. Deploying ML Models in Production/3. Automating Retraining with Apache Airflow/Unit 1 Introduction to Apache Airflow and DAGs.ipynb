{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 1 Introduction to Apache Airflow and DAGs\n",
                                    "\n",
                                    "# Introduction\n",
                                    "\n",
                                    "Welcome to our first lesson on \"Automating Retraining with Apache Airflow\"\\! In this lesson, we'll start our journey into the world of workflow automation with Apache Airflow, a powerful open-source platform that allows us to programmatically author, schedule, and monitor workflows.\n",
                                    "\n",
                                    "As machine learning practitioners, we often need to retrain our models regularly as new data becomes available. This process involves several steps: data extraction, preprocessing, model training, evaluation, and deployment. Manually executing these steps can be time-consuming and error-prone. This is where Apache Airflow comes in—it provides a framework to automate and orchestrate complex computational workflows.\n",
                                    "\n",
                                    "In this course, we focus on **Apache Airflow 2.x** and its modern **TaskFlow API**. The TaskFlow API, introduced in Airflow 2, allows us to define workflows using Python functions and decorators, making DAGs more readable and maintainable compared to the older operator-based approach. All examples and practices in this course will use Airflow 2 and the TaskFlow API.\n",
                                    "\n",
                                    "By the end of this lesson, we'll understand what Apache Airflow is, learn about **Directed Acyclic Graphs (DAGs)**, and implement a simple workflow using Airflow's **TaskFlow** API.\n",
                                    "\n",
                                    "# Understanding Apache Airflow and DAGs\n",
                                    "\n",
                                    "Apache Airflow is a platform created by Airbnb (now an Apache Software Foundation project) to programmatically author, schedule, and monitor workflows. At its core, Airflow uses Directed Acyclic Graphs (DAGs) to represent workflows. But what exactly is a DAG? Let's break it down:\n",
                                    "\n",
                                    "  * **Graph:** A graph is a mathematical structure made up of nodes connected by edges, and in the context of Airflow, it helps us visually and logically organize the sequence and dependencies of tasks in our workflow.\n",
                                    "  * **Directed:** The relationships between tasks have a specific direction. Task A may lead to Task B, but not vice versa.\n",
                                    "  * **Acyclic:** There are no cycles or loops in the workflow. You can't create circular dependencies where Task A depends on Task B, which depends on Task A.\n",
                                    "\n",
                                    "In Airflow, each task in a workflow is represented as a **node** in the DAG, and the dependencies between tasks are represented as **directed edges**. This allows us to define complex workflows with multiple tasks and their dependencies in a clear, programmatic way.\n",
                                    "\n",
                                    "For example, a simple ML retraining workflow might include these tasks in sequence: extract new data, preprocess data, train model, evaluate model, and deploy model (if evaluation metrics exceed a threshold). Airflow ensures these tasks execute in the correct order and handles scheduling, retries on failure, and provides visibility into the workflow's execution.\n",
                                    "\n",
                                    "# Creating Your First Airflow DAG\n",
                                    "\n",
                                    "Now that we understand the concept of DAGs, let's create a simple Airflow DAG. We'll start with the basic structure and imports:\n",
                                    "\n",
                                    "```python\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=1),\n",
                                    "}\n",
                                    "```\n",
                                    "\n",
                                    "In this code, we're importing the necessary modules from Python's `datetime` library and Airflow's decorators. The `default_args` dictionary defines global settings for our DAG, specifying who owns it (`owner`), whether it depends on past executions (`depends_on_past`), email notification preferences, and retry configurations. These settings help Airflow know how to handle the DAG's execution—for instance, it will automatically retry a failed task once after waiting for 1 minute.\n",
                                    "\n",
                                    "# Defining the DAG with TaskFlow API\n",
                                    "\n",
                                    "With our default arguments in place, we can now define the DAG using Airflow's TaskFlow API, which provides a more intuitive way to define workflows:\n",
                                    "\n",
                                    "```python\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='A simple two-task DAG using TaskFlow API',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'basic'],\n",
                                    ")\n",
                                    "def hello_airflow_dag():\n",
                                    "    \"\"\"\n",
                                    "    This DAG demonstrates the basics of Airflow with just two tasks:\n",
                                    "    1. A task that says hello\n",
                                    "    2. A task that says goodbye and uses the result from the first task\n",
                                    "    \"\"\"\n",
                                    "```\n",
                                    "\n",
                                    "Here, we use the `@dag` decorator to transform our Python function into an Airflow DAG. The parameters define critical aspects of our workflow's behavior: the `dag_id` provides a unique name, `schedule` sets it to run daily, and `start_date` indicates when scheduling should begin. Setting `catchup=False` prevents Airflow from executing the DAG for past periods, which is especially useful when first deploying a DAG with a start date in the past. The docstring clearly documents what our simple workflow will do, enhancing readability for anyone maintaining this code.\n",
                                    "\n",
                                    "# Creating Tasks in Your Workflow\n",
                                    "\n",
                                    "Now it's time to define the individual tasks for our DAG using the `@task` decorator:\n",
                                    "\n",
                                    "```python\n",
                                    "    @task(task_id=\"hello_task\")\n",
                                    "    def say_hello():\n",
                                    "        \"\"\"Simple function that prints a greeting.\"\"\"\n",
                                    "        print(\"Hello from Airflow!\")\n",
                                    "        return \"Hello\"\n",
                                    "\n",
                                    "    @task(task_id=\"goodbye_task\")\n",
                                    "    def say_goodbye(first_task_result):\n",
                                    "        \"\"\"\n",
                                    "        Function that uses the result from the first task.\n",
                                    "        \n",
                                    "        Args:\n",
                                    "            first_task_result: The result returned by the first task\n",
                                    "        \"\"\"\n",
                                    "        print(f\"Previous task said: {first_task_result}\")\n",
                                    "        print(\"Goodbye from Airflow!\")\n",
                                    "```\n",
                                    "\n",
                                    "The `@task` decorator transforms regular Python functions into Airflow tasks. Our first task, `say_hello`, simply prints a message and returns the string \"Hello\". The second task, `say_goodbye`, takes the output from the first task as a parameter, allowing us to demonstrate how data flows between tasks in Airflow. This is one of the **powerful features** of the TaskFlow API—it automatically handles the serialization, storage, and retrieval of data between tasks, making workflow development more intuitive and less boilerplate-heavy.\n",
                                    "\n",
                                    "**Note:** Although it looks like the value is passed directly as a Python variable, Airflow actually passes data between tasks using its XCom (cross-communication) mechanism. The TaskFlow API makes this seamless, but under the hood, the result is serialized and stored by Airflow, not passed in-memory like a normal Python function call.\n",
                                    "\n",
                                    "# Orchestrating the Workflow\n",
                                    "\n",
                                    "The final step is defining how our tasks should interact. In the TaskFlow API, this happens naturally through function calls:\n",
                                    "\n",
                                    "```python\n",
                                    "    # Define the task dependencies\n",
                                    "    # With TaskFlow API, dependencies are created by function calls\n",
                                    "    first_result = say_hello()  # Execute the first task\n",
                                    "    say_goodbye(first_result)   # Pass the result to the second task\n",
                                    "\n",
                                    "# Create the DAG instance\n",
                                    "dag = hello_airflow_dag()\n",
                                    "```\n",
                                    "\n",
                                    "This is where the **elegant simplicity** of the TaskFlow API shines. In older versions of Airflow, you had to use special symbols (like `>>` or `<<`) to manually set the order in which tasks run. With the TaskFlow API, you simply call your Python functions and pass data between them: this automatically creates the correct order and dependencies. When we call `say_hello()`, it returns the string \"Hello,\" which we assign to `first_result`. By passing this variable to `say_goodbye()`, we tell Airflow that the second task should wait for the first one to finish and use its result.\n",
                                    "\n",
                                    "The final line creates our DAG object and assigns it to the variable `dag`. This is a common convention, but it's not a strict requirement, since Airflow will still discover the DAG as long as the function is called at the module level.\n",
                                    "\n",
                                    "When the DAG runs, it executes `say_hello` first, then passes the returned value to `say_goodbye` (via XCom). In the Airflow UI logs, you'd see \"Hello from Airflow\\!\" from the first task, followed by \"Previous task said: Hello\" and \"Goodbye from Airflow\\!\" from the second task.\n",
                                    "\n",
                                    "# Understanding the DAG Run Output\n",
                                    "\n",
                                    "When your DAG runs, Airflow generates detailed logs that provide insight into the execution process. While these logs can be quite verbose, they are invaluable for debugging and monitoring. Let's look at a simplified version of the output you might see for our `hello_airflow_dag`:\n",
                                    "\n",
                                    "```text\n",
                                    "[2025-05-07T14:46:45.518+0000] {dag.py:4435} INFO - dagrun id: mlops_pipeline\n",
                                    "[2025-05-07T14:46:45.552+0000] {dag.py:4396} INFO - [DAG TEST] starting task_id=hello_task map_index=-1\n",
                                    "Hello from Airflow!\n",
                                    "[2025-05-07 14:46:45,606] {python.py:240} INFO - Done. Returned value was: Hello\n",
                                    "[2025-05-07T14:46:45.619+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=mlops_pipeline, task_id=hello_task, ...\n",
                                    "[2025-05-07T14:46:45.645+0000] {dag.py:4396} INFO - [DAG TEST] starting task_id=goodbye_task map_index=-1\n",
                                    "Previous task said: Hello\n",
                                    "Goodbye from Airflow!\n",
                                    "[2025-05-07 14:46:45,670] {python.py:240} INFO - Done. Returned value was: None\n",
                                    "[2025-05-07T14:46:45.673+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=mlops_pipeline, task_id=goodbye_task, ...\n",
                                    "[2025-05-07T14:46:45.686+0000] {dagrun.py:854} INFO - Marking run <DagRun mlops_pipeline ...> successful\n",
                                    "```\n",
                                    "\n",
                                    "Let's break down what these key lines tell us:\n",
                                    "\n",
                                    "  * `INFO - dagrun id: mlops_pipeline`: This indicates the start of a new DAG run for our `mlops_pipeline` DAG.\n",
                                    "  * The next set of lines shows the execution of `hello_task`:\n",
                                    "      * `INFO - [DAG TEST] starting task_id=hello_task ...`: Airflow begins executing the `hello_task`.\n",
                                    "      * `Hello from Airflow!`: This is the `print()` output from our `say_hello` function.\n",
                                    "      * `INFO - Done. Returned value was: Hello`: The task completed and returned \"Hello\", which Airflow passes via XComs.\n",
                                    "      * `INFO - Marking task as SUCCESS. ... task_id=hello_task, ...`: The `hello_task` finished successfully.\n",
                                    "  * Following that, we see the execution of `goodbye_task`:\n",
                                    "      * `INFO - [DAG TEST] starting task_id=goodbye_task ...`: Airflow starts the `goodbye_task`.\n",
                                    "      * `Previous task said: Hello` and `Goodbye from Airflow!`: These are the `print()` outputs from `say_goodbye`, confirming it received the \"Hello\" string from the first task.\n",
                                    "      * `INFO - Done. Returned value was: None`: The `say_goodbye` task completed (returning `None` as it has no explicit return).\n",
                                    "      * `INFO - Marking task as SUCCESS. ... task_id=goodbye_task, ...`: The `goodbye_task` also finished successfully.\n",
                                    "  * `INFO - Marking run <DagRun mlops_pipeline ...> successful`: Finally, this line confirms that the entire DAG run completed successfully.\n",
                                    "\n",
                                    "This output confirms that our tasks executed in the correct order, data was passed between them as expected, and the overall workflow was successful. In the Airflow UI, you would see a graphical representation of this execution, with green boxes indicating successfully completed tasks.\n",
                                    "\n",
                                    "# Where to Place Your DAG Code\n",
                                    "\n",
                                    "For Airflow to discover and execute your DAGs, your Python files must be placed in a specific directory known as the **DAGs folder**. By default, this is the `dags/` directory inside your Airflow home directory (`$AIRFLOW_HOME/dags/`), but it can be configured differently in your Airflow settings.\n",
                                    "\n",
                                    "When Airflow runs, it continuously scans the DAGs folder for Python files. Any file that contains a DAG definition (i.e., a variable or function that returns a DAG object) will be automatically detected and made available to the Airflow scheduler. This means you don't need to manually register your DAGs: just save your `.py` file in the correct folder, and Airflow will handle the rest.\n",
                                    "\n",
                                    "**Best practices** for organizing your DAG code:\n",
                                    "\n",
                                    "  * Place each DAG in its own Python file for clarity and maintainability.\n",
                                    "  * If you have shared code (such as utility functions or custom operators), consider placing them in a separate `utils/` or `plugins/` directory and importing them into your DAG files.\n",
                                    "\n",
                                    "Please note that, in this course, the CodeSignal environment is pre-configured so that any DAG code you write is automatically placed in the correct folder. You don't need to worry about file placement or Airflow configuration—just focus on writing your DAGs, and they'll be picked up and executed by Airflow behind the scenes.\n",
                                    "\n",
                                    "# Conclusion and Next Steps\n",
                                    "\n",
                                    "In this lesson, we've taken our first steps with Apache Airflow by creating a simple DAG with two tasks. We've learned about the core concepts of Airflow: defining workflows as DAGs, creating tasks with the `@task` decorator, and establishing dependencies between them. The TaskFlow API has made this process intuitive by letting us express workflows as regular Python functions while handling the complexities of data passing and dependency management behind the scenes.\n",
                                    "\n",
                                    "While we've built a simple example, these fundamentals form the building blocks for creating complex, production-grade ML retraining pipelines. As you practice these concepts, experiment with adding more tasks, passing different types of data between them, and visualizing the resulting workflow graphs. In the upcoming lessons, we'll expand on these basics to build more sophisticated workflows that handle real machine learning tasks from data processing to model deployment."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Transform a Function into a DAG\n",
                                    "\n",
                                    "You’ve just learned how Apache Airflow 2.x uses the TaskFlow API to define workflows as Python functions and how tasks are organized in a DAG. Now, let’s put that knowledge into practice by turning a regular Python function into a real Airflow DAG.\n",
                                    "\n",
                                    "Your task is to add the missing @dag decorator above the hello_airflow_dag function. This decorator tells Airflow to treat the function as a workflow. Make sure to include all the required parameters:\n",
                                    "\n",
                                    "dag_id: set this to 'mlops_pipeline'\n",
                                    "description: provide a short explanation of the DAG\n",
                                    "default_args: use the default_args dictionary already defined in the code\n",
                                    "schedule: set to '@daily'\n",
                                    "start_date: set to some date using datetime(YYYY, MM, DD)\n",
                                    "catchup: set to False\n",
                                    "tags: include two tags in a list\n",
                                    "Once you add the decorator with the correct parameters, your function will become a proper Airflow DAG. This is a key step in automating workflows with Airflow, so take your time and ensure each parameter is included.\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Basic Introduction to Airflow - Simple DAG\n",
                                    "\n",
                                    "This module defines a minimal Airflow DAG with only two tasks to demonstrate\n",
                                    "the core concepts of Directed Acyclic Graphs (DAGs) in Airflow using the TaskFlow API.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=1),\n",
                                    "}\n",
                                    "\n",
                                    "# TODO: Add the @dag decorator above the function below.\n",
                                    "# The decorator should include these parameters:\n",
                                    "# - dag_id: set to 'mlops_pipeline'\n",
                                    "# - description: a short description of the DAG\n",
                                    "# - default_args: use the default_args dictionary above\n",
                                    "# - schedule: set to '@daily'\n",
                                    "# - start_date: set to datetime(2023, 1, 1)\n",
                                    "# - catchup: set to False\n",
                                    "# - tags: include 'intro' and 'basic' in a list\n",
                                    "def hello_airflow_dag():\n",
                                    "    \"\"\"\n",
                                    "    This DAG demonstrates the basics of Airflow with just two tasks:\n",
                                    "    1. A task that says hello\n",
                                    "    2. A task that says goodbye and uses the result from the first task\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define the first task using the @task decorator\n",
                                    "    @task(task_id=\"hello_task\")\n",
                                    "    def say_hello():\n",
                                    "        \"\"\"Simple function that prints a greeting.\"\"\"\n",
                                    "        print(\"Hello from Airflow!\")\n",
                                    "        return \"Hello\"\n",
                                    "    \n",
                                    "    # Define the second task using the @task decorator\n",
                                    "    @task(task_id=\"goodbye_task\")\n",
                                    "    def say_goodbye(first_task_result):\n",
                                    "        \"\"\"\n",
                                    "        Function that uses the result from the first task.\n",
                                    "        \n",
                                    "        Args:\n",
                                    "            first_task_result: The result returned by the first task\n",
                                    "        \"\"\"\n",
                                    "        print(f\"Previous task said: {first_task_result}\")\n",
                                    "        print(\"Goodbye from Airflow!\")\n",
                                    "    \n",
                                    "    # Define the task dependencies\n",
                                    "    # With TaskFlow API, dependencies are created by function calls\n",
                                    "    first_result = say_hello()  # Execute the first task\n",
                                    "    say_goodbye(first_result)   # Pass the result to the second task\n",
                                    "\n",
                                    "# Create the DAG instance\n",
                                    "hello_airflow_dag()\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\"\"\"\n",
                                    "Basic Introduction to Airflow - Simple DAG\n",
                                    "\n",
                                    "This module defines a minimal Airflow DAG with only two tasks to demonstrate\n",
                                    "the core concepts of Directed Acyclic Graphs (DAGs) in Airflow using the TaskFlow API.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=1),\n",
                                    "}\n",
                                    "\n",
                                    "# TODO: Add the @dag decorator above the function below.\n",
                                    "# The decorator should include these parameters:\n",
                                    "# - dag_id: set to 'mlops_pipeline'\n",
                                    "# - description: a short description of the DAG\n",
                                    "# - default_args: use the default_args dictionary above\n",
                                    "# - schedule: set to '@daily'\n",
                                    "# - start_date: set to datetime(2023, 1, 1)\n",
                                    "# - catchup: set to False\n",
                                    "# - tags: include 'intro' and 'basic' in a list\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',\n",
                                    "    description='A simple two-task DAG using TaskFlow API',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule='@daily',\n",
                                    "    start_date=datetime(2023, 1, 1),\n",
                                    "    catchup=False,\n",
                                    "    tags=['intro', 'basic'],\n",
                                    ")\n",
                                    "def hello_airflow_dag():\n",
                                    "    \"\"\"\n",
                                    "    This DAG demonstrates the basics of Airflow with just two tasks:\n",
                                    "    1. A task that says hello\n",
                                    "    2. A task that says goodbye and uses the result from the first task\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define the first task using the @task decorator\n",
                                    "    @task(task_id=\"hello_task\")\n",
                                    "    def say_hello():\n",
                                    "        \"\"\"Simple function that prints a greeting.\"\"\"\n",
                                    "        print(\"Hello from Airflow!\")\n",
                                    "        return \"Hello\"\n",
                                    "    \n",
                                    "    # Define the second task using the @task decorator\n",
                                    "    @task(task_id=\"goodbye_task\")\n",
                                    "    def say_goodbye(first_task_result):\n",
                                    "        \"\"\"\n",
                                    "        Function that uses the result from the first task.\n",
                                    "        \n",
                                    "        Args:\n",
                                    "            first_task_result: The result returned by the first task\n",
                                    "        \"\"\"\n",
                                    "        print(f\"Previous task said: {first_task_result}\")\n",
                                    "        print(\"Goodbye from Airflow!\")\n",
                                    "    \n",
                                    "    # Define the task dependencies\n",
                                    "    # With TaskFlow API, dependencies are created by function calls\n",
                                    "    first_result = say_hello()  # Execute the first task\n",
                                    "    say_goodbye(first_result)   # Pass the result to the second task\n",
                                    "\n",
                                    "# Create the DAG instance\n",
                                    "hello_airflow_dag()"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Turn Functions into Airflow Tasks\n",
                                    "\n",
                                    "You’ve just seen how Airflow uses the TaskFlow API to organize workflows as Python functions inside a DAG. Now, let’s focus on how individual steps in your workflow become Airflow tasks.\n",
                                    "\n",
                                    "In the code below, the two functions that should act as tasks are missing their @task decorators. Your job is to add these decorators so Airflow knows to treat them as tasks within the DAG.\n",
                                    "\n",
                                    "Here’s what you need to do:\n",
                                    "\n",
                                    "Add the @task decorator above the say_hello function, and set its task_id to \"hello_task\".\n",
                                    "Add the @task decorator above the say_goodbye function, and set its task_id to \"goodbye_task\".\n",
                                    "Define the task dependencies in the body of the function hello_airflow_dag function.\n",
                                    "The @task decorator is what turns a regular Python function into a task that Airflow can schedule and monitor. Make sure you place the decorator directly above each function definition.\n",
                                    "\n",
                                    "Once you’ve added the decorators and defined the task dependencies, your DAG will be ready to run both tasks as part of the workflow. This is a key step in building automated pipelines with Airflow!\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Basic Introduction to Airflow - Simple DAG\n",
                                    "\n",
                                    "This module defines a minimal Airflow DAG with only two tasks to demonstrate\n",
                                    "the core concepts of Directed Acyclic Graphs (DAGs) in Airflow using the TaskFlow API.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=1),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG using the TaskFlow API\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='A simple two-task DAG using TaskFlow API',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'basic'],\n",
                                    ")\n",
                                    "def hello_airflow_dag():\n",
                                    "    \"\"\"\n",
                                    "    This DAG demonstrates the basics of Airflow with just two tasks:\n",
                                    "    1. A task that says hello\n",
                                    "    2. A task that says goodbye and uses the result from the first task\n",
                                    "    \"\"\"\n",
                                    "\n",
                                    "    # TODO: Add the @task decorator above this function.\n",
                                    "    # This decorator turns the function into an Airflow task.\n",
                                    "    def say_hello():\n",
                                    "        \"\"\"Simple function that prints a greeting.\"\"\"\n",
                                    "        print(\"Hello from Airflow!\")\n",
                                    "        return \"Hello\"\n",
                                    "\n",
                                    "    # TODO: Add the @task decorator above this function.\n",
                                    "    # This decorator turns the function into an Airflow task.\n",
                                    "    def say_goodbye(first_task_result):\n",
                                    "        \"\"\"\n",
                                    "        Function that uses the result from the first task.\n",
                                    "\n",
                                    "        Args:\n",
                                    "            first_task_result: The result returned by the first task\n",
                                    "        \"\"\"\n",
                                    "        print(f\"Previous task said: {first_task_result}\")\n",
                                    "        print(\"Goodbye from Airflow!\")\n",
                                    "\n",
                                    "    # TODO: Define the task dependencies using function calls.\n",
                                    "    # With TaskFlow API, dependencies are created by calling the task functions\n",
                                    "    # and passing the result from one to the next.\n",
                                    "\n",
                                    "\n",
                                    "# Create the DAG instance\n",
                                    "hello_airflow_dag()\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Basic Introduction to Airflow - Simple DAG\n",
                                    "\n",
                                    "This module defines a minimal Airflow DAG with only two tasks to demonstrate\n",
                                    "the core concepts of Directed Acyclic Graphs (DAGs) in Airflow using the TaskFlow API.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=1),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG using the TaskFlow API\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='A simple two-task DAG using TaskFlow API',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'basic'],\n",
                                    ")\n",
                                    "def hello_airflow_dag():\n",
                                    "    \"\"\"\n",
                                    "    This DAG demonstrates the basics of Airflow with just two tasks:\n",
                                    "    1. A task that says hello\n",
                                    "    2. A task that says goodbye and uses the result from the first task\n",
                                    "    \"\"\"\n",
                                    "\n",
                                    "    # TODO: Add the @task decorator above this function.\n",
                                    "    # This decorator turns the function into an Airflow task.\n",
                                    "    @task(task_id=\"hello_task\")\n",
                                    "    def say_hello():\n",
                                    "        \"\"\"Simple function that prints a greeting.\"\"\"\n",
                                    "        print(\"Hello from Airflow!\")\n",
                                    "        return \"Hello\"\n",
                                    "\n",
                                    "    # TODO: Add the @task decorator above this function.\n",
                                    "    # This decorator turns the function into an Airflow task.\n",
                                    "    @task(task_id=\"goodbye_task\")\n",
                                    "    def say_goodbye(first_task_result):\n",
                                    "        \"\"\"\n",
                                    "        Function that uses the result from the first task.\n",
                                    "\n",
                                    "        Args:\n",
                                    "            first_task_result: The result returned by the first task\n",
                                    "        \"\"\"\n",
                                    "        print(f\"Previous task said: {first_task_result}\")\n",
                                    "        print(\"Goodbye from Airflow!\")\n",
                                    "\n",
                                    "    # TODO: Define the task dependencies using function calls.\n",
                                    "    # With TaskFlow API, dependencies are created by calling the task functions\n",
                                    "    # and passing the result from one to the next.\n",
                                    "    first_result = say_hello()\n",
                                    "    say_goodbye(first_result)\n",
                                    "\n",
                                    "\n",
                                    "# Create the DAG instance\n",
                                    "hello_airflow_dag()\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Controlling Workflow Timing in Airflow\n",
                                    "\n",
                                    "Cosmo\n",
                                    "Just now\n",
                                    "Read message aloud\n",
                                    "You’ve just practiced turning Python functions into Airflow tasks using the TaskFlow API. Now, let’s focus on how to control when your workflow runs by adjusting the DAG’s schedule.\n",
                                    "\n",
                                    "In the code below, the DAG is set to run once per day using the @daily schedule. Your task is to change this so the DAG runs every hour instead. This is done by updating the schedule parameter in the @dag decorator.\n",
                                    "\n",
                                    "Understanding how to set the schedule is an important part of building reliable workflows in Airflow.\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Basic Introduction to Airflow - Simple DAG\n",
                                    "\n",
                                    "This module defines a minimal Airflow DAG with only two tasks to demonstrate\n",
                                    "the core concepts of Directed Acyclic Graphs (DAGs) in Airflow using the TaskFlow API.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=1),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG using the TaskFlow API\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='A simple two-task DAG using TaskFlow API',\n",
                                    "    default_args=default_args,\n",
                                    "    # TODO: Change the schedule parameter below from '@daily' to '@hourly'\n",
                                    "    # This will make the DAG run every hour instead of once per day.\n",
                                    "    schedule='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'basic'],\n",
                                    ")\n",
                                    "def hello_airflow_dag():\n",
                                    "    \"\"\"\n",
                                    "    This DAG demonstrates the basics of Airflow with just two tasks:\n",
                                    "    1. A task that says hello\n",
                                    "    2. A task that says goodbye and uses the result from the first task\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define the first task using the @task decorator\n",
                                    "    @task(task_id=\"hello_task\")\n",
                                    "    def say_hello():\n",
                                    "        \"\"\"Simple function that prints a greeting.\"\"\"\n",
                                    "        print(\"Hello from Airflow!\")\n",
                                    "        return \"Hello\"\n",
                                    "    \n",
                                    "    # Define the second task using the @task decorator\n",
                                    "    @task(task_id=\"goodbye_task\")\n",
                                    "    def say_goodbye(first_task_result):\n",
                                    "        \"\"\"\n",
                                    "        Function that uses the result from the first task.\n",
                                    "        \n",
                                    "        Args:\n",
                                    "            first_task_result: The result returned by the first task\n",
                                    "        \"\"\"\n",
                                    "        print(f\"Previous task said: {first_task_result}\")\n",
                                    "        print(\"Goodbye from Airflow!\")\n",
                                    "    \n",
                                    "    # Define the task dependencies\n",
                                    "    # With TaskFlow API, dependencies are created by function calls\n",
                                    "    first_result = say_hello()  # Execute the first task\n",
                                    "    say_goodbye(first_result)   # Pass the result to the second task\n",
                                    "\n",
                                    "# Create the DAG instance\n",
                                    "hello_airflow_dag()\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Basic Introduction to Airflow - Simple DAG\n",
                                    "\n",
                                    "This module defines a minimal Airflow DAG with only two tasks to demonstrate\n",
                                    "the core concepts of Directed Acyclic Graphs (DAGs) in Airflow using the TaskFlow API.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=1),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG using the TaskFlow API\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='A simple two-task DAG using TaskFlow API',\n",
                                    "    default_args=default_args,\n",
                                    "    # TODO: Change the schedule parameter below from '@daily' to '@hourly'\n",
                                    "    # This will make the DAG run every hour instead of once per day.\n",
                                    "    schedule='@hourly',  # Run hourly\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'basic'],\n",
                                    ")\n",
                                    "def hello_airflow_dag():\n",
                                    "    \"\"\"\n",
                                    "    This DAG demonstrates the basics of Airflow with just two tasks:\n",
                                    "    1. A task that says hello\n",
                                    "    2. A task that says goodbye and uses the result from the first task\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define the first task using the @task decorator\n",
                                    "    @task(task_id=\"hello_task\")\n",
                                    "    def say_hello():\n",
                                    "        \"\"\"Simple function that prints a greeting.\"\"\"\n",
                                    "        print(\"Hello from Airflow!\")\n",
                                    "        return \"Hello\"\n",
                                    "    \n",
                                    "    # Define the second task using the @task decorator\n",
                                    "    @task(task_id=\"goodbye_task\")\n",
                                    "    def say_goodbye(first_task_result):\n",
                                    "        \"\"\"\n",
                                    "        Function that uses the result from the first task.\n",
                                    "        \n",
                                    "        Args:\n",
                                    "            first_task_result: The result returned by the first task\n",
                                    "        \"\"\"\n",
                                    "        print(f\"Previous task said: {first_task_result}\")\n",
                                    "        print(\"Goodbye from Airflow!\")\n",
                                    "    \n",
                                    "    # Define the task dependencies\n",
                                    "    # With TaskFlow API, dependencies are created by function calls\n",
                                    "    first_result = say_hello()  # Execute the first task\n",
                                    "    say_goodbye(first_result)   # Pass the result to the second task\n",
                                    "\n",
                                    "# Create the DAG instance\n",
                                    "hello_airflow_dag()\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adding a Third Task to Your DAG\n",
                                    "\n",
                                    "You’ve just practiced building a simple Airflow DAG with two connected tasks and learned how to pass data from one task to another. Now, let’s make your workflow a bit more interesting by adding a third step.\n",
                                    "\n",
                                    "Your goal is to introduce a new task called finalize_greeting into the DAG. This task should take the output from say_goodbye, perform a transformation on it (for example, add extra text or reformat the message), and return the new result. Here’s what you need to do:\n",
                                    "\n",
                                    "Define a new function called finalize_greeting and decorate it with @task(task_id=\"finalize_task\").\n",
                                    "Ensure this function takes the output from say_goodbye as its input.\n",
                                    "Inside the function, transform the input in some way (for example, add a friendly remark or change the format), print the final message, and return it.\n",
                                    "Update the task dependencies so that the output from say_goodbye is passed to finalize_greeting.\n",
                                    "This will help you see how to chain multiple tasks together and build more flexible workflows in Airflow.\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Basic Introduction to Airflow - Simple DAG\n",
                                    "\n",
                                    "This module defines a minimal Airflow DAG with two tasks to demonstrate\n",
                                    "the core concepts of Directed Acyclic Graphs (DAGs) in Airflow using the TaskFlow API.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=1),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG using the TaskFlow API\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='A simple two-task DAG using TaskFlow API',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'basic'],\n",
                                    ")\n",
                                    "def hello_airflow_dag():\n",
                                    "    \"\"\"\n",
                                    "    This DAG demonstrates the basics of Airflow with just two tasks:\n",
                                    "    1. A task that says hello\n",
                                    "    2. A task that says goodbye and uses the result from the first task\n",
                                    "    \"\"\"\n",
                                    "\n",
                                    "    @task(task_id=\"hello_task\")\n",
                                    "    def say_hello():\n",
                                    "        \"\"\"Simple function that prints a greeting.\"\"\"\n",
                                    "        print(\"Hello from Airflow!\")\n",
                                    "        return \"Hello\"\n",
                                    "\n",
                                    "    @task(task_id=\"goodbye_task\")\n",
                                    "    def say_goodbye(first_task_result):\n",
                                    "        \"\"\"\n",
                                    "        Function that uses the result from the first task.\n",
                                    "\n",
                                    "        Args:\n",
                                    "            first_task_result: The result returned by the first task\n",
                                    "        \"\"\"\n",
                                    "        print(f\"Previous task said: {first_task_result}\")\n",
                                    "        print(\"Goodbye from Airflow!\")\n",
                                    "        # The goodbye task should return a value so the next task can use it\n",
                                    "        return \"Goodbye\"\n",
                                    "\n",
                                    "    # TODO: Define a new task called finalize_greeting using the @task decorator.\n",
                                    "    # - The function should take the output from say_goodbye as input\n",
                                    "    # - It should transform this input in some way, creating a reformatted \"final message\"\n",
                                    "    # - It should print this final message and return it\n",
                                    "\n",
                                    "    # Define the task dependencies\n",
                                    "    first_result = say_hello()  # Execute the first task\n",
                                    "    goodbye_result = say_goodbye(first_result)\n",
                                    "    # TODO: Pass the goodbye_result to the finalize_greeting task\n",
                                    "    # finalize_greeting(goodbye_result)\n",
                                    "\n",
                                    "# Create the DAG instance\n",
                                    "hello_airflow_dag()\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Basic Introduction to Airflow - Simple DAG\n",
                                    "\n",
                                    "This module defines a minimal Airflow DAG with two tasks to demonstrate\n",
                                    "the core concepts of Directed Acyclic Graphs (DAGs) in Airflow using the TaskFlow API.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=1),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG using the TaskFlow API\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='A simple two-task DAG using TaskFlow API',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'basic'],\n",
                                    ")\n",
                                    "def hello_airflow_dag():\n",
                                    "    \"\"\"\n",
                                    "    This DAG demonstrates the basics of Airflow with just two tasks:\n",
                                    "    1. A task that says hello\n",
                                    "    2. A task that says goodbye and uses the result from the first task\n",
                                    "    \"\"\"\n",
                                    "\n",
                                    "    @task(task_id=\"hello_task\")\n",
                                    "    def say_hello():\n",
                                    "        \"\"\"Simple function that prints a greeting.\"\"\"\n",
                                    "        print(\"Hello from Airflow!\")\n",
                                    "        return \"Hello\"\n",
                                    "\n",
                                    "    @task(task_id=\"goodbye_task\")\n",
                                    "    def say_goodbye(first_task_result):\n",
                                    "        \"\"\"\n",
                                    "        Function that uses the result from the first task.\n",
                                    "\n",
                                    "        Args:\n",
                                    "            first_task_result: The result returned by the first task\n",
                                    "        \"\"\"\n",
                                    "        print(f\"Previous task said: {first_task_result}\")\n",
                                    "        print(\"Goodbye from Airflow!\")\n",
                                    "        # The goodbye task should return a value so the next task can use it\n",
                                    "        return \"Goodbye\"\n",
                                    "\n",
                                    "    # TODO: Define a new task called finalize_greeting using the @task decorator.\n",
                                    "    # - The function should take the output from say_goodbye as input\n",
                                    "    # - It should transform this input in some way, creating a reformatted \"final message\"\n",
                                    "    # - It should print this final message and return it\n",
                                    "    @task(task_id=\"finalize_task\")\n",
                                    "    def finalize_greeting(goodbye_message):\n",
                                    "        \"\"\"\n",
                                    "        Function that takes the output from the previous task and finalizes the message.\n",
                                    "\n",
                                    "        Args:\n",
                                    "            goodbye_message: The result returned by the previous task\n",
                                    "        \"\"\"\n",
                                    "        final_message = f\"Final message: '{goodbye_message}' - It was a pleasure!\"\n",
                                    "        print(final_message)\n",
                                    "        return final_message\n",
                                    "\n",
                                    "    # Define the task dependencies\n",
                                    "    first_result = say_hello()  # Execute the first task\n",
                                    "    goodbye_result = say_goodbye(first_result)\n",
                                    "    # TODO: Pass the goodbye_result to the finalize_greeting task\n",
                                    "    finalize_greeting(goodbye_result)\n",
                                    "\n",
                                    "# Create the DAG instance\n",
                                    "hello_airflow_dag()\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Measuring Your Workflow Output"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Build a Time Formatting Workflow"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Build a Three Step Greeting Workflow"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
