{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 3 Testing and Running ML Pipelines with Airflow CLI\n",
                                    "\n",
                                    "Based on the previous conversation, here is the information converted to English:\n",
                                    "\n",
                                    "Welcome to the third lesson in our \"Automating Retraining with Apache Airflow\" course.\n",
                                    "\n",
                                    "In our previous lessons, we discussed the core concepts of Apache Airflow and how to design an ML pipeline DAG using the TaskFlow API. Now, let's learn how to test and run that pipeline effectively.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Understanding the Airflow CLI\n",
                                    "\n",
                                    "The **Airflow Command Line Interface (CLI)** is a tool that allows you to interact with Airflow directly from the terminal. The CLI is critical for your ML pipeline development workflow for several reasons:\n",
                                    "\n",
                                    "  * **Development and debugging**: You can quickly test DAGs and tasks during development without waiting for a scheduled run.\n",
                                    "  * **CI/CD pipelines**: You can automate DAG validation in continuous integration workflows.\n",
                                    "  * **Troubleshooting**: You can investigate issues in your workflows with detailed logging.\n",
                                    "\n",
                                    "Using the CLI to test each component of your pipeline will help you catch issues early, such as a model trained on corrupted data or with incorrect hyperparameters, which could affect business decisions.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Exploring Available DAGs\n",
                                    "\n",
                                    "Before you can test a DAG, you need to know what's available in your environment.\n",
                                    "\n",
                                    "To list all available DAGs, run the following command:\n",
                                    "\n",
                                    "```bash\n",
                                    "airflow dags list\n",
                                    "```\n",
                                    "\n",
                                    "This command will display a tabular list of all DAGs, including the DAG ID, schedule interval, and whether each DAG is paused. Remember that the DAG code must be saved correctly in `$AIRFLOW_HOME/dags` to be discovered.\n",
                                    "\n",
                                    "To examine the details of your ML pipeline DAG, use the command:\n",
                                    "\n",
                                    "```bash\n",
                                    "airflow dags show mlops_pipeline\n",
                                    "```\n",
                                    "\n",
                                    "This command reveals the architecture of your DAG, showing all tasks and their connections, allowing you to verify its structure before execution.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Testing Complete DAGs\n",
                                    "\n",
                                    "Once you have a view of the architecture, you can test if the DAG runs correctly from start to finish. Use this command for a test run:\n",
                                    "\n",
                                    "```bash\n",
                                    "airflow dags test mlops_pipeline\n",
                                    "```\n",
                                    "\n",
                                    "When you run this command, your entire pipeline will be executed in sequence, from data extraction to model deployment. However, this test execution has a few unique characteristics:\n",
                                    "\n",
                                    "  * It runs immediately instead of waiting for the next scheduled interval.\n",
                                    "  * It shows all logs directly in your console.\n",
                                    "  * It doesn't record the run in Airflow's database, keeping your metadata clean.\n",
                                    "\n",
                                    "This is a safe way to observe every step without affecting your production environment.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Working with Individual Tasks\n",
                                    "\n",
                                    "Sometimes, you may only need to test a specific part of the pipeline. To list all tasks within a DAG, use the command:\n",
                                    "\n",
                                    "```bash\n",
                                    "airflow tasks list mlops_pipeline\n",
                                    "```\n",
                                    "\n",
                                    "This command shows all individual task IDs in your pipeline, which is useful when you need to test a specific task.\n",
                                    "\n",
                                    "To test a single task, use the `tasks test` command with the task ID and a date context, as in the example below:\n",
                                    "\n",
                                    "```bash\n",
                                    "airflow tasks test mlops_pipeline train_model 2023-09-15\n",
                                    "```\n",
                                    "\n",
                                    "This way, you can inspect and debug individual components without running the entire workflow.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Troubleshooting Common Issues\n",
                                    "\n",
                                    "When testing ML pipelines in Airflow, you may encounter some common issues:\n",
                                    "\n",
                                    "  * **Task dependencies not working correctly?** Double-check how you've defined the relationships between tasks. With the TaskFlow API, dependencies are created automatically when you pass the result of one task to the next.\n",
                                    "  * **Tasks failing unexpectedly?** Try isolating and testing the problematic task. The detailed logs often reveal the source of the problem.\n",
                                    "\n",
                                    "When troubleshooting ML pipeline issues specifically, pay close attention to domain-specific concerns like data quality issues, resource constraints (ML training often requires significant memory), and model performance thresholds that might trigger failures.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Conclusion\n",
                                    "\n",
                                    "The Airflow CLI is an essential tool for thoroughly testing your ML pipelines. By mastering these commands, you can ensure your model retraining system functions correctly before it's deployed to production. In the next lesson, we'll discuss more advanced Airflow features for handling large datasets and optimizing resource usage."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Discover Your Airflow Pipelines\n",
                                    "\n",
                                    "You have just learned how to use the Airflow CLI to explore and manage your ML pipelines. Now, let's put that knowledge into practice.\n",
                                    "\n",
                                    "Your task is to write a command in the solution.sh file that will list all the available DAGs in your Airflow environment. When you run it, you should see the mlops_pipeline DAG. This is a simple but important first step in working with Airflow from the command line.\n",
                                    "\n",
                                    "```python\n",
                                    "#airflow_dags.py\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule_interval='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date (in the past)\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"    \n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the next unit.\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define tasks using the TaskFlow API\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "    \n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            # Determine if model meets quality threshold\n",
                                    "            if accuracy >= 0.8:\n",
                                    "                return {\"validation\": \"passed\", \"accuracy\": accuracy}\n",
                                    "            else:\n",
                                    "                return {\"validation\": \"failed\", \"accuracy\": accuracy}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            logging.warning(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "    \n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_model(validation_result)\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "```\n",
                                    "```bash\n",
                                    "# solution.sh\n",
                                    "```\n",
                                    "\n",
                                    "To list all available DAGs in your Airflow environment, you need to use the `airflow dags list` command. This command will show a list of DAGs, including the `mlops_pipeline` DAG that is defined in the `airflow_dags.py` file.\n",
                                    "\n",
                                    "Here is the command you should write in `solution.sh`:\n",
                                    "\n",
                                    "```bash\n",
                                    "# solution.sh\n",
                                    "airflow dags list\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Inspecting Your ML Pipeline Structure\n",
                                    "\n",
                                    "You’ve just learned how to use the Airflow CLI to explore your available DAGs and view their details. Now, let’s take the next step and work with a specific DAG in your environment.\n",
                                    "\n",
                                    "The solution.sh file contains a command that should show the structure of our DAG defined in airflow_dag.py. However, for some reason it appears to result in an error when it is run. Can you find the issue and fix it?\n",
                                    "\n",
                                    "```python\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule_interval='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date (in the past)\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"    \n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the next unit.\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define tasks using the TaskFlow API\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "    \n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            # Determine if model meets quality threshold\n",
                                    "            if accuracy >= 0.8:\n",
                                    "                return {\"validation\": \"passed\", \"accuracy\": accuracy}\n",
                                    "            else:\n",
                                    "                return {\"validation\": \"failed\", \"accuracy\": accuracy}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            logging.warning(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "    \n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_model(validation_result)\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "````bash\n",
                                    "# solution.sh\n",
                                    "airflow show dags ml_pipeline\n",
                                    "```\n",
                                    "\n",
                                    "Pemasalahan pada file `solution.sh` adalah nama DAG yang salah. Dalam file `airflow_dag.py`, `dag_id` yang didefinisikan adalah `mlops_pipeline`, bukan `ml_pipeline`.\n",
                                    "\n",
                                    "Untuk memperbaiki masalah tersebut, Anda perlu mengganti `ml_pipeline` dengan `mlops_pipeline` di perintah `airflow show dags`.\n",
                                    "\n",
                                    "**File `solution.sh` yang sudah diperbaiki:**\n",
                                    "\n",
                                    "```bash\n",
                                    "airflow dags show mlops_pipeline\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Test Your Full ML Pipeline\n",
                                    "\n",
                                    "You’ve already learned how to list and inspect your Airflow DAGs using the CLI. Now, let’s move forward and try running your entire machine learning pipeline from start to finish.\n",
                                    "\n",
                                    "Your task is to write a command in the solution.sh file that will test the entire mlops_pipeline DAG using the Airflow CLI. This is a key step in ensuring your pipeline works as expected before you schedule it for regular runs.\n",
                                    "\n",
                                    "```python\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule_interval='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date (in the past)\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"    \n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the next unit.\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define tasks using the TaskFlow API\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "    \n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            # Determine if model meets quality threshold\n",
                                    "            if accuracy >= 0.8:\n",
                                    "                return {\"validation\": \"passed\", \"accuracy\": accuracy}\n",
                                    "            else:\n",
                                    "                return {\"validation\": \"failed\", \"accuracy\": accuracy}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            logging.warning(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "    \n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_model(validation_result)\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "```\n",
                                    "\n",
                                    "```bash\n",
                                    "# solution.sh\n",
                                    "# TODO: Write the command to test the entire mlops_pipeline DAG\n",
                                    "```\n",
                                    "\n",
                                    "Untuk menjalankan seluruh DAG dari awal hingga akhir menggunakan Airflow CLI, Anda harus menggunakan perintah `dags test` dengan ID DAG dan tanggal eksekusi. Tanggal harus berada di masa lalu agar Airflow tidak menunggu jadwal.\n",
                                    "\n",
                                    "Berikut adalah perintah yang benar untuk file `solution.sh`:\n",
                                    "\n",
                                    "```bash\n",
                                    "airflow dags test mlops_pipeline 2023-01-01\n",
                                    "```\n",
                                    "\n",
                                    "Perintah ini akan menjalankan seluruh DAG `mlops_pipeline` secara lokal untuk tanggal yang ditentukan, tanpa melibatkan scheduler atau mencatat hasilnya ke database metadata Airflow. Ini adalah cara yang ideal untuk menguji alur kerja secara lengkap sebelum menjalankannya di lingkungan produksi.\n",
                                    "\n",
                                    "Untuk mempelajari lebih lanjut tentang pengujian DAG Airflow secara interaktif, tonton video ini [How to easily test your Airflow DAGs with the new dag.test() function](https://www.youtube.com/watch?v=eDHQ2d1wqD4).\n",
                                    "http://googleusercontent.com/youtube_content/4"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## See All Tasks in Your Pipeline\n",
                                    "\n",
                                    "After learning how to test your complete ML pipeline using the Airflow CLI, it's time to focus on the individual building blocks of your workflow.\n",
                                    "\n",
                                    "Your task is to write a command in the solution.sh file that will list all the tasks defined in the mlops_pipeline DAG. This will help you see each step in your pipeline and is useful when you want to test or inspect specific tasks.\n",
                                    "\n",
                                    "```python\n",
                                    "from datetime import datetime, timedelta\n",
                                    "from airflow.decorators import dag, task\n",
                                    "\n",
                                    "# Define default arguments for the DAG\n",
                                    "default_args = {\n",
                                    "    'owner': 'airflow',\n",
                                    "    'depends_on_past': False,\n",
                                    "    'email_on_failure': False,\n",
                                    "    'email_on_retry': False,\n",
                                    "    'retries': 1,\n",
                                    "    'retry_delay': timedelta(minutes=5),\n",
                                    "}\n",
                                    "\n",
                                    "# Define the DAG\n",
                                    "@dag(\n",
                                    "    dag_id='mlops_pipeline',  # Unique identifier for the DAG\n",
                                    "    description='Introduction to Airflow for ML pipelines',\n",
                                    "    default_args=default_args,\n",
                                    "    schedule_interval='@daily',  # Run daily\n",
                                    "    start_date=datetime(2023, 1, 1),  # Start date (in the past)\n",
                                    "    catchup=False,  # Don't run for past dates\n",
                                    "    tags=['intro', 'ml'],\n",
                                    ")\n",
                                    "def training_pipeline():\n",
                                    "    \"\"\"    \n",
                                    "    This DAG introduces Airflow concepts for ML workflows.\n",
                                    "    It demonstrates task definition, dependencies, and flow control\n",
                                    "    before we implement our actual ML pipeline in the next unit.\n",
                                    "    \"\"\"\n",
                                    "    \n",
                                    "    # Define tasks using the TaskFlow API\n",
                                    "    @task(task_id=\"extract_data\")\n",
                                    "    def extract_data():\n",
                                    "        \"\"\"Simulate extracting data from a source.\"\"\"\n",
                                    "        print(\"Extracting data from source...\")\n",
                                    "        # In a real scenario, this would connect to a data source\n",
                                    "        return {\"data_extracted\": True, \"records\": 1000}\n",
                                    "    \n",
                                    "    @task(task_id=\"transform_data\")\n",
                                    "    def transform_data(extract_result):\n",
                                    "        \"\"\"Simulate transforming the extracted data.\"\"\"\n",
                                    "        if extract_result[\"data_extracted\"]:\n",
                                    "            num_records = extract_result[\"records\"]\n",
                                    "            print(f\"Transforming {num_records} records...\")\n",
                                    "            # Simulate data transformation\n",
                                    "            return {\"data_transformed\": True, \"features\": 10}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data extraction failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"train_model\")\n",
                                    "    def train_model(transform_result):\n",
                                    "        \"\"\"Simulate training a machine learning model.\"\"\"\n",
                                    "        if transform_result[\"data_transformed\"]:\n",
                                    "            num_features = transform_result[\"features\"]\n",
                                    "            print(f\"Training model with {num_features} features...\")\n",
                                    "            # Simulate model training\n",
                                    "            return {\"model_trained\": True, \"accuracy\": 0.85}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Data transformation failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"validate_model\")\n",
                                    "    def validate_model(train_result):\n",
                                    "        \"\"\"Simulate validating the model's performance.\"\"\"\n",
                                    "        if train_result[\"model_trained\"]:\n",
                                    "            accuracy = train_result[\"accuracy\"]\n",
                                    "            print(f\"Validating model. Accuracy: {accuracy}\")\n",
                                    "            # Determine if model meets quality threshold\n",
                                    "            if accuracy >= 0.8:\n",
                                    "                return {\"validation\": \"passed\", \"accuracy\": accuracy}\n",
                                    "            else:\n",
                                    "                return {\"validation\": \"failed\", \"accuracy\": accuracy}\n",
                                    "        else:\n",
                                    "            raise ValueError(\"Model training failed\")\n",
                                    "    \n",
                                    "    @task(task_id=\"deploy_model\", trigger_rule=\"none_failed\")\n",
                                    "    def deploy_model(validation_result):\n",
                                    "        \"\"\"Simulate deploying the model if validation passed.\"\"\"\n",
                                    "        if validation_result[\"validation\"] == \"passed\":\n",
                                    "            print(f\"Deploying model with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": True}\n",
                                    "        else:\n",
                                    "            logging.warning(f\"Model validation failed with accuracy: {validation_result['accuracy']}\")\n",
                                    "            return {\"deployed\": False}\n",
                                    "    \n",
                                    "    # Define the task dependencies using TaskFlow\n",
                                    "    # This creates the DAG structure automatically based on function calls\n",
                                    "    extract_result = extract_data()\n",
                                    "    transform_result = transform_data(extract_result)\n",
                                    "    train_result = train_model(transform_result)\n",
                                    "    validation_result = validate_model(train_result)\n",
                                    "    deploy_model(validation_result)\n",
                                    "\n",
                                    "# Now actually enable this to be run as a DAG\n",
                                    "training_pipeline()\n",
                                    "```\n",
                                    "\n",
                                    "```bash\n",
                                    "# solution.sh\n",
                                    "# TODO: Write the command to list all tasks in the mlops_pipeline DAG\n",
                                    "```\n",
                                    "\n",
                                    "Untuk membuat daftar semua task dalam DAG `mlops_pipeline` menggunakan Airflow CLI, Anda dapat menggunakan perintah `tasks list` diikuti dengan ID DAG.\n",
                                    "\n",
                                    "Berikut adalah perintah yang benar untuk file `solution.sh`:\n",
                                    "\n",
                                    "```bash\n",
                                    "airflow tasks list mlops_pipeline\n",
                                    "```\n",
                                    "\n",
                                    "Perintah ini akan menampilkan daftar semua task\\_id yang ada di dalam DAG yang ditentukan, memberikan gambaran yang jelas tentang struktur alur kerja Anda."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Test an Individual Pipeline Task"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
