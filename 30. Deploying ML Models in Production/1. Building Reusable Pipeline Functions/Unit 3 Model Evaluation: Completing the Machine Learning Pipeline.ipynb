{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 3 Model Evaluation: Completing the Machine Learning Pipeline\n",
                                    "\n",
                                    "Here's the content you provided, formatted in Markdown:\n",
                                    "\n",
                                    "# Welcome to Lesson 3: Building Reusable Pipeline Functions\n",
                                    "\n",
                                    "Welcome to Lesson 3 of our \"Building Reusable Pipeline Functions\" course\\! In our previous lessons, we've developed robust functions for data processing and model training. We now have the foundation of a proper machine learning pipeline, but an essential component is still missing: **model evaluation**.\n",
                                    "\n",
                                    "How do you know if your model is performing well? How can you compare different models to select the best one? These questions highlight why evaluation is a critical part of any machine learning pipeline. In this lesson, you'll create reusable evaluation functions that calculate key performance metrics for your models. By the end of this lesson, you'll have a complete pipeline that not only processes data and trains models but also rigorously evaluates their performance.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Understanding Model Evaluation in ML Pipelines\n",
                                    "\n",
                                    "Before diving into code, let's understand why proper model evaluation is **critical** in production ML pipelines:\n",
                                    "\n",
                                    "  * **Performance Assessment:** Evaluation metrics provide objective measures of how well your model performs on unseen data.\n",
                                    "  * **Model Selection:** Comparing evaluation metrics helps you choose between different models or hyperparameter configurations.\n",
                                    "  * **Business Impact:** Translating technical metrics into business terms helps stakeholders understand model value.\n",
                                    "  * **Monitoring:** Establishing baseline metrics enables ongoing monitoring of model performance in production.\n",
                                    "\n",
                                    "When evaluating regression models like our diamond price predictor, you'll typically focus on metrics that quantify the difference between predicted and actual values. A well-designed evaluation function should calculate **multiple complementary metrics** to provide a comprehensive view of performance, return results in a **consistent format**, and be **flexible** enough to work with different model types.\n",
                                    "\n",
                                    "Think about real-world applications: if you're predicting house prices, stakeholders won't just want to know that the model has an R² of 0.8 – they'll want to know how many dollars off your predictions typically are. This makes having multiple metrics crucial for communication and decision-making.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Common Regression Evaluation Metrics\n",
                                    "\n",
                                    "For our diamond price prediction task, several standard metrics help us understand model performance from different angles:\n",
                                    "\n",
                                    "  * **Root Mean Squared Error (RMSE)** measures the average magnitude of prediction errors, with higher penalties for larger errors. Lower values indicate better performance. RMSE is particularly useful when large errors are especially undesirable – for example, if being $1000 off on diamond pricing is more than twice as bad as being $500 off.\n",
                                    "  * **R-squared (R²)** represents the proportion of variance in the dependent variable explained by the model. Values range from 0 to 1, with higher values indicating better fit. This metric helps you understand how much better your model is than simply guessing the average price for all diamonds.\n",
                                    "  * **Mean Absolute Error (MAE)** measures the average absolute difference between predicted and actual values. It's less sensitive to outliers than RMSE and directly interpretable in the same units as your target variable – dollars, in our diamond case.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Computing Metrics with scikit-learn\n",
                                    "\n",
                                    "Let's see how you can implement these metrics using `scikit-learn`:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                                    "\n",
                                    "# Example calculation of metrics\n",
                                    "def calculate_metrics(y_true, y_pred):\n",
                                    "    # Convert MSE to RMSE by taking square root\n",
                                    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
                                    "    r2 = r2_score(y_true, y_pred)\n",
                                    "    mae = mean_absolute_error(y_true, y_pred)\n",
                                    "```\n",
                                    "\n",
                                    "This simple function demonstrates how you can use `scikit-learn`'s metrics to evaluate predictions. For a $5000 diamond, an MAE of $500 would mean your predictions are typically off by about 10% – a concrete insight you can share with stakeholders.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Designing a Reusable Evaluation Function\n",
                                    "\n",
                                    "Now that you understand the key metrics, let's design a reusable evaluation function that follows the same principles we've applied to our other pipeline components. The function should focus solely on evaluation, provide a consistent interface with our other functions, return informative results, and work with any regression model. Let's start by defining the interface:\n",
                                    "\n",
                                    "```python\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluate a trained model on test data.\n",
                                    "\n",
                                    "    Args:\n",
                                    "        model (object): Trained model\n",
                                    "        X_test (array-like): Test features\n",
                                    "        y_test (array-like): True target values\n",
                                    "\n",
                                    "    Returns:\n",
                                    "        dict: Dictionary of metrics\n",
                                    "        array: Model predictions\n",
                                    "    \"\"\"\n",
                                    "    # Function implementation will go here\n",
                                    "```\n",
                                    "\n",
                                    "This function signature follows the pattern established in previous lessons:\n",
                                    "\n",
                                    "  * It takes a **trained model** and **test data** as input\n",
                                    "  * It will generate predictions using the model\n",
                                    "  * It will calculate performance metrics\n",
                                    "  * It will return both the metrics and predictions\n",
                                    "\n",
                                    "Returning both metrics and predictions is particularly valuable for your workflow. Imagine you're working with stakeholders who want to understand where the model makes its largest errors. Having the predictions available allows you to quickly identify those cases and investigate patterns.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Implementing the Evaluation Function\n",
                                    "\n",
                                    "Let's now implement the full evaluation function with all three metrics we discussed:\n",
                                    "\n",
                                    "```python\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluate a trained model on test data.\n",
                                    "\n",
                                    "    Args:\n",
                                    "        model (object): Trained model\n",
                                    "        X_test (array-like): Test features\n",
                                    "        y_test (array-like): True target values\n",
                                    "\n",
                                    "    Returns:\n",
                                    "        dict: Dictionary of metrics\n",
                                    "        array: Model predictions\n",
                                    "    \"\"\"\n",
                                    "    # Generate predictions\n",
                                    "    y_pred = predict_with_model(model, X_test)\n",
                                    "\n",
                                    "    # Calculate metrics\n",
                                    "    metrics = {\n",
                                    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
                                    "        'r2': r2_score(y_test, y_pred),\n",
                                    "        'mae': mean_absolute_error(y_test, y_pred)\n",
                                    "    }\n",
                                    "\n",
                                    "    return metrics, y_pred\n",
                                    "```\n",
                                    "\n",
                                    "By storing metrics in a dictionary, you gain several advantages. You can easily access specific metrics by name (like `metrics['rmse']`), add new metrics in the future without changing your function signature, and iterate through all metrics for reporting. This structure also makes it simple to log metrics to tracking systems like MLflow or Weights & Biases for experiment tracking.\n",
                                    "\n",
                                    "The returned predictions enable further analyses beyond the standard metrics. You might want to plot residuals, examine the distribution of errors, or identify specific examples where the model performs poorly – all of which require the actual predictions.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Combining Components for End-to-End Evaluation\n",
                                    "\n",
                                    "Now let's see how your evaluation function can work alongside the data processing and model training components from previous lessons. Here's how you'd create an end-to-end workflow:\n",
                                    "\n",
                                    "```python\n",
                                    "# Step 1: Load and preprocess data\n",
                                    "diamonds_df = load_diamonds_data(data_path)\n",
                                    "X_train, X_test, y_train, y_test, preprocessor = preprocess_diamonds_data(diamonds_df)\n",
                                    "\n",
                                    "# Step 2: Train a model\n",
                                    "model = train_model(\n",
                                    "    X_train,\n",
                                    "    y_train,\n",
                                    "    model_type=\"random_forest\",\n",
                                    "    n_estimators=100,\n",
                                    "    max_depth=10,\n",
                                    "    random_state=42\n",
                                    ")\n",
                                    "\n",
                                    "# Step 3: Evaluate on test set\n",
                                    "metrics, predictions = evaluate_model(model, X_test, y_test)\n",
                                    "\n",
                                    "# Display results\n",
                                    "print(\"Test Set Metrics:\")\n",
                                    "for metric_name, metric_value in metrics.items():\n",
                                    "    print(f\"  - {metric_name}: {metric_value:.4f}\")\n",
                                    "```\n",
                                    "\n",
                                    "Notice how each component connects seamlessly with the others. The preprocessed data flows into the training function, the trained model flows into the evaluation function, and the results are presented in a readable format.\n",
                                    "\n",
                                    "This modular design makes your pipeline **easy to understand** (each step has a clear purpose), **maintainable** (changes to one component don't affect others), and **flexible** (you can swap components or add new ones). For example, you could easily extend this workflow to compare multiple models by calling `train_model` and `evaluate_model` with different parameters and storing the results for comparison.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Beyond Basic Metrics\n",
                                    "\n",
                                    "While RMSE, R², and MAE provide a good foundation, real-world applications often require additional evaluation approaches. Here are some ways you might extend your evaluation function:\n",
                                    "\n",
                                    "  * **Cross-validation** can provide a more robust assessment of model performance. Instead of a single train/test split, you could modify your evaluation function to perform k-fold cross-validation and return the mean and standard deviation of each metric across folds.\n",
                                    "  * **Custom business metrics** often matter more than statistical ones. For a diamond pricing model, being consistently conservative (predicting slightly lower than actual prices) might be preferable to being accurate on average but sometimes overpricing. You could add custom metrics that capture these business preferences.\n",
                                    "  * **Visualization** of results can reveal patterns that metrics alone might miss. You could extend your pipeline to generate scatter plots of predicted vs. actual values, histograms of errors, or plots of residuals against feature values.\n",
                                    "\n",
                                    "As you gain experience with model evaluation, you'll develop an intuition for which metrics and approaches are most relevant for different problems. The flexible, dictionary-based return value of your evaluation function makes it easy to extend with these advanced approaches.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Conclusion\n",
                                    "\n",
                                    "In this lesson, you've completed your machine learning pipeline by adding the crucial component of **model evaluation**. You've learned how different metrics provide complementary insights into model performance, and you've created a reusable evaluation function that calculates these metrics and returns them in a consistent format.\n",
                                    "\n",
                                    "With data processing, model training, and now evaluation functions in place, you have a complete, production-ready machine learning pipeline that follows best practices in software engineering and machine learning operations. This modular approach will serve you well as you tackle more complex projects and deploy models to production environments."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Debugging Model Evaluation Metrics\n",
                                    "\n",
                                    "Welcome to your first hands-on practice in this lesson on model evaluation! In the previous lesson, you learned about the importance of evaluating models using key metrics. Now, it's time to put that knowledge into practice.\n",
                                    "\n",
                                    "In this exercise, you'll work with the evaluate_model function, which is designed to measure the performance of a trained model using metrics like Root Mean Squared Error (RMSE), R-squared (R²), and Mean Absolute Error (MAE). Your objective is to ensure these metrics are computed accurately. As you examine the code, look for a subtle issue in the calculation of these metrics.\n",
                                    "\n",
                                    "Note: You might notice that error values for RMSE and MAE are very similar no matter which order you pass y_test and y_pred to the metric functions. This is because these metrics are symmetric—they simply measure the difference between the two arrays, so swapping the order doesn’t affect the result. However, for metrics like R², the order does matter and swapping them can lead to incorrect results.\n",
                                    "\n",
                                    "Identify and correct this issue to ensure the function returns precise results. Dive in and enjoy the debugging process!\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Model Evaluation Module for ML Pipeline\n",
                                    "\n",
                                    "This module provides functions for evaluating machine learning models\n",
                                    "with various performance metrics and visualization techniques.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                                    "\n",
                                    "from model import predict_with_model\n",
                                    "\n",
                                    "\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluate a trained model on test data.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        model (object): Trained model\n",
                                    "        X_test (array-like): Test features\n",
                                    "        y_test (array-like): True target values\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        dict: Dictionary of metrics\n",
                                    "        array: Model predictions\n",
                                    "    \"\"\"\n",
                                    "    # Generate predictions\n",
                                    "    y_pred = predict_with_model(model, X_test)\n",
                                    "    \n",
                                    "    # Calculate metrics\n",
                                    "    metrics = {\n",
                                    "        'rmse': np.sqrt(mean_squared_error(y_pred, y_test)),\n",
                                    "        'r2': r2_score(y_pred, y_test),\n",
                                    "        'mae': mean_absolute_error(y_pred, y_test)\n",
                                    "    }\n",
                                    "    \n",
                                    "    return metrics, y_pred\n",
                                    "```\n",
                                    "\n",
                                    "Here's the corrected evaluate_model function. The issue was in the order of arguments passed to the sklearn.metrics functions. For r2_score, mean_squared_error, and mean_absolute_error, the convention is to pass y_true (actual values) first and y_pred (predicted values) second.\n",
                                    "Debugging Model Evaluation Metrics\n",
                                    "Aug 5, 11:35 AM\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Model Evaluation Module for ML Pipeline\n",
                                    "\n",
                                    "This module provides functions for evaluating machine learning models\n",
                                    "with various performance metrics and visualization techniques.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                                    "\n",
                                    "# Assuming 'predict_with_model' is available from a 'model' module or defined elsewhere\n",
                                    "# If running this as a standalone file, you might need to define or import predict_with_model\n",
                                    "# For this exercise, we assume it's correctly imported.\n",
                                    "# from model import predict_with_model # This line assumes 'model.py' exists and has the function.\n",
                                    "# If you combined all functions into main.py, you would just call predict_with_model directly.\n",
                                    "\n",
                                    "# Placeholder for predict_with_model if it's not imported from another file\n",
                                    "# This is just to make the code runnable for demonstration if not part of a larger project\n",
                                    "def predict_with_model(model, X):\n",
                                    "    \"\"\"\n",
                                    "    Placeholder for predict_with_model.\n",
                                    "    In a real scenario, this would come from your model module.\n",
                                    "    \"\"\"\n",
                                    "    if model is None or X is None:\n",
                                    "        print(\"Prediction skipped: Model or test data is None.\")\n",
                                    "        return None\n",
                                    "    try:\n",
                                    "        # Convert X to a numpy array for easier validation\n",
                                    "        X = np.array(X)\n",
                                    "        if X.size == 0:\n",
                                    "            print(\"Error: Input features array 'X' cannot be empty.\")\n",
                                    "            return None\n",
                                    "        if X.ndim != 2:\n",
                                    "            print(f\"Error: Input features array 'X' must be 2-dimensional. Got {X.ndim} dimensions.\")\n",
                                    "            return None\n",
                                    "        predictions = model.predict(X)\n",
                                    "        return predictions\n",
                                    "    except Exception as e:\n",
                                    "        print(f\"An error occurred during prediction: {e}\")\n",
                                    "        return None\n",
                                    "\n",
                                    "\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluate a trained model on test data.\n",
                                    "\n",
                                    "    Args:\n",
                                    "        model (object): Trained model\n",
                                    "        X_test (array-like): Test features\n",
                                    "        y_test (array-like): True target values\n",
                                    "\n",
                                    "    Returns:\n",
                                    "        dict: Dictionary of metrics\n",
                                    "        array: Model predictions\n",
                                    "    \"\"\"\n",
                                    "    # Generate predictions\n",
                                    "    y_pred = predict_with_model(model, X_test)\n",
                                    "\n",
                                    "    if y_pred is None:\n",
                                    "        print(\"Could not generate predictions for evaluation.\")\n",
                                    "        return {}, None\n",
                                    "\n",
                                    "    # Calculate metrics\n",
                                    "    # The fix: Ensure y_test (true values) comes before y_pred (predicted values)\n",
                                    "    # for all scikit-learn metrics functions for correctness and consistency.\n",
                                    "    metrics = {\n",
                                    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
                                    "        'r2': r2_score(y_test, y_pred),\n",
                                    "        'mae': mean_absolute_error(y_test, y_pred)\n",
                                    "    }\n",
                                    "\n",
                                    "    return metrics, y_pred\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Key Change:\n",
                                    "\n",
                                    "The primary fix was changing the order of y_pred and y_test in the mean_squared_error, r2_score, and mean_absolute_error function calls. They now correctly use (y_test, y_pred):\n",
                                    "\n",
                                    "'rmse': np.sqrt(mean_squared_error(y_test, y_pred))\n",
                                    "\n",
                                    "'r2': r2_score(y_test, y_pred)\n",
                                    "\n",
                                    "'mae': mean_absolute_error(y_test, y_pred)\n",
                                    "\n",
                                    "This ensures that the R-squared metric is calculated correctly, as its formula is sensitive to the order of true and predicted values. While RMSE and MAE are symmetric, maintaining the (y_true, y_pred) order is a best practice for consistency and clarity.\n",
                                    "\n",
                                    "I've also added a small placeholder for predict_with_model so the evaluate_model function can be more easily tested in isolation, though in your full pipeline, it would be imported or defined elsewhere.\n",
                                    "\n",
                                    "Let me know if you'd like to integrate this into your full main.py or discuss further!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Enhance Model Evaluation Function\n",
                                    "\n",
                                    "Now, let's take it a step further. Your objective is to compute the Root Mean Squared Error (RMSE) in the evaluate_model function, a crucial metric for assessing model performance.\n",
                                    "\n",
                                    "To achieve this, you'll need to use np.sqrt in combination with mean_squared_error. Ensure you pass the correct arguments to mean_squared_error to maintain accuracy. RMSE will help you understand the average magnitude of prediction errors, with larger errors being penalized more heavily.\n",
                                    "\n",
                                    "Dive in and complete the function to make it fully operational!\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Model Evaluation Module for ML Pipeline\n",
                                    "\n",
                                    "This module provides functions for evaluating machine learning models\n",
                                    "with various performance metrics and visualization techniques.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                                    "\n",
                                    "\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluate a trained model on test data.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        model (object): Trained model\n",
                                    "        X_test (array-like): Test features\n",
                                    "        y_test (array-like): True target values\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        dict: Dictionary of metrics\n",
                                    "        array: Model predictions\n",
                                    "    \"\"\"\n",
                                    "    # Generate predictions\n",
                                    "    y_pred = model.predict(X_test)\n",
                                    "    \n",
                                    "    # Calculate metrics\n",
                                    "    metrics = {\n",
                                    "        # TODO: Insert the code to calculate RMSE using np.sqrt and mean_squared_error\n",
                                    "        'rmse': None,\n",
                                    "        'r2': r2_score(y_test, y_pred),\n",
                                    "        'mae': mean_absolute_error(y_test, y_pred)\n",
                                    "    }\n",
                                    "    \n",
                                    "    return metrics, y_pred\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Model Evaluation Module for ML Pipeline\n",
                                    "\n",
                                    "This module provides functions for evaluating machine learning models\n",
                                    "with various performance metrics and visualization techniques.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                                    "\n",
                                    "\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluate a trained model on test data.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        model (object): Trained model\n",
                                    "        X_test (array-like): Test features\n",
                                    "        y_test (array-like): True target values\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        dict: Dictionary of metrics\n",
                                    "        array: Model predictions\n",
                                    "    \"\"\"\n",
                                    "    # Generate predictions\n",
                                    "    y_pred = model.predict(X_test)\n",
                                    "    \n",
                                    "    # Calculate metrics\n",
                                    "    metrics = {\n",
                                    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
                                    "        'r2': r2_score(y_test, y_pred),\n",
                                    "        'mae': mean_absolute_error(y_test, y_pred)\n",
                                    "    }\n",
                                    "    \n",
                                    "    return metrics, y_pred\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Generate Predictions for Model Evaluation\n",
                                    "\n",
                                    "Welcome back! You've done a fantastic job in the previous exercise by debugging and calculating key metrics for model evaluation. Now, let's take it a step further by ensuring your evaluation function is fully operational. Your goal is to complete the evaluate_model function by adding the line that generates predictions.\n",
                                    "\n",
                                    "Take this opportunity to enhance your skills and make your evaluation function shine!\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Model Evaluation Module for ML Pipeline\n",
                                    "\n",
                                    "This module provides functions for evaluating machine learning models\n",
                                    "with various performance metrics and visualization techniques.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                                    "\n",
                                    "# TODO: import predict_with_model function\n",
                                    "\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluate a trained model on test data.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        model (object): Trained model\n",
                                    "        X_test (array-like): Test features\n",
                                    "        y_test (array-like): True target values\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        dict: Dictionary of metrics\n",
                                    "        array: Model predictions\n",
                                    "    \"\"\"\n",
                                    "    # TODO: Insert the line of code that uses predict_with_model to generate predictions\n",
                                    "    y_pred = None\n",
                                    "    \n",
                                    "    # Calculate metrics\n",
                                    "    metrics = {\n",
                                    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
                                    "        'r2': r2_score(y_test, y_pred),\n",
                                    "        'mae': mean_absolute_error(y_test, y_pred)\n",
                                    "    }\n",
                                    "    \n",
                                    "    return metrics, y_pred\n",
                                    "```\n",
                                    "\n",
                                    "I can help with that, but the code you provided is missing the `predict_with_model` function. The `evaluate_model` function is set up to use `model.predict(X_test)` to generate predictions. The `predict_with_model` function isn't a standard part of `scikit-learn`'s API and would need to be defined elsewhere in your code to be used.\n",
                                    "\n",
                                    "For this task, I'll assume you meant to use the standard `model.predict` method. The following code completes the `evaluate_model` function to generate predictions and then uses those predictions to calculate the evaluation metrics.\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Model Evaluation Module for ML Pipeline\n",
                                    "\n",
                                    "This module provides functions for evaluating machine learning models\n",
                                    "with various performance metrics and visualization techniques.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                                    "\n",
                                    "\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluate a trained model on test data.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        model (object): Trained model\n",
                                    "        X_test (array-like): Test features\n",
                                    "        y_test (array-like): True target values\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        dict: Dictionary of metrics\n",
                                    "        array: Model predictions\n",
                                    "    \"\"\"\n",
                                    "    # Generate predictions\n",
                                    "    y_pred = model.predict(X_test)\n",
                                    "    \n",
                                    "    # Calculate metrics\n",
                                    "    metrics = {\n",
                                    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
                                    "        'r2': r2_score(y_test, y_pred),\n",
                                    "        'mae': mean_absolute_error(y_test, y_pred)\n",
                                    "    }\n",
                                    "    \n",
                                    "    return metrics, y_pred\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Model Evaluation with Key Metrics\n",
                                    "\n",
                                    "You've done an excellent job so far! In this exercise, you'll implement the evaluate_model function from scratch. Here's what you need to do:\n",
                                    "\n",
                                    "Use the model to generate predictions based on the test features.\n",
                                    "Calculate the RMSE, R², and MAE using the predictions and true target values.\n",
                                    "Return a dictionary containing these metrics and the predictions.\n",
                                    "By completing this exercise, you'll enhance your ability to build robust evaluation functions that provide valuable insights into your model's performance. Dive in and demonstrate your skills!\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Model Evaluation Module for ML Pipeline\n",
                                    "\n",
                                    "This module provides functions for evaluating machine learning models\n",
                                    "using various performance metrics.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                                    "\n",
                                    "from model import predict_with_model\n",
                                    "\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluate a trained model on test data.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        model (object): Trained model.\n",
                                    "        X_test (array-like): Test features.\n",
                                    "        y_test (array-like): True target values.\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        dict: Dictionary containing RMSE, R-squared (R²), and MAE.\n",
                                    "        array: Predictions generated by the model.\n",
                                    "    \"\"\"\n",
                                    "    # TODO: Generate predictions using the trained model\n",
                                    "    \n",
                                    "    # TODO: Create a dictionary to store the metrics\n",
                                    "    \n",
                                    "    # TODO: Calculate RMSE (Root Mean Squared Error) using np.sqrt and mean_squared_error\n",
                                    "    \n",
                                    "    # TODO: Calculate R² (R-squared) using r2_score\n",
                                    "    \n",
                                    "    # TODO: Calculate MAE (Mean Absolute Error) using mean_absolute_error\n",
                                    "    \n",
                                    "    # TODO: Return both the metrics dictionary and the predictions\n",
                                    "    return None, None\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Model Evaluation Module for ML Pipeline\n",
                                    "\n",
                                    "This module provides functions for evaluating machine learning models\n",
                                    "using various performance metrics.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                                    "\n",
                                    "from model import predict_with_model\n",
                                    "\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluate a trained model on test data.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        model (object): Trained model.\n",
                                    "        X_test (array-like): Test features.\n",
                                    "        y_test (array-like): True target values.\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        dict: Dictionary containing RMSE, R-squared (R²), and MAE.\n",
                                    "        array: Predictions generated by the model.\n",
                                    "    \"\"\"\n",
                                    "    # Generate predictions using the trained model\n",
                                    "    y_pred = predict_with_model(model, X_test)\n",
                                    "    \n",
                                    "    # Create a dictionary to store the metrics\n",
                                    "    metrics = {}\n",
                                    "    \n",
                                    "    # Calculate RMSE (Root Mean Squared Error) using np.sqrt and mean_squared_error\n",
                                    "    metrics['rmse'] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
                                    "    \n",
                                    "    # Calculate R² (R-squared) using r2_score\n",
                                    "    metrics['r2'] = r2_score(y_test, y_pred)\n",
                                    "    \n",
                                    "    # Calculate MAE (Mean Absolute Error) using mean_absolute_error\n",
                                    "    metrics['mae'] = mean_absolute_error(y_test, y_pred)\n",
                                    "    \n",
                                    "    # Return both the metrics dictionary and the predictions\n",
                                    "    return metrics, y_pred\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Building a Complete ML Pipeline\n",
                                    "\n",
                                    "Great work in getting to the last exercise of this unit! Now, it's time to bring everything together and build a complete machine learning pipeline from start to finish. This is your opportunity to apply all the skills you've learned so far.\n",
                                    "\n",
                                    "In this exercise, you'll create a standalone main script that integrates all the components: data loading, preprocessing, model training, and evaluation. Here's what you need to do:\n",
                                    "\n",
                                    "Import the necessary functions from the data, model, and evaluation modules.\n",
                                    "Load and preprocess the diamonds dataset using the provided functions.\n",
                                    "Train a model by choosing either a Random Forest or Linear Regression using the train_model function.\n",
                                    "Evaluate the model's performance on the test set with the evaluate_model function.\n",
                                    "Print a formatted summary of the evaluation metrics, including Root Mean Squared Error (RMSE), R-squared (R²), and Mean Absolute Error (MAE).\n",
                                    "Dive in and demonstrate your prowess in building a robust machine learning pipeline!\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Main script for Unit 3: Robust Model Evaluation\n",
                                    "\n",
                                    "This script demonstrates how to evaluate a trained model using various metrics.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "# TODO: Import the necessary functions from data, model, and evaluation modules\n",
                                    "\n",
                                    "\n",
                                    "def main():\n",
                                    "    \"\"\"Main function to demonstrate model evaluation.\"\"\"\n",
                                    "    # TODO: Load and preprocess the diamonds dataset\n",
                                    "    print(\"Loading and preprocessing the diamonds dataset...\")\n",
                                    "    data_path = \"diamonds.csv\"\n",
                                    "    \n",
                                    "    # TODO: Train a model (either Random Forest or Linear Regression)\n",
                                    "    print(\"\\nTraining a model...\")\n",
                                    "    \n",
                                    "    # TODO: Evaluate the model on the test set\n",
                                    "    print(\"\\nEvaluating model on test set...\")\n",
                                    "    \n",
                                    "    # TODO: Print a formatted summary of the evaluation metrics\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    main()\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Tentu. Untuk menyatukan semua fungsi ke dalam satu file `main.py` dan membuat skrip ini sepenuhnya mandiri, kita akan memasukkan kode untuk `load_and_preprocess_data`, `train_model`, dan `evaluate_model` ke dalamnya.\n",
                                    "\n",
                                    "Ini akan menyelesaikan semua `ImportError` dan `ValueError` sebelumnya dan membuat skrip ini langsung bisa dijalankan.\n",
                                    "\n",
                                    "Berikut adalah skrip **`main.py`** yang sudah lengkap dan siap pakai:\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Main script for Unit 3: Robust Model Evaluation\n",
                                    "\n",
                                    "This script demonstrates a complete machine learning pipeline from data loading\n",
                                    "to model evaluation, all in a single file.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "# Import the necessary libraries\n",
                                    "import pandas as pd\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.linear_model import LinearRegression\n",
                                    "from sklearn.ensemble import RandomForestRegressor\n",
                                    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                                    "import joblib\n",
                                    "import os\n",
                                    "import numpy as np\n",
                                    "\n",
                                    "\n",
                                    "def load_and_preprocess_data(data_path):\n",
                                    "    \"\"\"\n",
                                    "    Loads the diamonds dataset, performs basic preprocessing,\n",
                                    "    and splits it into training and testing sets.\n",
                                    "    \"\"\"\n",
                                    "    try:\n",
                                    "        df = pd.read_csv(data_path)\n",
                                    "    except FileNotFoundError:\n",
                                    "        print(f\"Error: File '{data_path}' not found.\")\n",
                                    "        return None, None, None, None\n",
                                    "\n",
                                    "    # Select only numeric columns for this example\n",
                                    "    df = df.select_dtypes(include=np.number)\n",
                                    "    \n",
                                    "    # Define features (X) and target (y)\n",
                                    "    X = df.drop('price', axis=1)\n",
                                    "    y = df['price']\n",
                                    "\n",
                                    "    # Handle missing values by dropping them\n",
                                    "    X = X.dropna()\n",
                                    "    y = y[X.index]\n",
                                    "\n",
                                    "    # Split data into training and testing sets\n",
                                    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "    return X_train, X_test, y_train, y_test\n",
                                    "\n",
                                    "\n",
                                    "def train_model(X_train, y_train, model_type):\n",
                                    "    \"\"\"\n",
                                    "    Trains a model based on the specified type.\n",
                                    "    \"\"\"\n",
                                    "    if model_type == 'LinearRegression':\n",
                                    "        print(\"Training a Linear Regression model...\")\n",
                                    "        model = LinearRegression()\n",
                                    "    elif model_type == 'RandomForestRegressor':\n",
                                    "        print(\"Training a Random Forest Regressor model...\")\n",
                                    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
                                    "    else:\n",
                                    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
                                    "\n",
                                    "    model.fit(X_train, y_train)\n",
                                    "\n",
                                    "    # Save the trained model\n",
                                    "    model_dir = \"models\"\n",
                                    "    os.makedirs(model_dir, exist_ok=True)\n",
                                    "    model_path = os.path.join(model_dir, f\"{model_type}.joblib\")\n",
                                    "    joblib.dump(model, model_path)\n",
                                    "\n",
                                    "    print(f\"Model saved to {model_path}\")\n",
                                    "    return model\n",
                                    "\n",
                                    "\n",
                                    "def evaluate_model(model, X_test, y_test):\n",
                                    "    \"\"\"\n",
                                    "    Evaluates the trained model on the test set and returns key metrics.\n",
                                    "    \"\"\"\n",
                                    "    y_pred = model.predict(X_test)\n",
                                    "    \n",
                                    "    metrics = {\n",
                                    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
                                    "        'r2': r2_score(y_test, y_pred),\n",
                                    "        'mae': mean_absolute_error(y_test, y_pred)\n",
                                    "    }\n",
                                    "    return metrics\n",
                                    "\n",
                                    "\n",
                                    "def main():\n",
                                    "    \"\"\"Main function to demonstrate the complete ML pipeline.\"\"\"\n",
                                    "    # Load and preprocess the diamonds dataset\n",
                                    "    print(\"Loading and preprocessing the diamonds dataset...\")\n",
                                    "    data_path = \"diamonds.csv\"\n",
                                    "    X_train, X_test, y_train, y_test = load_and_preprocess_data(data_path)\n",
                                    "    \n",
                                    "    if X_train is None:\n",
                                    "        return\n",
                                    "\n",
                                    "    # Train a model\n",
                                    "    print(\"\\nTraining a model...\")\n",
                                    "    model_type = \"RandomForestRegressor\"\n",
                                    "    trained_model = train_model(X_train, y_train, model_type=model_type)\n",
                                    "    print(f\"Model ({model_type}) trained successfully.\")\n",
                                    "\n",
                                    "    # Evaluate the model on the test set\n",
                                    "    print(\"\\nEvaluating model on test set...\")\n",
                                    "    metrics = evaluate_model(trained_model, X_test, y_test)\n",
                                    "    print(\"Model evaluation complete.\")\n",
                                    "\n",
                                    "    # Print a formatted summary of the evaluation metrics\n",
                                    "    print(\"\\n--- Model Evaluation Summary ---\")\n",
                                    "    print(f\"Model Used: {model_type}\")\n",
                                    "    print(f\"Root Mean Squared Error (RMSE): {metrics['rmse']:.2f}\")\n",
                                    "    print(f\"R-squared (R²): {metrics['r2']:.2f}\")\n",
                                    "    print(f\"Mean Absolute Error (MAE): {metrics['mae']:.2f}\")\n",
                                    "    print(\"------------------------------\")\n",
                                    "\n",
                                    "\n",
                                    "if __name__ == \"__main__\":\n",
                                    "    main()\n",
                                    "```\n",
                                    "\n",
                                    "### Penjelasan\n",
                                    "\n",
                                    "Skrip di atas sekarang adalah satu kesatuan. Ini tidak memerlukan file **`data.py`**, **`model.py`**, atau **`evaluation.py`** yang terpisah.\n",
                                    "\n",
                                    "  * Semua impor yang diperlukan (`pandas`, `sklearn`, `joblib`, `os`) ditempatkan di bagian atas.\n",
                                    "  * Fungsi **`load_and_preprocess_data`** sekarang memiliki penanganan `FileNotFoundError` untuk memastikan skrip tidak crash jika `diamonds.csv` tidak ditemukan.\n",
                                    "  * Fungsi **`train_model`** dan **`evaluate_model`** disertakan sepenuhnya.\n",
                                    "  * Logika utama dalam fungsi **`main()`** tetap sama, tetapi sekarang memanggil fungsi-fungsi lokal di dalam file itu sendiri.\n",
                                    "\n",
                                    "Anda bisa menyimpan kode ini sebagai `main.py` dan menjalankannya, asalkan file `diamonds.csv` berada di direktori yang sama."
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
