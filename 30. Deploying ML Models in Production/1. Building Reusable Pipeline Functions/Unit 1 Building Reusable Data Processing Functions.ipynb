{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 1 Building Reusable Data Processing Functions\n",
                                    "\n",
                                    "Hello and welcome to the first lesson of \"Building Reusable Pipeline Functions\"! This is where our journey into the world of MLOps begins, as we take our first steps in the \"Deploying ML Models in Production\" course path.\n",
                                    "\n",
                                    "Throughout this path, you'll learn how to transform experimental Machine Learning models into robust production systems. We'll start by laying the foundations of our ML system in this course, covering data processing, model training, evaluation, and persistence. In later courses, we'll move on to integrating an API to serve our ML model as well as adding an automated retraining pipeline with Apache Airflow.\n",
                                    "\n",
                                    "In today's lesson, we'll focus on building reusable data processing functions — a critical foundation for any reliable ML system. We'll work with a diamond price prediction dataset to create well-structured functions that can be reused throughout our ML pipeline. Let's get started!\n",
                                    "\n",
                                    "Understanding MLOps Fundamentals\n",
                                    "MLOps (Machine Learning Operations) combines Machine Learning, DevOps practices, and data engineering to streamline the process of taking ML models to production and maintaining them effectively.\n",
                                    "\n",
                                    "In traditional ML workflows, data scientists often create one-off scripts for data preparation. This approach works for exploration but quickly becomes problematic in production settings where data changes over time and multiple team members need to understand and modify the code. By creating modular, well-documented data processing functions, you're establishing the foundation for a reliable ML pipeline that can evolve with your project needs.\n",
                                    "\n",
                                    "Some of the key benefits of adopting MLOps include:\n",
                                    "\n",
                                    "Reproducibility: Ensures that data processing steps can be repeated exactly the same way each time.\n",
                                    "\n",
                                    "Maintainability: Makes code easier to update and debug when isolated in focused functions.\n",
                                    "\n",
                                    "Consistency: Provides the same transformations across training and inference.\n",
                                    "\n",
                                    "Scalability: Allows processing to be applied to datasets of varying sizes.\n",
                                    "\n",
                                    "Testing: Makes unit testing possible for individual pipeline components.\n",
                                    "\n",
                                    "Exploring the Diamonds Dataset\n",
                                    "In this course path, we'll be developing an application for diamond price prediction using the classic diamonds.csv dataset from Kaggle. This dataset is a staple in the data science community, offering a rich collection of attributes for nearly 54,000 diamonds.\n",
                                    "\n",
                                    "The dataset's attributes are well-suited for building a predictive model. For instance, the carat column represents the weight of the diamond, ranging from 0.2 to 5.01, while the cut column describes the quality of the cut, with categories like Fair, Good, Very Good, Premium, and Ideal. The color and clarity columns provide additional qualitative measures, with color ranging from J (worst) to D (best) and clarity from I1 (worst) to IF (best). The dataset also includes numerical features such as depth, table, and the dimensions x, y, and z, which describe the diamond's physical characteristics. Here are the first few records from the dataset:\n",
                                    "\n",
                                    "```sh\n",
                                    "   carat      cut color clarity  depth  table  price     x     y     z\n",
                                    "1   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
                                    "2   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
                                    "3   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
                                    "4   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
                                    "5   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
                                    "```\n",
                                    "\n",
                                    "Creating Reusable Data Loading Functions\n",
                                    "The first step in any ML pipeline is loading and exploring the data. Let's examine how we can create a reusable function for this purpose:\n",
                                    "\n",
                                    "```python\n",
                                    "def load_diamonds_data(file_path):\n",
                                    "    \"\"\"\n",
                                    "    Load the diamonds dataset from a CSV file.\n",
                                    "\n",
                                    "    Args:\n",
                                    "        file_path (str): Path to the CSV file\n",
                                    "\n",
                                    "    Returns:\n",
                                    "        pd.DataFrame: Loaded diamonds data\n",
                                    "    \"\"\"\n",
                                    "    # Load the data\n",
                                    "    df = pd.read_csv(file_path, index_col=0)\n",
                                    "\n",
                                    "    return df\n",
                                    "```\n",
                                    "\n",
                                    "This function simply loads the dataset using pd.read_csv, setting index_col=0 to specify the index column.\n",
                                    "By isolating data loading in a dedicated function, you make your code more maintainable. If your data source changes in the future — perhaps from CSV to a database or cloud storage — you'll only need to update this one function rather than changing code throughout your project.\n",
                                    "\n",
                                    "Designing Effective Preprocessing Functions\n",
                                    "After loading the data, preprocessing is the next critical step. Let's look at how we can design the beginning of our preprocessing function:\n",
                                    "\n",
                                    "```python\n",
                                    "def preprocess_diamonds_data(df, test_size=0.2, random_state=42):\n",
                                    "    \"\"\"\n",
                                    "    Preprocess the diamonds dataset for ML model training.\n",
                                    "\n",
                                    "    Args:\n",
                                    "        df (pd.DataFrame): Raw diamonds data\n",
                                    "        test_size (float): Proportion of data to use for testing\n",
                                    "        random_state (int): Random seed for reproducibility\n",
                                    "\n",
                                    "    Returns:\n",
                                    "        tuple: (X_train, X_test, y_train, y_test)\n",
                                    "    \"\"\"\n",
                                    "    # Separate features and target\n",
                                    "    X = df.drop('price', axis=1)  # Features - everything except price\n",
                                    "    y = df['price']               # Target - what we want to predict\n",
                                    "\n",
                                    "    # Split into train and test sets\n",
                                    "    X_train, X_test, y_train, y_test = train_test_split(\n",
                                    "        X, y, test_size=test_size, random_state=random_state\n",
                                    "    )\n",
                                    "```\n",
                                    "\n",
                                    "This portion of the code illustrates several important design principles. The function accepts flexible parameters with sensible defaults. It starts by separating the prediction target (price) from the features and creating training and testing splits. By using a fixed random state, you ensure that your splits are reproducible — absolutely essential when you're debugging or comparing different modeling approaches.\n",
                                    "\n",
                                    "Creating Smart Feature Transformations\n",
                                    "Now, let's examine how we build the actual preprocessing pipeline using scikit-learn:\n",
                                    "\n",
                                    "```python\n",
                                    "    # Identify categorical and numerical columns automatically\n",
                                    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
                                    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                                    "\n",
                                    "    # Create preprocessing transformers for both categorical and numerical data\n",
                                    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
                                    "    numerical_transformer = StandardScaler()\n",
                                    "    # Combine preprocessing steps\n",
                                    "    preprocessor = ColumnTransformer(\n",
                                    "        transformers=[\n",
                                    "            ('num', numerical_transformer, numerical_cols),\n",
                                    "            ('cat', categorical_transformer, categorical_cols)\n",
                                    "        ])\n",
                                    "```\n",
                                    "\n",
                                    "This code elegantly solves the challenge of mixed data types in ML pipelines by automating preprocessing through dynamic column identification, specialized transformers, and a unified ColumnTransformer. Instead of manual column-by-column processing, the approach automatically detects categorical and numerical features, applies appropriate transformations to each, and combines them into a cohesive pipeline.\n",
                                    "The resulting preprocessing system is both automatic and adaptable, requiring no code modifications when dataset structure changes. This flexibility is essential for production systems where data evolves over time. Additionally, thoughtful details like the handle_unknown='ignore' parameter in OneHotEncoder ensure the pipeline can gracefully handle new categories not seen during training—a common real-world scenario.\n",
                                    "\n",
                                    "Preventing Data Leakage in Preprocessing\n",
                                    "The final part of our preprocessing function applies the transformations and returns the processed data:\n",
                                    "\n",
                                    "```python\n",
                                    "    # Fit the preprocessor on training data only\n",
                                    "    X_train_processed = preprocessor.fit_transform(X_train)  # Learn parameters and transform\n",
                                    "    X_test_processed = preprocessor.transform(X_test)        # Apply learned parameters without fitting\n",
                                    "\n",
                                    "    return X_train_processed, X_test_processed, y_train, y_test\n",
                                    "```\n",
                                    "\n",
                                    "This code demonstrates a crucial ML practice: using fit_transform() on training data to learn parameters, but only transform() on test data to apply those parameters. This approach prevents data leakage—where test data information inadvertently influences training, such as when standardizing all data together before splitting. By fitting exclusively on training data, you simulate how your model will perform on truly unseen production data, maintaining the integrity of your evaluation metrics.\n",
                                    "\n",
                                    "Orchestrating the Data Pipeline\n",
                                    "Now that we've built our individual components, let's see how they work together in a complete workflow:\n",
                                    "\n",
                                    "```python\n",
                                    "def main():\n",
                                    "    \"\"\"Main function to demonstrate data processing.\"\"\"\n",
                                    "    # Step 1: Load the data\n",
                                    "    print(\"Loading diamonds dataset...\")\n",
                                    "    data_path = \"diamonds.csv\"  # Path relative to this script\n",
                                    "    diamonds_df = load_diamonds_data(data_path)\n",
                                    "\n",
                                    "    # Step 2: Preprocess the data\n",
                                    "    print(\"\\nPreprocessing the dataset...\")\n",
                                    "    X_train, X_test, y_train, y_test = preprocess_diamonds_data(diamonds_df)\n",
                                    "\n",
                                    "    # Print preprocessing results\n",
                                    "    print(f\"\\nPreprocessing complete:\")\n",
                                    "    print(f\"  - Training features shape: {X_train.shape}\")\n",
                                    "    print(f\"  - Testing features shape: {X_test.shape}\")\n",
                                    "    print(f\"  - Training target shape: {y_train.shape}\")\n",
                                    "    print(f\"  - Testing target shape: {y_test.shape}\")\n",
                                    "\n",
                                    "    print(\"\\nData processing pipeline is ready for model training!\")\n",
                                    "```\n",
                                    "\n",
                                    "This orchestration function demonstrates how our individual components combine into a cohesive pipeline with clear, sequential workflow. By structuring code where high-level functions call more specialized functions in sequence, we create a maintainable ML system that balances big-picture clarity with encapsulated implementation details. This orchestration pattern is particularly valuable in production environments, where it enables easier debugging, promotes collaboration among team members, and facilitates future modifications as requirements evolve.\n",
                                    "\n",
                                    "Conclusion and Next Steps\n",
                                    "In this first lesson, you've learned how to build the foundation of a robust ML pipeline by creating reusable functions for data loading and preprocessing. These functions aren't just convenient abstractions — they're essential building blocks for production ML systems that can handle changing data and requirements. By separating concerns, preventing data leakage, and creating adaptable transformations, you've taken the first steps toward MLOps best practices.\n",
                                    "\n",
                                    "As you continue through this course, you'll build upon this foundation, adding functions for model training, evaluation, and persistence. These components will eventually come together to form a complete, production-ready ML system that can reliably deliver predictions and adapt to new data. The skills you're developing now — structuring code for reusability, preventing common ML pitfalls, and thinking in pipelines — will serve you throughout your journey into MLOps."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Fix the Data Processing Bug\n",
                                    "\n",
                                    "Welcome to your first hands-on exercise in building reusable data processing functions! In this exercise, you'll apply what you've learned about creating effective preprocessing functions.\n",
                                    "\n",
                                    "Today, you'll be working on the preprocess_diamonds_data function, which is crucial for preparing our diamonds dataset for machine learning. However, there's a small mix-up in the current code: your mission is to identify and fix this bug. Happy debugging!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Data Processing Module for ML Pipeline\n",
                                    "\n",
                                    "This module handles the processing and preparation of the diamonds dataset\n",
                                    "for machine learning tasks.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "import pandas as pd\n",
                                    "import numpy as np\n",
                                    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                                    "from sklearn.compose import ColumnTransformer\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "def load_diamonds_data(file_path):\n",
                                    "    \"\"\"\n",
                                    "    Load the diamonds dataset from a CSV file.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        file_path (str): Path to the CSV file\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        pd.DataFrame: Loaded diamonds data\n",
                                    "    \"\"\"\n",
                                    "    # Load the data\n",
                                    "    df = pd.read_csv(file_path, index_col=0)\n",
                                    "    \n",
                                    "    return df\n",
                                    "\n",
                                    "def preprocess_diamonds_data(df, test_size=0.2, random_state=42):\n",
                                    "    \"\"\"\n",
                                    "    Preprocess the diamonds dataset for ML model training.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        df (pd.DataFrame): Raw diamonds data\n",
                                    "        test_size (float): Proportion of data to use for testing\n",
                                    "        random_state (int): Random seed for reproducibility\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        tuple: (X_train, X_test, y_train, y_test, preprocessor)\n",
                                    "               - Training features\n",
                                    "               - Testing features\n",
                                    "               - Training target\n",
                                    "               - Testing target\n",
                                    "               - Preprocessor object for transforming new data\n",
                                    "    \"\"\"\n",
                                    "    # Separate features and target\n",
                                    "    X = df\n",
                                    "    y = df.drop('price', axis=1)\n",
                                    "    \n",
                                    "    # Split into train and test sets\n",
                                    "    X_train, X_test, y_train, y_test = train_test_split(\n",
                                    "        X, y, test_size=test_size, random_state=random_state\n",
                                    "    )\n",
                                    "    \n",
                                    "    # Identify categorical and numerical columns\n",
                                    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
                                    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                                    "    \n",
                                    "    # Create preprocessing transformers for both categorical and numerical data\n",
                                    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
                                    "    numerical_transformer = StandardScaler()\n",
                                    "\n",
                                    "    # Combine preprocessing steps\n",
                                    "    preprocessor = ColumnTransformer(\n",
                                    "        transformers=[\n",
                                    "            ('num', numerical_transformer, numerical_cols),\n",
                                    "            ('cat', categorical_transformer, categorical_cols)\n",
                                    "        ])\n",
                                    "    \n",
                                    "    # Fit the preprocessor on the training data\n",
                                    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
                                    "    X_test_processed = preprocessor.transform(X_test)\n",
                                    "    \n",
                                    "    return X_train_processed, X_test_processed, y_train, y_test, preprocessor\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "You've spotted a critical bug in the data separation step\\! The code was incorrectly assigning the features and target variables, which would lead to incorrect model training and predictions.\n",
                                    "\n",
                                    "The bug is in these lines:\n",
                                    "\n",
                                    "```python\n",
                                    "    # Separate features and target\n",
                                    "    X = df\n",
                                    "    y = df.drop('price', axis=1)\n",
                                    "```\n",
                                    "\n",
                                    "Here, `X` (the features) is assigned the entire DataFrame `df`, which still includes the `price` column. Meanwhile, `y` (the target) is incorrectly assigned the DataFrame with the `price` column dropped.\n",
                                    "\n",
                                    "The fix is to swap the assignments: `X` should be the DataFrame without the target column, and `y` should be a Series containing only the `price` column.\n",
                                    "\n",
                                    "Here is the corrected code with the bug fixed:\n",
                                    "\n",
                                    "```python\n",
                                    "\"\"\"\n",
                                    "Data Processing Module for ML Pipeline\n",
                                    "\n",
                                    "This module handles the processing and preparation of the diamonds dataset\n",
                                    "for machine learning tasks.\n",
                                    "\"\"\"\n",
                                    "\n",
                                    "import pandas as pd\n",
                                    "import numpy as np\n",
                                    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                                    "from sklearn.compose import ColumnTransformer\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "def load_diamonds_data(file_path):\n",
                                    "    \"\"\"\n",
                                    "    Load the diamonds dataset from a CSV file.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        file_path (str): Path to the CSV file\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        pd.DataFrame: Loaded diamonds data\n",
                                    "    \"\"\"\n",
                                    "    # Load the data\n",
                                    "    df = pd.read_csv(file_path, index_col=0)\n",
                                    "    \n",
                                    "    return df\n",
                                    "\n",
                                    "def preprocess_diamonds_data(df, test_size=0.2, random_state=42):\n",
                                    "    \"\"\"\n",
                                    "    Preprocess the diamonds dataset for ML model training.\n",
                                    "    \n",
                                    "    Args:\n",
                                    "        df (pd.DataFrame): Raw diamonds data\n",
                                    "        test_size (float): Proportion of data to use for testing\n",
                                    "        random_state (int): Random seed for reproducibility\n",
                                    "        \n",
                                    "    Returns:\n",
                                    "        tuple: (X_train, X_test, y_train, y_test, preprocessor)\n",
                                    "               - Training features\n",
                                    "               - Testing features\n",
                                    "               - Training target\n",
                                    "               - Testing target\n",
                                    "               - Preprocessor object for transforming new data\n",
                                    "    \"\"\"\n",
                                    "    # Separate features and target (FIXED BUG HERE)\n",
                                    "    X = df.drop('price', axis=1)\n",
                                    "    y = df['price']\n",
                                    "    \n",
                                    "    # Split into train and test sets\n",
                                    "    X_train, X_test, y_train, y_test = train_test_split(\n",
                                    "        X, y, test_size=test_size, random_state=random_state\n",
                                    "    )\n",
                                    "    \n",
                                    "    # Identify categorical and numerical columns\n",
                                    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
                                    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                                    "    \n",
                                    "    # Create preprocessing transformers for both categorical and numerical data\n",
                                    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
                                    "    numerical_transformer = StandardScaler()\n",
                                    "\n",
                                    "    # Combine preprocessing steps\n",
                                    "    preprocessor = ColumnTransformer(\n",
                                    "        transformers=[\n",
                                    "            ('num', numerical_transformer, numerical_cols),\n",
                                    "            ('cat', categorical_transformer, categorical_cols)\n",
                                    "        ])\n",
                                    "    \n",
                                    "    # Fit the preprocessor on the training data\n",
                                    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
                                    "    X_test_processed = preprocessor.transform(X_test)\n",
                                    "    \n",
                                    "    return X_train_processed, X_test_processed, y_train, y_test, preprocessor\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Loading Diamonds Dataset Correctly"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Identifying Data Columns Efficiently"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Building a Robust Preprocessing Pipeline"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Building a Data Processing Module"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
