{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 3 Classification Metrics\n",
                                    "\n",
                                    "\n",
                                    "Classification metrics help us evaluate model performance on classification tasks like predicting spam emails or diagnosing diseases. They help us determine if our model performs well. By the end, you'll understand:\n",
                                    "\n",
                                    "* **Confusion Matrix** and its interpretation.\n",
                                    "* **Accuracy**, **Precision**, and **Recall**.\n",
                                    "* **F1-score**\n",
                                    "* How to compute these metrics using **Python** and **SciKit Learn**.\n",
                                    "\n",
                                    "Let's dive in!\n",
                                    "\n",
                                    "## Confusion Matrix\n",
                                    "\n",
                                    "A **Confusion Matrix** describes the performance of a classification model. In the context of a confusion matrix, a positive prediction is predicting the class labeled `1`, and a negative prediction is predicting the class labeled `0`. The confusion matrix is a 2x2 table (for binary classification) that shows:\n",
                                    "\n",
                                    "* **True Positives (TP):** The number of **correct** positive predictions.\n",
                                    "* **True Negatives (TN):** The number of **correct** negative predictions.\n",
                                    "* **False Positives (FP):** The number of **incorrect** positive predictions.\n",
                                    "* **False Negatives (FN):** The number of **incorrect** negative predictions.\n",
                                    "\n",
                                    "Imagine that you need to classify emails as spam (1) or not-spam (0). Let's define an example of predictions and then create our confusion matrix:\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import confusion_matrix\n",
                                    "\n",
                                    "# Sample classification dataset\n",
                                    "y_true = np.array([0, 1, 0, 1, 0, 1, 1, 0, 1, 0])  # True labels\n",
                                    "y_pred = np.array([1, 1, 1, 1, 0, 0, 1, 0, 1, 0])  # Predicted labels\n",
                                    "\n",
                                    "# Calculating confusion matrix\n",
                                    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
                                    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
                                    "````\n",
                                    "\n",
                                    "**Output:**\n",
                                    "\n",
                                    "```\n",
                                    "Confusion Matrix:\n",
                                    "[[3 2]\n",
                                    " [1 4]]\n",
                                    "```\n",
                                    "\n",
                                    "This tells us:\n",
                                    "\n",
                                    "  * **True Positives (TP):** 4 (model correctly predicted spam four times)\n",
                                    "  * **True Negatives (TN):** 3 (model correctly predicted not spam three times)\n",
                                    "  * **False Positives (FP):** 2 (model incorrectly predicted spam two times)\n",
                                    "  * **False Negatives (FN):** 1 (model incorrectly predicted not spam one time)\n",
                                    "\n",
                                    "Note that the values in the confusion matrix are stored this way:\n",
                                    "\n",
                                    "```\n",
                                    "[[TN FP]\n",
                                    " [FN TP]]\n",
                                    "```\n",
                                    "\n",
                                    "## What is Accuracy?\n",
                                    "\n",
                                    "**Accuracy** is the ratio of correctly predicted instances out of all instances. It's useful but can be misleading for imbalanced datasets.\n",
                                    "\n",
                                    "**Formula:** $Accuracy = \\\\frac{TP + TN}{TP + TN + FP + FN}$\n",
                                    "\n",
                                    "Let's compute accuracy using **SciKit Learn**:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Calculating accuracy\n",
                                    "accuracy = accuracy_score(y_true, y_pred)\n",
                                    "print(f\"Accuracy: {accuracy}\")\n",
                                    "```\n",
                                    "\n",
                                    "**Output:**\n",
                                    "\n",
                                    "```\n",
                                    "Accuracy: 0.7\n",
                                    "```\n",
                                    "\n",
                                    "Our model is 70% accurate. But sometimes accuracy alone isn't enough. Accuracy can be deceptive in imbalanced datasets where one class significantly outnumbers the other. For example, if 95% of emails are not spam and only 5% are spam, a model that never classifies any email as spam would still be 95% accurate. Thus, accuracy doesn't always reflect the real performance on minority classes.\n",
                                    "\n",
                                    "In such cases of imbalanced data, we need to use other metrics. Let's look at the other options: the **Precision** and **Recall** metrics.\n",
                                    "\n",
                                    "## What is Precision?\n",
                                    "\n",
                                    "**Precision** is the ratio of correctly predicted positive cases out of all predicted positives. It's crucial when false positives are costly (e.g., spam detection).\n",
                                    "\n",
                                    "**Formula:** $Precision = \\\\frac{TP}{TP + FP}$\n",
                                    "\n",
                                    "Here's how to calculate precision:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import precision_score\n",
                                    "\n",
                                    "# Calculating precision\n",
                                    "precision = precision_score(y_true, y_pred)\n",
                                    "print(f\"Precision: {precision}\")\n",
                                    "```\n",
                                    "\n",
                                    "**Output:**\n",
                                    "\n",
                                    "```\n",
                                    "Precision: 0.6666666666666666\n",
                                    "```\n",
                                    "\n",
                                    "Approximately 67% of instances predicted as spam were actually spam.\n",
                                    "\n",
                                    "Use precision when the cost of false positives is high. This metric is crucial in scenarios where the consequences of incorrectly predicting a positive are significant. **Example:** In spam detection, marking an important email as spam (a false positive) can result in the user missing critical information. Therefore, we prioritize obtaining a high precision to minimize false positives.\n",
                                    "\n",
                                    "## What is Recall?\n",
                                    "\n",
                                    "**Recall** is the ratio of correctly predicted positive cases out of all actual positives. It's essential when false negatives are costly (e.g., disease detection).\n",
                                    "\n",
                                    "**Formula:** $Recall = \\\\frac{TP}{TP + FN}$\n",
                                    "\n",
                                    "Let's compute recall:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import recall_score\n",
                                    "\n",
                                    "# Calculating recall\n",
                                    "recall = recall_score(y_true, y_pred)\n",
                                    "print(f\"Recall: {recall}\")\n",
                                    "```\n",
                                    "\n",
                                    "**Output:**\n",
                                    "\n",
                                    "```\n",
                                    "Recall: 0.8\n",
                                    "```\n",
                                    "\n",
                                    "80% of actual spam emails were correctly predicted as spam.\n",
                                    "\n",
                                    "Use recall when the cost of false negatives is high. This metric is essential in situations where missing actual positive cases is more detrimental than having false positives. **Example:** In disease diagnosis, failing to identify a disease (a false negative) can have severe consequences on patient health. In such cases, we aim for high recall to ensure as many actual positive cases as possible are correctly identified.\n",
                                    "\n",
                                    "## F1-score\n",
                                    "\n",
                                    "Sometimes you want to pay attention to both Precision and Recall, finding an optimal balance between them. In these cases, we use the **F1-Score** metric.\n",
                                    "\n",
                                    "**F1-Score** is the harmonic mean of Precision and Recall. It balances the two metrics to provide a single measure of a model's performance.\n",
                                    "\n",
                                    "**Formula:** $F1-Score = 2 \\\\times \\\\frac{Precision \\\\times Recall}{Precision + Recall}$\n",
                                    "\n",
                                    "The F1-Score is high only if both Precision and Recall are high. It's particularly useful for imbalanced datasets where a high score for one metric might be misleading without considering the other.\n",
                                    "\n",
                                    "Here's how to calculate the F1-Score:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import f1_score\n",
                                    "\n",
                                    "# Calculating F1-Score\n",
                                    "f1 = f1_score(y_true, y_pred)\n",
                                    "print(f\"F1-Score: {f1}\")\n",
                                    "```\n",
                                    "\n",
                                    "**Output:**\n",
                                    "\n",
                                    "```\n",
                                    "F1-Score: 0.7272727272727273\n",
                                    "```\n",
                                    "\n",
                                    "An F1-Score of approximately 0.73 indicates a good balance between Precision and Recall, offering a more comprehensive measure of the model's performance in scenarios where both false positives and false negatives are important.\n",
                                    "\n",
                                    "## Lesson Summary\n",
                                    "\n",
                                    "We've covered:\n",
                                    "\n",
                                    "  * **Confusion Matrix:** Breakdown of predictions.\n",
                                    "  * **Accuracy:** Ratio of correct predictions.\n",
                                    "  * **Precision:** Correct predictions out of all positive predictions.\n",
                                    "  * **Recall:** Correct predictions out of all actual positives.\n",
                                    "  * **F1-score:** Combination of Precision and Recall.\n",
                                    "  * The pitfalls of using **Accuracy** with imbalanced datasets.\n",
                                    "\n",
                                    "These metrics help evaluate different aspects of your model's performance. Now it's your turn\\! You'll compute classification metrics on new datasets, reinforcing your understanding. Ready to practice? Let's go\\!\n"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Confusion Matrix Values in Spam Classification\n",
                                    "\n",
                                    "Stellar work, Space Explorer! Let’s take it a step further. Complete the TODO to finish the classification code and print the confusion matrix values. This will help you understand how to extract specific values from the confusion matrix\n",
                                    "\n",
                                    "May cosmic knowledge guide you!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import confusion_matrix\n",
                                    "\n",
                                    "# Sample classification dataset\n",
                                    "y_true = np.array([0, 1, 0, 1, 0, 1, 1, 0, 1, 0])\n",
                                    "y_pred = np.array([1, 1, 0, 1, 0, 0, 1, 0, 1, 0])\n",
                                    "\n",
                                    "# Calculating confusion matrix\n",
                                    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
                                    "\n",
                                    "# TODO: Calculate and assign TP, FP, TN, and FN\n",
                                    "\n",
                                    "print(f\"TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Alright, Space Explorer\\! Let's extract those confusion matrix values.\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import confusion_matrix\n",
                                    "\n",
                                    "# Sample classification dataset\n",
                                    "y_true = np.array([0, 1, 0, 1, 0, 1, 1, 0, 1, 0])\n",
                                    "y_pred = np.array([1, 1, 0, 1, 0, 0, 1, 0, 1, 0])\n",
                                    "\n",
                                    "# Calculating confusion matrix\n",
                                    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
                                    "\n",
                                    "# TODO: Calculate and assign TP, FP, TN, and FN\n",
                                    "# The confusion matrix is structured as:\n",
                                    "# [[TN, FP],\n",
                                    "#  [FN, TP]]\n",
                                    "\n",
                                    "TN = conf_matrix[0, 0]\n",
                                    "FP = conf_matrix[0, 1]\n",
                                    "FN = conf_matrix[1, 0]\n",
                                    "TP = conf_matrix[1, 1]\n",
                                    "\n",
                                    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
                                    "print(f\"TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Calculate Precision and Recall\n",
                                    "\n",
                                    "Great job so far, Stellar Navigator!\n",
                                    "\n",
                                    "Now it's time to fill in the missing pieces. Implement the missing code to calculate Precision and Recall based on the email classification results. Your goal is here is to calculate precision and recall without implemented functions, using formulas. It will help you understand better what these metrics actually mean.\n",
                                    "\n",
                                    "Remember, Precision is the ratio of correctly predicted positive cases to all predicted positives, and Recall is the ratio of correctly predicted positive cases to all actual positives.\n",
                                    "\n",
                                    "Happy coding!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import confusion_matrix\n",
                                    "\n",
                                    "# Assume y_true and y_pred are given\n",
                                    "y_true = [0, 1, 0, 1, 0, 1, 1, 0, 1, 0]  # True labels\n",
                                    "y_pred = [1, 1, 0, 1, 0, 0, 1, 0, 1, 0]  # Predicted labels\n",
                                    "\n",
                                    "# Use sklearn's confusion matrix\n",
                                    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
                                    "\n",
                                    "# TODO: Calculate and print Precision and Recall\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Great job so far, Stellar Navigator\\! Let's calculate Precision and Recall using the fundamental formulas.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import confusion_matrix\n",
                                    "\n",
                                    "# Assume y_true and y_pred are given\n",
                                    "y_true = [0, 1, 0, 1, 0, 1, 1, 0, 1, 0]  # True labels\n",
                                    "y_pred = [1, 1, 0, 1, 0, 0, 1, 0, 1, 0]  # Predicted labels\n",
                                    "\n",
                                    "# Use sklearn's confusion matrix to get TN, FP, FN, TP\n",
                                    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
                                    "\n",
                                    "# TODO: Calculate and print Precision and Recall\n",
                                    "\n",
                                    "# Calculate Precision\n",
                                    "# Formula: Precision = TP / (TP + FP)\n",
                                    "if (tp + fp) == 0:\n",
                                    "    precision = 0.0 # Handle division by zero if no positive predictions\n",
                                    "else:\n",
                                    "    precision = tp / (tp + fp)\n",
                                    "\n",
                                    "# Calculate Recall\n",
                                    "# Formula: Recall = TP / (TP + FN)\n",
                                    "if (tp + fn) == 0:\n",
                                    "    recall = 0.0 # Handle division by zero if no actual positives\n",
                                    "else:\n",
                                    "    recall = tp / (tp + fn)\n",
                                    "\n",
                                    "print(f\"True Positives (TP): {tp}\")\n",
                                    "print(f\"False Positives (FP): {fp}\")\n",
                                    "print(f\"False Negatives (FN): {fn}\")\n",
                                    "print(f\"True Negatives (TN): {tn}\")\n",
                                    "print(f\"Precision: {precision:.4f}\")\n",
                                    "print(f\"Recall: {recall:.4f}\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Calculate Accuracy for Email Classification\n",
                                    "\n",
                                    "Cosmo\n",
                                    "Just now\n",
                                    "Read message aloud\n",
                                    "Hey Space Explorer!\n",
                                    "\n",
                                    "Let's explore a scenario with an unbalanced dataset. Modify the given code to calculate and print the precision and recall instead of the accuracy score. This will help you understand that while accuracy might be high, the model can still perform poorly in identifying both classes correctly.\n",
                                    "\n",
                                    "Go ahead and make the change!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Email classification labels: 1 is spam, 0 is not spam\n",
                                    "y_true = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 1])  # Actual labels\n",
                                    "y_pred = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])  # Predicted labels (dumb model)\n",
                                    "\n",
                                    "# Calculate accuracy for the email classification model\n",
                                    "accuracy = accuracy_score(y_true, y_pred)\n",
                                    "\n",
                                    "print(f\"Accuracy: {accuracy}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Hey Space Explorer\\!\n",
                                    "\n",
                                    "You're absolutely right; with unbalanced datasets, accuracy can be very misleading. Let's modify the code to calculate and print precision and recall instead, to get a clearer picture of our \"dumb model's\" performance.\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
                                    "\n",
                                    "# Email classification labels: 1 is spam, 0 is not spam\n",
                                    "y_true = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 1])  # Actual labels\n",
                                    "y_pred = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])  # Predicted labels (dumb model)\n",
                                    "\n",
                                    "# Calculate precision for the email classification model\n",
                                    "# Setting zero_division=0 to handle cases where there are no true positive predictions\n",
                                    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
                                    "\n",
                                    "# Calculate recall for the email classification model\n",
                                    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
                                    "\n",
                                    "print(f\"Precision: {precision:.4f}\")\n",
                                    "print(f\"Recall: {recall:.4f}\")\n",
                                    "\n",
                                    "# Optional: Print Confusion Matrix to see the breakdown\n",
                                    "# tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
                                    "# print(f\"Confusion Matrix:\\n{confusion_matrix(y_true, y_pred)}\")\n",
                                    "# print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
                                    "```\n",
                                    "\n",
                                    "### Explanation of the Output:\n",
                                    "\n",
                                    "When you run this code, you'll observe:\n",
                                    "\n",
                                    "  * **Precision:** This will likely be low (0.0 in this specific case). Even though our \"dumb model\" correctly identifies most of the \"not spam\" emails (0s), it makes a False Positive (predicts spam when it's not) and misses all actual spam emails, leading to a low precision for the positive class (spam). In this example, the model predicts '1' (spam) only once, and that prediction is incorrect (actual label is '0'). Hence, TP is 0 and FP is 1, leading to Precision = 0 / (0 + 1) = 0.\n",
                                    "  * **Recall:** This will also be low (0.0 in this specific case). The model fails to identify *any* of the actual spam emails (the `1`s in `y_true`). Since our model never correctly predicts a positive, its recall will be 0. Recall = TP / (TP + FN) = 0 / (0 + 2) = 0.\n",
                                    "\n",
                                    "This clearly demonstrates why just looking at accuracy can be deceptive with unbalanced datasets. Our model has high accuracy (80% if you were to calculate it) because it correctly predicts the majority class (not spam) most of the time. However, it completely fails at its core task of identifying the minority class (spam), as evidenced by the zero precision and recall. This is a crucial lesson in evaluating classification models\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Calculate Classification Metrics\n",
                                    "\n",
                                    "Greetings, Space Voyager!\n",
                                    "\n",
                                    "You're doing great. Now it's your turn to fill in the blanks and calculate the metrics that show the model's performance. Complete the TODO to compute the missing confusion matrix, accuracy, precision, and recall.\n",
                                    "\n",
                                    "May the stars guide you!\n",
                                    "\n",
                                    "```python\n",
                                    "# TODO: import necessary functions\n",
                                    "\n",
                                    "# True labels and predicted labels for spam classification\n",
                                    "y_true = [1, 0, 1, 0, 1, 1, 0, 0, 0, 1]\n",
                                    "y_pred = [1, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
                                    "\n",
                                    "# TODO: Calculate and print the Confusion Matrix\n",
                                    "\n",
                                    "# TODO: Calculate and print the accuracy, precision, and recall\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
                                    "\n",
                                    "# True labels and predicted labels for spam classification\n",
                                    "y_true = [1, 0, 1, 0, 1, 1, 0, 0, 0, 1]\n",
                                    "y_pred = [1, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
                                    "\n",
                                    "# Calculate and print the Confusion Matrix\n",
                                    "cm = confusion_matrix(y_true, y_pred)\n",
                                    "print(\"Confusion Matrix:\")\n",
                                    "print(cm)\n",
                                    "\n",
                                    "# Calculate and print the accuracy, precision, and recall\n",
                                    "accuracy = accuracy_score(y_true, y_pred)\n",
                                    "precision = precision_score(y_true, y_pred)\n",
                                    "recall = recall_score(y_true, y_pred)\n",
                                    "\n",
                                    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
                                    "print(f\"Precision: {precision:.2f}\")\n",
                                    "print(f\"Recall: {recall:.2f}\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Compute Confusion Matrix and F1-Score\n",
                                    "\n",
                                    "Good job learning classification metrics, Space Voyager!\n",
                                    "\n",
                                    "Let's compute the F1-score for our spam email classifier. Replace the precision and recall calculation with the necessary code to compute the F1-score, representing both metrics with one value.\n",
                                    "\n",
                                    "May your analytics soar through the cosmos!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
                                    "\n",
                                    "# True labels and predicted labels\n",
                                    "y_true = np.array([0, 1, 0, 1, 0, 1, 1, 0, 1, 0])\n",
                                    "y_pred = np.array([1, 1, 1, 1, 0, 0, 1, 0, 1, 0])\n",
                                    "\n",
                                    "# Compute the confusion matrix\n",
                                    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
                                    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
                                    "\n",
                                    "# TODO: Replace the precision and recall calculation with F1-score\n",
                                    "precision = precision_score(y_true, y_pred)\n",
                                    "recall = recall_score(y_true, y_pred)\n",
                                    "print(f\"Precision: {precision}\")\n",
                                    "print(f\"Recall: {recall}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "from sklearn.metrics import confusion_matrix, f1_score\n",
                                    "\n",
                                    "# True labels and predicted labels\n",
                                    "y_true = np.array([0, 1, 0, 1, 0, 1, 1, 0, 1, 0])\n",
                                    "y_pred = np.array([1, 1, 1, 1, 0, 0, 1, 0, 1, 0])\n",
                                    "\n",
                                    "# Compute the confusion matrix\n",
                                    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
                                    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
                                    "\n",
                                    "# Replace the precision and recall calculation with F1-score\n",
                                    "f1 = f1_score(y_true, y_pred)\n",
                                    "print(f\"F1-score: {f1}\")\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
