{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 5 Building Full Preprocessing Pipeline for the Titanic Dataset\n",
                                    "\n",
                                    "# Lesson Introduction\n",
                                    "\n",
                                    "Welcome\\! Today, we'll learn how to build a full **preprocessing pipeline** for the Titanic dataset. In real work, you're going to deal with big datasets with lots of features and rows.\n",
                                    "\n",
                                    "We aim to learn how to prepare real data for machine learning models by handling missing values, encoding categorical features, scaling numerical features, and splitting the data into training and test sets.\n",
                                    "\n",
                                    "Imagine you have a messy jigsaw puzzle. You need to organize the pieces, find the edges first, and then start assembling. Data preprocessing is like organizing the pieces before starting the puzzle.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Load and Prepare the Data\n",
                                    "\n",
                                    "Let's start by loading the **Titanic dataset** using **Seaborn**, which has information about passengers like age, fare, and whether they survived. We'll drop some columns we won't use.\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "\n",
                                    "# Load the Titanic dataset\n",
                                    "df = sns.load_dataset('titanic')\n",
                                    "\n",
                                    "# Drop columns that won't be used\n",
                                    "df = df.drop(columns=['deck', 'embarked', 'alive'])\n",
                                    "\n",
                                    "print(df.head())\n",
                                    "```\n",
                                    "\n",
                                    "Expected output:\n",
                                    "\n",
                                    "```\n",
                                    "   survived  pclass     sex   age  sibsp  parch     fare  who  adult_male  \\\n",
                                    "0         0       3    male  22.0      1      0   7.2500  man        True   \n",
                                    "1         1       1  female  38.0      1      0  71.2833  woman      False   \n",
                                    "2         1       3  female  26.0      0      0   7.9250  woman      False   \n",
                                    "3         1       1  female  35.0      1      0  53.1000  woman      False   \n",
                                    "4         0       3    male  35.0      0      0   8.0500  man        True   \n",
                                    " \n",
                                    "     embark_town  alone  \n",
                                    "0  Southampton    False  \n",
                                    "1    Cherbourg    False  \n",
                                    "2  Southampton     True  \n",
                                    "3  Southampton    False  \n",
                                    "4  Southampton     True  \n",
                                    "```\n",
                                    "\n",
                                    "We loaded the dataset and dropped columns `deck`, `embarked`, and `alive` because they have too many missing values or aren't useful. For example, the `embarked` column shouldn't affect the passenger's survival rate, so it's questionable as a feature.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Handle Missing Values\n",
                                    "\n",
                                    "Next, let's handle missing values using **SimpleImputer** from **SciKit Learn**.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.impute import SimpleImputer\n",
                                    "\n",
                                    "# Handle missing values\n",
                                    "imputer_num = SimpleImputer(strategy='mean')\n",
                                    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
                                    "\n",
                                    "df['age'] = imputer_num.fit_transform(df[['age']])\n",
                                    "df['embark_town'] = imputer_cat.fit_transform(df[['embark_town']].values.reshape(-1, 1)).ravel()\n",
                                    "df['fare'] = imputer_num.fit_transform(df[['fare']])\n",
                                    "\n",
                                    "print(df.head())\n",
                                    "```\n",
                                    "\n",
                                    "As a reminder, `ravel()` is a method in NumPy that returns a contiguous flattened array. In this context, it's used to flatten the column vector returned by `fit_transform()` into a 1-dimensional array. This ensures that the `embark_town` column is reshaped back into a 1-D array that fits into the DataFrame correctly.\n",
                                    "\n",
                                    "Expected output:\n",
                                    "\n",
                                    "```\n",
                                    "   survived  pclass     sex   age  sibsp  parch     fare  who  adult_male  \\\n",
                                    "0         0       3    male  22.0      1      0   7.2500  man        True   \n",
                                    "1         1       1  female  38.0      1      0  71.2833  woman      False   \n",
                                    "2         1       3  female  26.0      0      0   7.9250  woman      False   \n",
                                    "3         1       1  female  35.0      1      0  53.1000  woman      False   \n",
                                    "4         0       3    male  35.0      0      0   8.0500  man        True   \n",
                                    " \n",
                                    "     embark_town  alone  \n",
                                    "0  Southampton    False  \n",
                                    "1    Cherbourg    False  \n",
                                    "2  Southampton     True  \n",
                                    "3  Southampton    False  \n",
                                    "4  Southampton     True  \n",
                                    "```\n",
                                    "\n",
                                    "We filled missing numerical data (`age`, `fare`) using the mean and categorical data (`embark_town`) using the most frequent value. This is like guessing a missing puzzle piece based on surrounding ones.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Encode Categorical Features: Part 1\n",
                                    "\n",
                                    "Machine learning models need numerical data. So, we use **OneHotEncoder** to convert categorical features into numbers.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.preprocessing import OneHotEncoder\n",
                                    "\n",
                                    "# Encode categorical features\n",
                                    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
                                    "encoded_columns = encoder.fit_transform(df[['sex', 'class', 'embark_town', 'who', 'adult_male', 'alone']])\n",
                                    "encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out(['sex', 'class', 'embark_town', 'who', 'adult_male', 'alone']))\n",
                                    "```\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Encode Categorical Features: Part 2\n",
                                    "\n",
                                    "Next, we drop the original categorical columns and concatenate the new encoded columns with the DataFrame.\n",
                                    "\n",
                                    "```python\n",
                                    "# Drop and concatenate\n",
                                    "df = df.drop(columns=['sex', 'class', 'embark_town', 'who', 'adult_male', 'alone'])\n",
                                    "df = pd.concat([df.reset_index(drop=True), encoded_df], axis=1)\n",
                                    "\n",
                                    "print(df.head())\n",
                                    "```\n",
                                    "\n",
                                    "Expected output:\n",
                                    "\n",
                                    "```\n",
                                    "   survived  pclass   age  sibsp  parch     fare  alone  sex_male  \\\n",
                                    "0         0       3  22.0      1      0   7.2500  False       1.0   \n",
                                    "1         1       1  38.0      1      0  71.2833  False       0.0   \n",
                                    "2         1       3  26.0      0      0   7.9250   True       0.0   \n",
                                    "3         1       1  35.0      1      0  53.1000  False       0.0   \n",
                                    "4         0       3  35.0      0      0   8.0500   True       1.0   \n",
                                    " \n",
                                    "   class_2  class_3  embark_town_Queenstown  embark_town_Southampton  \\\n",
                                    "0      0.0      1.0                     0.0                      1.0   \n",
                                    "1      0.0      0.0                     0.0                      0.0   \n",
                                    "2      0.0      1.0                     0.0                      1.0   \n",
                                    "3      0.0      0.0                     0.0                      1.0   \n",
                                    "4      0.0      1.0                     0.0                      1.0   \n",
                                    " \n",
                                    "   who_man  who_woman  adult_male_True  \n",
                                    "0      1.0        0.0              1.0  \n",
                                    "1      0.0        1.0              0.0  \n",
                                    "2      0.0        1.0              0.0  \n",
                                    "3      0.0        1.0              0.0  \n",
                                    "4      1.0        0.0              1.0  \n",
                                    "```\n",
                                    "\n",
                                    "We converted the categorical columns into numerical ones, dropped the originals, and added the new encoded columns. It's like translating words into a secret code for a robot.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Feature Scaling\n",
                                    "\n",
                                    "Feature scaling ensures all numerical values are on a similar scale. We use **StandardScaler** for this.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.preprocessing import StandardScaler\n",
                                    "\n",
                                    "# Feature scaling\n",
                                    "scaler = StandardScaler()\n",
                                    "scaled_columns = scaler.fit_transform(df[['age', 'fare']])\n",
                                    "scaled_df = pd.DataFrame(scaled_columns, columns=['age', 'fare'])\n",
                                    "\n",
                                    "# Drop and concatenate\n",
                                    "df = df.drop(columns=['age', 'fare'])\n",
                                    "df = pd.concat([df.reset_index(drop=True), scaled_df], axis=1)\n",
                                    "\n",
                                    "print(df.head())\n",
                                    "```\n",
                                    "\n",
                                    "Expected output:\n",
                                    "\n",
                                    "```\n",
                                    "   survived  pclass  sibsp  parch  alone  sex_male  class_2  class_3  \\\n",
                                    "0         0       3      1      0  False       1.0      0.0      1.0   \n",
                                    "1         1       1      1      0  False       0.0      0.0      0.0   \n",
                                    "2         1       3      0      0   True       0.0      0.0      1.0   \n",
                                    "3         1       1      1      0  False       0.0      0.0      0.0   \n",
                                    "4         0       3      0      0   True       1.0      0.0      1.0   \n",
                                    " \n",
                                    "   embark_town_Queenstown  embark_town_Southampton  who_man  who_woman  \\\n",
                                    "0                     0.0                      1.0      1.0        0.0   \n",
                                    "1                     0.0                      0.0      0.0        1.0   \n",
                                    "2                     0.0                      1.0      0.0        1.0   \n",
                                    "3                     0.0                      1.0      0.0        1.0   \n",
                                    "4                     0.0                      1.0      1.0        0.0   \n",
                                    " \n",
                                    "   adult_male_True       age      fare  \n",
                                    "0              1.0 -0.530376 -0.502445  \n",
                                    "1              0.0  0.571829  0.788947  \n",
                                    "2              0.0 -0.254596 -0.488854  \n",
                                    "3              0.0  0.400810  0.420731  \n",
                                    "4              1.0  0.400810 -0.486337  \n",
                                    "```\n",
                                    "\n",
                                    "We scaled our numerical data (`age`, `fare`) to have a mean of 0 and a standard deviation of 1. This is like resizing puzzle pieces to fit perfectly.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Separate Features and Target Variable\n",
                                    "\n",
                                    "Next, we separate our features (used for predictions) and the target variable (the outcome we predict).\n",
                                    "\n",
                                    "```python\n",
                                    "# Separate features and target variable\n",
                                    "X = df.drop(columns=['survived'])\n",
                                    "y = df['survived']\n",
                                    "\n",
                                    "print(\"X:\\n\", X.head())\n",
                                    "print(\"\\ny:\\n\", y.head())\n",
                                    "```\n",
                                    "\n",
                                    "Expected output:\n",
                                    "\n",
                                    "```\n",
                                    "X:\n",
                                    "    pclass  sibsp  parch  alone  sex_male  class_2  class_3  embark_town_Queenstown  \\\n",
                                    "0       3      1      0  False       1.0      0.0      1.0                     0.0   \n",
                                    "1       1      1      0  False       0.0      0.0      0.0                     0.0   \n",
                                    "2       3      0      0   True       0.0      0.0      1.0                     0.0   \n",
                                    "3       1      1      0  False       0.0      0.0      0.0                     0.0   \n",
                                    "4       3      0      0   True       1.0      0.0      1.0                     0.0   \n",
                                    " \n",
                                    "   embark_town_Southampton  who_man  who_woman  adult_male_True       age  \\\n",
                                    "0                      1.0      1.0        0.0              1.0 -0.530376   \n",
                                    "1                      0.0      0.0        1.0              0.0  0.571829   \n",
                                    "2                      1.0      0.0        1.0              0.0 -0.254596   \n",
                                    "3                      1.0      0.0        1.0              0.0  0.400810   \n",
                                    "4                      1.0      1.0        0.0              1.0  0.400810   \n",
                                    " \n",
                                    "       fare  \n",
                                    "0 -0.502445  \n",
                                    "1  0.788947  \n",
                                    "2 -0.488854  \n",
                                    "3  0.420731  \n",
                                    "4 -0.486337  \n",
                                    "\n",
                                    "y:\n",
                                    " 0    0\n",
                                    "1    1\n",
                                    "2    1\n",
                                    "3    1\n",
                                    "4    0\n",
                                    "Name: survived, dtype: int64\n",
                                    "```\n",
                                    "\n",
                                    "Here, `X` contains all features except `survived`, and `y` contains the `survived` column. This helps in training the model more efficiently.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Train-Test Split\n",
                                    "\n",
                                    "Finally, we split the dataset into training and test sets using **train\\_test\\_split**. This lets us train the model on one part of the data and test it on another.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Train-test split\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "print(f\"Training set size: {len(X_train)}, Test set size: {len(X_test)}\")\n",
                                    "```\n",
                                    "\n",
                                    "Expected output:\n",
                                    "\n",
                                    "```\n",
                                    "Training set size: 712, Test set size: 179\n",
                                    "```\n",
                                    "\n",
                                    "We split the data so 80% is used for training and 20% for testing. This step is like practicing with some pieces before trying the whole puzzle.\n",
                                    "\n",
                                    "-----\n",
                                    "\n",
                                    "## Lesson Summary\n",
                                    "\n",
                                    "Today, we:\n",
                                    "\n",
                                    "  * **Loaded and prepared** the Titanic dataset.\n",
                                    "  * **Handled missing values**.\n",
                                    "  * **Encoded categorical features**.\n",
                                    "  * **Scaled numerical features**.\n",
                                    "  * **Separated features and the target variable**.\n",
                                    "  * **Split the dataset** into training and test sets.\n",
                                    "\n",
                                    "Now, you'll get to practice these steps hands-on. Happy learning\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Drop Unwanted Titanic Columns\n",
                                    "\n",
                                    "Hey Space Navigator, let's continue our journey! We need to load the Titanic dataset and drop some columns we don't need. Fill in the missing lines to load the Titanic dataset using Seaborn and drop the specified columns. Let's ace this mission!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "\n",
                                    "# Load the Titanic dataset\n",
                                    "df = sns.load_dataset('titanic')\n",
                                    "\n",
                                    "# TODO: Drop columns that won't be used: deck, embarked and alive\n",
                                    "\n",
                                    "# Display the modified dataset\n",
                                    "print(df.head())\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "\n",
                                    "# Load the Titanic dataset\n",
                                    "df = sns.load_dataset('titanic')\n",
                                    "\n",
                                    "# Drop columns that won't be used: deck, embarked and alive\n",
                                    "df = df.drop(columns=['deck', 'embarked', 'alive'])\n",
                                    "\n",
                                    "# Display the modified dataset\n",
                                    "print(df.head())\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Handle Missing Values in Titanic Dataset\n",
                                    "\n",
                                    "Great job so far, Galactic Pioneer! Let's handle some missing data before we move ahead. Fill in the TODOs to complete the code.\n",
                                    "\n",
                                    "Cleaning your dataset by filling in missing values helps ensure that the analysis is accurate and meaningful.\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "from sklearn.impute import SimpleImputer\n",
                                    "\n",
                                    "# Load the Titanic dataset\n",
                                    "df = sns.load_dataset('titanic')\n",
                                    "\n",
                                    "# Drop some columns for simplicity\n",
                                    "df = df.drop(columns=['deck', 'embarked', 'alive'])\n",
                                    "\n",
                                    "# TODO: Handle missing values in 'age' and 'fare' using mean\n",
                                    "\n",
                                    "print(df[['age', 'fare']].isna().sum())  # should be 0 if NaNs are handled!\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "from sklearn.impute import SimpleImputer\n",
                                    "\n",
                                    "# Load the Titanic dataset\n",
                                    "df = sns.load_dataset('titanic')\n",
                                    "\n",
                                    "# Drop some columns for simplicity\n",
                                    "df = df.drop(columns=['deck', 'embarked', 'alive'])\n",
                                    "\n",
                                    "# Handle missing values in 'age' and 'fare' using mean\n",
                                    "imputer_num = SimpleImputer(strategy='mean')\n",
                                    "\n",
                                    "# Apply to 'age' column\n",
                                    "df['age'] = imputer_num.fit_transform(df[['age']])\n",
                                    "\n",
                                    "# Apply to 'fare' column\n",
                                    "# Note: While 'fare' has very few missing values (or sometimes none in the default dataset),\n",
                                    "# it's good practice to apply the imputer if you expect missing values in real-world scenarios.\n",
                                    "df['fare'] = imputer_num.fit_transform(df[['fare']])\n",
                                    "\n",
                                    "\n",
                                    "print(df[['age', 'fare']].isna().sum())\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Encode Categorical Features and Concatenate\n",
                                    "\n",
                                    "Let's dive deeper, Stellar Navigator! Fill in the blanks to encode the categorical features and concatenate them with the original DataFrame. Use OneHotEncoder for the sex column and LabelEncoder for the class column.\n",
                                    "\n",
                                    "It's time to show your mastery!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
                                    "\n",
                                    "# Load the Titanic dataset\n",
                                    "df = sns.load_dataset('titanic')\n",
                                    "   \n",
                                    "# Encode 'sex' column with OneHotEncoder\n",
                                    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
                                    "# TODO: Encode the 'sex' column from the dataset with OneHotEncoder\n",
                                    "columns=one_hot_encoder.get_feature_names_out(['sex']))\n",
                                    "\n",
                                    "# Encode 'class' column with LabelEncoder\n",
                                    "label_encoder = LabelEncoder()\n",
                                    "# TODO: Encode the 'class' column from the dataset with LabelEncoder\n",
                                    "\n",
                                    "# Concatenate the encoded columns with the original dataframe\n",
                                    "df = pd.concat([df.reset_index(drop=True), encoded_sex_df, encoded_class_df], axis=1)\n",
                                    "print(df.head())\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
                                    "\n",
                                    "# Load the Titanic dataset\n",
                                    "df = sns.load_dataset('titanic')\n",
                                    "\n",
                                    "# Encode 'sex' column with OneHotEncoder\n",
                                    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
                                    "encoded_sex = one_hot_encoder.fit_transform(df[['sex']])\n",
                                    "encoded_sex_df = pd.DataFrame(encoded_sex, columns=one_hot_encoder.get_feature_names_out(['sex']))\n",
                                    "\n",
                                    "# Encode 'class' column with LabelEncoder\n",
                                    "label_encoder = LabelEncoder()\n",
                                    "encoded_class = label_encoder.fit_transform(df['class'])\n",
                                    "encoded_class_df = pd.DataFrame(encoded_class, columns=['class_encoded'])\n",
                                    "\n",
                                    "# Concatenate the encoded columns with the original dataframe\n",
                                    "df = pd.concat([df.reset_index(drop=True), encoded_sex_df, encoded_class_df], axis=1)\n",
                                    "print(df.head())\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Handle Missing Values and Feature Scaling\n",
                                    "\n",
                                    "Hey Space Voyager! You're making great progress so far. Now, let’s kick it up a notch.\n",
                                    "\n",
                                    "You'll need to complete the missing pieces of code to scale numeric features. Take a look at the TODO comments and fill in the blanks.\n",
                                    "\n",
                                    "May your aim be true!\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "from sklearn.preprocessing import StandardScaler\n",
                                    "\n",
                                    "# Load the Titanic dataset\n",
                                    "df = sns.load_dataset('titanic')\n",
                                    "\n",
                                    "# Drop columns that won't be used\n",
                                    "df = df.drop(columns=['deck', 'embarked', 'alive'])\n",
                                    "\n",
                                    "# Feature scaling\n",
                                    "# TODO: fit and transform scaler on the 'age' and 'fare' columns\n",
                                    "\n",
                                    "print(df[['age', 'fare']].head())\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "from sklearn.preprocessing import StandardScaler\n",
                                    "\n",
                                    "# Load the Titanic dataset\n",
                                    "df = sns.load_dataset('titanic')\n",
                                    "\n",
                                    "# Drop columns that won't be used\n",
                                    "df = df.drop(columns=['deck', 'embarked', 'alive'])\n",
                                    "\n",
                                    "# Handle missing values in 'age' and 'fare' before scaling\n",
                                    "# For simplicity, let's fill with the mean. A more robust approach might be needed for real-world data.\n",
                                    "df['age'].fillna(df['age'].mean(), inplace=True)\n",
                                    "df['fare'].fillna(df['fare'].mean(), inplace=True)\n",
                                    "\n",
                                    "# Feature scaling\n",
                                    "scaler = StandardScaler()\n",
                                    "df[['age', 'fare']] = scaler.fit_transform(df[['age', 'fare']])\n",
                                    "\n",
                                    "print(df[['age', 'fare']].head())\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "2b0e69d9",
                           "metadata": {},
                           "source": [
                                    "Celestial Traveler, let's finalize our Titanic preprocessing adventure! Follow the #TODO steps to load, clean, encode, scale, and split the Titanic dataset as we've learned.\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "from sklearn.impute import SimpleImputer\n",
                                    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# TODO: Load Titanic dataset and drop unnecessary columns 'deck', 'embarked', and 'alive'\n",
                                    "\n",
                                    "# TODO: Handle missing values for both numerical and categorical data\n",
                                    "#      Use: SimpleImputer with strategy='mean' for numerical columns 'age' and 'fare'\n",
                                    "#      Use: SimpleImputer with strategy='most_frequent' for categorical column 'embark_town'\n",
                                    "\n",
                                    "# TODO: Encode categorical features using one-hot encoding\n",
                                    "\n",
                                    "# TODO: Perform feature scaling on numerical features using standard scaling\n",
                                    "\n",
                                    "# TODO: Separate features from the target variable\n",
                                    "\n",
                                    "# TODO: Split the dataset into training and testing sets\n",
                                    "\n",
                                    "# TODO: Print sizes of training and testing sets\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "import pandas as pd\n",
                                    "import seaborn as sns\n",
                                    "from sklearn.impute import SimpleImputer\n",
                                    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Load Titanic dataset and drop unnecessary columns 'deck', 'embarked', and 'alive'\n",
                                    "df = sns.load_dataset('titanic')\n",
                                    "df = df.drop(columns=['deck', 'embarked', 'alive'])\n",
                                    "\n",
                                    "# Handle missing values for both numerical and categorical data\n",
                                    "# Numerical Imputer\n",
                                    "numerical_imputer = SimpleImputer(strategy='mean')\n",
                                    "df[['age', 'fare']] = numerical_imputer.fit_transform(df[['age', 'fare']])\n",
                                    "\n",
                                    "# Categorical Imputer (for 'embark_town' which has missing values)\n",
                                    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
                                    "df[['embark_town']] = categorical_imputer.fit_transform(df[['embark_town']])\n",
                                    "\n",
                                    "\n",
                                    "# Identify categorical and numerical features after imputation\n",
                                    "categorical_features = ['sex', 'pclass', 'who', 'alone', 'class', 'adult_male', 'embark_town']\n",
                                    "numerical_features = ['age', 'fare', 'sibsp', 'parch']\n",
                                    "\n",
                                    "# Encode categorical features using one-hot encoding\n",
                                    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
                                    "encoded_features = encoder.fit_transform(df[categorical_features])\n",
                                    "encoded_feature_names = encoder.get_feature_names_out(categorical_features)\n",
                                    "encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=df.index)\n",
                                    "\n",
                                    "# Perform feature scaling on numerical features using standard scaling\n",
                                    "scaler = StandardScaler()\n",
                                    "scaled_numerical_features = scaler.fit_transform(df[numerical_features])\n",
                                    "scaled_numerical_df = pd.DataFrame(scaled_numerical_features, columns=numerical_features, index=df.index)\n",
                                    "\n",
                                    "# Concatenate all processed features\n",
                                    "# First, drop original categorical and numerical columns that have been transformed\n",
                                    "df_processed = df.drop(columns=categorical_features + numerical_features)\n",
                                    "df_processed = pd.concat([df_processed, scaled_numerical_df, encoded_df], axis=1)\n",
                                    "\n",
                                    "# Separate features from the target variable\n",
                                    "X = df_processed.drop('survived', axis=1)\n",
                                    "y = df_processed['survived']\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# Print sizes of training and testing sets\n",
                                    "print(f\"X_train shape: {X_train.shape}\")\n",
                                    "print(f\"X_test shape: {X_test.shape}\")\n",
                                    "print(f\"y_train shape: {y_train.shape}\")\n",
                                    "print(f\"y_test shape: {y_test.shape}\")\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
