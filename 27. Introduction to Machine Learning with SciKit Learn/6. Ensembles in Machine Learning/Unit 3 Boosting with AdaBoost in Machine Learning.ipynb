{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 3 Boosting with AdaBoost in Machine Learning\n",
                                    "\n",
                                    "Welcome\\! In today's lesson, we'll explore Boosting, focusing on AdaBoost. Boosting improves model accuracy by combining weak models. By the end, you'll understand AdaBoost and how to use it to improve your machine learning models.\n",
                                    "\n",
                                    "## Introduction to Boosting and AdaBoost\n",
                                    "\n",
                                    "Boosting increases model accuracy by combining weak models. Think of a group of not-so-great basketball players; individually, they may not win, but together they can be strong.\n",
                                    "\n",
                                    "**AdaBoost** (Adaptive Boosting) combines several weak classifiers into a strong one. A weak classifier is slightly better than guessing. AdaBoost focuses on correcting errors made by previous classifiers. Here's how it works:\n",
                                    "\n",
                                    "1.  **Initialize Weights:** Assign equal weights to all training samples.\n",
                                    "2.  **Train Weak Classifier:** Train a weak classifier on the weighted data.\n",
                                    "3.  **Calculate Error:** Compute the classification error of the weak classifier.\n",
                                    "4.  **Update Weights:** Increase the weights of misclassified samples and decrease the weights of correctly classified samples. This ensures that subsequent classifiers focus more on the difficult samples.\n",
                                    "5.  **Combine Classifiers:** Combine all the weak classifiers to form a strong classifier, with each classifier's vote weighted according to its accuracy.\n",
                                    "\n",
                                    "## Loading the Dataset and Splitting the Dataset\n",
                                    "\n",
                                    "Before training our model, we need data. We'll use the **wine dataset**, which contains chemical properties of wines. This data helps us train and test our model.\n",
                                    "\n",
                                    "To load the dataset, use `load_wine` from `sklearn.datasets`, which returns features `X` and labels `y`. Features describe the properties, while labels indicate the type of wine.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "```\n",
                                    "\n",
                                    "Next, we split the data into training and testing sets using `train_test_split` from `sklearn.model_selection`. We use 80% for training and 20% for testing.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Split dataset\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "```\n",
                                    "\n",
                                    "## Training an AdaBoost Classifier\n",
                                    "\n",
                                    "Now, let's train our AdaBoost model using `AdaBoostClassifier` from `sklearn.ensemble`. We’ll use `DecisionTreeClassifier` from `sklearn.tree` as the weak classifier. In this case, each decision tree will have just one node.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.ensemble import AdaBoostClassifier\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "\n",
                                    "# Train AdaBoost classifier\n",
                                    "ada_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, algorithm='SAMME')\n",
                                    "ada_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Make predictions\n",
                                    "y_pred_ada = ada_clf.predict(X_test)\n",
                                    "```\n",
                                    "\n",
                                    "In the code:\n",
                                    "\n",
                                    "  * `base_estimator=DecisionTreeClassifier()` specifies the weak classifier.\n",
                                    "  * `n_estimators=100` combines 100 weak classifiers.\n",
                                    "  * `algorithm='SAMME'` specifies what algorithm to use. There is essentially only one option, which is `'SAMME'`. If it is not set, the program will use another algorithm, called `'SAMME.R'`. However, this algorithm is deprecated and will be removed in the future versions of sklearn, so you shouldn't use it. Always specify `algorithm='SAMME'` when using the AdaBoost classifier.\n",
                                    "  * `fit(X_train, y_train)` trains the model.\n",
                                    "  * `predict(X_test)` makes predictions on the test set.\n",
                                    "\n",
                                    "## Comparing AdaBoost with RandomForest\n",
                                    "\n",
                                    "To understand the effectiveness of AdaBoost, let’s compare it with `RandomForestClassifier` from `sklearn.ensemble`.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.ensemble import RandomForestClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Train RandomForest classifier\n",
                                    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                                    "rf_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Make predictions with RandomForest\n",
                                    "y_pred_rf = rf_clf.predict(X_test)\n",
                                    "\n",
                                    "# Calculate and compare accuracies\n",
                                    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
                                    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
                                    "\n",
                                    "print(f\"AdaBoost accuracy: {accuracy_ada}\")  # 0.94\n",
                                    "print(f\"RandomForest accuracy: {accuracy_rf}\")  # 1.0\n",
                                    "```\n",
                                    "\n",
                                    "In this code, we initialize the `RandomForestClassifier` with 100 trees, which we already know to perform perfectly on this dataset. Then, we make predictions with Random Forest and compare accuracies of the Random Forest and the AdaBoost models.\n",
                                    "\n",
                                    "In this case, AdaBoost shows slightly lower performance, but its accuracy is still very high and outperforms simple models.\n",
                                    "\n",
                                    "## Lesson Summary\n",
                                    "\n",
                                    "Great job\\! You've learned about Boosting and how AdaBoost uses weak classifiers to create a strong model. We covered:\n",
                                    "\n",
                                    "  * What Boosting and AdaBoost are.\n",
                                    "  * Loading the wine dataset.\n",
                                    "  * Splitting the dataset.\n",
                                    "  * Training an AdaBoost classifier with decision trees.\n",
                                    "  * Comparing the accuracies of AdaBoost and RandomForest.\n",
                                    "\n",
                                    "Next, you'll practice by loading data, splitting it, and training your own AdaBoost classifier. Ready to boost your skills? Let's dive in\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Change the Weak Classifier in AdaBoost\n",
                                    "\n",
                                    "Galactic Pioneer, it's time to change the weak classifier! Replace the DecisionTreeClassifier with a RandomForestClassifier as the base estimator for the AdaBoostClassifier. This will help you see how different weak classifiers perform in boosting.\n",
                                    "\n",
                                    "Let's code!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import AdaBoostClassifier\n",
                                    "from sklearn.ensemble import RandomForestClassifier\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split dataset\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# Train AdaBoost classifier with DecisionTreeClassifier as the base estimator\n",
                                    "dt_clf = DecisionTreeClassifier()\n",
                                    "ada_clf = AdaBoostClassifier(estimator=dt_clf, n_estimators=100, algorithm='SAMME')\n",
                                    "ada_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Make predictions\n",
                                    "y_pred_ada = ada_clf.predict(X_test)\n",
                                    "\n",
                                    "# Calculate accuracy\n",
                                    "accuracy = accuracy_score(y_test, y_pred_ada)\n",
                                    "print(f\"AdaBoost Classifier accuracy: {accuracy}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import AdaBoostClassifier\n",
                                    "from sklearn.ensemble import RandomForestClassifier\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split dataset\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# Train AdaBoost classifier with RandomForestClassifier as the base estimator\n",
                                    "rf_clf_weak = RandomForestClassifier(n_estimators=10, random_state=42) # Using a small number of estimators for the weak learner\n",
                                    "ada_clf_rf = AdaBoostClassifier(estimator=rf_clf_weak, n_estimators=100, algorithm='SAMME', random_state=42)\n",
                                    "ada_clf_rf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Make predictions\n",
                                    "y_pred_ada_rf = ada_clf_rf.predict(X_test)\n",
                                    "\n",
                                    "# Calculate accuracy\n",
                                    "accuracy_rf_base = accuracy_score(y_test, y_pred_ada_rf)\n",
                                    "print(f\"AdaBoost Classifier with RandomForest base estimator accuracy: {accuracy_rf_base}\")\n",
                                    "\n",
                                    "# Original AdaBoost Classifier with DecisionTreeClassifier (for comparison)\n",
                                    "dt_clf = DecisionTreeClassifier()\n",
                                    "ada_clf_dt = AdaBoostClassifier(estimator=dt_clf, n_estimators=100, algorithm='SAMME', random_state=42)\n",
                                    "ada_clf_dt.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Make predictions\n",
                                    "y_pred_ada_dt = ada_clf_dt.predict(X_test)\n",
                                    "\n",
                                    "# Calculate accuracy\n",
                                    "accuracy_dt_base = accuracy_score(y_test, y_pred_ada_dt)\n",
                                    "print(f\"AdaBoost Classifier with DecisionTree base estimator accuracy: {accuracy_dt_base}\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Train and Predict with AdaBoost\n",
                                    "\n",
                                    "Hey Galactic Pioneer, ready to boost your skills?\n",
                                    "\n",
                                    "Here's your mission: complete the code to train an AdaBoost classifier and make predictions. Can you also calculate the accuracy of the model? Fill in the TODO comments and make the code work!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import AdaBoostClassifier\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split dataset\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# TODO: Initialize AdaBoostClassifier with DecisionTreeClassifier as a base model, 100 estimators, and 'SAMME' algorithm.\n",
                                    "\n",
                                    "# TODO: Fit the AdaBoost classifier with the training data.\n",
                                    "\n",
                                    "# TODO: Make predictions on test dataset and calculate accuracy.\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import AdaBoostClassifier\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split dataset\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# TODO: Initialize AdaBoostClassifier with DecisionTreeClassifier as a base model, 100 estimators, and 'SAMME' algorithm.\n",
                                    "ada_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, algorithm='SAMME', random_state=42)\n",
                                    "\n",
                                    "# TODO: Fit the AdaBoost classifier with the training data.\n",
                                    "ada_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# TODO: Make predictions on test dataset and calculate accuracy.\n",
                                    "y_pred_ada = ada_clf.predict(X_test)\n",
                                    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
                                    "\n",
                                    "print(f\"AdaBoost Classifier accuracy: {accuracy_ada}\")\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import AdaBoostClassifier\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split dataset\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# TODO: Initialize AdaBoostClassifier with DecisionTreeClassifier as a base model, 100 estimators, and 'SAMME' algorithm.\n",
                                    "ada_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, algorithm='SAMME', random_state=42)\n",
                                    "\n",
                                    "# TODO: Fit the AdaBoost classifier with the training data.\n",
                                    "ada_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# TODO: Make predictions on test dataset and calculate accuracy.\n",
                                    "y_pred_ada = ada_clf.predict(X_test)\n",
                                    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
                                    "\n",
                                    "print(f\"AdaBoost Classifier accuracy: {accuracy_ada}\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## AdaBoost vs RandomForest\n",
                                    "\n",
                                    "Great job, Space Explorer!\n",
                                    "\n",
                                    "Let's make it more engaging. Fill in the missing pieces of code to train an AdaBoost classifier using a synthetically generated dataset. The generated dataset is imbalanced, and has 0 redundant features. Boosting techniques usually perform better than RandomForest here.\n",
                                    "\n",
                                    "After completing the AdaBoost classifier, also train a RandomForest classifier on the same dataset and compare their performances.\n",
                                    "\n",
                                    "May the stars guide your way!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import make_classification\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Generate an imbalanced synthetic dataset\n",
                                    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
                                    "                           n_redundant=0, weights=[0.9, 0.1], random_state=42)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# Initialize AdaBoost with DecisionTree as the base estimator\n",
                                    "ada_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3), n_estimators=100, algorithm='SAMME')\n",
                                    "# TODO: Train the AdaBoost classifier\n",
                                    "\n",
                                    "# TODO: Make predictions and calculate accuracy for AdaBoost\n",
                                    "\n",
                                    "# TODO: Initialize and train the RandomForest classifier\n",
                                    "\n",
                                    "# TODO: Make predictions and calculate accuracy for RandomForest\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import make_classification\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Generate an imbalanced synthetic dataset\n",
                                    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
                                    "                           n_redundant=0, weights=[0.9, 0.1], random_state=42)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# Initialize AdaBoost with DecisionTree as the base estimator\n",
                                    "ada_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3), n_estimators=100, algorithm='SAMME', random_state=42)\n",
                                    "# TODO: Train the AdaBoost classifier\n",
                                    "ada_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# TODO: Make predictions and calculate accuracy for AdaBoost\n",
                                    "y_pred_ada = ada_clf.predict(X_test)\n",
                                    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
                                    "print(f\"AdaBoost Classifier Accuracy: {accuracy_ada:.4f}\")\n",
                                    "\n",
                                    "# TODO: Initialize and train the RandomForest classifier\n",
                                    "# Using similar number of estimators for a fair comparison, and adjusting class_weight for imbalanced data\n",
                                    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
                                    "rf_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# TODO: Make predictions and calculate accuracy for RandomForest\n",
                                    "y_pred_rf = rf_clf.predict(X_test)\n",
                                    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
                                    "print(f\"RandomForest Classifier Accuracy: {accuracy_rf:.4f}\")\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
