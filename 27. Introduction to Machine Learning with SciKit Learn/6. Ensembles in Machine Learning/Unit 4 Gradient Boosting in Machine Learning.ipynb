{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 4 Gradient Boosting in Machine Learning\n",
                                    "\n",
                                    "Hello there\\! Today, we're going to explore Gradient Boosting, a powerful technique that improves the accuracy of machine learning models. Our goal is to understand what **Gradient Boosting** is, how it works, and how to use it with a real example in Python. By the end, you'll know how to implement **Gradient Boosting** and apply it to a dataset.\n",
                                    "\n",
                                    "## What is Gradient Boosting?\n",
                                    "\n",
                                    "**Gradient Boosting** is an ensemble technique that combines multiple weak learners, usually decision trees, to form a stronger, more accurate model. Unlike Bagging and **Random Forests**, which create models independently, **Gradient Boosting** builds models sequentially. Each new model aims to correct errors made by the previous ones.\n",
                                    "\n",
                                    "Imagine baking a cake. The first cake might not be perfect — maybe too dry or not sweet enough. The next time, you make changes to improve it based on previous errors. Over time, you get closer to perfection. This is how **Gradient Boosting** works.\n",
                                    "\n",
                                    "Here's a step-by-step explanation of **Gradient Boosting**:\n",
                                    "\n",
                                    "1.  **Start with an initial model:** This can be a simple model like a single decision tree.\n",
                                    "2.  **Calculate errors:** Find out where the initial model makes mistakes.\n",
                                    "3.  **Build the next model:** Create a new model that focuses on correcting the errors from the initial model using gradients.\n",
                                    "4.  **Combine models:** Add the new model to the existing ones to create a stronger model.\n",
                                    "5.  **Repeat:** Continue this process until desired accuracy is achieved or a specified number of models is built.\n",
                                    "\n",
                                    "Consider tuning a musical instrument. Initially, it may be out of tune. By fine-tuning each string separately, you reduce the overall error (or off-tune sound) until the instrument sounds perfect.\n",
                                    "\n",
                                    "### Gradient Boosting vs. AdaBoost\n",
                                    "\n",
                                    "**Gradient Boosting** and **AdaBoost** are both boosting techniques but they differ in their approach to combining weak learners.\n",
                                    "\n",
                                    "  * **AdaBoost:** Each subsequent model focuses more on the instances that previous models misclassified. It assigns weights to instances, increasing weights for those that are hard to classify.\n",
                                    "  * **Gradient Boosting:** Each subsequent model tries to minimize the loss function (usually the residual error) directly through gradient descent. It builds the new learner in the direction that reduces the error of the whole ensemble.\n",
                                    "\n",
                                    "## Loading and Preparing the Dataset\n",
                                    "\n",
                                    "Before we dive into coding, let's understand why datasets are crucial. A good dataset allows us to train and test our machine learning models effectively. We'll use the `load_digits` function from `scikit-learn`, which provides a real-world dataset for digit classification (0 to 9) from images.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_digits\n",
                                    "\n",
                                    "# Load real dataset\n",
                                    "X, y = load_digits(return_X_y=True)\n",
                                    "```\n",
                                    "\n",
                                    "We need to split this dataset into training and testing sets to evaluate our model properly. Here's how we do it in Python:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Split dataset\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "```\n",
                                    "\n",
                                    "While this task is usually solved using deep learning, namely Convolutional Neural Networks, we can also approach it using simpler classifiers, including `GradientBoostingClassifier` and `AdaBoostClassifier`.\n",
                                    "\n",
                                    "## Training and Testing a Gradient Boosting Classifier\n",
                                    "\n",
                                    "Now let's train a Gradient Boosting model using `GradientBoostingClassifier` from `scikit-learn`. Here's the basic code to train our model:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.ensemble import GradientBoostingClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Train a gradient boosting classifier\n",
                                    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
                                    "gb_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "y_pred = gb_clf.predict(X_test)\n",
                                    "accuracy = accuracy_score(y_pred, y_test)\n",
                                    "print(f\"Accuracy on test data: {accuracy:.2f}\")  # Accuracy on test data: 0.97\n",
                                    "```\n",
                                    "\n",
                                    "In this code:\n",
                                    "\n",
                                    "  * `GradientBoostingClassifier` is the model we're using.\n",
                                    "  * `n_estimators=100` means we'll build 100 weak learners (decision trees).\n",
                                    "  * `random_state=42` ensures reproducibility.\n",
                                    "  * `fit(X_train, y_train)` trains the model on our training data.\n",
                                    "  * `predict(X_test)` generates predictions on the test data.\n",
                                    "\n",
                                    "Lastly, we calculate the accuracy using the `accuracy_score` function.\n",
                                    "\n",
                                    "The `n_estimators` parameter is crucial because it determines the number of boosting stages, or how many times we refine our model. If set too low, our model might not be accurate enough (underfitting). If set too high, our model might become too complex (overfitting).\n",
                                    "\n",
                                    "## Comparing Gradient Boosting to AdaBoost and RandomForest\n",
                                    "\n",
                                    "Let's compare **GradientBoosting**, **AdaBoost**, and **RandomForest** classifiers using the same dataset and parameters:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
                                    "\n",
                                    "# Train an AdaBoost classifier\n",
                                    "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42, algorithm='SAMME')\n",
                                    "ada_clf.fit(X_train, y_train)\n",
                                    "y_pred_ada = ada_clf.predict(X_test)\n",
                                    "accuracy_ada = accuracy_score(y_pred_ada, y_test)\n",
                                    "print(f\"Accuracy for AdaBoost on test data: {accuracy_ada:.2f}\")  # Accuracy for AdaBoost on test data: 0.83\n",
                                    "\n",
                                    "# Train a Random Forest classifier\n",
                                    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                                    "rf_clf.fit(X_train, y_train)\n",
                                    "y_pred_rf = rf_clf.predict(X_test)\n",
                                    "accuracy_rf = accuracy_score(y_pred_rf, y_test)\n",
                                    "print(f\"Accuracy for RandomForest on test data: {accuracy_rf:.2f}\")  # Accuracy for RandomForest on test data: 0.97\n",
                                    "```\n",
                                    "\n",
                                    "We train all models on the same data and compare their accuracies on the testing set. Main conclusions:\n",
                                    "\n",
                                    "  * The `GradientBoostingClassifier` outperforms the `AdaBoostClassifier` on this dataset\n",
                                    "  * The `GradientBoostingClassifier` shows comparable performance to the `RandomForestClassifier`\n",
                                    "  * The `GradientBoostingClassifier` is a candidate to be the main model for this task\n",
                                    "\n",
                                    "## Lesson Summary\n",
                                    "\n",
                                    "Well done\\! In this lesson, we learned about **Gradient Boosting**. We covered what it is, how it works, and how to implement it in Python using a real dataset.\n",
                                    "\n",
                                    "To recap:\n",
                                    "\n",
                                    "  * **Gradient Boosting** builds models sequentially to correct errors from previous models.\n",
                                    "  * We used the `load_digits` dataset to train and test our Gradient Boosting model.\n",
                                    "  * The `GradientBoostingClassifier` from `scikit-learn` allowed us to easily implement this technique.\n",
                                    "  * We compared **Gradient Boosting** with **AdaBoost** and **RandomForest** to see how they perform on the same dataset.\n",
                                    "\n",
                                    "Now that we've covered the theory, it's time for some hands-on practice. You'll apply these concepts to new datasets and fine-tune model parameters to see how **Gradient Boosting** can improve model performance.\n",
                                    "\n",
                                    "Ready to get started? Let's move to the practice session\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adjust Gradient Boosting Estimators\n",
                                    "\n",
                                    "n this task, you'll change the number of weak learners in the Gradient Boosting model from 5 to 25. This small tweak demonstrates how the number of boosting stages impacts model performance.\n",
                                    "\n",
                                    "Let's see the difference!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_digits\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import GradientBoostingClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Load and split dataset\n",
                                    "X, y = load_digits(return_X_y=True)\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# Train Gradient Boosting model\n",
                                    "gb_clf = GradientBoostingClassifier(n_estimators=5, random_state=42)\n",
                                    "gb_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Model evaluation\n",
                                    "accuracy = accuracy_score(gb_clf.predict(X_test), y_test)\n",
                                    "print(f\"Gradient Boosting accuracy: {accuracy:.2f}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "In this task, you'll change the number of weak learners in the Gradient Boosting model from 5 to 25. This small tweak demonstrates how the number of boosting stages impacts model performance.\n",
                                    "\n",
                                    "Let's see the difference\\!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_digits\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import GradientBoostingClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Load and split dataset\n",
                                    "X, y = load_digits(return_X_y=True)\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# Train Gradient Boosting model with n_estimators changed from 5 to 25\n",
                                    "gb_clf = GradientBoostingClassifier(n_estimators=25, random_state=42)\n",
                                    "gb_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Model evaluation\n",
                                    "accuracy = accuracy_score(gb_clf.predict(X_test), y_test)\n",
                                    "print(f\"Gradient Boosting accuracy: {accuracy:.2f}\")\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Complete the Gradient Boosting Setup for Digit Classification\n",
                                    "\n",
                                    "Great job, fellow Space Explorer! Let's level up the challenge.\n",
                                    "\n",
                                    "Complete the missing pieces of the code to train and evaluate a Gradient Boosting classifier on the load_digits dataset and evaluate its performance.\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_digits\n",
                                    "from sklearn.ensemble import GradientBoostingClassifier\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "X, y = load_digits(return_X_y=True)\n",
                                    "\n",
                                    "#TODO:  Split dataset and train Gradient Boosting classifier\n",
                                    "\n",
                                    "# Evaluate model performance with accuracy\n",
                                    "accuracy = accuracy_score(gb_clf.predict(X_test), y_test)\n",
                                    "print(f\"Gradient Boosting Accuracy: {accuracy:.2f}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Great job, fellow Space Explorer\\! Let's level up the challenge.\n",
                                    "\n",
                                    "Complete the missing pieces of the code to train and evaluate a Gradient Boosting classifier on the `load_digits` dataset and evaluate its performance.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_digits\n",
                                    "from sklearn.ensemble import GradientBoostingClassifier\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
                                    "\n",
                                    "# Load dataset\n",
                                    "X, y = load_digits(return_X_y=True)\n",
                                    "\n",
                                    "# Split dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# Initialize and train Gradient Boosting classifier\n",
                                    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42) # Using 100 estimators as a common default\n",
                                    "gb_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Evaluate model performance with accuracy\n",
                                    "accuracy = accuracy_score(gb_clf.predict(X_test), y_test)\n",
                                    "print(f\"Gradient Boosting Accuracy: {accuracy:.2f}\")\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Gradient Boosting vs. AdaBoost on Synthetic Data\n",
                                    "\n",
                                    "Great!\n",
                                    "\n",
                                    "Now, let's compare our two boosting models.\n",
                                    "\n",
                                    "Complete the missing parts to train the an AdaBoost and a GradientBoosting classifier and evaluate its accuracy using the given synthetic dataset.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import make_classification\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Generate a synthetic dataset\n",
                                    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
                                    "\n",
                                    "# Split data into training and test sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# TODO: Train an AdaBoost classifier with 50 estimators, fit to training data, and evaluate accuracy\n",
                                    "\n",
                                    "# TODO: Train a Gradient Boosting classifier with 50 estimators, fit to training data, and evaluate accuracy\n",
                                    "\n",
                                    "# Print accuracies\n",
                                    "print(f\"Accuracy for AdaBoost: {accuracy_ada:.2f}\")\n",
                                    "print(f\"Accuracy for Gradient Boosting: {accuracy_gb:.2f}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Great\\!\n",
                                    "\n",
                                    "Now, let's compare our two boosting models.\n",
                                    "\n",
                                    "Complete the missing parts to train an AdaBoost and a GradientBoosting classifier and evaluate its accuracy using the given synthetic dataset.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import make_classification\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Generate a synthetic dataset\n",
                                    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
                                    "\n",
                                    "# Split data into training and test sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# Train an AdaBoost classifier with 50 estimators, fit to training data, and evaluate accuracy\n",
                                    "ada_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
                                    "ada_clf.fit(X_train, y_train)\n",
                                    "y_pred_ada = ada_clf.predict(X_test)\n",
                                    "accuracy_ada = accuracy_score(y_pred_ada, y_test)\n",
                                    "\n",
                                    "# Train a Gradient Boosting classifier with 50 estimators, fit to training data, and evaluate accuracy\n",
                                    "gb_clf = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
                                    "gb_clf.fit(X_train, y_train)\n",
                                    "y_pred_gb = gb_clf.predict(X_test)\n",
                                    "accuracy_gb = accuracy_score(y_pred_gb, y_test)\n",
                                    "\n",
                                    "# Print accuracies\n",
                                    "print(f\"Accuracy for AdaBoost: {accuracy_ada:.2f}\")\n",
                                    "print(f\"Accuracy for Gradient Boosting: {accuracy_gb:.2f}\")\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Comparing Models Efficiency\n",
                                    "\n",
                                    "Every explorer needs the right tools to succeed. In this task, we create a small synthetic dataset and train Gradient Boosting, Random Forest, and AdaBoost models on it. Your goal will be to compare the time it takes to train each model. Remember, good predictions is not the only factor when choosing the right model, we also care about efficiency!\n",
                                    "\n",
                                    "The first one is done for you, use it as an example\n",
                                    "\n",
                                    "Embark on your journey, Space Voyager!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "import time\n",
                                    "from sklearn.datasets import make_classification\n",
                                    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
                                    "\n",
                                    "# Create a small synthetic dataset\n",
                                    "X, y = make_classification(n_samples=5000, n_features=20, random_state=42)\n",
                                    "\n",
                                    "# Train and time the Gradient Boosting model\n",
                                    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
                                    "start_time = time.time()\n",
                                    "gb_clf.fit(X, y)\n",
                                    "gb_time = time.time() - start_time\n",
                                    "\n",
                                    "# TODO: Train and time the Random Forest model\n",
                                    "\n",
                                    "# TODO: Train and time the AdaBoost model\n",
                                    "\n",
                                    "print(f\"Gradient Boosting training time: {gb_time:.2f} seconds\")\n",
                                    "print(f\"Random Forest training time: {rf_time:.2f} seconds\")\n",
                                    "print(f\"AdaBoost training time: {ab_time:.2f} seconds\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "\n",
                                    "Every explorer needs the right tools to succeed. In this task, we create a small synthetic dataset and train Gradient Boosting, Random Forest, and AdaBoost models on it. Your goal will be to compare the time it takes to train each model. Remember, good predictions is not the only factor when choosing the right model, we also care about efficiency\\!\n",
                                    "\n",
                                    "The first one is done for you, use it as an example\n",
                                    "\n",
                                    "Embark on your journey, Space Voyager\\!\n",
                                    "\n",
                                    "```python\n",
                                    "import numpy as np\n",
                                    "import time\n",
                                    "from sklearn.datasets import make_classification\n",
                                    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
                                    "\n",
                                    "# Create a small synthetic dataset\n",
                                    "X, y = make_classification(n_samples=5000, n_features=20, random_state=42)\n",
                                    "\n",
                                    "# Train and time the Gradient Boosting model\n",
                                    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
                                    "start_time = time.time()\n",
                                    "gb_clf.fit(X, y)\n",
                                    "gb_time = time.time() - start_time\n",
                                    "\n",
                                    "# Train and time the Random Forest model\n",
                                    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                                    "start_time = time.time()\n",
                                    "rf_clf.fit(X, y)\n",
                                    "rf_time = time.time() - start_time\n",
                                    "\n",
                                    "# Train and time the AdaBoost model\n",
                                    "ab_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
                                    "start_time = time.time()\n",
                                    "ab_clf.fit(X, y)\n",
                                    "ab_time = time.time() - start_time\n",
                                    "\n",
                                    "print(f\"Gradient Boosting training time: {gb_time:.2f} seconds\")\n",
                                    "print(f\"Random Forest training time: {rf_time:.2f} seconds\")\n",
                                    "print(f\"AdaBoost training time: {ab_time:.2f} seconds\")\n",
                                    "\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Gradient Boosting with Varying Estimators\n",
                                    "\n",
                                    "Hey there, Space Pioneer! You're doing great so far.\n",
                                    "\n",
                                    "Let’s tweak the model a bit. Add the missing pieces to train and evaluate the GradientBoostingClassifier.\n",
                                    "\n",
                                    "Good luck, and may the force of data be with you!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "from sklearn.datasets import load_digits\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import GradientBoostingClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Load the digits dataset\n",
                                    "X, y = load_digits(return_X_y=True)\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# List of n_estimators to try\n",
                                    "n_estimators_list = list(range(1, 31, 7))  # Trying fewer values for simplicity\n",
                                    "accuracies = []\n",
                                    "\n",
                                    "# Train and test models with different n_estimators\n",
                                    "for n in n_estimators_list:\n",
                                    "    # TODO: define the GradientBoostingClassifier with the given n_estimators (n)\n",
                                    "    # TODO: make predictions on the test set and calculate the accuracy\n",
                                    "    # TODO: append the obtained accuracy to the accuracies list\n",
                                    "\n",
                                    "# Plot the results\n",
                                    "plt.plot(n_estimators_list, accuracies, marker='o')\n",
                                    "plt.xlabel('Number of Estimators')\n",
                                    "plt.ylabel('Accuracy')\n",
                                    "plt.title('Gradient Boosting Accuracy vs Number of Estimators')\n",
                                    "plt.show()\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Hey there, Space Pioneer\\! You're doing great so far.\n",
                                    "\n",
                                    "Let’s tweak the model a bit. Add the missing pieces to train and evaluate the GradientBoostingClassifier.\n",
                                    "\n",
                                    "Good luck, and may the force of data be with you\\!\n",
                                    "\n",
                                    "```python\n",
                                    "import matplotlib.pyplot as plt\n",
                                    "from sklearn.datasets import load_digits\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.ensemble import GradientBoostingClassifier\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Load the digits dataset\n",
                                    "X, y = load_digits(return_X_y=True)\n",
                                    "\n",
                                    "# Split data into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                                    "\n",
                                    "# List of n_estimators to try\n",
                                    "n_estimators_list = list(range(1, 31, 7))  # Trying fewer values for simplicity\n",
                                    "accuracies = []\n",
                                    "\n",
                                    "# Train and test models with different n_estimators\n",
                                    "for n in n_estimators_list:\n",
                                    "    # define the GradientBoostingClassifier with the given n_estimators (n)\n",
                                    "    gb_clf = GradientBoostingClassifier(n_estimators=n, random_state=42)\n",
                                    "    gb_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "    # make predictions on the test set and calculate the accuracy\n",
                                    "    y_pred = gb_clf.predict(X_test)\n",
                                    "    accuracy = accuracy_score(y_test, y_pred)\n",
                                    "    \n",
                                    "    # append the obtained accuracy to the accuracies list\n",
                                    "    accuracies.append(accuracy)\n",
                                    "\n",
                                    "# Plot the results\n",
                                    "plt.plot(n_estimators_list, accuracies, marker='o')\n",
                                    "plt.xlabel('Number of Estimators')\n",
                                    "plt.ylabel('Accuracy')\n",
                                    "plt.title('Gradient Boosting Accuracy vs Number of Estimators')\n",
                                    "plt.show()\n",
                                    "\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
