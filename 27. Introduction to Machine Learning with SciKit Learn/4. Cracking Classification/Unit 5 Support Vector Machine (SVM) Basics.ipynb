{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 5 Support Vector Machine (SVM) Basics\n",
                                    "\n",
                                    "Welcome to our lesson on Support Vector Machines (SVM)\\! Today, we will learn about a powerful tool used for classifying data in machine learning. Have you ever thought about how a computer could tell the difference between pictures of cats and dogs? SVM is one way to make that possible by drawing a line, or more accurately, a hyperplane, to separate different categories. By the end of this lesson, you’ll be able to load a dataset, split it into training and testing sets, and train an SVM model using Python and Scikit-Learn. Let’s get started\\!\n",
                                    "\n",
                                    "## Introduction to SVM\n",
                                    "\n",
                                    "First, let’s talk about what an SVM is. Imagine you have a bag full of apples and oranges on a table. You want to separate them into two groups using a straight line. This line that separates the two groups in such a way is called a **hyperplane**. SVM is a classification algorithm that finds the best hyperplane that separates different classes in the data.\n",
                                    "\n",
                                    "But wait, what if the data can’t be separated by a straight line? That’s when SVM can use something called a **kernel trick** to transform the data into a higher dimension where a hyperplane can be used. The data points that are closest to the hyperplane are called **support vectors** because they “support” the hyperplane. Essentially, SVM seeks to maximize the margin between the classes through the hyperplane, leading to better generalization in classification.\n",
                                    "\n",
                                    "Here is an example image:\n",
                                    "\n",
                                    "Dots of different classes are separated using the hyperplane (which is just a line in this 2d case), which we call the \"decision boundary\". The closest samples form the support vectors, drawn as dashed lines. They help to keep the decision boundary in the optimal equally-distanced position.\n",
                                    "\n",
                                    "In Python, we use the `SVC` class from the `Scikit-Learn` library to create an SVM.\n",
                                    "\n",
                                    "## Quick Reminder on Dataset Loading and Splitting\n",
                                    "\n",
                                    "Remember, to work with any dataset, we need to load it and then split it into training and testing sets. Here’s a quick reminder using the wine dataset:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Load the wine dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets (60% training, 40% testing)\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
                                    "```\n",
                                    "\n",
                                    "## Training the SVM Classifier\n",
                                    "\n",
                                    "Now that we have our training and testing data ready, it’s time to create and train the SVM classifier. Think of this as teaching the computer to draw the best line that separates the data points into their correct categories.\n",
                                    "\n",
                                    "We will use the `SVC` class with a linear kernel:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.svm import SVC\n",
                                    "\n",
                                    "# Create an SVM classifier with a linear kernel\n",
                                    "svm_clf = SVC(kernel='linear')\n",
                                    "\n",
                                    "# Train the classifier\n",
                                    "svm_clf.fit(X_train, y_train)\n",
                                    "```\n",
                                    "\n",
                                    "In this code, `kernel='linear'` specifies that we want to use a linear kernel. We then train the classifier using `fit(X_train, y_train)`. The other common option to use for the kernel is `rbf`, which stands for the radial basis function.\n",
                                    "\n",
                                    "## Comparing with Other Models\n",
                                    "\n",
                                    "Finally, let's compare how different models perform on this dataset. We will train and evaluate Logistic Regression, Decision Tree, Naive Bayes, and k-Nearest Neighbors (kNN) models in addition to our SVM model.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.linear_model import LogisticRegression\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.naive_bayes import GaussianNB\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "from sklearn.svm import SVC\n",
                                    "\n",
                                    "# Dictionary of models to compare\n",
                                    "models = {\n",
                                    "    \"Logistic Regression\": LogisticRegression(max_iter=10000),\n",
                                    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
                                    "    \"Naive Bayes\": GaussianNB(),\n",
                                    "    \"kNN\": KNeighborsClassifier(),\n",
                                    "    \"SVM\": SVC(kernel='linear')\n",
                                    "}\n",
                                    "\n",
                                    "# Train each model and print their accuracy on the test set\n",
                                    "for name, model in models.items():\n",
                                    "    model.fit(X_train, y_train)\n",
                                    "    accuracy = model.score(X_test, y_test)\n",
                                    "    print(f\"{name} Accuracy: {accuracy:.2f}\")\n",
                                    "```\n",
                                    "\n",
                                    "The output is:\n",
                                    "\n",
                                    "```\n",
                                    "Logistic Regression Accuracy: 0.96\n",
                                    "Decision Tree Accuracy: 0.94\n",
                                    "Naive Bayes Accuracy: 1.00\n",
                                    "kNN Accuracy: 0.69\n",
                                    "SVM Accuracy: 0.94\n",
                                    "```\n",
                                    "\n",
                                    "Here we can see all the models compared, and they show similar results. Remember that you can always improve a model by tuning it, which we will discuss in the last course of this course path. In this case, we might choose the Naive Bayes classifier as the best model based on its current performance or opt to tune the Decision Tree classifier for potential improvements. Choosing the right model involves experimenting with the data and comparing results.\n",
                                    "\n",
                                    "## Lesson Summary\n",
                                    "\n",
                                    "Great job\\! Let’s recap what we’ve learned today:\n",
                                    "\n",
                                    "  * **SVM (Support Vector Machine)** is used to classify data by finding the best hyperplane that separates different classes.\n",
                                    "  * We used the **Wine dataset** to get some data to work with.\n",
                                    "  * We split the dataset into **training and testing sets** using `train_test_split`.\n",
                                    "  * We created and trained an SVM classifier using the `SVC` class from **Scikit-Learn**.\n",
                                    "  * We compared the performance of SVM with **Logistic Regression, Decision Tree, Naive Bayes, and k-Nearest Neighbors** classifiers.\n",
                                    "\n",
                                    "Now it’s your turn to put this knowledge into practice\\! Up next, you'll get hands-on experience to solidify what you’ve learned by loading a dataset, splitting it, and training an SVM model just like we did here. Dive in and start coding\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Changing SVM Kernel\n",
                                    "\n",
                                    "Time to explore different kernels!\n",
                                    "\n",
                                    "Modify the given code to use a linear kernel for the SVM classifier instead of the RBF (Radial Basis Function). This will help you see how choosing a right kernel may affect the model's performance\n",
                                    "\n",
                                    "Let’s code!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.svm import SVC\n",
                                    "\n",
                                    "# Load the wine dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
                                    "\n",
                                    "# Create an SVM classifier with an rbf kernel\n",
                                    "# TODO: change the kernel\n",
                                    "svm_clf = SVC(kernel='rbf')\n",
                                    "\n",
                                    "# Train the classifier\n",
                                    "svm_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Print the accuracy of the classifier\n",
                                    "print(f\"SVM Accuracy: {svm_clf.score(X_test, y_test)}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "To change the SVM kernel to linear, you just need to modify the `kernel` parameter in the `SVC` constructor from `'rbf'` to `'linear'`.\n",
                                    "\n",
                                    "Here's the modified code:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.svm import SVC\n",
                                    "\n",
                                    "# Load the wine dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
                                    "\n",
                                    "# Create an SVM classifier with a linear kernel\n",
                                    "# TODO: change the kernel\n",
                                    "svm_clf = SVC(kernel='linear') # Changed from 'rbf' to 'linear'\n",
                                    "\n",
                                    "# Train the classifier\n",
                                    "svm_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Print the accuracy of the classifier\n",
                                    "print(f\"SVM Accuracy: {svm_clf.score(X_test, y_test)}\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Complete the Wine Classification SVM\n",
                                    "\n",
                                    "All right, Space Explorer, let's add some missing code to complete our SVM classifier!\n",
                                    "\n",
                                    "Fill in the blanks to:\n",
                                    "\n",
                                    "Load the wine dataset.\n",
                                    "Create and train an SVM classifier with a linear kernel.\n",
                                    "You're almost there; keep pushing!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.svm import SVC\n",
                                    "\n",
                                    "# TODO: Load the wine dataset\n",
                                    "X, y = ____\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
                                    "\n",
                                    "# TODO: Create and train an SVM classifier with a linear kernel\n",
                                    "\n",
                                    "# TODO: Print the accuracy of the trained model\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Sure, I'll help you complete the SVM classifier for wine classification.\n",
                                    "\n",
                                    "Here's the completed code:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine # Import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.svm import SVC\n",
                                    "\n",
                                    "# TODO: Load the wine dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
                                    "\n",
                                    "# TODO: Create and train an SVM classifier with a linear kernel\n",
                                    "svm_clf = SVC(kernel='linear') # Create SVM classifier\n",
                                    "svm_clf.fit(X_train, y_train) # Train the classifier\n",
                                    "\n",
                                    "# TODO: Print the accuracy of the trained model\n",
                                    "print(f\"SVM Accuracy: {svm_clf.score(X_test, y_test)}\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Bringing Out the Power of the RBF Kernel\n",
                                    "\n",
                                    "Space Wanderer, it's time to explore! Your mission is to classify data using SVM with different kernels and understand the benefits of the RBF kernel. We will use a synthetic dataset designed to highlight the strengths of the RBF kernel over the linear one.\n",
                                    "\n",
                                    "In this dataset, classes are distributed in circles—a complex non-linear pattern that the linear kernel should not be able to distinguish.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import make_circles\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.svm import SVC\n",
                                    "\n",
                                    "# Generate a synthetic dataset\n",
                                    "X, y = make_circles(n_samples=1000, factor=0.3, noise=0.1, random_state=42)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
                                    "\n",
                                    "# Train and test SVM with linear kernel\n",
                                    "svm_linear = SVC(kernel='linear')\n",
                                    "svm_linear.fit(X_train, y_train)\n",
                                    "linear_accuracy = svm_linear.score(X_test, y_test)\n",
                                    "\n",
                                    "# Train and test SVM with rbf kernel\n",
                                    "svm_rbf = SVC(kernel='rbf')\n",
                                    "# TODO: Train the classifier with rbf kernel\n",
                                    "\n",
                                    "# TODO: Get the accuracy of the classifier with rbf kernel\n",
                                    "\n",
                                    "# Print the accuracies for comparison\n",
                                    "print(f\"SVM Linear Kernel Accuracy: {linear_accuracy}\")\n",
                                    "print(f\"SVM RBF Kernel Accuracy: {rbf_accuracy}\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import make_circles\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.svm import SVC\n",
                                    "\n",
                                    "# Generate a synthetic dataset\n",
                                    "X, y = make_circles(n_samples=1000, factor=0.3, noise=0.1, random_state=42)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
                                    "\n",
                                    "# Train and test SVM with linear kernel\n",
                                    "svm_linear = SVC(kernel='linear')\n",
                                    "svm_linear.fit(X_train, y_train)\n",
                                    "linear_accuracy = svm_linear.score(X_test, y_test)\n",
                                    "\n",
                                    "# Train and test SVM with rbf kernel\n",
                                    "svm_rbf = SVC(kernel='rbf')\n",
                                    "# TODO: Train the classifier with rbf kernel\n",
                                    "svm_rbf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# TODO: Get the accuracy of the classifier with rbf kernel\n",
                                    "rbf_accuracy = svm_rbf.score(X_test, y_test)\n",
                                    "\n",
                                    "# Print the accuracies for comparison\n",
                                    "print(f\"SVM Linear Kernel Accuracy: {linear_accuracy}\")\n",
                                    "print(f\"SVM RBF Kernel Accuracy: {rbf_accuracy}\")\n",
                                    "```"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "id": "ea0829e0",
                           "metadata": {},
                           "source": [
                                    "Alright, Galactic Pioneer! It’s time to classify different types of wine based on their chemical properties using three different classifiers: SVM, Decision Tree, and KNN. You'll also perform simple hyperparameter tuning for each model to find the best configuration. Follow the instructions in the TODO comments to load, split, train, and tune the classifiers using the wine dataset.\n",
                                    "\n",
                                    "Let's get started!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.svm import SVC\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "\n",
                                    "# Load the wine dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
                                    "\n",
                                    "# Hyperparameter ranges\n",
                                    "svm_kernels = ['linear', 'rbf']\n",
                                    "dt_max_depths = [3, 5, 10, None]\n",
                                    "knn_neighbors = [3, 5, 7, 9]\n",
                                    "\n",
                                    "# Decision Tree hyperparameter tuning\n",
                                    "best_dt_accuracy = 0\n",
                                    "best_dt_model = None\n",
                                    "for max_depth in dt_max_depths:\n",
                                    "    dt_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
                                    "    dt_clf.fit(X_train, y_train)\n",
                                    "    accuracy = dt_clf.score(X_test, y_test)\n",
                                    "    if accuracy > best_dt_accuracy:\n",
                                    "        best_dt_accuracy = accuracy\n",
                                    "        best_dt_model = dt_clf\n",
                                    "\n",
                                    "print(f\"Best Decision Tree Accuracy: {best_dt_accuracy}\")\n",
                                    "\n",
                                    "# KNN hyperparameter tuning\n",
                                    "best_knn_accuracy = 0\n",
                                    "best_knn_model = None\n",
                                    "for n_neighbors in knn_neighbors:\n",
                                    "    knn_clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
                                    "    knn_clf.fit(X_train, y_train)\n",
                                    "    accuracy = knn_clf.score(X_test, y_test)\n",
                                    "    if accuracy > best_knn_accuracy:\n",
                                    "        best_knn_accuracy = accuracy\n",
                                    "        best_knn_model = knn_clf\n",
                                    "\n",
                                    "print(f\"Best KNN Accuracy: {best_knn_accuracy}\")\n",
                                    "\n",
                                    "\n",
                                    "# TODO: Perform the SVM hypertuning in the same manner.\n",
                                    "# Hint: use \"for kernel in svm_kernels:\" loop for iteration\n",
                                    "# TODO: find and print the best accuracy for the SVM\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Alright, Galactic Pioneer\\! Let's complete the SVM hyperparameter tuning.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.svm import SVC\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "\n",
                                    "# Load the wine dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
                                    "\n",
                                    "# Hyperparameter ranges\n",
                                    "svm_kernels = ['linear', 'rbf']\n",
                                    "dt_max_depths = [3, 5, 10, None]\n",
                                    "knn_neighbors = [3, 5, 7, 9]\n",
                                    "\n",
                                    "# Decision Tree hyperparameter tuning\n",
                                    "best_dt_accuracy = 0\n",
                                    "best_dt_model = None\n",
                                    "for max_depth in dt_max_depths:\n",
                                    "    dt_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
                                    "    dt_clf.fit(X_train, y_train)\n",
                                    "    accuracy = dt_clf.score(X_test, y_test)\n",
                                    "    if accuracy > best_dt_accuracy:\n",
                                    "        best_dt_accuracy = accuracy\n",
                                    "        best_dt_model = dt_clf\n",
                                    "\n",
                                    "print(f\"Best Decision Tree Accuracy: {best_dt_accuracy}\")\n",
                                    "\n",
                                    "# KNN hyperparameter tuning\n",
                                    "best_knn_accuracy = 0\n",
                                    "best_knn_model = None\n",
                                    "for n_neighbors in knn_neighbors:\n",
                                    "    knn_clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
                                    "    knn_clf.fit(X_train, y_train)\n",
                                    "    accuracy = knn_clf.score(X_test, y_test)\n",
                                    "    if accuracy > best_knn_accuracy:\n",
                                    "        best_knn_accuracy = accuracy\n",
                                    "        best_knn_model = knn_clf\n",
                                    "\n",
                                    "print(f\"Best KNN Accuracy: {best_knn_accuracy}\")\n",
                                    "\n",
                                    "\n",
                                    "# TODO: Perform the SVM hypertuning in the same manner.\n",
                                    "# Hint: use \"for kernel in svm_kernels:\" loop for iteration\n",
                                    "best_svm_accuracy = 0\n",
                                    "best_svm_model = None\n",
                                    "for kernel in svm_kernels:\n",
                                    "    svm_clf = SVC(kernel=kernel)\n",
                                    "    svm_clf.fit(X_train, y_train)\n",
                                    "    accuracy = svm_clf.score(X_test, y_test)\n",
                                    "    if accuracy > best_svm_accuracy:\n",
                                    "        best_svm_accuracy = accuracy\n",
                                    "        best_svm_model = svm_clf\n",
                                    "\n",
                                    "# TODO: find and print the best accuracy for the SVM\n",
                                    "print(f\"Best SVM Accuracy: {best_svm_accuracy}\")\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
