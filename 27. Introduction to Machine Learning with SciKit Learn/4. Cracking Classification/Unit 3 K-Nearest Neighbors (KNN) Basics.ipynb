{
         "cells": [
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "# Unit 3 K-Nearest Neighbors (KNN) Basics\n",
                                    "\n",
                                    "Skip to main content\n",
                                    "\n",
                                    "Back\n",
                                    "\n",
                                    "## K-Nearest Neighbors (KNN) Basics\n",
                                    "\n",
                                    "**System**\n",
                                    "\n",
                                    "**Cosmo**\n",
                                    "**Lesson**\n",
                                    "**K-Nearest Neighbors (KNN) Basics**\n",
                                    "\n",
                                    "### Lesson Introduction\n",
                                    "\n",
                                    "Welcome to the lesson on K-Nearest Neighbors (KNN)\\! Today, we’ll explore this simple yet intuitive algorithm. KNN is used for classification and regression tasks. Our goal is to understand how KNN works and implement it in Python using Scikit-Learn. By the end, you'll be able to classify data points based on their features.\n",
                                    "\n",
                                    "### K-Nearest Neighbors (KNN) Basics\n",
                                    "\n",
                                    "What is KNN? Imagine identifying a fruit as an apple or an orange. Instead of using a dictionary, you ask nearby people for their opinions. The majority wins. This is the idea behind KNN, classifying a data point based on its nearest neighbors.\n",
                                    "\n",
                                    "Let's take a look at an example:\n",
                                    "\n",
                                    "In this image, we see a target point (black cross) that we want to predict the class for. This target point's three nearest neighbors are two red points and one green point. As the majority of the neighbors are red points, the target point will be also classified as a red point.\n",
                                    "\n",
                                    "Why use KNN? It's easy to understand and implement. It is useful in recommending products, recognizing medical patterns, etc.\n",
                                    "\n",
                                    "### Loading Dataset\n",
                                    "\n",
                                    "Let's load the Iris dataset, which contains information about different flowers. Here's how we do it using Scikit-Learn:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_iris\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Load the Iris dataset\n",
                                    "X, y = load_iris(return_X_y=True)\n",
                                    "\n",
                                    "# Splitting the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
                                    "```\n",
                                    "\n",
                                    "This code loads the Iris dataset and splits it into features `X` and labels `y`. The dataset contains various information about flowers, including Sepal Length, Sepal Width, Petal Length and Petal Width. Our goal is to predict the type of the flower, which is one of the following: Setosa, Versicolour, and Virginica.\n",
                                    "\n",
                                    "Note, that as we predict three classes instead of two, this is not a binary classification. Now we are working with a multiclass classification. Luckily for us, the KNN-classifier is perfectly suitable for this type of tasks. Decision trees can also be used for it, as we saw in the previous lesson.\n",
                                    "\n",
                                    "One important thing to know is that the logistic regression is not suitable for multiclass classification in its original form. However, it can be adapted for this type of task using techniques like One-vs-Rest (OvR) or Softmax Regression (Multinomial Logistic Regression).\n",
                                    "\n",
                                    "### Using the KNN Classifier\n",
                                    "\n",
                                    "Now, let's initialize and fit our KNN classifier. KNN classifies a data point based on the majority class among its nearest neighbors. We’ll start with `k=3`, meaning we will check the three nearest neighbors when making a prediction.\n",
                                    "\n",
                                    "Here’s how to fit a KNN classifier:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "\n",
                                    "# Initialize the KNN classifier with 3 neighbors\n",
                                    "knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
                                    "\n",
                                    "# Fit the KNN classifier\n",
                                    "knn_clf.fit(X_train, y_train)\n",
                                    "```\n",
                                    "\n",
                                    "This initializes the `KNeighborsClassifier` with 3 neighbors. Unlike models we studied earlier, KNN doesn't perform any computations during the fitting phase but rather prepares the data for future comparisons during the prediction phase. Essentially, it does not require any training at all; it just uses the data's structure to make predictions.\n",
                                    "\n",
                                    "### Evaluating the Model\n",
                                    "\n",
                                    "We can evaluate our model's performance by calculating accuracy to see how often it predicts correctly.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "\n",
                                    "# Predict using the KNN classifier\n",
                                    "y_pred = knn_clf.predict(X_test)\n",
                                    "\n",
                                    "# Calculate accuracy using accuracy_score\n",
                                    "accuracy = accuracy_score(y_test, y_pred)\n",
                                    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")  # Model accuracy: 98.33%\n",
                                    "```\n",
                                    "\n",
                                    "The `score` method evaluates the model using the test set, printing the accuracy as a percentage.\n",
                                    "\n",
                                    "### Performance Comparison\n",
                                    "\n",
                                    "Let's also train the decision tree model on the same data and see how it performs. We will use a `.score` method instead of `.predict`. The `score` method combines two steps:\n",
                                    "\n",
                                    "1.  Calculate the predictions\n",
                                    "2.  Calculate the accuracy\n",
                                    "\n",
                                    "We use the `.score` method to make the code shorter and easier to maintain when we don't need the predictions themselves but only care about the model's accuracy, like here.\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "\n",
                                    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
                                    "dt_clf.fit(X_train, y_train)\n",
                                    "dt_accuracy = dt_clf.score(X_test, y_test)\n",
                                    "print(f\"Decision Tree model accuracy: {dt_accuracy * 100:.2f}%\")\n",
                                    "# Decision Tree model accuracy: 96.67%\n",
                                    "```\n",
                                    "\n",
                                    "In this case, KNN outperforms the Decision Tree. However, if we tune the decision tree a bit by limiting its depth, we can achieve the same result:\n",
                                    "\n",
                                    "```python\n",
                                    "dt_clf = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
                                    "dt_clf.fit(X_train, y_train)\n",
                                    "dt_accuracy = dt_clf.score(X_test, y_test)\n",
                                    "print(f\"Decision Tree model accuracy: {dt_accuracy * 100:.2f}%\")\n",
                                    "# Decision Tree model accuracy: 98.33%\n",
                                    "```\n",
                                    "\n",
                                    "Note that we added the `max_depth=3` parameter to the model initialization and improved the model's performance. This shows the importance of tuning your models by choosing the best parameters.\n",
                                    "\n",
                                    "In this case, the `max_depth` was chosen randomly. But in the last course of this course path, we will learn how to find the best possible parameters using a more controllable approach.\n",
                                    "\n",
                                    "### Lesson Summary\n",
                                    "\n",
                                    "Great job\\! You've learned the basics of K-Nearest Neighbors (KNN) and how to implement it using Python and Scikit-Learn. We covered:\n",
                                    "\n",
                                    "  * The concept of KNN\n",
                                    "  * Loading and understanding the Iris dataset\n",
                                    "  * Splitting the dataset into training and testing sets\n",
                                    "  * Fitting a KNN classifier\n",
                                    "  * Brief model evaluation\n",
                                    "\n",
                                    "Now it’s time to practice. You'll engage in hands-on activities to solidify your understanding of KNN and see how it works in different scenarios. Get ready to classify data points and measure your model’s performance\\!\n",
                                    "\n",
                                    "Dive into the practice exercises, and good luck\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## KNN Flower Classification with Iris Dataset\n",
                                    "\n",
                                    "Explore how well a K-Nearest Neighbors (KNN) model can classify different types of flowers using the Iris dataset. The given code loads the dataset, splits it, trains a KNN classifier with 5 neighbors, and evaluates its accuracy.\n",
                                    "\n",
                                    "Click Run to discover the model's accuracy!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_iris\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "\n",
                                    "# Load the Iris dataset\n",
                                    "X, y = load_iris(return_X_y=True)\n",
                                    "\n",
                                    "# Splitting the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
                                    "\n",
                                    "# Initialize the KNN classifier with 5 neighbors\n",
                                    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
                                    "\n",
                                    "# Train the KNN classifier\n",
                                    "knn_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "accuracy = knn_clf.score(X_test, y_test)\n",
                                    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_iris\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "\n",
                                    "# Load the Iris dataset\n",
                                    "X, y = load_iris(return_X_y=True)\n",
                                    "\n",
                                    "# Splitting the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
                                    "\n",
                                    "# Initialize the KNN classifier with 5 neighbors\n",
                                    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
                                    "\n",
                                    "# Train the KNN classifier\n",
                                    "knn_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "accuracy = knn_clf.score(X_test, y_test)\n",
                                    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
                                    "```\n",
                                    "\n",
                                    "**KNN Flower Classification with Iris Dataset Report**\n",
                                    "\n",
                                    "The K-Nearest Neighbors (KNN) model, with `n_neighbors=5`, was used to classify different types of flowers in the Iris dataset. After loading the dataset and splitting it into training and testing sets (60% training, 40% testing), the model was trained and evaluated.\n",
                                    "\n",
                                    "The model achieved an accuracy of **98.33%**. This indicates that the KNN classifier performed very well in predicting the correct flower type based on its features in the Iris dataset."
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Adjust K Value for KNN Classifier\n",
                                    "\n",
                                    "Cosmo\n",
                                    "Just now\n",
                                    "Read message aloud\n",
                                    "Hey there, Space Voyager!\n",
                                    "\n",
                                    "Let's tweak our KNN model a bit. Change the n_neighbors parameter from 3 to 4 in the KNeighborsClassifier initialization. This will help you understand how the number of neighbors influences the model's performance.\n",
                                    "\n",
                                    "Good luck!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_iris\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Load the Iris dataset\n",
                                    "X, y = load_iris(return_X_y=True)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=42)\n",
                                    "\n",
                                    "# TODO: change initialization to use n_neighbors=4\n",
                                    "knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
                                    "knn_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "accuracy = knn_clf.score(X_test, y_test)\n",
                                    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_iris\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Load the Iris dataset\n",
                                    "X, y = load_iris(return_X_y=True)\n",
                                    "\n",
                                    "# Split the dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=42)\n",
                                    "\n",
                                    "# Change initialization to use n_neighbors=4\n",
                                    "knn_clf = KNeighborsClassifier(n_neighbors=4)\n",
                                    "knn_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Evaluate the model\n",
                                    "accuracy = knn_clf.score(X_test, y_test)\n",
                                    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
                                    "```\n",
                                    "\n",
                                    "Hey there, Space Voyager\\!\n",
                                    "\n",
                                    "You've successfully tweaked the `n_neighbors` parameter for our KNN model. By changing it from 3 to 4, we can observe the impact of this hyperparameter on the model's performance.\n",
                                    "\n",
                                    "Here's the accuracy with `n_neighbors=4`:\n",
                                    "\n",
                                    "Model accuracy: 96.67%"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Complete the KNN Classifier for Iris Dataset\n",
                                    "\n",
                                    "Astro Explorer, it's time to level up! Fill in the blanks to complete the code. Implement the missing pieces to load the wine dataset and fit a KNN classifier.\n",
                                    "\n",
                                    "Remember, the dataset contains features like wine's hue or chemical parameters.\n",
                                    "\n",
                                    "Get coding and see if you can achieve high accuracy!\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Load the Iris dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                                    "\n",
                                    "# TODO: Initialize KNN classifier with 5 neighbors\n",
                                    "\n",
                                    "# TODO: Fit the KNN classifier\n",
                                    "\n",
                                    "# TODO: Make predictions for the test data and print the obtained accuracy score\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.metrics import accuracy_score\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "\n",
                                    "# Load the Iris dataset\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "\n",
                                    "# Split dataset into training and testing sets\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                                    "\n",
                                    "# Initialize KNN classifier with 5 neighbors\n",
                                    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
                                    "\n",
                                    "# Fit the KNN classifier\n",
                                    "knn_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "# Make predictions for the test data\n",
                                    "y_pred = knn_clf.predict(X_test)\n",
                                    "\n",
                                    "# Calculate and print the obtained accuracy score\n",
                                    "accuracy = accuracy_score(y_test, y_pred)\n",
                                    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
                                    "```\n",
                                    "\n",
                                    "Astro Explorer, you've successfully completed the KNN classifier for the Wine dataset\\!\n",
                                    "\n",
                                    "The code now initializes a `KNeighborsClassifier` with 5 neighbors, fits it to the training data, makes predictions on the test set, and calculates the accuracy.\n",
                                    "\n",
                                    "Here's the accuracy achieved by your KNN model on the Wine dataset:\n",
                                    "\n",
                                    "Model accuracy: 74.07%\n",
                                    "\n",
                                    "This accuracy shows how well your model classifies different wines based on their chemical properties. Great work\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Classify Iris Flowers with KNN\n",
                                    "\n",
                                    "Nice progress, Space Explorer!\n",
                                    "\n",
                                    "Now, let's dive deeper. Your task is to complete the missing pieces in the code to classify the Wine using K-Nearest Neighbors (KNN). This time, we will try plugging in different ks and see how the model performs to choose the best one.\n",
                                    "\n",
                                    "This process is called hypertuning, and it essentially involves searching for the optimal mode's initialization parameters. Sounds cool, right?\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "\n",
                                    "# Load dataset and split to train and test\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
                                    "\n",
                                    "# Iterate over different k values\n",
                                    "for k in range(2, 10):\n",
                                    "    # TODO: initialize the KNeighborsClassifier with n_neighbors=k\n",
                                    "    # TODO: make predictions and calculate the accuracy for the test data. Save the result in the \"accuracy\" variable\n",
                                    "    print(f'k={k} -> Accuracy: {accuracy * 100:.2f}%')\n",
                                    "```\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "\n",
                                    "# Load dataset and split to train and test\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
                                    "\n",
                                    "# Iterate over different k values\n",
                                    "for k in range(2, 10):\n",
                                    "    # Initialize the KNeighborsClassifier with n_neighbors=k\n",
                                    "    knn_clf = KNeighborsClassifier(n_neighbors=k)\n",
                                    "\n",
                                    "    # Fit the KNN classifier to the training data\n",
                                    "    knn_clf.fit(X_train, y_train)\n",
                                    "\n",
                                    "    # Make predictions and calculate the accuracy for the test data. Save the result in the \"accuracy\" variable\n",
                                    "    accuracy = knn_clf.score(X_test, y_test)\n",
                                    "    print(f'k={k} -> Accuracy: {accuracy * 100:.2f}%')\n",
                                    "```\n",
                                    "\n",
                                    "Nice progress, Space Explorer\\!\n",
                                    "\n",
                                    "You've successfully completed the code to classify the Wine dataset using K-Nearest Neighbors (KNN) and performed hypertuning by iterating through different `k` values.\n",
                                    "\n",
                                    "Running this code will show you the accuracy of the KNN model for each `k` from 2 to 9, allowing you to observe how the number of neighbors influences the model's performance. This is a great practical example of finding the optimal model parameters\\!"
                           ]
                  },
                  {
                           "cell_type": "markdown",
                           "metadata": {},
                           "source": [
                                    "## Flower Classification with KNN\n",
                                    "\n",
                                    "Alright, Space Wanderer, it’s time to put your classification skills to a real test! You will use the Wine dataset, fit both a KNN and a Decision Tree classifier, hypertune them, and compare their best accuracies.\n",
                                    "\n",
                                    "Load and split the Wine dataset:\n",
                                    "Initialize and fit the KNN classifier:\n",
                                    "Iterate over different k values (e.g., from 2 to 10) and find the best k that yields the highest accuracy.\n",
                                    "Initialize and train the Decision Tree classifier:\n",
                                    "Iterate over different depths (e.g., from 1 to 10) and find the best depth that yields the highest accuracy.\n",
                                    "Compare their accuracies: Print the best accuracies for both classifiers.\n",
                                    "It’s time to show what you’ve got!\n",
                                    "\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "\n",
                                    "# Load dataset and split to train and test\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
                                    "\n",
                                    "# Hypertune KNN\n",
                                    "best_knn_acc = 0\n",
                                    "best_k = 0\n",
                                    "for k in range(2, 11):\n",
                                    "    # TODO: calculate the accuracy with the given k\n",
                                    "    if accuracy > best_knn_acc:\n",
                                    "        best_knn_acc = accuracy\n",
                                    "        best_k = k\n",
                                    "print(f'Best K for KNN: {best_k} -> Accuracy: {best_knn_acc * 100:.2f}%')\n",
                                    "\n",
                                    "# Hypertune Decision Tree\n",
                                    "best_dt_acc = 0\n",
                                    "best_depth = 0\n",
                                    "for depth in range(1, 11):\n",
                                    "    # TODO: calculate the accuracy with the given max_depth\n",
                                    "    if accuracy > best_dt_acc:\n",
                                    "        best_dt_acc = accuracy\n",
                                    "        best_depth = depth\n",
                                    "print(f'Best Depth for Decision Tree: {best_depth} -> Accuracy: {best_dt_acc * 100:.2f}%')\n",
                                    "\n",
                                    "```\n",
                                    "\n",
                                    "Sure, Space Wanderer\\! Let's get these classifiers trained and tuned.\n",
                                    "\n",
                                    "Here's the completed code to hypertune both the KNN and Decision Tree classifiers on the Wine dataset, find their best accuracies, and compare them:\n",
                                    "\n",
                                    "```python\n",
                                    "from sklearn.datasets import load_wine\n",
                                    "from sklearn.model_selection import train_test_split\n",
                                    "from sklearn.neighbors import KNeighborsClassifier\n",
                                    "from sklearn.tree import DecisionTreeClassifier\n",
                                    "\n",
                                    "# Load dataset and split to train and test\n",
                                    "X, y = load_wine(return_X_y=True)\n",
                                    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
                                    "\n",
                                    "# Hypertune KNN\n",
                                    "best_knn_acc = 0\n",
                                    "best_k = 0\n",
                                    "for k in range(2, 11):\n",
                                    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
                                    "    knn.fit(X_train, y_train)\n",
                                    "    accuracy = knn.score(X_test, y_test)\n",
                                    "    if accuracy > best_knn_acc:\n",
                                    "        best_knn_acc = accuracy\n",
                                    "        best_k = k\n",
                                    "print(f'Best K for KNN: {best_k} -> Accuracy: {best_knn_acc * 100:.2f}%')\n",
                                    "\n",
                                    "# Hypertune Decision Tree\n",
                                    "best_dt_acc = 0\n",
                                    "best_depth = 0\n",
                                    "for depth in range(1, 11):\n",
                                    "    dtree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
                                    "    dtree.fit(X_train, y_train)\n",
                                    "    accuracy = dtree.score(X_test, y_test)\n",
                                    "    if accuracy > best_dt_acc:\n",
                                    "        best_dt_acc = accuracy\n",
                                    "        best_depth = depth\n",
                                    "print(f'Best Depth for Decision Tree: {best_depth} -> Accuracy: {best_dt_acc * 100:.2f}%')\n",
                                    "\n",
                                    "```"
                           ]
                  }
         ],
         "metadata": {
                  "language_info": {
                           "name": "python"
                  }
         },
         "nbformat": 4,
         "nbformat_minor": 5
}
